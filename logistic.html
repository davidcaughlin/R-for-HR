<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 49 Identifying Predictors of Turnover Using Logistic Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION</title>
  <meta name="description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 49 Identifying Predictors of Turnover Using Logistic Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://rforhr.com" />
  <meta property="og:image" content="https://rforhr.com/cover.png" />
  <meta property="og:description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="github-repo" content="davidcaughlin/R-for-HR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 49 Identifying Predictors of Turnover Using Logistic Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION" />
  
  <meta name="twitter:description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="twitter:image" content="https://rforhr.com/cover.png" />

<meta name="author" content="David E. Caughlin" />


<meta name="date" content="2022-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="turnoverchisquare.html"/>
<link rel="next" href="kfold.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for HR</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#hragrowth"><i class="fa fa-check"></i><b>0.1</b> Growth of HR Analytics</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#hranalytics_skillsgap"><i class="fa fa-check"></i><b>0.2</b> Skills Gap</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#hraplc"><i class="fa fa-check"></i><b>0.3</b> Project Life Cycle Perspective</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#hranalytics_hris_overview"><i class="fa fa-check"></i><b>0.4</b> Overview of HRIS &amp; HR Analytics</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#philosophy"><i class="fa fa-check"></i><b>0.5</b> My Philosophy for This Book</a>
<ul>
<li class="chapter" data-level="0.5.1" data-path="index.html"><a href="index.html#rationalerpref"><i class="fa fa-check"></i><b>0.5.1</b> Rationale for Using R</a></li>
<li class="chapter" data-level="0.5.2" data-path="index.html"><a href="index.html#audiencepref"><i class="fa fa-check"></i><b>0.5.2</b> Audience</a></li>
</ul></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#structurepref"><i class="fa fa-check"></i><b>0.6</b> Structure</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#aboutauthor"><i class="fa fa-check"></i><b>0.7</b> About the Author</a></li>
<li class="chapter" data-level="0.8" data-path="index.html"><a href="index.html#contactauthor"><i class="fa fa-check"></i><b>0.8</b> Contacting the Author</a></li>
<li class="chapter" data-level="0.9" data-path="index.html"><a href="index.html#acknowpref"><i class="fa fa-check"></i><b>0.9</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I HR Analytics Project Life Cycle</b></span></li>
<li class="chapter" data-level="1" data-path="overviewhraplc.html"><a href="overviewhraplc.html"><i class="fa fa-check"></i><b>1</b> Overview of HR Analytics Project Life Cycle</a></li>
<li class="chapter" data-level="2" data-path="questionformulation.html"><a href="questionformulation.html"><i class="fa fa-check"></i><b>2</b> Question Formulation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="questionformulation.html"><a href="questionformulation.html#adoptstrategicmindset"><i class="fa fa-check"></i><b>2.1</b> Adopting a Strategic Mindset</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="questionformulation.html"><a href="questionformulation.html#strategy"><i class="fa fa-check"></i><b>2.1.1</b> Strategy</a></li>
<li class="chapter" data-level="2.1.2" data-path="questionformulation.html"><a href="questionformulation.html#strategyformulation"><i class="fa fa-check"></i><b>2.1.2</b> Strategy Formulation</a></li>
<li class="chapter" data-level="2.1.3" data-path="questionformulation.html"><a href="questionformulation.html#strategyimplementation"><i class="fa fa-check"></i><b>2.1.3</b> Strategy Implementation</a></li>
<li class="chapter" data-level="2.1.4" data-path="questionformulation.html"><a href="questionformulation.html#strategichrm"><i class="fa fa-check"></i><b>2.1.4</b> Strategic Human Resource Management</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="questionformulation.html"><a href="questionformulation.html#defining-problems-formulating-questions"><i class="fa fa-check"></i><b>2.2</b> Defining Problems &amp; Formulating Questions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="questionformulation.html"><a href="questionformulation.html#definingaproblem"><i class="fa fa-check"></i><b>2.2.1</b> Defining a Problem</a></li>
<li class="chapter" data-level="2.2.2" data-path="questionformulation.html"><a href="questionformulation.html#formulatingaquestion"><i class="fa fa-check"></i><b>2.2.2</b> Formulating a Question</a></li>
<li class="chapter" data-level="2.2.3" data-path="questionformulation.html"><a href="questionformulation.html#divergentconvergentthinking"><i class="fa fa-check"></i><b>2.2.3</b> Thinking Divergently &amp; Convergently</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="questionformulation.html"><a href="questionformulation.html#summary"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dataacquisition.html"><a href="dataacquisition.html"><i class="fa fa-check"></i><b>3</b> Data Acquisition</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dataacquisition.html"><a href="dataacquisition.html#employee-surveys"><i class="fa fa-check"></i><b>3.1</b> Employee Surveys</a></li>
<li class="chapter" data-level="3.2" data-path="dataacquisition.html"><a href="dataacquisition.html#rating-forms"><i class="fa fa-check"></i><b>3.2</b> Rating Forms</a></li>
<li class="chapter" data-level="3.3" data-path="dataacquisition.html"><a href="dataacquisition.html#surveillance-monitoring"><i class="fa fa-check"></i><b>3.3</b> Surveillance &amp; Monitoring</a></li>
<li class="chapter" data-level="3.4" data-path="dataacquisition.html"><a href="dataacquisition.html#database-queries"><i class="fa fa-check"></i><b>3.4</b> Database Queries</a></li>
<li class="chapter" data-level="3.5" data-path="dataacquisition.html"><a href="dataacquisition.html#scraping"><i class="fa fa-check"></i><b>3.5</b> Scraping</a></li>
<li class="chapter" data-level="3.6" data-path="dataacquisition.html"><a href="dataacquisition.html#summary-1"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datamanagement.html"><a href="datamanagement.html"><i class="fa fa-check"></i><b>4</b> Data Management</a>
<ul>
<li class="chapter" data-level="4.1" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_clean"><i class="fa fa-check"></i><b>4.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="4.2" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_manipulate"><i class="fa fa-check"></i><b>4.2</b> Data Manipulation &amp; Structuring</a></li>
<li class="chapter" data-level="4.3" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_tools"><i class="fa fa-check"></i><b>4.3</b> Common Data-Management Tools</a></li>
<li class="chapter" data-level="4.4" data-path="datamanagement.html"><a href="datamanagement.html#summary-2"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dataanalysis.html"><a href="dataanalysis.html"><i class="fa fa-check"></i><b>5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="dataanalysis.html"><a href="dataanalysis.html#dataanalysis_toolstechniques"><i class="fa fa-check"></i><b>5.1</b> Tools &amp; Techniques</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="dataanalysis.html"><a href="dataanalysis.html#mathematics"><i class="fa fa-check"></i><b>5.1.1</b> Mathematics</a></li>
<li class="chapter" data-level="5.1.2" data-path="dataanalysis.html"><a href="dataanalysis.html#statistics"><i class="fa fa-check"></i><b>5.1.2</b> Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="dataanalysis.html"><a href="dataanalysis.html#machinelearning"><i class="fa fa-check"></i><b>5.1.3</b> Machine Learning</a></li>
<li class="chapter" data-level="5.1.4" data-path="dataanalysis.html"><a href="dataanalysis.html#computationalmodelsimulation"><i class="fa fa-check"></i><b>5.1.4</b> Computational Modeling &amp; Simulations</a></li>
<li class="chapter" data-level="5.1.5" data-path="dataanalysis.html"><a href="dataanalysis.html#textqualitativeanalyses"><i class="fa fa-check"></i><b>5.1.5</b> Text Analyses &amp; Qualitative Analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataanalysis.html"><a href="dataanalysis.html#continuum_dataanalytics"><i class="fa fa-check"></i><b>5.2</b> Continuum of Data Analytics</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="dataanalysis.html"><a href="dataanalysis.html#descriptive_analytics"><i class="fa fa-check"></i><b>5.2.1</b> Descriptive Analytics</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataanalysis.html"><a href="dataanalysis.html#predictish_analytics"><i class="fa fa-check"></i><b>5.2.2</b> Predict-ish Analytics</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataanalysis.html"><a href="dataanalysis.html#predictive_analytics"><i class="fa fa-check"></i><b>5.2.3</b> Predictive Analytics</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataanalysis.html"><a href="dataanalysis.html#prescriptive_analytics"><i class="fa fa-check"></i><b>5.2.4</b> Prescriptive Analytics</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="dataanalysis.html"><a href="dataanalysis.html#summary-3"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html"><i class="fa fa-check"></i><b>6</b> Data Interpretation &amp; Storytelling</a>
<ul>
<li class="chapter" data-level="6.1" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#datainterpretation"><i class="fa fa-check"></i><b>6.1</b> Data Interpretation</a></li>
<li class="chapter" data-level="6.2" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#storytelling"><i class="fa fa-check"></i><b>6.2</b> Storytelling</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#structure"><i class="fa fa-check"></i><b>6.2.1</b> Structure</a></li>
<li class="chapter" data-level="6.2.2" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#clarity-parsimony"><i class="fa fa-check"></i><b>6.2.2</b> Clarity &amp; Parsimony</a></li>
<li class="chapter" data-level="6.2.3" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#influence-persuasion"><i class="fa fa-check"></i><b>6.2.3</b> Influence &amp; Persuasion</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#storytelling_withdata"><i class="fa fa-check"></i><b>6.3</b> Storytelling with Data</a></li>
<li class="chapter" data-level="6.4" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#summary-4"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deploymentimplementation.html"><a href="deploymentimplementation.html"><i class="fa fa-check"></i><b>7</b> Deployment &amp; Implementation</a></li>
<li class="part"><span><b>II Employee Compensation &amp; Reward Systems</b></span></li>
<li class="chapter" data-level="8" data-path="compensation.html"><a href="compensation.html"><i class="fa fa-check"></i><b>8</b> Introduction to Employee Compensation &amp; Reward Systems</a></li>
<li class="chapter" data-level="9" data-path="marketsurvey.html"><a href="marketsurvey.html"><i class="fa fa-check"></i><b>9</b> Preparing Market Survey Data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="marketsurvey.html"><a href="marketsurvey.html#conceptualoverview_marketsurvey"><i class="fa fa-check"></i><b>9.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="marketsurvey.html"><a href="marketsurvey.html#agingdata_marketsurvey"><i class="fa fa-check"></i><b>9.1.1</b> Aging Market Survey Data</a></li>
<li class="chapter" data-level="9.1.2" data-path="marketsurvey.html"><a href="marketsurvey.html#applyingweights_marketsurvey"><i class="fa fa-check"></i><b>9.1.2</b> Applying Market Survey Weights</a></li>
<li class="chapter" data-level="9.1.3" data-path="marketsurvey.html"><a href="marketsurvey.html#conceptualvideo_marketsurvey"><i class="fa fa-check"></i><b>9.1.3</b> Conceptual Videos</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="marketsurvey.html"><a href="marketsurvey.html#tutorial_marketsurvey"><i class="fa fa-check"></i><b>9.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="marketsurvey.html"><a href="marketsurvey.html#videotutorial_marketsurvey"><i class="fa fa-check"></i><b>9.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="9.2.2" data-path="marketsurvey.html"><a href="marketsurvey.html#functions_marketsurvey"><i class="fa fa-check"></i><b>9.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="9.2.3" data-path="marketsurvey.html"><a href="marketsurvey.html#initsteps_marketsurvey"><i class="fa fa-check"></i><b>9.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="9.2.4" data-path="marketsurvey.html"><a href="marketsurvey.html#agedata_marketsurvey"><i class="fa fa-check"></i><b>9.2.4</b> Age the Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="marketsurvey.html"><a href="marketsurvey.html#weightdata_marketsurvey"><i class="fa fa-check"></i><b>9.2.5</b> Compute the Sample-Weighted Means</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="marketsurvey.html"><a href="marketsurvey.html#weightdata_withfunction_marketsurvey"><i class="fa fa-check"></i><b>9.3</b> Optional: Compute the Sample-Weighted Means with a Function</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="marketsurvey.html"><a href="marketsurvey.html#summary_marketsurvey"><i class="fa fa-check"></i><b>9.3.1</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Introduction to R</b></span></li>
<li class="chapter" data-level="10" data-path="overviewR.html"><a href="overviewR.html"><i class="fa fa-check"></i><b>10</b> Overview of R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overviewR.html"><a href="overviewR.html#R_overview"><i class="fa fa-check"></i><b>10.1</b> R Programming Language</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="overviewR.html"><a href="overviewR.html#R_what"><i class="fa fa-check"></i><b>10.1.1</b> What Is R?</a></li>
<li class="chapter" data-level="10.1.2" data-path="overviewR.html"><a href="overviewR.html#R_why"><i class="fa fa-check"></i><b>10.1.2</b> Why Use R?</a></li>
<li class="chapter" data-level="10.1.3" data-path="overviewR.html"><a href="overviewR.html#R_who"><i class="fa fa-check"></i><b>10.1.3</b> Who Uses R?</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="overviewR.html"><a href="overviewR.html#RStudio_overview"><i class="fa fa-check"></i><b>10.2</b> RStudio</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="overviewR.html"><a href="overviewR.html#RStudio_what"><i class="fa fa-check"></i><b>10.2.1</b> What is RStudio?</a></li>
<li class="chapter" data-level="10.2.2" data-path="overviewR.html"><a href="overviewR.html#RStudio_why"><i class="fa fa-check"></i><b>10.2.2</b> Why RStudio?</a></li>
<li class="chapter" data-level="10.2.3" data-path="overviewR.html"><a href="overviewR.html#RStudio_who"><i class="fa fa-check"></i><b>10.2.3</b> Who Uses RStudio?</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="overviewR.html"><a href="overviewR.html#packages_overview"><i class="fa fa-check"></i><b>10.3</b> Packages</a></li>
<li class="chapter" data-level="10.4" data-path="overviewR.html"><a href="overviewR.html#summary-5"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="install.html"><a href="install.html"><i class="fa fa-check"></i><b>11</b> Installing R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="11.1" data-path="install.html"><a href="install.html#videotutorial_install"><i class="fa fa-check"></i><b>11.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="11.2" data-path="install.html"><a href="install.html#installR"><i class="fa fa-check"></i><b>11.2</b> Downloading &amp; Installing R</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="install.html"><a href="install.html#installR_windows"><i class="fa fa-check"></i><b>11.2.1</b> For Windows Operation Systems</a></li>
<li class="chapter" data-level="11.2.2" data-path="install.html"><a href="install.html#installR_macos"><i class="fa fa-check"></i><b>11.2.2</b> For Mac Operating Systems</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="install.html"><a href="install.html#installRStudio"><i class="fa fa-check"></i><b>11.3</b> Downloading &amp; Installing RStudio</a></li>
<li class="chapter" data-level="11.4" data-path="install.html"><a href="install.html#summary_install"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="gettingstarted.html"><a href="gettingstarted.html"><i class="fa fa-check"></i><b>12</b> Getting Started with R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="12.1" data-path="gettingstarted.html"><a href="gettingstarted.html#orientation_gettingstarted"><i class="fa fa-check"></i><b>12.1</b> Orientation to RStudio</a></li>
<li class="chapter" data-level="12.2" data-path="gettingstarted.html"><a href="gettingstarted.html#createRscript"><i class="fa fa-check"></i><b>12.2</b> Creating &amp; Saving an R Script</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="gettingstarted.html"><a href="gettingstarted.html#createnewRscript_gettingstarted"><i class="fa fa-check"></i><b>12.2.1</b> Creating a New R Script</a></li>
<li class="chapter" data-level="12.2.2" data-path="gettingstarted.html"><a href="gettingstarted.html#usenewRscript_gettingstarted"><i class="fa fa-check"></i><b>12.2.2</b> Using an R Script</a></li>
<li class="chapter" data-level="12.2.3" data-path="gettingstarted.html"><a href="gettingstarted.html#saveRscript_gettingstarted"><i class="fa fa-check"></i><b>12.2.3</b> Saving an R Script</a></li>
<li class="chapter" data-level="12.2.4" data-path="gettingstarted.html"><a href="gettingstarted.html#openRscript_gettingstarted"><i class="fa fa-check"></i><b>12.2.4</b> Opening a Saved R Script</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="gettingstarted.html"><a href="gettingstarted.html#RStudioproject_gettingstarted"><i class="fa fa-check"></i><b>12.3</b> Creating an RStudio Project</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="gettingstarted.html"><a href="gettingstarted.html#createRStudioproject_gettingstarted"><i class="fa fa-check"></i><b>12.3.1</b> Creating a New RStudio Project</a></li>
<li class="chapter" data-level="12.3.2" data-path="gettingstarted.html"><a href="gettingstarted.html#openRStudioproject_gettingstarted"><i class="fa fa-check"></i><b>12.3.2</b> Opening an Existing RStudio Project</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="gettingstarted.html"><a href="gettingstarted.html#writtentutorials_gettingstarted"><i class="fa fa-check"></i><b>12.4</b> Orientation to Written Tutorials</a></li>
<li class="chapter" data-level="12.5" data-path="gettingstarted.html"><a href="gettingstarted.html#summary_gettingstarted"><i class="fa fa-check"></i><b>12.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="gentleintro.html"><a href="gentleintro.html"><i class="fa fa-check"></i><b>13</b> Basic Features and Operations of the R Language</a>
<ul>
<li class="chapter" data-level="13.1" data-path="gentleintro.html"><a href="gentleintro.html#videotutorial_gentleintro"><i class="fa fa-check"></i><b>13.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="13.2" data-path="gentleintro.html"><a href="gentleintro.html#functions_gentleintro"><i class="fa fa-check"></i><b>13.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="13.3" data-path="gentleintro.html"><a href="gentleintro.html#r_as_calculator"><i class="fa fa-check"></i><b>13.3</b> R as a Calculator</a></li>
<li class="chapter" data-level="13.4" data-path="gentleintro.html"><a href="gentleintro.html#functions"><i class="fa fa-check"></i><b>13.4</b> Functions</a></li>
<li class="chapter" data-level="13.5" data-path="gentleintro.html"><a href="gentleintro.html#packages"><i class="fa fa-check"></i><b>13.5</b> Packages</a></li>
<li class="chapter" data-level="13.6" data-path="gentleintro.html"><a href="gentleintro.html#variableassignment"><i class="fa fa-check"></i><b>13.6</b> Variable Assignment</a></li>
<li class="chapter" data-level="13.7" data-path="gentleintro.html"><a href="gentleintro.html#typesofdata"><i class="fa fa-check"></i><b>13.7</b> Types of Data</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="gentleintro.html"><a href="gentleintro.html#numeric-data"><i class="fa fa-check"></i><b>13.7.1</b> <code>numeric</code> Data</a></li>
<li class="chapter" data-level="13.7.2" data-path="gentleintro.html"><a href="gentleintro.html#character-data"><i class="fa fa-check"></i><b>13.7.2</b> <code>character</code> Data</a></li>
<li class="chapter" data-level="13.7.3" data-path="gentleintro.html"><a href="gentleintro.html#date-data"><i class="fa fa-check"></i><b>13.7.3</b> <code>Date</code> Data</a></li>
<li class="chapter" data-level="13.7.4" data-path="gentleintro.html"><a href="gentleintro.html#logical-data"><i class="fa fa-check"></i><b>13.7.4</b> <code>logical</code> Data</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="gentleintro.html"><a href="gentleintro.html#vectors"><i class="fa fa-check"></i><b>13.8</b> Vectors</a></li>
<li class="chapter" data-level="13.9" data-path="gentleintro.html"><a href="gentleintro.html#lists"><i class="fa fa-check"></i><b>13.9</b> Lists</a></li>
<li class="chapter" data-level="13.10" data-path="gentleintro.html"><a href="gentleintro.html#dataframes"><i class="fa fa-check"></i><b>13.10</b> Data Frames</a></li>
<li class="chapter" data-level="13.11" data-path="gentleintro.html"><a href="gentleintro.html#annotate"><i class="fa fa-check"></i><b>13.11</b> Annotations</a></li>
<li class="chapter" data-level="13.12" data-path="gentleintro.html"><a href="gentleintro.html#summary_gentleintro"><i class="fa fa-check"></i><b>13.12</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="setwd.html"><a href="setwd.html"><i class="fa fa-check"></i><b>14</b> Setting a Working Directory</a>
<ul>
<li class="chapter" data-level="14.1" data-path="setwd.html"><a href="setwd.html#videotutorial_setwd"><i class="fa fa-check"></i><b>14.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="14.2" data-path="setwd.html"><a href="setwd.html#functions_setwd"><i class="fa fa-check"></i><b>14.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="14.3" data-path="setwd.html"><a href="setwd.html#getwd_setwd"><i class="fa fa-check"></i><b>14.3</b> Identify the Current Working Directory</a></li>
<li class="chapter" data-level="14.4" data-path="setwd.html"><a href="setwd.html#setwd_setwd"><i class="fa fa-check"></i><b>14.4</b> Set a New Working Directory</a></li>
<li class="chapter" data-level="14.5" data-path="setwd.html"><a href="setwd.html#summary_setwd"><i class="fa fa-check"></i><b>14.5</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>IV Data Acquisition &amp; Management</b></span></li>
<li class="chapter" data-level="15" data-path="read.html"><a href="read.html"><i class="fa fa-check"></i><b>15</b> Reading Data into R</a>
<ul>
<li class="chapter" data-level="15.1" data-path="read.html"><a href="read.html#conceptualoverview_read"><i class="fa fa-check"></i><b>15.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="15.2" data-path="read.html"><a href="read.html#tutorial_read"><i class="fa fa-check"></i><b>15.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="read.html"><a href="read.html#videotutorial_read"><i class="fa fa-check"></i><b>15.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="15.2.2" data-path="read.html"><a href="read.html#functions_read"><i class="fa fa-check"></i><b>15.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="15.2.3" data-path="read.html"><a href="read.html#initialsteps_read"><i class="fa fa-check"></i><b>15.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="15.2.4" data-path="read.html"><a href="read.html#readcsv"><i class="fa fa-check"></i><b>15.2.4</b> Read a .csv File</a></li>
<li class="chapter" data-level="15.2.5" data-path="read.html"><a href="read.html#readxlsx"><i class="fa fa-check"></i><b>15.2.5</b> Read a .xlsx File</a></li>
<li class="chapter" data-level="15.2.6" data-path="read.html"><a href="read.html#read_summary"><i class="fa fa-check"></i><b>15.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="read.html"><a href="read.html#read_supplement"><i class="fa fa-check"></i><b>15.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="read.html"><a href="read.html#read_supplement_functions"><i class="fa fa-check"></i><b>15.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="15.3.2" data-path="read.html"><a href="read.html#read_initsteps_supplement"><i class="fa fa-check"></i><b>15.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="15.3.3" data-path="read.html"><a href="read.html#read_additionalfunctions"><i class="fa fa-check"></i><b>15.3.3</b> Additional Functions for Reading a .csv File</a></li>
<li class="chapter" data-level="15.3.4" data-path="read.html"><a href="read.html#read_skiprows"><i class="fa fa-check"></i><b>15.3.4</b> Skip Rows of Data During Read</a></li>
<li class="chapter" data-level="15.3.5" data-path="read.html"><a href="read.html#listdatafiles"><i class="fa fa-check"></i><b>15.3.5</b> List Data File Names in Working Directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="addnames.html"><a href="addnames.html"><i class="fa fa-check"></i><b>16</b> Removing, Adding, &amp; Changing Variable Names</a>
<ul>
<li class="chapter" data-level="16.1" data-path="addnames.html"><a href="addnames.html#conceptualoverview_addnames"><i class="fa fa-check"></i><b>16.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="16.2" data-path="addnames.html"><a href="addnames.html#tutorial_addnames"><i class="fa fa-check"></i><b>16.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="addnames.html"><a href="addnames.html#videotutorial_addnames"><i class="fa fa-check"></i><b>16.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="16.2.2" data-path="addnames.html"><a href="addnames.html#function_addnames"><i class="fa fa-check"></i><b>16.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="16.2.3" data-path="addnames.html"><a href="addnames.html#initsteps_addnames"><i class="fa fa-check"></i><b>16.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="16.2.4" data-path="addnames.html"><a href="addnames.html#remove_variablenames"><i class="fa fa-check"></i><b>16.2.4</b> Remove Variable Names from a Data Frame Object</a></li>
<li class="chapter" data-level="16.2.5" data-path="addnames.html"><a href="addnames.html#add_variablenames"><i class="fa fa-check"></i><b>16.2.5</b> Add Variable Names to a Data Frame Object</a></li>
<li class="chapter" data-level="16.2.6" data-path="addnames.html"><a href="addnames.html#change_variablenames"><i class="fa fa-check"></i><b>16.2.6</b> Change Specific Variable Names in a Data Frame Object</a></li>
<li class="chapter" data-level="16.2.7" data-path="addnames.html"><a href="addnames.html#summary_addnames"><i class="fa fa-check"></i><b>16.2.7</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="write.html"><a href="write.html"><i class="fa fa-check"></i><b>17</b> Writing Data from R</a>
<ul>
<li class="chapter" data-level="17.1" data-path="write.html"><a href="write.html#conceptualoverview_write"><i class="fa fa-check"></i><b>17.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="17.2" data-path="write.html"><a href="write.html#tutorial_write"><i class="fa fa-check"></i><b>17.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="write.html"><a href="write.html#videotutorial_write"><i class="fa fa-check"></i><b>17.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="17.2.2" data-path="write.html"><a href="write.html#functions_write"><i class="fa fa-check"></i><b>17.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="17.2.3" data-path="write.html"><a href="write.html#initialsteps_write"><i class="fa fa-check"></i><b>17.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="17.2.4" data-path="write.html"><a href="write.html#write_dataframe"><i class="fa fa-check"></i><b>17.2.4</b> Write Data Frame to Working Directory</a></li>
<li class="chapter" data-level="17.2.5" data-path="write.html"><a href="write.html#write_table"><i class="fa fa-check"></i><b>17.2.5</b> Write Table to Working Directory</a></li>
<li class="chapter" data-level="17.2.6" data-path="write.html"><a href="write.html#summary_write"><i class="fa fa-check"></i><b>17.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="arrange.html"><a href="arrange.html"><i class="fa fa-check"></i><b>18</b> Arranging (Sorting) Data</a>
<ul>
<li class="chapter" data-level="18.1" data-path="arrange.html"><a href="arrange.html#conceptualoverview_arrange"><i class="fa fa-check"></i><b>18.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="18.2" data-path="arrange.html"><a href="arrange.html#tutorial_arrange"><i class="fa fa-check"></i><b>18.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="arrange.html"><a href="arrange.html#videotutorial_arrange"><i class="fa fa-check"></i><b>18.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="18.2.2" data-path="arrange.html"><a href="arrange.html#functions_arrange"><i class="fa fa-check"></i><b>18.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="18.2.3" data-path="arrange.html"><a href="arrange.html#initsteps_arrange"><i class="fa fa-check"></i><b>18.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="18.2.4" data-path="arrange.html"><a href="arrange.html#arrangebyvalues"><i class="fa fa-check"></i><b>18.2.4</b> Arrange (Sort) Data</a></li>
<li class="chapter" data-level="18.2.5" data-path="arrange.html"><a href="arrange.html#summary_arrange"><i class="fa fa-check"></i><b>18.2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="arrange.html"><a href="arrange.html#arrange_supplement"><i class="fa fa-check"></i><b>18.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="arrange.html"><a href="arrange.html#arrange_supplement_functions"><i class="fa fa-check"></i><b>18.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="18.3.2" data-path="arrange.html"><a href="arrange.html#arrange_initsteps_supplement"><i class="fa fa-check"></i><b>18.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="18.3.3" data-path="arrange.html"><a href="arrange.html#arrange_orderfunction"><i class="fa fa-check"></i><b>18.3.3</b> <code>order</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="join.html"><a href="join.html"><i class="fa fa-check"></i><b>19</b> Joining (Merging) Data</a>
<ul>
<li class="chapter" data-level="19.1" data-path="join.html"><a href="join.html#conceptualoverview_join"><i class="fa fa-check"></i><b>19.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="join.html"><a href="join.html#review_horizontaljoin"><i class="fa fa-check"></i><b>19.1.1</b> Review of Horizontal Joins (Merges)</a></li>
<li class="chapter" data-level="19.1.2" data-path="join.html"><a href="join.html#review_verticaljoin"><i class="fa fa-check"></i><b>19.1.2</b> Review of Vertical Joins (Merges)</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="join.html"><a href="join.html#tutorial_join"><i class="fa fa-check"></i><b>19.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="join.html"><a href="join.html#videotutorial_join"><i class="fa fa-check"></i><b>19.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="19.2.2" data-path="join.html"><a href="join.html#functions-packages-introduced"><i class="fa fa-check"></i><b>19.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="19.2.3" data-path="join.html"><a href="join.html#initsteps_join"><i class="fa fa-check"></i><b>19.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="19.2.4" data-path="join.html"><a href="join.html#horizontaljoin"><i class="fa fa-check"></i><b>19.2.4</b> Horizontal Join (Merge)</a></li>
<li class="chapter" data-level="19.2.5" data-path="join.html"><a href="join.html#verticaljoin"><i class="fa fa-check"></i><b>19.2.5</b> Vertical Join (Merge)</a></li>
<li class="chapter" data-level="19.2.6" data-path="join.html"><a href="join.html#summary_join"><i class="fa fa-check"></i><b>19.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="join.html"><a href="join.html#join_supplement"><i class="fa fa-check"></i><b>19.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="join.html"><a href="join.html#supp_join_video"><i class="fa fa-check"></i><b>19.3.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="19.3.2" data-path="join.html"><a href="join.html#join_supplement_functions"><i class="fa fa-check"></i><b>19.3.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="19.3.3" data-path="join.html"><a href="join.html#join_initsteps_supplement"><i class="fa fa-check"></i><b>19.3.3</b> Initial Steps</a></li>
<li class="chapter" data-level="19.3.4" data-path="join.html"><a href="join.html#join_mergefunction"><i class="fa fa-check"></i><b>19.3.4</b> <code>merge</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="filter.html"><a href="filter.html"><i class="fa fa-check"></i><b>20</b> Filtering (Subsetting) Data</a>
<ul>
<li class="chapter" data-level="20.1" data-path="filter.html"><a href="filter.html#conceptualoverview_filter"><i class="fa fa-check"></i><b>20.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="filter.html"><a href="filter.html#review_logicaloperators"><i class="fa fa-check"></i><b>20.1.1</b> Review of Logical Operators</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="filter.html"><a href="filter.html#tutorial_filter"><i class="fa fa-check"></i><b>20.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="filter.html"><a href="filter.html#videotutorial_filter"><i class="fa fa-check"></i><b>20.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="20.2.2" data-path="filter.html"><a href="filter.html#function_filter"><i class="fa fa-check"></i><b>20.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="20.2.3" data-path="filter.html"><a href="filter.html#initsteps_filter"><i class="fa fa-check"></i><b>20.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="20.2.4" data-path="filter.html"><a href="filter.html#filter_cases"><i class="fa fa-check"></i><b>20.2.4</b> Filter Cases from Data Frame</a></li>
<li class="chapter" data-level="20.2.5" data-path="filter.html"><a href="filter.html#remove-single-variable-from-data-frame"><i class="fa fa-check"></i><b>20.2.5</b> Remove Single Variable from Data Frame</a></li>
<li class="chapter" data-level="20.2.6" data-path="filter.html"><a href="filter.html#select_multiplevariables"><i class="fa fa-check"></i><b>20.2.6</b> Select Multiple Variables from Data Frame</a></li>
<li class="chapter" data-level="20.2.7" data-path="filter.html"><a href="filter.html#remove_multiplevariables"><i class="fa fa-check"></i><b>20.2.7</b> Remove Multiple Variables from Data Frame</a></li>
<li class="chapter" data-level="20.2.8" data-path="filter.html"><a href="filter.html#summary_filter"><i class="fa fa-check"></i><b>20.2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="filter.html"><a href="filter.html#filter_supplement"><i class="fa fa-check"></i><b>20.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="filter.html"><a href="filter.html#supp_filter_video"><i class="fa fa-check"></i><b>20.3.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="20.3.2" data-path="filter.html"><a href="filter.html#filter_supplement_functions"><i class="fa fa-check"></i><b>20.3.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="20.3.3" data-path="filter.html"><a href="filter.html#filter_initsteps_supplement"><i class="fa fa-check"></i><b>20.3.3</b> Initial Steps</a></li>
<li class="chapter" data-level="20.3.4" data-path="filter.html"><a href="filter.html#filter_subset_supplement"><i class="fa fa-check"></i><b>20.3.4</b> <code>subset</code> Function from Base R</a></li>
<li class="chapter" data-level="20.3.5" data-path="filter.html"><a href="filter.html#str_detect_supp"><i class="fa fa-check"></i><b>20.3.5</b> Filter by Pattern Contained within String</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="clean.html"><a href="clean.html"><i class="fa fa-check"></i><b>21</b> Cleaning Data</a>
<ul>
<li class="chapter" data-level="21.1" data-path="clean.html"><a href="clean.html#conceptualoverview_clean"><i class="fa fa-check"></i><b>21.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="21.2" data-path="clean.html"><a href="clean.html#tutorial_clean"><i class="fa fa-check"></i><b>21.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="clean.html"><a href="clean.html#videotutorial_clean"><i class="fa fa-check"></i><b>21.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="21.2.2" data-path="clean.html"><a href="clean.html#functions_clean"><i class="fa fa-check"></i><b>21.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="21.2.3" data-path="clean.html"><a href="clean.html#initsteps_clean"><i class="fa fa-check"></i><b>21.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="21.2.4" data-path="clean.html"><a href="clean.html#reviewdata_clean"><i class="fa fa-check"></i><b>21.2.4</b> Review Data</a></li>
<li class="chapter" data-level="21.2.5" data-path="clean.html"><a href="clean.html#cleandata_clean"><i class="fa fa-check"></i><b>21.2.5</b> Clean Data</a></li>
<li class="chapter" data-level="21.2.6" data-path="clean.html"><a href="clean.html#renamevariables_clean"><i class="fa fa-check"></i><b>21.2.6</b> Rename Variables</a></li>
<li class="chapter" data-level="21.2.7" data-path="clean.html"><a href="clean.html#otherapproaches_clean"><i class="fa fa-check"></i><b>21.2.7</b> Other Approaches to Cleaning Data</a></li>
<li class="chapter" data-level="21.2.8" data-path="clean.html"><a href="clean.html#summary_clean"><i class="fa fa-check"></i><b>21.2.8</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="manipulate.html"><a href="manipulate.html"><i class="fa fa-check"></i><b>22</b> Manipulating &amp; Restructuring Data</a>
<ul>
<li class="chapter" data-level="22.1" data-path="manipulate.html"><a href="manipulate.html#conceptualoverview_manipulate"><i class="fa fa-check"></i><b>22.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="22.2" data-path="manipulate.html"><a href="manipulate.html#tutorial_manipulate"><i class="fa fa-check"></i><b>22.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="manipulate.html"><a href="manipulate.html#videotutorial_manipulate"><i class="fa fa-check"></i><b>22.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="22.2.2" data-path="manipulate.html"><a href="manipulate.html#functions_manipulate"><i class="fa fa-check"></i><b>22.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="22.2.3" data-path="manipulate.html"><a href="manipulate.html#initsteps_manipulate"><i class="fa fa-check"></i><b>22.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="22.2.4" data-path="manipulate.html"><a href="manipulate.html#manipulate_widetolong"><i class="fa fa-check"></i><b>22.2.4</b> Wide-to-Long Format Data Manipulation</a></li>
<li class="chapter" data-level="22.2.5" data-path="manipulate.html"><a href="manipulate.html#manipulate_longtowide"><i class="fa fa-check"></i><b>22.2.5</b> Long-to-Wide Format Data Manipulation</a></li>
<li class="chapter" data-level="22.2.6" data-path="manipulate.html"><a href="manipulate.html#summary_manipulate"><i class="fa fa-check"></i><b>22.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="center.html"><a href="center.html"><i class="fa fa-check"></i><b>23</b> Centering &amp; Standardizing Variables</a>
<ul>
<li class="chapter" data-level="23.1" data-path="center.html"><a href="center.html#conceptualoverview_center"><i class="fa fa-check"></i><b>23.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="center.html"><a href="center.html#review_center"><i class="fa fa-check"></i><b>23.1.1</b> Review of Centering Variables</a></li>
<li class="chapter" data-level="23.1.2" data-path="center.html"><a href="center.html#review_standardize"><i class="fa fa-check"></i><b>23.1.2</b> Review of Standardizing Variables</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="center.html"><a href="center.html#tutorial_center"><i class="fa fa-check"></i><b>23.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="center.html"><a href="center.html#videotutorial_center"><i class="fa fa-check"></i><b>23.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="23.2.2" data-path="center.html"><a href="center.html#functions_center"><i class="fa fa-check"></i><b>23.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="23.2.3" data-path="center.html"><a href="center.html#initsteps_center"><i class="fa fa-check"></i><b>23.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="23.2.4" data-path="center.html"><a href="center.html#grandmean_center"><i class="fa fa-check"></i><b>23.2.4</b> Grand-Mean Center Variables</a></li>
<li class="chapter" data-level="23.2.5" data-path="center.html"><a href="center.html#groupmean_center"><i class="fa fa-check"></i><b>23.2.5</b> Group-Mean Center Variables</a></li>
<li class="chapter" data-level="23.2.6" data-path="center.html"><a href="center.html#standardize_center"><i class="fa fa-check"></i><b>23.2.6</b> Standardize Variables</a></li>
<li class="chapter" data-level="23.2.7" data-path="center.html"><a href="center.html#summary_center"><i class="fa fa-check"></i><b>23.2.7</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="removeobjects.html"><a href="removeobjects.html"><i class="fa fa-check"></i><b>24</b> Removing Objects from the R Environment</a>
<ul>
<li class="chapter" data-level="24.1" data-path="removeobjects.html"><a href="removeobjects.html#conceptualoverview_removeobjects"><i class="fa fa-check"></i><b>24.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="24.2" data-path="removeobjects.html"><a href="removeobjects.html#tutorial_removeobjects"><i class="fa fa-check"></i><b>24.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="removeobjects.html"><a href="removeobjects.html#videotutorial_removeobjects"><i class="fa fa-check"></i><b>24.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="24.2.2" data-path="removeobjects.html"><a href="removeobjects.html#function_removeobjects"><i class="fa fa-check"></i><b>24.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="24.2.3" data-path="removeobjects.html"><a href="removeobjects.html#initsteps_removeobjects"><i class="fa fa-check"></i><b>24.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="24.2.4" data-path="removeobjects.html"><a href="removeobjects.html#listobjects_removeobjects"><i class="fa fa-check"></i><b>24.2.4</b> List Objects in R Environment</a></li>
<li class="chapter" data-level="24.2.5" data-path="removeobjects.html"><a href="removeobjects.html#removeobjects_removeobjects"><i class="fa fa-check"></i><b>24.2.5</b> Remove Objects from R Environment</a></li>
<li class="chapter" data-level="24.2.6" data-path="removeobjects.html"><a href="removeobjects.html#summary_removeobjects"><i class="fa fa-check"></i><b>24.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Employee Demographics</b></span></li>
<li class="chapter" data-level="25" data-path="employeedemographics.html"><a href="employeedemographics.html"><i class="fa fa-check"></i><b>25</b> Introduction to Employee Demographics</a></li>
<li class="chapter" data-level="26" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>26</b> Describing Employee Demographics Using Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="26.1" data-path="descriptives.html"><a href="descriptives.html#conceptualoverview_descriptives"><i class="fa fa-check"></i><b>26.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="26.1.1" data-path="descriptives.html"><a href="descriptives.html#measurementscales"><i class="fa fa-check"></i><b>26.1.1</b> Review of Measurement Scales</a></li>
<li class="chapter" data-level="26.1.2" data-path="descriptives.html"><a href="descriptives.html#constructs_measures_measurementscales"><i class="fa fa-check"></i><b>26.1.2</b> Constructs, Measures, &amp; Measurement Scales</a></li>
<li class="chapter" data-level="26.1.3" data-path="descriptives.html"><a href="descriptives.html#typesof_descriptivestatistics"><i class="fa fa-check"></i><b>26.1.3</b> Types of Descriptive Statistics</a></li>
<li class="chapter" data-level="26.1.4" data-path="descriptives.html"><a href="descriptives.html#samplewriteup_descriptives"><i class="fa fa-check"></i><b>26.1.4</b> Sample Write-Up</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="descriptives.html"><a href="descriptives.html#tutorial_descriptives"><i class="fa fa-check"></i><b>26.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="26.2.1" data-path="descriptives.html"><a href="descriptives.html#videotutorial_descriptives"><i class="fa fa-check"></i><b>26.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="26.2.2" data-path="descriptives.html"><a href="descriptives.html#functions_descriptives"><i class="fa fa-check"></i><b>26.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="26.2.3" data-path="descriptives.html"><a href="descriptives.html#initsteps_descriptives"><i class="fa fa-check"></i><b>26.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="26.2.4" data-path="descriptives.html"><a href="descriptives.html#determine_measurementscale"><i class="fa fa-check"></i><b>26.2.4</b> Determine the Measurement Scale</a></li>
<li class="chapter" data-level="26.2.5" data-path="descriptives.html"><a href="descriptives.html#describe_nominal_ordinal"><i class="fa fa-check"></i><b>26.2.5</b> Describe Nominal &amp; Ordinal (Categorical) Variables</a></li>
<li class="chapter" data-level="26.2.6" data-path="descriptives.html"><a href="descriptives.html#describe_interval_ratio"><i class="fa fa-check"></i><b>26.2.6</b> Describe Interval &amp; Ratio (Continuous) Variables</a></li>
<li class="chapter" data-level="26.2.7" data-path="descriptives.html"><a href="descriptives.html#summary_descriptives"><i class="fa fa-check"></i><b>26.2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="descriptives.html"><a href="descriptives.html#descriptives_supplement"><i class="fa fa-check"></i><b>26.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="26.3.1" data-path="descriptives.html"><a href="descriptives.html#descriptives_supplement_functions"><i class="fa fa-check"></i><b>26.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="26.3.2" data-path="descriptives.html"><a href="descriptives.html#descriptives_initsteps_supplement"><i class="fa fa-check"></i><b>26.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="26.3.3" data-path="descriptives.html"><a href="descriptives.html#descriptives_coefficientofvariation_supplement"><i class="fa fa-check"></i><b>26.3.3</b> Compute Coefficient of Variation (CV)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="crosstabs.html"><a href="crosstabs.html"><i class="fa fa-check"></i><b>27</b> Summarizing Two or More Categorical Variables Using Cross-Tabulations</a>
<ul>
<li class="chapter" data-level="27.1" data-path="crosstabs.html"><a href="crosstabs.html#conceptualoverview_crosstabs"><i class="fa fa-check"></i><b>27.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="27.1.1" data-path="crosstabs.html"><a href="crosstabs.html#review_crosstabs"><i class="fa fa-check"></i><b>27.1.1</b> Review of Cross-Tabulation</a></li>
<li class="chapter" data-level="27.1.2" data-path="crosstabs.html"><a href="crosstabs.html#samplewriteup_crosstabs"><i class="fa fa-check"></i><b>27.1.2</b> Sample Write-Up</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="crosstabs.html"><a href="crosstabs.html#tutorial_crosstabs"><i class="fa fa-check"></i><b>27.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="27.2.1" data-path="crosstabs.html"><a href="crosstabs.html#videotutorial_crosstabs"><i class="fa fa-check"></i><b>27.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="27.2.2" data-path="crosstabs.html"><a href="crosstabs.html#functions_crosstabs"><i class="fa fa-check"></i><b>27.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="27.2.3" data-path="crosstabs.html"><a href="crosstabs.html#initsteps_tables"><i class="fa fa-check"></i><b>27.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="27.2.4" data-path="crosstabs.html"><a href="crosstabs.html#twoway_crosstabs"><i class="fa fa-check"></i><b>27.2.4</b> Two-Way Cross-Tabulation</a></li>
<li class="chapter" data-level="27.2.5" data-path="crosstabs.html"><a href="crosstabs.html#threeway_crosstabs"><i class="fa fa-check"></i><b>27.2.5</b> Three-Way Cross-Tabulation</a></li>
<li class="chapter" data-level="27.2.6" data-path="crosstabs.html"><a href="crosstabs.html#summary_crosstabs"><i class="fa fa-check"></i><b>27.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="pivottables.html"><a href="pivottables.html"><i class="fa fa-check"></i><b>28</b> Applying Pivot Tables to Explore Employee Demographic Data</a>
<ul>
<li class="chapter" data-level="28.1" data-path="pivottables.html"><a href="pivottables.html#conceptualoverview_pivottables"><i class="fa fa-check"></i><b>28.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="28.2" data-path="pivottables.html"><a href="pivottables.html#tutorial_pivottables"><i class="fa fa-check"></i><b>28.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="28.2.1" data-path="pivottables.html"><a href="pivottables.html#videotutorial_pivottables"><i class="fa fa-check"></i><b>28.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="28.2.2" data-path="pivottables.html"><a href="pivottables.html#functions_pivottables"><i class="fa fa-check"></i><b>28.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="28.2.3" data-path="pivottables.html"><a href="pivottables.html#initsteps_pivottables"><i class="fa fa-check"></i><b>28.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="28.2.4" data-path="pivottables.html"><a href="pivottables.html#create_pivottables"><i class="fa fa-check"></i><b>28.2.4</b> Create a Pivot Table</a></li>
<li class="chapter" data-level="28.2.5" data-path="pivottables.html"><a href="pivottables.html#summary_pivottables"><i class="fa fa-check"></i><b>28.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VI Employee Surveys</b></span></li>
<li class="chapter" data-level="29" data-path="employeesurveys.html"><a href="employeesurveys.html"><i class="fa fa-check"></i><b>29</b> Introduction to Employee Surveys</a></li>
<li class="chapter" data-level="30" data-path="aggregatesegment.html"><a href="aggregatesegment.html"><i class="fa fa-check"></i><b>30</b> Aggregating &amp; Segmenting Employee Survey Data</a>
<ul>
<li class="chapter" data-level="30.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#conceptualoverview_aggregatesegment"><i class="fa fa-check"></i><b>30.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="30.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#tutorial_aggregatesegment"><i class="fa fa-check"></i><b>30.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="30.2.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#videotutorial_aggregatesegment"><i class="fa fa-check"></i><b>30.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="30.2.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#functions_aggregatesegment"><i class="fa fa-check"></i><b>30.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="30.2.3" data-path="aggregatesegment.html"><a href="aggregatesegment.html#initsteps_aggregatesegment"><i class="fa fa-check"></i><b>30.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="30.2.4" data-path="aggregatesegment.html"><a href="aggregatesegment.html#counts_bygroup"><i class="fa fa-check"></i><b>30.2.4</b> Counts By Group</a></li>
<li class="chapter" data-level="30.2.5" data-path="aggregatesegment.html"><a href="aggregatesegment.html#centraldispersion_bygroup"><i class="fa fa-check"></i><b>30.2.5</b> Measures of Central Tendency and Dispersion By Group</a></li>
<li class="chapter" data-level="30.2.6" data-path="aggregatesegment.html"><a href="aggregatesegment.html#addaggregatedvariable"><i class="fa fa-check"></i><b>30.2.6</b> Add Variable to Data Frame Containing Aggregated Values</a></li>
<li class="chapter" data-level="30.2.7" data-path="aggregatesegment.html"><a href="aggregatesegment.html#visualize_bygroup"><i class="fa fa-check"></i><b>30.2.7</b> Visualize Data By Group</a></li>
<li class="chapter" data-level="30.2.8" data-path="aggregatesegment.html"><a href="aggregatesegment.html#summary_aggregatesegment"><i class="fa fa-check"></i><b>30.2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_supplement"><i class="fa fa-check"></i><b>30.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="30.3.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_supplement_functions"><i class="fa fa-check"></i><b>30.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="30.3.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_initsteps_supplement"><i class="fa fa-check"></i><b>30.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="30.3.3" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_describeby_supplement"><i class="fa fa-check"></i><b>30.3.3</b> <code>describeBy</code> Function from <code>psych</code> Package</a></li>
<li class="chapter" data-level="30.3.4" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_aggregate_supplement"><i class="fa fa-check"></i><b>30.3.4</b> <code>aggregate</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html"><i class="fa fa-check"></i><b>31</b> Estimating Internal Consistency Reliability Using Cronbach’s alpha</a>
<ul>
<li class="chapter" data-level="31.1" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#conceptualoverview_cronbachsalpha"><i class="fa fa-check"></i><b>31.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="31.2" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#tutorial_cronbachsalpha"><i class="fa fa-check"></i><b>31.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="31.2.1" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#videotutorial_cronbachsalpha"><i class="fa fa-check"></i><b>31.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="31.2.2" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#functions_cronbachsalpha"><i class="fa fa-check"></i><b>31.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="31.2.3" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#initsteps_cronbachsalpha"><i class="fa fa-check"></i><b>31.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="31.2.4" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#cronbachsalpha_alpha"><i class="fa fa-check"></i><b>31.2.4</b> Compute Cronbach’s alpha</a></li>
<li class="chapter" data-level="31.2.5" data-path="cronbachsalpha.html"><a href="cronbachsalpha.html#summary_cronbachsalpha"><i class="fa fa-check"></i><b>31.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="compositevariable.html"><a href="compositevariable.html"><i class="fa fa-check"></i><b>32</b> Creating a Composite Variable Based on a Multi-Item Measure</a>
<ul>
<li class="chapter" data-level="32.1" data-path="compositevariable.html"><a href="compositevariable.html#conceptualoverview_compositevariable"><i class="fa fa-check"></i><b>32.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="32.2" data-path="compositevariable.html"><a href="compositevariable.html#tutorial_compositevariable"><i class="fa fa-check"></i><b>32.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="32.2.1" data-path="compositevariable.html"><a href="compositevariable.html#videotutorial_compositevariable"><i class="fa fa-check"></i><b>32.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="32.2.2" data-path="compositevariable.html"><a href="compositevariable.html#functions_compositevariable"><i class="fa fa-check"></i><b>32.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="32.2.3" data-path="compositevariable.html"><a href="compositevariable.html#initsteps_compositevariable"><i class="fa fa-check"></i><b>32.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="32.2.4" data-path="compositevariable.html"><a href="compositevariable.html#compositevariable_cronbachsalpha"><i class="fa fa-check"></i><b>32.2.4</b> Compute Cronbach’s alpha</a></li>
<li class="chapter" data-level="32.2.5" data-path="compositevariable.html"><a href="compositevariable.html#compositevariable_composite"><i class="fa fa-check"></i><b>32.2.5</b> Create a Composite Variable</a></li>
<li class="chapter" data-level="32.2.6" data-path="compositevariable.html"><a href="compositevariable.html#summary_compositevariable"><i class="fa fa-check"></i><b>32.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Employee Training</b></span></li>
<li class="chapter" data-level="33" data-path="employeetraining.html"><a href="employeetraining.html"><i class="fa fa-check"></i><b>33</b> Introduction to Employee Training</a>
<ul>
<li class="chapter" data-level="33.1" data-path="employeetraining.html"><a href="employeetraining.html#trainingevaluation_employeetraining"><i class="fa fa-check"></i><b>33.1</b> Training Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="pretestposttest.html"><a href="pretestposttest.html"><i class="fa fa-check"></i><b>34</b> Evaluating a Pre-Test/Post-Test without Control Group Design Using Paired-Samples <em>t</em>-test</a>
<ul>
<li class="chapter" data-level="34.1" data-path="pretestposttest.html"><a href="pretestposttest.html#conceptualoverview_pretestposttest_psttest"><i class="fa fa-check"></i><b>34.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="34.1.1" data-path="pretestposttest.html"><a href="pretestposttest.html#review_pretestposttest"><i class="fa fa-check"></i><b>34.1.1</b> Review of Pre-Test/Post-Test without Control Group Design</a></li>
<li class="chapter" data-level="34.1.2" data-path="pretestposttest.html"><a href="pretestposttest.html#review_psttest"><i class="fa fa-check"></i><b>34.1.2</b> Review of Paired-Samples <em>t</em>-test</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="pretestposttest.html"><a href="pretestposttest.html#tutorial_psttest"><i class="fa fa-check"></i><b>34.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="34.2.1" data-path="pretestposttest.html"><a href="pretestposttest.html#videotutorial_psttest"><i class="fa fa-check"></i><b>34.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="34.2.2" data-path="pretestposttest.html"><a href="pretestposttest.html#functions_psttest"><i class="fa fa-check"></i><b>34.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="34.2.3" data-path="pretestposttest.html"><a href="pretestposttest.html#initsteps_psttest"><i class="fa fa-check"></i><b>34.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="34.2.4" data-path="pretestposttest.html"><a href="pretestposttest.html#estimate_psttest"><i class="fa fa-check"></i><b>34.2.4</b> Estimate Paired-Samples <em>t</em>-test</a></li>
<li class="chapter" data-level="34.2.5" data-path="pretestposttest.html"><a href="pretestposttest.html#barchart_psttest"><i class="fa fa-check"></i><b>34.2.5</b> Visualize Results Using Bar Chart</a></li>
<li class="chapter" data-level="34.2.6" data-path="pretestposttest.html"><a href="pretestposttest.html#summary_psttest"><i class="fa fa-check"></i><b>34.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="34.3" data-path="pretestposttest.html"><a href="pretestposttest.html#psttest_supplement"><i class="fa fa-check"></i><b>34.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="34.3.1" data-path="pretestposttest.html"><a href="pretestposttest.html#psttest_supplement_functions"><i class="fa fa-check"></i><b>34.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="34.3.2" data-path="pretestposttest.html"><a href="pretestposttest.html#psttest_initsteps_supplement"><i class="fa fa-check"></i><b>34.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="34.3.3" data-path="pretestposttest.html"><a href="pretestposttest.html#t.test_function_psttest"><i class="fa fa-check"></i><b>34.3.3</b> <code>t.test</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="posttestonly.html"><a href="posttestonly.html"><i class="fa fa-check"></i><b>35</b> Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples <em>t</em>-test</a>
<ul>
<li class="chapter" data-level="35.1" data-path="posttestonly.html"><a href="posttestonly.html#conceptualoverview_posttestonly_isttest"><i class="fa fa-check"></i><b>35.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="posttestonly.html"><a href="posttestonly.html#review_posttestonly"><i class="fa fa-check"></i><b>35.1.1</b> Review of Post-Test-Only with Control Group Design</a></li>
<li class="chapter" data-level="35.1.2" data-path="posttestonly.html"><a href="posttestonly.html#review_isttest"><i class="fa fa-check"></i><b>35.1.2</b> Review of Independent-Samples <em>t</em>-test</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="posttestonly.html"><a href="posttestonly.html#tutorial_isttest"><i class="fa fa-check"></i><b>35.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="posttestonly.html"><a href="posttestonly.html#videotutorial_isttest"><i class="fa fa-check"></i><b>35.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="35.2.2" data-path="posttestonly.html"><a href="posttestonly.html#functions_isttest"><i class="fa fa-check"></i><b>35.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="35.2.3" data-path="posttestonly.html"><a href="posttestonly.html#initsteps_isttest"><i class="fa fa-check"></i><b>35.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="35.2.4" data-path="posttestonly.html"><a href="posttestonly.html#estimate_isttest"><i class="fa fa-check"></i><b>35.2.4</b> Estimate Independent-Samples <em>t</em>-test</a></li>
<li class="chapter" data-level="35.2.5" data-path="posttestonly.html"><a href="posttestonly.html#barchart_isttest"><i class="fa fa-check"></i><b>35.2.5</b> Visualize Results Using Bar Chart</a></li>
<li class="chapter" data-level="35.2.6" data-path="posttestonly.html"><a href="posttestonly.html#summary_isttest"><i class="fa fa-check"></i><b>35.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="35.3" data-path="posttestonly.html"><a href="posttestonly.html#isttest_supplement"><i class="fa fa-check"></i><b>35.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="35.3.1" data-path="posttestonly.html"><a href="posttestonly.html#isttest_supplement_functions"><i class="fa fa-check"></i><b>35.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="35.3.2" data-path="posttestonly.html"><a href="posttestonly.html#isttest_initsteps_supplement"><i class="fa fa-check"></i><b>35.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="35.3.3" data-path="posttestonly.html"><a href="posttestonly.html#t.test_function_isttest"><i class="fa fa-check"></i><b>35.3.3</b> <code>t.test</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html"><i class="fa fa-check"></i><b>36</b> Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="36.1" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#conceptualoverview_posttestonly_threegroups_onewayanova"><i class="fa fa-check"></i><b>36.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#review_posttestonly_threegroups"><i class="fa fa-check"></i><b>36.1.1</b> Review of Post-Test-Only with Two Comparison Groups Design</a></li>
<li class="chapter" data-level="36.1.2" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#review_onewayanova"><i class="fa fa-check"></i><b>36.1.2</b> Review of One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#tutorial_onewayanova"><i class="fa fa-check"></i><b>36.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#videotutorial_onewayanova"><i class="fa fa-check"></i><b>36.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="36.2.2" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#functions_onewayanova"><i class="fa fa-check"></i><b>36.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="36.2.3" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#initsteps_onewayanova"><i class="fa fa-check"></i><b>36.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="36.2.4" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#teststatisticalassumptions_onewayanova"><i class="fa fa-check"></i><b>36.2.4</b> Test Statistical Assumptions</a></li>
<li class="chapter" data-level="36.2.5" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#estimate_onewayanova"><i class="fa fa-check"></i><b>36.2.5</b> Estimate One-Way ANOVA</a></li>
<li class="chapter" data-level="36.2.6" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#barchart_onewayanova"><i class="fa fa-check"></i><b>36.2.6</b> Visualize Results Using Bar Chart</a></li>
<li class="chapter" data-level="36.2.7" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#summary_onewayanova"><i class="fa fa-check"></i><b>36.2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="36.3" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#onewayanova_supplement"><i class="fa fa-check"></i><b>36.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="36.3.1" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#onewayanova_supplement_functions"><i class="fa fa-check"></i><b>36.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="36.3.2" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#onewayanova_initsteps_supplement"><i class="fa fa-check"></i><b>36.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="36.3.3" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#aov_function_onewayanova"><i class="fa fa-check"></i><b>36.3.3</b> <code>aov</code> Function from Base R</a></li>
<li class="chapter" data-level="36.3.4" data-path="posttestonly_threegroups.html"><a href="posttestonly_threegroups.html#apatable_onewayanova"><i class="fa fa-check"></i><b>36.3.4</b> APA-Style Table of Results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Employee Selection</b></span></li>
<li class="chapter" data-level="37" data-path="selection.html"><a href="selection.html"><i class="fa fa-check"></i><b>37</b> Introduction to Employee Selection</a></li>
<li class="chapter" data-level="38" data-path="disparateimpact.html"><a href="disparateimpact.html"><i class="fa fa-check"></i><b>38</b> Investigating Disparate Impact</a>
<ul>
<li class="chapter" data-level="38.1" data-path="disparateimpact.html"><a href="disparateimpact.html#conceptualoverview_disparateimpact"><i class="fa fa-check"></i><b>38.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="38.2" data-path="disparateimpact.html"><a href="disparateimpact.html#tutorial_disparateimpact"><i class="fa fa-check"></i><b>38.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="38.2.1" data-path="disparateimpact.html"><a href="disparateimpact.html#videotutorial_disparateimpact"><i class="fa fa-check"></i><b>38.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="38.2.2" data-path="disparateimpact.html"><a href="disparateimpact.html#functions_disparateimpact"><i class="fa fa-check"></i><b>38.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="38.2.3" data-path="disparateimpact.html"><a href="disparateimpact.html#initsteps_disparateimpact"><i class="fa fa-check"></i><b>38.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="38.2.4" data-path="disparateimpact.html"><a href="disparateimpact.html#fourfifthsrule"><i class="fa fa-check"></i><b>38.2.4</b> 4/5ths Rule</a></li>
<li class="chapter" data-level="38.2.5" data-path="disparateimpact.html"><a href="disparateimpact.html#chisquaretest_disparateimpact"><i class="fa fa-check"></i><b>38.2.5</b> chi-square (<span class="math inline">\(\chi^2\)</span>) Test of Independence</a></li>
<li class="chapter" data-level="38.2.6" data-path="disparateimpact.html"><a href="disparateimpact.html#fisherexacttest"><i class="fa fa-check"></i><b>38.2.6</b> Fisher Exact Test</a></li>
<li class="chapter" data-level="38.2.7" data-path="disparateimpact.html"><a href="disparateimpact.html#zdifference_test_disparateimpact"><i class="fa fa-check"></i><b>38.2.7</b> <span class="math inline">\(Z_{D}\)</span> Test</a></li>
<li class="chapter" data-level="38.2.8" data-path="disparateimpact.html"><a href="disparateimpact.html#z_impactratiotest"><i class="fa fa-check"></i><b>38.2.8</b> <span class="math inline">\(Z_{IR}\)</span> Test</a></li>
<li class="chapter" data-level="38.2.9" data-path="disparateimpact.html"><a href="disparateimpact.html#summary_disparatetreatment"><i class="fa fa-check"></i><b>38.2.9</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="39" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html"><i class="fa fa-check"></i><b>39</b> Estimating Criterion-Related Validity of a Selection Tool Using Correlation</a>
<ul>
<li class="chapter" data-level="39.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#conceptualoverview_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="39.1.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#review_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.1.1</b> Review of Criterion-Related Validity</a></li>
<li class="chapter" data-level="39.1.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#review_correlation"><i class="fa fa-check"></i><b>39.1.2</b> Review of Correlation</a></li>
</ul></li>
<li class="chapter" data-level="39.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#tutorial_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="39.2.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#videotutorial_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="39.2.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#functions_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="39.2.3" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#initsteps_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="39.2.4" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#scatterplot_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2.4</b> Visualize Association Using a Scatter Plot</a></li>
<li class="chapter" data-level="39.2.5" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#estimate_correlation"><i class="fa fa-check"></i><b>39.2.5</b> Estimate Correlation</a></li>
<li class="chapter" data-level="39.2.6" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#summary_criterionrelatedvalidity"><i class="fa fa-check"></i><b>39.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="39.3" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#criterionrelatedvalidity_supplement"><i class="fa fa-check"></i><b>39.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="39.3.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#criterionrelatedvalidity_supplement_functions"><i class="fa fa-check"></i><b>39.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="39.3.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#criterionrelatedvalidity_initsteps_supplement"><i class="fa fa-check"></i><b>39.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="39.3.3" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#cor_function"><i class="fa fa-check"></i><b>39.3.3</b> <code>cor</code> Function from Base R</a></li>
<li class="chapter" data-level="39.3.4" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#cortest_function"><i class="fa fa-check"></i><b>39.3.4</b> <code>cor.test</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="40" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html"><i class="fa fa-check"></i><b>40</b> Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="40.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#conceptualoverview_predictingcriterionscores"><i class="fa fa-check"></i><b>40.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="40.1.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#review_slr"><i class="fa fa-check"></i><b>40.1.1</b> Review of Simple Linear Regression</a></li>
<li class="chapter" data-level="40.1.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#review_prediction_slr"><i class="fa fa-check"></i><b>40.1.2</b> Predicting Future Criterion Scores Using Simple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="40.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#tutorial_predictingcriterionscores"><i class="fa fa-check"></i><b>40.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="40.2.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#videotutorial_predictingcriterionscores"><i class="fa fa-check"></i><b>40.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="40.2.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#functions_predictingcriterionscores"><i class="fa fa-check"></i><b>40.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="40.2.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#initsteps_slr"><i class="fa fa-check"></i><b>40.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="40.2.4" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#estimate_slr"><i class="fa fa-check"></i><b>40.2.4</b> Estimate Simple Linear Regression Model</a></li>
<li class="chapter" data-level="40.2.5" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_slr"><i class="fa fa-check"></i><b>40.2.5</b> Predict Criterion Scores</a></li>
<li class="chapter" data-level="40.2.6" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#summary_predictingcriterionscores"><i class="fa fa-check"></i><b>40.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_supplement"><i class="fa fa-check"></i><b>40.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="40.3.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_supplement_functions"><i class="fa fa-check"></i><b>40.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="40.3.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_initsteps_supplement"><i class="fa fa-check"></i><b>40.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="40.3.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#lm_function_slr"><i class="fa fa-check"></i><b>40.3.3</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="40.3.4" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predict_function_slr"><i class="fa fa-check"></i><b>40.3.4</b> <code>predict</code> Function from Base R</a></li>
<li class="chapter" data-level="40.3.5" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#apatable_slr"><i class="fa fa-check"></i><b>40.3.5</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="41" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html"><i class="fa fa-check"></i><b>41</b> Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="41.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#conceptualoverview_incrementalvalidity"><i class="fa fa-check"></i><b>41.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="41.1.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#review_mlr"><i class="fa fa-check"></i><b>41.1.1</b> Review of Multiple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="41.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#tutorial_incrementalvalidity"><i class="fa fa-check"></i><b>41.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="41.2.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#videotutorial_incrementalvalidity"><i class="fa fa-check"></i><b>41.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="41.2.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#function_incrementalvalidity"><i class="fa fa-check"></i><b>41.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="41.2.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#initsteps_mlr"><i class="fa fa-check"></i><b>41.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="41.2.4" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#estimate_mlr"><i class="fa fa-check"></i><b>41.2.4</b> Estimate Multiple Linear Regression Model</a></li>
<li class="chapter" data-level="41.2.5" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#summary_incrementalvalidity"><i class="fa fa-check"></i><b>41.2.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="41.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#incrementalvalidity_supplement"><i class="fa fa-check"></i><b>41.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="41.3.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#incrementalvalidity_supplement_functions"><i class="fa fa-check"></i><b>41.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="41.3.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#incrementalvalidity_initsteps_supplement"><i class="fa fa-check"></i><b>41.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="41.3.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#lm_function_mlr"><i class="fa fa-check"></i><b>41.3.3</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="41.3.4" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#apatable_mlr"><i class="fa fa-check"></i><b>41.3.4</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="42" data-path="compensatory.html"><a href="compensatory.html"><i class="fa fa-check"></i><b>42</b> Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="42.1" data-path="compensatory.html"><a href="compensatory.html#conceptualoverview_compensatory"><i class="fa fa-check"></i><b>42.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="42.1.1" data-path="compensatory.html"><a href="compensatory.html#review_compensatory_mlr"><i class="fa fa-check"></i><b>42.1.1</b> Review of Multiple Linear Regression</a></li>
<li class="chapter" data-level="42.1.2" data-path="compensatory.html"><a href="compensatory.html#review_compensatory"><i class="fa fa-check"></i><b>42.1.2</b> Review of Compensatory Approach</a></li>
</ul></li>
<li class="chapter" data-level="42.2" data-path="compensatory.html"><a href="compensatory.html#tutorial_compensatory"><i class="fa fa-check"></i><b>42.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="42.2.1" data-path="compensatory.html"><a href="compensatory.html#videotutorial_compensatory"><i class="fa fa-check"></i><b>42.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="42.2.2" data-path="compensatory.html"><a href="compensatory.html#function_compensatory"><i class="fa fa-check"></i><b>42.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="42.2.3" data-path="compensatory.html"><a href="compensatory.html#initsteps_compensatory"><i class="fa fa-check"></i><b>42.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="42.2.4" data-path="compensatory.html"><a href="compensatory.html#estimate_compensatory"><i class="fa fa-check"></i><b>42.2.4</b> Estimate Multiple Linear Regression Model</a></li>
<li class="chapter" data-level="42.2.5" data-path="compensatory.html"><a href="compensatory.html#predictcriterionscores_compensatory"><i class="fa fa-check"></i><b>42.2.5</b> Predict Criterion Scores</a></li>
<li class="chapter" data-level="42.2.6" data-path="compensatory.html"><a href="compensatory.html#summary_compensatory"><i class="fa fa-check"></i><b>42.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="42.3" data-path="compensatory.html"><a href="compensatory.html#compensatory_supplement"><i class="fa fa-check"></i><b>42.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="42.3.1" data-path="compensatory.html"><a href="compensatory.html#compensatory_supplement_functions"><i class="fa fa-check"></i><b>42.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="42.3.2" data-path="compensatory.html"><a href="compensatory.html#compensatory_initsteps_supplement"><i class="fa fa-check"></i><b>42.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="42.3.3" data-path="compensatory.html"><a href="compensatory.html#lm_predict_functions_compensatory"><i class="fa fa-check"></i><b>42.3.3</b> <code>lm</code> &amp; <code>predict</code> Functions from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="43" data-path="multiplecutoff.html"><a href="multiplecutoff.html"><i class="fa fa-check"></i><b>43</b> Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method</a>
<ul>
<li class="chapter" data-level="43.1" data-path="multiplecutoff.html"><a href="multiplecutoff.html#conceptualoverview_multiplecutoff"><i class="fa fa-check"></i><b>43.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="43.1.1" data-path="multiplecutoff.html"><a href="multiplecutoff.html#review_multiplecutoff"><i class="fa fa-check"></i><b>43.1.1</b> Review of Noncompensatory Approach</a></li>
</ul></li>
<li class="chapter" data-level="43.2" data-path="multiplecutoff.html"><a href="multiplecutoff.html#tutorial_multiplecutoff"><i class="fa fa-check"></i><b>43.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="43.2.1" data-path="multiplecutoff.html"><a href="multiplecutoff.html#function_multiplecutoff"><i class="fa fa-check"></i><b>43.2.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="43.2.2" data-path="multiplecutoff.html"><a href="multiplecutoff.html#initsteps_multiplecutoff"><i class="fa fa-check"></i><b>43.2.2</b> Initial Steps</a></li>
<li class="chapter" data-level="43.2.3" data-path="multiplecutoff.html"><a href="multiplecutoff.html#createcutoffscores_multiplecutoff"><i class="fa fa-check"></i><b>43.2.3</b> Create Cutoff Scores</a></li>
<li class="chapter" data-level="43.2.4" data-path="multiplecutoff.html"><a href="multiplecutoff.html#applycutoffscores_multiplecutoff"><i class="fa fa-check"></i><b>43.2.4</b> Apply Cutoff Scores to Make Selection Decisions</a></li>
<li class="chapter" data-level="43.2.5" data-path="multiplecutoff.html"><a href="multiplecutoff.html#summary_multiplecutoff"><i class="fa fa-check"></i><b>43.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="44" data-path="differentialprediction.html"><a href="differentialprediction.html"><i class="fa fa-check"></i><b>44</b> Testing for Differential Prediction Using Moderated Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="44.1" data-path="differentialprediction.html"><a href="differentialprediction.html#conceptualoverview_differentialprediction"><i class="fa fa-check"></i><b>44.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="44.1.1" data-path="differentialprediction.html"><a href="differentialprediction.html#review_mmlr"><i class="fa fa-check"></i><b>44.1.1</b> Review of Moderated Multiple Linear Regression</a></li>
<li class="chapter" data-level="44.1.2" data-path="differentialprediction.html"><a href="differentialprediction.html#review_differentialprediction"><i class="fa fa-check"></i><b>44.1.2</b> Review of Differential Prediction</a></li>
</ul></li>
<li class="chapter" data-level="44.2" data-path="differentialprediction.html"><a href="differentialprediction.html#tutorial_differentialprediction"><i class="fa fa-check"></i><b>44.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="44.2.1" data-path="differentialprediction.html"><a href="differentialprediction.html#videotutorial_differentialprediction"><i class="fa fa-check"></i><b>44.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="44.2.2" data-path="differentialprediction.html"><a href="differentialprediction.html#function_mmlr"><i class="fa fa-check"></i><b>44.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="44.2.3" data-path="differentialprediction.html"><a href="differentialprediction.html#initsteps_mmlr"><i class="fa fa-check"></i><b>44.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="44.2.4" data-path="differentialprediction.html"><a href="differentialprediction.html#center_mmlr"><i class="fa fa-check"></i><b>44.2.4</b> Grand-Mean Center Continuous Predictor Variables</a></li>
<li class="chapter" data-level="44.2.5" data-path="differentialprediction.html"><a href="differentialprediction.html#estimate_mmlr"><i class="fa fa-check"></i><b>44.2.5</b> Estimate Moderated Multiple Linear Regression Model</a></li>
<li class="chapter" data-level="44.2.6" data-path="differentialprediction.html"><a href="differentialprediction.html#summary_differentialprediction"><i class="fa fa-check"></i><b>44.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="44.3" data-path="differentialprediction.html"><a href="differentialprediction.html#differentialprediction_supplement"><i class="fa fa-check"></i><b>44.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="44.3.1" data-path="differentialprediction.html"><a href="differentialprediction.html#differentialprediction_supplement_functions"><i class="fa fa-check"></i><b>44.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="44.3.2" data-path="differentialprediction.html"><a href="differentialprediction.html#differentialprediction_initsteps_supplement"><i class="fa fa-check"></i><b>44.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="44.3.3" data-path="differentialprediction.html"><a href="differentialprediction.html#lm_function_mmlr"><i class="fa fa-check"></i><b>44.3.3</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="44.3.4" data-path="differentialprediction.html"><a href="differentialprediction.html#apatable_mmlr"><i class="fa fa-check"></i><b>44.3.4</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="45" data-path="crossvalidation.html"><a href="crossvalidation.html"><i class="fa fa-check"></i><b>45</b> Statistically &amp; Empirically Cross-Validating a Selection Tool</a>
<ul>
<li class="chapter" data-level="45.1" data-path="crossvalidation.html"><a href="crossvalidation.html#conceptualoverview_crossvalidation"><i class="fa fa-check"></i><b>45.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="45.1.1" data-path="crossvalidation.html"><a href="crossvalidation.html#review_statistical_crossvalidation"><i class="fa fa-check"></i><b>45.1.1</b> Review of Statistical Cross-Validation</a></li>
<li class="chapter" data-level="45.1.2" data-path="crossvalidation.html"><a href="crossvalidation.html#review_empirical_crossvalidation"><i class="fa fa-check"></i><b>45.1.2</b> Review of Empirical Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="45.2" data-path="crossvalidation.html"><a href="crossvalidation.html#tutorial_crossvalidation"><i class="fa fa-check"></i><b>45.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="45.2.1" data-path="crossvalidation.html"><a href="crossvalidation.html#function_crossvalidation"><i class="fa fa-check"></i><b>45.2.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="45.2.2" data-path="crossvalidation.html"><a href="crossvalidation.html#initsteps_crossvalidation"><i class="fa fa-check"></i><b>45.2.2</b> Initial Steps</a></li>
<li class="chapter" data-level="45.2.3" data-path="crossvalidation.html"><a href="crossvalidation.html#crossvalidation_statistical"><i class="fa fa-check"></i><b>45.2.3</b> Perform Statistical Cross-Validation</a></li>
<li class="chapter" data-level="45.2.4" data-path="crossvalidation.html"><a href="crossvalidation.html#crossvalidation_empirical"><i class="fa fa-check"></i><b>45.2.4</b> Perform Empirical Cross-Validation</a></li>
<li class="chapter" data-level="45.2.5" data-path="crossvalidation.html"><a href="crossvalidation.html#summary_crossvalidation"><i class="fa fa-check"></i><b>45.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IX Employee Separation &amp; Retention</b></span></li>
<li class="chapter" data-level="46" data-path="turnover.html"><a href="turnover.html"><i class="fa fa-check"></i><b>46</b> Introduction to Employee Separation &amp; Retention</a></li>
<li class="chapter" data-level="47" data-path="turnoverrate.html"><a href="turnoverrate.html"><i class="fa fa-check"></i><b>47</b> Computing Monthly &amp; Annual Turnover Rates</a>
<ul>
<li class="chapter" data-level="47.1" data-path="turnoverrate.html"><a href="turnoverrate.html#conceptualoverview_turnoverrate"><i class="fa fa-check"></i><b>47.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="47.2" data-path="turnoverrate.html"><a href="turnoverrate.html#tutorial_turnoverrate"><i class="fa fa-check"></i><b>47.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="47.2.1" data-path="turnoverrate.html"><a href="turnoverrate.html#videotutorial_turnoverrate"><i class="fa fa-check"></i><b>47.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="47.2.2" data-path="turnoverrate.html"><a href="turnoverrate.html#functions_turnoverrate"><i class="fa fa-check"></i><b>47.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="47.2.3" data-path="turnoverrate.html"><a href="turnoverrate.html#initsteps_turnoverrate"><i class="fa fa-check"></i><b>47.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="47.2.4" data-path="turnoverrate.html"><a href="turnoverrate.html#turnoverrate_monthlyturnoverrate"><i class="fa fa-check"></i><b>47.2.4</b> Compute Monthly Turnover Rates</a></li>
<li class="chapter" data-level="47.2.5" data-path="turnoverrate.html"><a href="turnoverrate.html#turnoverrate_annualturnoverrate"><i class="fa fa-check"></i><b>47.2.5</b> Compute Annual Turnover Rate</a></li>
<li class="chapter" data-level="47.2.6" data-path="turnoverrate.html"><a href="turnoverrate.html#summary_turnoverrate"><i class="fa fa-check"></i><b>47.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="48" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html"><i class="fa fa-check"></i><b>48</b> Estimating the Association Between Two Categorical Variables Using Chi-Square (<span class="math inline">\(\chi^2\)</span>) Test of Independence</a>
<ul>
<li class="chapter" data-level="48.1" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#conceptualoverview_turnoverchisquare"><i class="fa fa-check"></i><b>48.1</b> Conceptual Overview</a></li>
<li class="chapter" data-level="48.2" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#tutorial_turnoverchisquare"><i class="fa fa-check"></i><b>48.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="48.2.1" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#videotutorial_turnoverchisquare"><i class="fa fa-check"></i><b>48.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="48.2.2" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#functions_turnoverchisquare"><i class="fa fa-check"></i><b>48.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="48.2.3" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#initsteps_turnoverchisquare"><i class="fa fa-check"></i><b>48.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="48.2.4" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#turnoverchisquare_contingencytable"><i class="fa fa-check"></i><b>48.2.4</b> Create a Contingency Table for Observed Data</a></li>
<li class="chapter" data-level="48.2.5" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#turnoverchisquare_chisquare"><i class="fa fa-check"></i><b>48.2.5</b> Estimate Chi-Square (<span class="math inline">\(\chi^2\)</span>) Test of Independence</a></li>
<li class="chapter" data-level="48.2.6" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#summary_turnoverchisquare"><i class="fa fa-check"></i><b>48.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="48.3" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#turnoverchisquare_supplement"><i class="fa fa-check"></i><b>48.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="48.3.1" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#turnoverchisquare_supplement_functions"><i class="fa fa-check"></i><b>48.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="48.3.2" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#turnoverchisquare_initsteps_supplement"><i class="fa fa-check"></i><b>48.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="48.3.3" data-path="turnoverchisquare.html"><a href="turnoverchisquare.html#supplement_turnoverchisquare_oddsratio"><i class="fa fa-check"></i><b>48.3.3</b> Compute Odds Ratio for 2x2 Contingency Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="49" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>49</b> Identifying Predictors of Turnover Using Logistic Regression</a>
<ul>
<li class="chapter" data-level="49.1" data-path="logistic.html"><a href="logistic.html#conceptualoverview_logistic"><i class="fa fa-check"></i><b>49.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="49.1.1" data-path="logistic.html"><a href="logistic.html#review_logistic"><i class="fa fa-check"></i><b>49.1.1</b> Review of Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="49.2" data-path="logistic.html"><a href="logistic.html#tutorial_logistic"><i class="fa fa-check"></i><b>49.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="49.2.1" data-path="logistic.html"><a href="logistic.html#videotutorial_logistic"><i class="fa fa-check"></i><b>49.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="49.2.2" data-path="logistic.html"><a href="logistic.html#functions_logistic"><i class="fa fa-check"></i><b>49.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="49.2.3" data-path="logistic.html"><a href="logistic.html#initsteps_logistic"><i class="fa fa-check"></i><b>49.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="49.2.4" data-path="logistic.html"><a href="logistic.html#estimate_simple_logistic"><i class="fa fa-check"></i><b>49.2.4</b> Estimate Simple Logistic Regression Model</a></li>
<li class="chapter" data-level="49.2.5" data-path="logistic.html"><a href="logistic.html#estimate_multiple_logistic"><i class="fa fa-check"></i><b>49.2.5</b> Estimate Multiple Logistic Regression Model</a></li>
<li class="chapter" data-level="49.2.6" data-path="logistic.html"><a href="logistic.html#summary_logistic"><i class="fa fa-check"></i><b>49.2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="49.3" data-path="logistic.html"><a href="logistic.html#logistic_supplement"><i class="fa fa-check"></i><b>49.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="49.3.1" data-path="logistic.html"><a href="logistic.html#logistic_supplement_functions"><i class="fa fa-check"></i><b>49.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="49.3.2" data-path="logistic.html"><a href="logistic.html#logistic_initsteps_supplement"><i class="fa fa-check"></i><b>49.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="49.3.3" data-path="logistic.html"><a href="logistic.html#glm_function_simple_logistic"><i class="fa fa-check"></i><b>49.3.3</b> Simple Logistic Regression Model Using <code>glm</code> Function from Base R</a></li>
<li class="chapter" data-level="49.3.4" data-path="logistic.html"><a href="logistic.html#glm_function_multiple_logistic"><i class="fa fa-check"></i><b>49.3.4</b> Multiple Logistic Regression Using <code>glm</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="50" data-path="kfold.html"><a href="kfold.html"><i class="fa fa-check"></i><b>50</b> Applying <em>k</em>-Fold Cross-Validation to Logistic Regression</a>
<ul>
<li class="chapter" data-level="50.1" data-path="kfold.html"><a href="kfold.html#conceptualoverview_kfold"><i class="fa fa-check"></i><b>50.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="50.1.1" data-path="kfold.html"><a href="kfold.html#review_predictiveanalytics"><i class="fa fa-check"></i><b>50.1.1</b> Review of Predictive Analytics</a></li>
<li class="chapter" data-level="50.1.2" data-path="kfold.html"><a href="kfold.html#review_kfold"><i class="fa fa-check"></i><b>50.1.2</b> Review of <em>k</em>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="50.1.3" data-path="kfold.html"><a href="kfold.html#conceptualvideo_kfold"><i class="fa fa-check"></i><b>50.1.3</b> Conceptual Video</a></li>
</ul></li>
<li class="chapter" data-level="50.2" data-path="kfold.html"><a href="kfold.html#tutorial_kfold"><i class="fa fa-check"></i><b>50.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="50.2.1" data-path="kfold.html"><a href="kfold.html#videotutorial_kfold"><i class="fa fa-check"></i><b>50.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="50.2.2" data-path="kfold.html"><a href="kfold.html#functions_kfold"><i class="fa fa-check"></i><b>50.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="50.2.3" data-path="kfold.html"><a href="kfold.html#initsteps_kfold"><i class="fa fa-check"></i><b>50.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="50.2.4" data-path="kfold.html"><a href="kfold.html#estimate_kfold_logistic"><i class="fa fa-check"></i><b>50.2.4</b> Apply <em>k</em>-Fold Cross-Validation Using Logistic Regression</a></li>
<li class="chapter" data-level="50.2.5" data-path="kfold.html"><a href="kfold.html#summary_kfold"><i class="fa fa-check"></i><b>50.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="51" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>51</b> Understanding Length of Service Using Survival Analysis</a>
<ul>
<li class="chapter" data-level="51.1" data-path="survival.html"><a href="survival.html#conceptualoverview_survival"><i class="fa fa-check"></i><b>51.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="51.1.1" data-path="survival.html"><a href="survival.html#review_censoring_survival"><i class="fa fa-check"></i><b>51.1.1</b> Censoring</a></li>
<li class="chapter" data-level="51.1.2" data-path="survival.html"><a href="survival.html#review_types_survival"><i class="fa fa-check"></i><b>51.1.2</b> Types of Survival Analysis</a></li>
<li class="chapter" data-level="51.1.3" data-path="survival.html"><a href="survival.html#conceptualvideo_survival"><i class="fa fa-check"></i><b>51.1.3</b> Conceptual Video</a></li>
</ul></li>
<li class="chapter" data-level="51.2" data-path="survival.html"><a href="survival.html#tutorial_survival"><i class="fa fa-check"></i><b>51.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="51.2.1" data-path="survival.html"><a href="survival.html#videotutorial_survival"><i class="fa fa-check"></i><b>51.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="51.2.2" data-path="survival.html"><a href="survival.html#functions_survival"><i class="fa fa-check"></i><b>51.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="51.2.3" data-path="survival.html"><a href="survival.html#initsteps_survival"><i class="fa fa-check"></i><b>51.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="51.2.4" data-path="survival.html"><a href="survival.html#createcensoring_survival"><i class="fa fa-check"></i><b>51.2.4</b> Create a Censoring Variable</a></li>
<li class="chapter" data-level="51.2.5" data-path="survival.html"><a href="survival.html#inspectlosdistribution_survival"><i class="fa fa-check"></i><b>51.2.5</b> Inspect Distribution of Length of Service</a></li>
<li class="chapter" data-level="51.2.6" data-path="survival.html"><a href="survival.html#conductkmanalysis_lifetable_survival"><i class="fa fa-check"></i><b>51.2.6</b> Conduct Kaplan-Meier Analysis &amp; Create Life Table</a></li>
<li class="chapter" data-level="51.2.7" data-path="survival.html"><a href="survival.html#estimatecox_survival"><i class="fa fa-check"></i><b>51.2.7</b> Estimate Cox Proportional Hazards Model</a></li>
<li class="chapter" data-level="51.2.8" data-path="survival.html"><a href="survival.html#summary_survival"><i class="fa fa-check"></i><b>51.2.8</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>X Employee Performance Management</b></span></li>
<li class="chapter" data-level="52" data-path="performancemanagement.html"><a href="performancemanagement.html"><i class="fa fa-check"></i><b>52</b> Introduction to Employee Performance Management</a></li>
<li class="chapter" data-level="53" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html"><i class="fa fa-check"></i><b>53</b> Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations</a>
<ul>
<li class="chapter" data-level="53.1" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#conceptualoverview_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="53.1.1" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#review_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.1.1</b> Review of Concurrent &amp; Discriminant Validity</a></li>
<li class="chapter" data-level="53.1.2" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#review_pearson_pointbiserial_correlation"><i class="fa fa-check"></i><b>53.1.2</b> Review of Pearson Product-Moment &amp; Point-Biserial Correlation</a></li>
<li class="chapter" data-level="53.1.3" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#review_bivariatescatterplot"><i class="fa fa-check"></i><b>53.1.3</b> Review of Bivariate Scatter Plot</a></li>
</ul></li>
<li class="chapter" data-level="53.2" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#tutorial_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="53.2.1" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#videotutorial_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="53.2.2" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#functions_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="53.2.3" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#initsteps_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="53.2.4" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#bivariatescatterplot_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.4</b> Visualize Association Using a Bivariate Scatter Plot</a></li>
<li class="chapter" data-level="53.2.5" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#estimate_correlation_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.5</b> Estimate Correlations</a></li>
<li class="chapter" data-level="53.2.6" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#correlationmatrix_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.6</b> Create Correlation Matrix</a></li>
<li class="chapter" data-level="53.2.7" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#summary_convergentdiscriminantvalidity"><i class="fa fa-check"></i><b>53.2.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="53.3" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#convergentdiscriminantvalidity_supplement"><i class="fa fa-check"></i><b>53.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="53.3.1" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#convergentdiscriminantvalidity_supplement_functions"><i class="fa fa-check"></i><b>53.3.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="53.3.2" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#convergentdiscriminantvalidity_initsteps_supplement"><i class="fa fa-check"></i><b>53.3.2</b> Initial Steps</a></li>
<li class="chapter" data-level="53.3.3" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#shapiro.test_function"><i class="fa fa-check"></i><b>53.3.3</b> <code>shapiro.test</code> Function from Base R</a></li>
<li class="chapter" data-level="53.3.4" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#apatable_correlationmatrix"><i class="fa fa-check"></i><b>53.3.4</b> APA-Style Results Table</a></li>
<li class="chapter" data-level="53.3.5" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#cor.plot_psych_function"><i class="fa fa-check"></i><b>53.3.5</b> <code>cor.plot</code> Function from <code>psych</code> package</a></li>
<li class="chapter" data-level="53.3.6" data-path="convergentdiscriminantvalidity.html"><a href="convergentdiscriminantvalidity.html#corrgram_corrgram_function"><i class="fa fa-check"></i><b>53.3.6</b> <code>corrgram</code> Function from <code>corrgram</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="54" data-path="polynomialregression.html"><a href="polynomialregression.html"><i class="fa fa-check"></i><b>54</b> Investigating Nonlinear Associations Using Polynomial Regression</a>
<ul>
<li class="chapter" data-level="54.1" data-path="polynomialregression.html"><a href="polynomialregression.html#conceptualoverview_polynomialregression"><i class="fa fa-check"></i><b>54.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="54.1.1" data-path="polynomialregression.html"><a href="polynomialregression.html#statisticalassumptions_polynomialregression"><i class="fa fa-check"></i><b>54.1.1</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="54.1.2" data-path="polynomialregression.html"><a href="polynomialregression.html#statisticalsignficance_polynomialregression"><i class="fa fa-check"></i><b>54.1.2</b> Statistical Significance</a></li>
</ul></li>
<li class="chapter" data-level="54.2" data-path="polynomialregression.html"><a href="polynomialregression.html#tutorial_polynomialregression"><i class="fa fa-check"></i><b>54.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="54.2.1" data-path="polynomialregression.html"><a href="polynomialregression.html#functions_polynomialregression"><i class="fa fa-check"></i><b>54.2.1</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="54.2.2" data-path="polynomialregression.html"><a href="polynomialregression.html#initsteps_polynomialregression"><i class="fa fa-check"></i><b>54.2.2</b> Initial Steps</a></li>
<li class="chapter" data-level="54.2.3" data-path="polynomialregression.html"><a href="polynomialregression.html#bivariatescatterplot_polynomialregression"><i class="fa fa-check"></i><b>54.2.3</b> Visualize Association Using a Bivariate Scatter Plot</a></li>
<li class="chapter" data-level="54.2.4" data-path="polynomialregression.html"><a href="polynomialregression.html#estimate_polynomialregression"><i class="fa fa-check"></i><b>54.2.4</b> Estimate Polynomial Regression Model</a></li>
<li class="chapter" data-level="54.2.5" data-path="polynomialregression.html"><a href="polynomialregression.html#summary_polynomialregression"><i class="fa fa-check"></i><b>54.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="55" data-path="lassoregression.html"><a href="lassoregression.html"><i class="fa fa-check"></i><b>55</b> Supervised Statistical Learning Using Lasso Regression</a>
<ul>
<li class="chapter" data-level="55.1" data-path="lassoregression.html"><a href="lassoregression.html#conceptualoverview_lassoregression"><i class="fa fa-check"></i><b>55.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="55.1.1" data-path="lassoregression.html"><a href="lassoregression.html#shrinkage_lassoregression"><i class="fa fa-check"></i><b>55.1.1</b> Shrinkage</a></li>
<li class="chapter" data-level="55.1.2" data-path="lassoregression.html"><a href="lassoregression.html#regularization_lassoregression"><i class="fa fa-check"></i><b>55.1.2</b> Regularization</a></li>
<li class="chapter" data-level="55.1.3" data-path="lassoregression.html"><a href="lassoregression.html#tuning_lassoregression"><i class="fa fa-check"></i><b>55.1.3</b> Tuning</a></li>
<li class="chapter" data-level="55.1.4" data-path="lassoregression.html"><a href="lassoregression.html#modeltype_selection_lassoregression"><i class="fa fa-check"></i><b>55.1.4</b> Model Type Selection</a></li>
<li class="chapter" data-level="55.1.5" data-path="lassoregression.html"><a href="lassoregression.html#crossvalidation_lassoregression"><i class="fa fa-check"></i><b>55.1.5</b> Cross-Validation</a></li>
<li class="chapter" data-level="55.1.6" data-path="lassoregression.html"><a href="lassoregression.html#predictiveanalytics_lassoregression"><i class="fa fa-check"></i><b>55.1.6</b> Predictive Analytics</a></li>
<li class="chapter" data-level="55.1.7" data-path="lassoregression.html"><a href="lassoregression.html#conceptualvideo_lassoregression"><i class="fa fa-check"></i><b>55.1.7</b> Conceptual Video</a></li>
</ul></li>
<li class="chapter" data-level="55.2" data-path="lassoregression.html"><a href="lassoregression.html#tutorial_lassoregression"><i class="fa fa-check"></i><b>55.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="55.2.1" data-path="lassoregression.html"><a href="lassoregression.html#videotutorial_lassoregression"><i class="fa fa-check"></i><b>55.2.1</b> Video Tutorials</a></li>
<li class="chapter" data-level="55.2.2" data-path="lassoregression.html"><a href="lassoregression.html#functions_lassoregression"><i class="fa fa-check"></i><b>55.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="55.2.3" data-path="lassoregression.html"><a href="lassoregression.html#initsteps_lassoregression"><i class="fa fa-check"></i><b>55.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="55.2.4" data-path="lassoregression.html"><a href="lassoregression.html#processoverview_lasso"><i class="fa fa-check"></i><b>55.2.4</b> Process Overview</a></li>
<li class="chapter" data-level="55.2.5" data-path="lassoregression.html"><a href="lassoregression.html#partitiondata_lasso"><i class="fa fa-check"></i><b>55.2.5</b> Partition the Data</a></li>
<li class="chapter" data-level="55.2.6" data-path="lassoregression.html"><a href="lassoregression.html#specify-k-fold-cross-validation"><i class="fa fa-check"></i><b>55.2.6</b> Specify <em>k</em>-Fold Cross-Validation</a></li>
<li class="chapter" data-level="55.2.7" data-path="lassoregression.html"><a href="lassoregression.html#trainlasso_lasso"><i class="fa fa-check"></i><b>55.2.7</b> Specify and Train Lasso Regression Model</a></li>
<li class="chapter" data-level="55.2.8" data-path="lassoregression.html"><a href="lassoregression.html#compare_ols_lasso"><i class="fa fa-check"></i><b>55.2.8</b> Optional: Compare to Lasso Model to OLS Multiple Linear Regression Model</a></li>
<li class="chapter" data-level="55.2.9" data-path="lassoregression.html"><a href="lassoregression.html#summary_lasso"><i class="fa fa-check"></i><b>55.2.9</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="56" data-path="pathanalysis.html"><a href="pathanalysis.html"><i class="fa fa-check"></i><b>56</b> Investigating Processes Using Path Analysis</a>
<ul>
<li class="chapter" data-level="56.1" data-path="pathanalysis.html"><a href="pathanalysis.html#conceptualoverview_pathanalysis"><i class="fa fa-check"></i><b>56.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="56.1.1" data-path="pathanalysis.html"><a href="pathanalysis.html#pathdiagram_pathanalysis"><i class="fa fa-check"></i><b>56.1.1</b> Path Diagram</a></li>
<li class="chapter" data-level="56.1.2" data-path="pathanalysis.html"><a href="pathanalysis.html#modelidentification_pathanalysis"><i class="fa fa-check"></i><b>56.1.2</b> Model Identification</a></li>
<li class="chapter" data-level="56.1.3" data-path="pathanalysis.html"><a href="pathanalysis.html#modelfit_pathanalysis"><i class="fa fa-check"></i><b>56.1.3</b> Model Fit</a></li>
<li class="chapter" data-level="56.1.4" data-path="pathanalysis.html"><a href="pathanalysis.html#parameterestimatese_pathanalysis"><i class="fa fa-check"></i><b>56.1.4</b> Parameter Estimates</a></li>
<li class="chapter" data-level="56.1.5" data-path="pathanalysis.html"><a href="pathanalysis.html#statisticalassumptions_pathanalysis"><i class="fa fa-check"></i><b>56.1.5</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="56.1.6" data-path="pathanalysis.html"><a href="pathanalysis.html#conceptualvideo_pathanalysis"><i class="fa fa-check"></i><b>56.1.6</b> Conceptual Video</a></li>
</ul></li>
<li class="chapter" data-level="56.2" data-path="pathanalysis.html"><a href="pathanalysis.html#tutorial_pathanalysis"><i class="fa fa-check"></i><b>56.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="56.2.1" data-path="pathanalysis.html"><a href="pathanalysis.html#videotutorial_pathanalysis"><i class="fa fa-check"></i><b>56.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="56.2.2" data-path="pathanalysis.html"><a href="pathanalysis.html#functions_pathanalysis"><i class="fa fa-check"></i><b>56.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="56.2.3" data-path="pathanalysis.html"><a href="pathanalysis.html#initsteps_pathanalysis"><i class="fa fa-check"></i><b>56.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="56.2.4" data-path="pathanalysis.html"><a href="pathanalysis.html#specifymodel_pathanalysis"><i class="fa fa-check"></i><b>56.2.4</b> Specify &amp; Estimate Path Analysis Models</a></li>
<li class="chapter" data-level="56.2.5" data-path="pathanalysis.html"><a href="pathanalysis.html#additionalinfo_pathanalysis"><i class="fa fa-check"></i><b>56.2.5</b> Additional Information on Model Specification Notation</a></li>
<li class="chapter" data-level="56.2.6" data-path="pathanalysis.html"><a href="pathanalysis.html#summary_pathanalysis"><i class="fa fa-check"></i><b>56.2.6</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="57" data-path="mediationanalysis.html"><a href="mediationanalysis.html"><i class="fa fa-check"></i><b>57</b> Estimating a Mediation Model Using Path Analysis</a>
<ul>
<li class="chapter" data-level="57.1" data-path="mediationanalysis.html"><a href="mediationanalysis.html#conceptualoverview_mediationanalysis"><i class="fa fa-check"></i><b>57.1</b> Conceptual Overview</a>
<ul>
<li class="chapter" data-level="57.1.1" data-path="mediationanalysis.html"><a href="mediationanalysis.html#indirecteffect_mediationanalysis"><i class="fa fa-check"></i><b>57.1.1</b> Estimation of Indirect Effect</a></li>
<li class="chapter" data-level="57.1.2" data-path="mediationanalysis.html"><a href="mediationanalysis.html#modelidentification_mediationanalysis"><i class="fa fa-check"></i><b>57.1.2</b> Model Identification</a></li>
<li class="chapter" data-level="57.1.3" data-path="mediationanalysis.html"><a href="mediationanalysis.html#modelfit_mediationanalysis"><i class="fa fa-check"></i><b>57.1.3</b> Model Fit</a></li>
<li class="chapter" data-level="57.1.4" data-path="mediationanalysis.html"><a href="mediationanalysis.html#parameterestimatese_mediationanalysis"><i class="fa fa-check"></i><b>57.1.4</b> Parameter Estimates</a></li>
<li class="chapter" data-level="57.1.5" data-path="mediationanalysis.html"><a href="mediationanalysis.html#statisticalassumptions_mediationanalysis"><i class="fa fa-check"></i><b>57.1.5</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="57.1.6" data-path="mediationanalysis.html"><a href="mediationanalysis.html#conceptualvideo_mediationanalysis"><i class="fa fa-check"></i><b>57.1.6</b> Conceptual Video</a></li>
</ul></li>
<li class="chapter" data-level="57.2" data-path="mediationanalysis.html"><a href="mediationanalysis.html#tutorial_mediationanalysis"><i class="fa fa-check"></i><b>57.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="57.2.1" data-path="mediationanalysis.html"><a href="mediationanalysis.html#videotutorial_mediationanalysis"><i class="fa fa-check"></i><b>57.2.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="57.2.2" data-path="mediationanalysis.html"><a href="mediationanalysis.html#functions_mediationanalysis"><i class="fa fa-check"></i><b>57.2.2</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="57.2.3" data-path="mediationanalysis.html"><a href="mediationanalysis.html#initsteps_mediationanalysis"><i class="fa fa-check"></i><b>57.2.3</b> Initial Steps</a></li>
<li class="chapter" data-level="57.2.4" data-path="mediationanalysis.html"><a href="mediationanalysis.html#specifymodel_mediationanalysis"><i class="fa fa-check"></i><b>57.2.4</b> Specify &amp; Estimate a Mediation Analysis Model</a></li>
<li class="chapter" data-level="57.2.5" data-path="mediationanalysis.html"><a href="mediationanalysis.html#summary_mediationanalysis"><i class="fa fa-check"></i><b>57.2.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>XI Odds &amp; Ends</b></span></li>
<li class="chapter" data-level="58" data-path="create_portfolio.html"><a href="create_portfolio.html"><i class="fa fa-check"></i><b>58</b> Creating a Data Analytics Portfolio</a></li>
<li class="chapter" data-level="59" data-path="literature_search_review.html"><a href="literature_search_review.html"><i class="fa fa-check"></i><b>59</b> Conducting a Literature Search &amp; Review</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R for HR:<br />
<em>An Introduction to Human Resource Analytics Using R</em><br />
BOOK UNDER CONSTRUCTION</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic" class="section level1" number="49">
<h1><span class="header-section-number">Chapter 49</span> Identifying Predictors of Turnover Using Logistic Regression</h1>
<p>In this chapter, we will learn how to estimate a (binary) logistic regression model in order to identify potential predictors of employee voluntary turnover, when voluntary turnover is operationalized as a dichotomous (i.e., binary) variable (e.g., stay vs. quit).</p>
<div id="conceptualoverview_logistic" class="section level2" number="49.1">
<h2><span class="header-section-number">49.1</span> Conceptual Overview</h2>
<p><strong>Logistic regression (logit model)</strong> is part of the family of <em>generalized linear models (GLMs)</em>. Assuming statistical assumptions have been satisfied, a binary logistic regression model is appropriate when the outcome variable of interest is dichotomous (i.e., binary) and when the predictor variable(s) of interest is/are continuous (interval, ratio) or categorical (nominal, ordinal). Unlike ordinary least squares (OLS) estimation, which was covered previously in the context of <a href="predictingcriterionscores.html#predictingcriterionscores">simple linear regression</a> and <a href="incrementalvalidity.html#incrementalvalidity">multiple linear regression</a>, logistic regression coefficients are typically estimated using <em>maximum likelihood (ML)</em>. There are also extensions of the logistic regression like multinomial and ordinal logistic regression, where these extensions are appropriate when the categorical outcome variable is nominal or ordinal with three or more levels/categories.</p>
<p>Logistic regression can be used to determine the odds that a dichotomous event occurs (e.g., stay vs. quit) given higher or lower values/levels on one or more predictor variables, where <strong>odds</strong> refers to the probability of an event occurring (<em>p</em>) relative to the probability of the event <em>not</em> occurring (1 - <em>p</em>). As such, an odds value of 1 can be interpreted as 1 to 1 odds; or in other words, the probability of the event occurring is equal to the probability of the event not occurring, which would be akin to flipping a fair coin. For example, if we find that the probability of quitting is .75 (<em>p</em> = .75; i.e., event occurring), then by extension, the probability of not quitting is .25 (1 - <em>p</em> = .25; i.e., event not occurring). Given these probabilities, the odds of quitting are 3 (.75 / .25 = 3), and the odds of not quitting are 1/3 or .33, which is calculated as: .25 / (1 - .25). A logit transformation of the odds of quitting and of the odds of not quitting will be symmetrical, where a <strong>logit transformation</strong> refers to taking the natural log (<span class="math inline">\(\ln\)</span>) of both values (i.e., logarithmic transformation). For example, a logit transformation of 3 and 1/3 yields 1.10 and -1.10, which are symmetrical: <span class="math inline">\(\ln(3) = 1.10\)</span> and <span class="math inline">\(\ln(1/3) = -1.10\)</span>.</p>
<div id="review_logistic" class="section level3" number="49.1.1">
<h3><span class="header-section-number">49.1.1</span> Review of Logistic Regression</h3>
<p>Just as there is a distinction between simple and multiple <em>linear</em> regression models, we can also draw a distinction between simple and multiple <em>logistic</em> regression models. When there is a single predictor variable and a dichotomous outcome variable, we can apply what is referred to as a <em>simple logistic regression</em> model. More specifically, a simple logistic regression refers to the bivariate linear association between a predictor variable and a dichotomous outcome variable that has undergone a logit transformation, as shown in the equation below.</p>
<p><span class="math inline">\(logit(p) = \log(odds) = \ln(\frac{p}{1-p}) = b_0 + b_1(X_1)\)</span></p>
<p>where <span class="math inline">\(logit(p)\)</span> represents the logit transformation of the outcome, <span class="math inline">\(\log(odds)\)</span> represents the log(arithmic) odds, <span class="math inline">\(\ln\)</span> represents the natural log, <span class="math inline">\(p\)</span> represents the probability of an event occurring, <span class="math inline">\(b_0\)</span> represents the intercept value, and <span class="math inline">\(b_1\)</span> represents the regression coefficient (i.e., slope, weight) of the association between the predictor variable <span class="math inline">\(X_1\)</span> and the logit transformation of the outcome variable.</p>
<p>And when we have two or more predictor variables and a single dichotomous outcome variable, we estimate what is called a <em>multiple logistic regression</em> model, where an example of a multiple logistic regression model follows.</p>
<p><span class="math inline">\(logit(p) = \log(odds) = \ln(\frac{p}{1-p}) = b_0 + b_1(X_1) + b_2(X_2)\)</span></p>
<p>where <span class="math inline">\(b_2\)</span> represents the regression coefficient (i.e., slope, weight) of the association between the <em>second</em> predictor variable <span class="math inline">\(X_2\)</span> and the logit transformation of the outcome variable.</p>
<p><strong>Logistic Function:</strong> Logistic regression is predicated on the <em>logistic function</em>. The scatter plot figure shown below illustrates the logistic function when there is a continuous (interval, ratio) predictor variable called <span class="math inline">\(X\)</span> and a dichotomous outcome variable called <span class="math inline">\(Y\)</span>. Because a linear function would not not closely approximate the association between these two variables, we instead use a logistic function, which is a <strong>sigmoidal</strong> (or <strong>sigmoid</strong>) function and takes the visual form of an <em>S</em>-curve.</p>
<div class="figure">
<img src="Sigmoidal.png" alt="" />
<p class="caption">The sigmoidal (sigmoid) function is shown in red and represents the probability of an event occurring at each level/value of the predictor variable.</p>
</div>
<p>Just like simple and multiple linear regression models, regression coefficients are estimated in simple logistic regression models, but these coefficients are not perhaps as easily interpretable in their original form because the outcome variable undergoes a logarithmic transformation, as noted above. The regression coefficients in a logistic regression model represent the change in log odds (i.e., logit transformation) for every one unit change in the predictor variable.</p>
<p><strong>Odds Ratio:</strong> To make a statistically significant regression coefficient (<span class="math inline">\(b_i\)</span>) easier to interpret, we often convert the coefficient to an <strong>odds ratio</strong>. To do so, we exponentiate the coefficient using <em>Euler’s number</em> (<span class="math inline">\(e\)</span>). Euler’s number (<span class="math inline">\(e\)</span>) is an irrational number and mathematical constant that is approximately equal to 2.71828.</p>
<p><span class="math inline">\(e^{b_i}\)</span></p>
<p>An odds ratio that is <em>less than 1.00</em> indicates a <em>negative</em> association between the predictor variable and outcome variable, and an odds ratio that is <em>greater than 1.00</em> indicates a <em>positive</em> association. An odds ratio <em>equal to 1.00</em> indicates that there is <em>no association</em> between the variables.</p>
<p><em>Note: In the case of a multiple logistic regression model where we have two or more predictor variables, we would describe an odds ratios more accurately as an <strong>adjusted odds ratio</strong> because we are statistically controlling for the other predictor variable(s) in the model.</em></p>
<p><strong>Example of Interpreting a Negative Association:</strong> As an example, let’s imagine in a simple logistic regression model we find that the coefficient for the association between job satisfaction (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is -.42 – that is, we find a <em>negative</em> association. If we interpret the coefficient in its original form, we might say something like: “For every one unit increase in job satisfaction, there is as .42 unit reduction in log odds of quitting.” Such language will not typically be well-received by organizational stakeholders; however, if we convert the coefficient to an odds ratio by exponentiating it, we might find it easier to explain the finding to ourselves and others.</p>
<p><span class="math inline">\(e^{-.42} = .66\)</span></p>
<p>In this example, the odds ratio of .66 is less than 1.00, which reflects back to us that the association is indeed negative, which we already knew from the original log odds coefficient value of -.42. We can interpret this odds ratio as follows: “For every unit increase in job satisfaction, the odds of quitting is reduced by 34% (1 - .66 = .34).” Alternatively, we can interpret the odds ratio from a different vantage point if we compute its reciprocal, which is 1.52 (1 / .66 = 1.52); in doing so, we can interpret the finding as: “For every unit increase in job satisfaction, the odds of quitting is reduced by 1 in 1.52.” Or, we could frame the finding in terms of <em>not</em> quitting: “For every unit increase in job satisfaction, the odds of not quitting is 1.52 times greater – or has a 1.52 times higher likelihood.”</p>
<p><strong>Example of Interpreting a Positive Association:</strong> Now that we’ve worked through an example of a negative association, let’s practice interpreting a <em>positive</em> association. Let’s imagine a different simple logistic regression model in which we find that the coefficient for the association between <em>turnover intentions</em> (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is .89. When interpreting the coefficient in its original form, we might say: “For every one unit increase in turnover intentions, there is as .89 unit increase in log odds of quitting.” Just as we did before, we can exponentiate the coefficient to find the odds ratio.</p>
<p><span class="math inline">\(e^{.99} = 2.44\)</span></p>
<p>Because the odds ratio of 2.44 is greater than 1.00, it confirms to us that the association between turnover intentions and voluntary turnover is positive. We can interpret this finding as: “For every unit increase in turnover intentions, the odds of quitting is 2.44 times greater – or has a 2.44 times higher likelihood.”</p>
<p>As another example, let’s imagine that we observe an odds ratio of 1.29 with respect to negative affectivity in relation to voluntary turnover. We could interpret this finding as: “For every unit increase in negative affectivity, the odds of quitting is 1.29 times greater.” Or, because 1.29 times greater is equivalent to saying that there was a 29% increase, we could say: “For every unit increase in negative affectivity, the odds of quitting increases by 29%.”</p>
<p><strong>Predicted Probabilities:</strong> Our logistic regression coefficients can also be used to predict the probability of the event occurring for different value(s) of the predictor variable(s). The equations that follow can help us to understand algebraically how we can determine the probability (<span class="math inline">\(p\)</span>) of an event occurring based on the coefficients we might estimate for simple logistic regression model.</p>
<p><span class="math inline">\(\ln(\frac{p}{1-p}) = b_0 + b_1(X_1)\)</span></p>
<p><span class="math inline">\(\frac{p}{1-p} = e^{b_0 + b_1(X_1)}\)</span></p>
<p><span class="math inline">\(p = \frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}}\)</span></p>
<p>where <span class="math inline">\(e\)</span> represents Euler’s number.</p>
<p>As an example, let’s first imagine that our estimated intercept (<span class="math inline">\(b_0\)</span>) is .32 and our estimated regression coefficient associated with the predictor variable (<span class="math inline">\(b_1\)</span>) is -.09. Next, let’s imagine that someone has a score of 3 on the predictor variable (<span class="math inline">\(X_1\)</span>). If we plug those values into the equation, we will find that the probability of a person with a score of 3 on the predictor variable is .51.</p>
<p><span class="math inline">\(p = \frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}} = \frac{e^{.32 + (-.09 \times 3)}}{1+e^{.32 + (-.09 \times 3)}} = .51\)</span></p>
<p>If we set a probability threshold of .50 for experiencing the event, then we would classify anyone who scores a 3 on the predictor variable as having been predicted to experience the event in question. If however, the computed probability had been less than the probability threshold of .50, then we would have classified any associated with cases as having been predicted to <em>not</em> experience the event in question.</p>
<div id="statisticalassumptions_logistic" class="section level4" number="49.1.1.1">
<h4><span class="header-section-number">49.1.1.1</span> Statistical Assumptions</h4>
<p>The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple or multiple logistic regression model include:</p>
<ul>
<li>Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual;</li>
<li>Data are free of bivariate/multivariate outliers;</li>
<li>The association between any continuous predictor variable(s) and the logit transformation of the outcome variable is linear;</li>
<li>The outcome variable is dichotomous;</li>
<li>For a multiple logistic regression model, there is no (multi)collinearity between predictor variables.</li>
</ul>
<p>The fifth statistical assumption refers to the concept of <strong>collinearity</strong> (<strong>multicollinearity</strong>). This can be a tricky concept to understand, so let’s take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple logistic regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights – and even the signs – of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The <strong>tolerance</strong> statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that <em>R</em><sup>2</sup> value from 1 (i.e., 1 - <em>R</em><sup>2</sup>). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the <strong>variance inflation factor</strong> (<strong>VIF</strong>) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated.</p>
<p>Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., multilevel logit model), which is beyond the scope of this tutorial. An <em>intraclass correlation (ICC)</em> can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the <em>p</em>-values and inferences of statistical significance.</p>
</div>
<div id="statisticalsignficance_logistic" class="section level4" number="49.1.1.2">
<h4><span class="header-section-number">49.1.1.2</span> Statistical Signficance</h4>
<p>Using null hypothesis significance testing (NHST), we interpret a <em>p</em>-value that is <em>less than .05</em> (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficient’s <em>p</em>-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficient’s <em>p</em>-value is <em>equal to or greater than .05</em>, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficient’s <em>p</em>-value is equal to or greater than .05, we conclude that the regression coefficient does <em>not</em> differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. Keep in mind that in the context of a multiple logistic regression model, the association between each predictor variable and the logit transformation of the outcome variable must be interpreted with statistical control in mind, as we are effectively testing whether each predictor variable shows evidence of incremental validity in the presence of any other predictor variables.</p>
<p>When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline <em>p</em>-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, <em>p</em> = .049 would be considered statistically significant, and <em>p</em> = .050 would be considered statistically nonsignificant.</p>
<p>Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (<em>b</em>) is a <em>point estimate</em> of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do <em>not</em> include zero, then this tells us the same thing as a <em>p</em>-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error.</p>
<p><em>Note:</em> In a logistic regression model, we may also construct confidence intervals around the odds ratios.</p>
</div>
<div id="practicalsignficance_logistic" class="section level4" number="49.1.1.3">
<h4><span class="header-section-number">49.1.1.3</span> Practical Significance</h4>
<p>In its original form, a logistic regression coefficient is <em>not</em> an effect size; that is, it doesn’t provide an indication of practical significance. The <strong>odds ratio</strong>, however, can be conceptualized as an effect size. With that being said, there are some caveats. First, an odds ratio that is computed directly from an <em>unstandardized</em> logistic regression coefficient needs to be interpreted based on the raw scaling of the predictor variable, as the interpretation of the odds ratio has to do with the change in odds for unit change in the predictor variable. If we wish to compare odds ratios within or between models, we need to take this scaling issue into account. Second, in the case of a multiple logistic regression model, statistical control is at play, which means that the odds ratios are more accurately described as <strong>adjusted odds ratios</strong>.</p>
<p>There are different thresholds we can apply when interpreting the magnitude of an odds ratio, and below I provide some thresholds that we will use in this tutorial. With that being said, thresholds for qualitatively interpreting effect sizes should be context dependent, and there are other thresholds we might apply.</p>
<table>
<thead>
<tr class="header">
<th align="center">Odds Ratio &gt; 1</th>
<th align="center">Odds Ratio &lt; 1</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.2</td>
<td align="center">.8</td>
<td align="center">Small</td>
</tr>
<tr class="even">
<td align="center">2.5</td>
<td align="center">.4</td>
<td align="center">Medium</td>
</tr>
<tr class="odd">
<td align="center">4.3</td>
<td align="center">.2</td>
<td align="center">Large</td>
</tr>
</tbody>
</table>
<p>At the model level, we can’t compute a true <em>R</em><sup>2</sup> when estimating a logistic regression model. We can, however, compute a pseudo-<em>R</em><sup>2</sup>, There are different formulas available for computing pseudo-<em>R</em><sup>2</sup> (e.g., Cox &amp; Snell, McFadden), and in this chapter we’ll focus on the following Nagelkerke <span class="citation">(<a href="#ref-nagelkerke1991note" role="doc-biblioref">1991</a>)</span> formula:</p>
<p><span class="math inline">\(pseudo-R^2 = \frac{1 - (\frac{L(M_{null})}{L(M_{full})})^{2/N}}{1-L(M_{null})^{2/N}}\)</span></p>
<p>where <span class="math inline">\(L(M_{null})\)</span> is the likelihood of the outcome variable given a null, intercept-only model, <span class="math inline">\(L(M_{full})\)</span> is the likelihood of the outcome variable given the predictor variable(s) in the model, and <span class="math inline">\(N\)</span> is the sample size.</p>
<p>It’s important to note, though, that as the name implies, a pseudo-<em>R</em><sup>2</sup> is not the same thing as a true <em>R</em><sup>2</sup>. Thus, while we need to be cautious in our interpretations.</p>
<p>In addition to pseudo-<em>R</em><sup>2</sup>, we can also describe the classification accuracy of the model by using a <strong>confusion matrix</strong> (or <strong>classification table</strong>). A confusion matrix presents the percentage of correctly predicted values on the outcome variable; if a probability for a case based on the model is equal to or greater than .50, then it would be classified as a probability of 1, and all else would be classified as 0. For example, using a confusion matrix, we can make statements like: “The model correctly classified 55.9% of the employees as either stay or quit.”</p>
<p>To indicate how well your model fit the data and performed, I recommend reporting either pseudo-<em>R</em><sup>2</sup>, model classification accuracy, or both.</p>
<p><em>Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect.</em></p>
</div>
<div id="samplewriteup_logistic" class="section level4" number="49.1.1.4">
<h4><span class="header-section-number">49.1.1.4</span> Sample Write-Up</h4>
<p>A voluntary turnover study was conducted based on a sample 99 employees from the past year, some of whom quit the company and some of whom stayed. The focal outcome variable is turnover behavior (quit vs. stay), and because it is dichotomous, we used logistic regression. We were, specifically, interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that job satisfaction was not associated with turnover behavior to a statistically significant extent (<em>b</em> = -.233, <em>p</em> = .293, 95% CI[-.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (<em>b</em> = 1.195, <em>p</em> = .017, 95% CI[.216, 2.174]). For every one unit increase in negative affectivity, the odds of quitting were 3.304 times greater, when controlling for the other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (<em>b</em> = .897, <em>p</em> = .005, 95% CI[.276, 1.517]). For every one unit increase in turnover intentions, the odds of quitting were 2.451 times greater, when controlling for other predictor variables in the model. Both of these significant associations can be described as medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to correct classify 78.9% of employees from our sample using the estimated multiple logistic regression model. Finally, the estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover.</p>
</div>
</div>
</div>
<div id="tutorial_logistic" class="section level2" number="49.2">
<h2><span class="header-section-number">49.2</span> Tutorial</h2>
<p>This chapter’s tutorial demonstrates how to estimate simple and multiple logistic regression models using R.</p>
<div id="videotutorial_logistic" class="section level3" number="49.2.1">
<h3><span class="header-section-number">49.2.1</span> Video Tutorials</h3>
<p>As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below.</p>
<iframe src="https://youtube.com/embed/O7gRceyeyT8?rel=0" width="672" height="400px" data-external="1">
</iframe>
<p>Link to video tutorial: <a href="https://youtu.be/O7gRceyeyT8" class="uri">https://youtu.be/O7gRceyeyT8</a></p>
</div>
<div id="functions_logistic" class="section level3" number="49.2.2">
<h3><span class="header-section-number">49.2.2</span> Functions &amp; Packages Introduced</h3>
<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>Logit</code></td>
<td><code>lessR</code></td>
</tr>
<tr class="even">
<td><code>log</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>PseudoR2</code></td>
<td><code>DescTools</code></td>
</tr>
<tr class="even">
<td><code>exp</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>glm</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>merge</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>data.frame</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>mutate</code></td>
<td><code>dplyr</code></td>
</tr>
<tr class="odd">
<td><code>ifelse</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>c</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>predict</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>detach</code></td>
<td>base R</td>
</tr>
</tbody>
</table>
</div>
<div id="initsteps_logistic" class="section level3" number="49.2.3">
<h3><span class="header-section-number">49.2.3</span> Initial Steps</h3>
<p>If you haven’t already, save the file called <strong>“Turnover.csv”</strong> into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., <code>"H:/RWorkshop"</code>). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: <a href="https://github.com/davidcaughlin/R-Tutorial-Data-Files" class="uri">https://github.com/davidcaughlin/R-Tutorial-Data-Files</a>; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP,” which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book.</p>
<p>Next, using the <code>setwd</code> function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to <em>Session &gt; Set Working Directory &gt; Choose Directory…</em>. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to <a href="setwd.html#setwd">Setting a Working Directory</a> and <a href="gettingstarted.html#createRscript">Creating &amp; Saving an R Script</a>.</p>
<div class="sourceCode" id="cb1798"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1798-1"><a href="logistic.html#cb1798-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set your working directory</span></span>
<span id="cb1798-2"><a href="logistic.html#cb1798-2" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(<span class="st">&quot;H:/RWorkshop&quot;</span>)</span></code></pre></div>
<p>Next, read in the .csv data file called <strong>“Turnover.csv”</strong> using your choice of read function. In this example, I use the <code>read_csv</code> function from the <code>readr</code> package <span class="citation">(<a href="#ref-R-readr" role="doc-biblioref">Wickham, Hester, and Bryan 2021</a>)</span>. If you choose to use the <code>read_csv</code> function, be sure that you have installed and accessed the <code>readr</code> package using the <code>install.packages</code> and <code>library</code> functions. <em>Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months.</em> For refreshers on installing packages and reading data into R, please refer to <a href="gentleintro.html#packages">Packages</a> and <a href="read.html#read">Reading Data into R</a>.</p>
<div class="sourceCode" id="cb1799"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1799-1"><a href="logistic.html#cb1799-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install readr package if you haven&#39;t already</span></span>
<span id="cb1799-2"><a href="logistic.html#cb1799-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [Note: You don&#39;t need to install a package every </span></span>
<span id="cb1799-3"><a href="logistic.html#cb1799-3" aria-hidden="true" tabindex="-1"></a><span class="co"># time you wish to access it]</span></span>
<span id="cb1799-4"><a href="logistic.html#cb1799-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;readr&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1800"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1800-1"><a href="logistic.html#cb1800-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access readr package</span></span>
<span id="cb1800-2"><a href="logistic.html#cb1800-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb1800-3"><a href="logistic.html#cb1800-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1800-4"><a href="logistic.html#cb1800-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data and name data frame (tibble) object</span></span>
<span id="cb1800-5"><a href="logistic.html#cb1800-5" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;Turnover.csv&quot;</span>)</span></code></pre></div>
<pre><code>## Rows: 99 Columns: 6</code></pre>
<pre><code>## -- Column specification ------------------------------------------------------------------------------------------------------------------------
## Delimiter: &quot;,&quot;
## chr (1): ID
## dbl (5): Turnover, JS, OC, TI, NAff</code></pre>
<pre><code>## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb1804"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1804-1"><a href="logistic.html#cb1804-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the names of the variables in the data frame (tibble) objects</span></span>
<span id="cb1804-2"><a href="logistic.html#cb1804-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(td)</span></code></pre></div>
<pre><code>## [1] &quot;ID&quot;       &quot;Turnover&quot; &quot;JS&quot;       &quot;OC&quot;       &quot;TI&quot;       &quot;NAff&quot;</code></pre>
<div class="sourceCode" id="cb1806"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1806-1"><a href="logistic.html#cb1806-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View variable type for each variable in data frame</span></span>
<span id="cb1806-2"><a href="logistic.html#cb1806-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(td)</span></code></pre></div>
<pre><code>## spec_tbl_df [99 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ ID      : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ...
##  $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ...
##  $ JS      : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ...
##  $ OC      : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ...
##  $ TI      : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ...
##  $ NAff    : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ...
##  - attr(*, &quot;spec&quot;)=
##   .. cols(
##   ..   ID = col_character(),
##   ..   Turnover = col_double(),
##   ..   JS = col_double(),
##   ..   OC = col_double(),
##   ..   TI = col_double(),
##   ..   NAff = col_double()
##   .. )
##  - attr(*, &quot;problems&quot;)=&lt;externalptr&gt;</code></pre>
<div class="sourceCode" id="cb1808"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1808-1"><a href="logistic.html#cb1808-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View first 6 rows of data frame</span></span>
<span id="cb1808-2"><a href="logistic.html#cb1808-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   ID     Turnover    JS    OC    TI  NAff
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 EMP559        1  4.96  5.32  0.51  1.87
## 2 EMP561        1  1.72  1.47  4.08  2.48
## 3 EMP571        1  1.64  0.87  2.65  2.84
## 4 EMP589        1  3.01  2.15  4.17  2.43
## 5 EMP592        1  3.04  1.94  3.27  2.76
## 6 EMP601        1  3.81  3.81  3.01  3.67</code></pre>
<p>There are 5 variables and 99 cases (i.e., employees) in the <code>td</code> data frame: <code>ID</code>, <code>Turnover</code>, <code>JS</code>, <code>OC</code>, <code>TI</code>, and <code>NAff</code>. Per the output of the <code>str</code> (structure) function above, all of the variables are of type <em>numeric</em> (continuous: interval/ratio), except for the <code>ID</code> variable, which is of type <em>character</em>. <code>ID</code> is the unique employee identifier variable. Imagine that these data were collected as part of a turnover study within an organization to determine the drivers/predictors of turnover based on a sample of employees who stayed and leaved during the past year. The variables <code>JS</code>, <code>OC</code>, <code>TI</code>, and <code>NAff</code> were collected as part of an annual survey and were later joined with the <code>Turnover</code> variable. Survey respondents rated each survey item using a 7-point response scale, ranging from strongly disagree (0) to strongly agree (6). <code>JS</code> contains the average of each employee’s responses to 10 job satisfaction items. <code>OC</code> contains the average of each employee’s responses to 7 organizational commitment items. <code>TI</code> contains the average of each employee’s responses to 3 turnover intentions items, where higher scores indicate higher levels of turnover intentions. <code>NAff</code> contains the average of each employee’s responses to 10 negative affectivity items. <code>Turnover</code> is a variable that indicates whether these individuals left the organization during the prior year, with 1 = quit and 0 = stayed. <strong>Note: If the <code>Turnover</code> variable were to include the character values of <code>quit</code> and <code>stay</code> instead of 1 and 0, the functions covered in this tutorial would automatically convert the character values to 0 and 1 (behind the scenes), where 0 would be assigned to the character value that comes first alphabetically.</strong></p>
</div>
<div id="estimate_simple_logistic" class="section level3" number="49.2.4">
<h3><span class="header-section-number">49.2.4</span> Estimate Simple Logistic Regression Model</h3>
<p>We’ll begin by specifying a simple logistic regression model, which means the model will include just one predictor variable. Let’s begin by regressing <code>Turnover</code> on <code>JS</code> using the <code>Logit</code> function from the <code>lessR</code> package. If you haven’t already, install and access the <code>lessR</code> package.</p>
<div class="sourceCode" id="cb1810"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1810-1"><a href="logistic.html#cb1810-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1810-2"><a href="logistic.html#cb1810-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;lessR&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1811"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1811-1"><a href="logistic.html#cb1811-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1811-2"><a href="logistic.html#cb1811-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lessR)</span></code></pre></div>
<p>As the first argument in the <code>Logit</code> function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variable is typed to the right of the <code>~</code> symbol. As the second argument, type <code>data=</code> followed by the name of the data frame to which both of the variables belong (<code>td</code>). Let’s begin by specifying <code>JS</code> as the predictor variable.</p>
<div class="sourceCode" id="cb1812"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1812-1"><a href="logistic.html#cb1812-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model</span></span>
<span id="cb1812-2"><a href="logistic.html#cb1812-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    1.8554     0.6883    2.695    0.007      0.5063      3.2044 
##          JS   -0.4378     0.1958   -2.236    0.025     -0.8216     -0.0540 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      6.3939      1.6591     24.6415 
##          JS      0.6455      0.4397      0.9475 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 126.341 on 96 degrees of freedom
## 
## AIC: 130.3413 
## 
## Number of iterations to convergence: 4 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 98 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits   cooks
## 69 6.00        1 0.3162   0.6838   1.5688  0.3725 0.08496
## 7  1.38        0 0.7775  -0.7775  -1.7682 -0.2877 0.06241
## 73 5.48        1 0.3673   0.6327   1.4476  0.2949 0.04889
## 58 5.43        1 0.3724   0.6276   1.4363  0.2877 0.04618
## 12 1.72        0 0.7507  -0.7507  -1.6920 -0.2486 0.04353
## 31 1.77        0 0.7466  -0.7466  -1.6810 -0.2429 0.04117
## 13 1.96        0 0.7305  -0.7305  -1.6393 -0.2219 0.03314
## 1  4.96        1 0.4217   0.5783   1.3332  0.2239 0.02609
## 33 4.88        1 0.4302   0.5698   1.3162  0.2138 0.02353
## 84 4.66        1 0.4540   0.5460   1.2703  0.1875 0.01757
## 63 4.65        1 0.4551   0.5449   1.2682  0.1863 0.01733
## 61 2.52        0 0.6797  -0.6797  -1.5199 -0.1668 0.01693
## 97 5.59        0 0.3562  -0.3562  -0.9554 -0.2021 0.01693
## 70 5.48        0 0.3673  -0.3673  -0.9731 -0.1985 0.01648
## 74 2.56        0 0.6758  -0.6758  -1.5115 -0.1635 0.01615
## 75 2.57        0 0.6749  -0.6749  -1.5095 -0.1626 0.01596
## 67 2.65        0 0.6671  -0.6671  -1.4929 -0.1563 0.01454
## 80 5.04        0 0.4131  -0.4131  -1.0457 -0.1813 0.01431
## 77 4.46        1 0.4757   0.5243   1.2296  0.1656 0.01336
## 39 4.43        1 0.4790   0.5210   1.2235  0.1625 0.01282
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 69 6.00        1       0 0.3162  0.1215
## 97 5.59        0       0 0.3562  0.1120
## 70 5.48        0       0 0.3673  0.1090
## 73 5.48        1       0 0.3673  0.1090
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS Turnover predict fitted std.err
## 39 4.43        1       0 0.4790 0.07497
## 83 4.41        0       0 0.4812 0.07431
## 64 4.26        1       0 0.4976 0.06946
## 27 4.15        0       1 0.5097 0.06609
## 14 4.14        0       1 0.5107 0.06579
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS Turnover predict fitted std.err
## 66 1.19        1       1 0.7916 0.07790
## 48 1.05        1       1 0.8015 0.07904
## 88 0.67        1       1 0.8266 0.08096
## 24 0.23        1       1 0.8525 0.08116
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       39  39.8        8     31     20.5 
## Turnover   1       59  60.2       10     49     83.1 
## ---------------------------------------------------
##          Total     98                           58.2 
## 
## Accuracy: 58.16 
## Recall: 83.05 
## Precision: 61.25</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1015-1.png" width="672" /></p>
<p><strong>Note: In some instances, you might receive the error message shown below. You can ignore this message, as it just indicates that you have a poor predictor variable in the model that results in fitted/predicted values that are all the same. If you get this message, proceed forward with your interpretation of the output.</strong></p>
<p><span class="math inline">\(\color{red}{\text{Error:}}\)</span>
<span class="math inline">\(\color{red}{\text{All predicted values are 0.}}\)</span>
<span class="math inline">\(\color{red}{\text{Something is wrong here.}}\)</span></p>
<p>The output generates the model coefficient estimates, the odds ratios and their confidence intervals, model fit information (i.e., AIC), outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model (which we probably already knew), the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis after excluding cases with missing data (<em>N</em> = 98).</p>
<div id="teststatisticalassumptions_simple_logistic" class="section level4" number="49.2.4.1">
<h4><span class="header-section-number">49.2.4.1</span> Test Statistical Assumptions</h4>
<p>To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a>.</p>
<p><strong>Cases Are Randomly Sampled from the Population:</strong> As mentioned in the <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a> section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied.</p>
<p><strong>Outcome Variable Is Dichotomous:</strong> We already know that the outcome variable called <code>Turnover</code> is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption.</p>
<p><strong>Data Are Free of Bivariate Outliers:</strong> To determine whether the data are free of bivariate outliers, let’s take a look at the text output section called <em>Analysis of Residuals and Influence</em>. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cook’s distance (cooks). The case associated with row number <em>66</em> has the highest Cook’s distance value (.085), followed by the cases associated with row numbers <em>67</em> and <em>71</em>, which have Cook’s distance values of .062 and .049. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we may want to estimate our model once more after removing the cases associated with row numbers <em>66</em>, <em>66</em>, and <em>71</em> from our data frame; however, these Cook’s distance values don’t look too concerning or out of the ordinary, and thus I wouldn’t recommend removing the associated cases. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so.</p>
<p><strong>Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear:</strong> To test the assumption of linearity between a <em>continuous</em> predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. <strong>[Note: We do not perform the following test/approach for <em>categorical</em> predictor variables.]</strong> We will use an approach that is commonly referred to as the Box-Tidwell approach <span class="citation">(<a href="#ref-hosmerlemeshow2000" role="doc-biblioref">Hosmer and Lemeshow 2000</a>)</span>. To apply this approach, we need to add the interaction term between our predictor variable <code>JS</code> and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of <code>JS</code>. In our regression model formula, specify the dichotomous outcome variable <code>Turnover</code> to the left of the <code>~</code> operator. To the right of the <code>~</code> operator, type the name of the predictor variable <code>JS</code> followed by the <code>+</code> operator. After the <code>+</code> operator, type the name of the predictor variable <code>JS</code>, followed by the <code>:</code> operator and the <code>log</code> function from base R with the predictor variable <code>JS</code> as its sole parenthetical argument.</p>
<div class="sourceCode" id="cb1814"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1814-1"><a href="logistic.html#cb1814-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1814-2"><a href="logistic.html#cb1814-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable</span></span>
<span id="cb1814-3"><a href="logistic.html#cb1814-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> JS<span class="sc">:</span><span class="fu">log</span>(JS), <span class="at">data=</span>td)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    5.4461     3.2062    1.699    0.089     -0.8379     11.7300 
##          JS   -2.9840     2.1693   -1.376    0.169     -7.2357      1.2677 
##  JS:log(JS)    1.1696     0.9778    1.196    0.232     -0.7467      3.0860 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)    231.8457      0.4326   124248.7675 
##          JS      0.0506      0.0007      3.5527 
##  JS:log(JS)      3.2208      0.4739     21.8896 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 124.621 on 95 degrees of freedom
## 
## AIC: 130.6211 
## 
## Number of iterations to convergence: 5 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 98 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits    cooks
## 7  1.38        0 0.8639  -0.8639   -2.105 -0.4827 0.158262
## 69 6.00        1 0.5290   0.4710    1.221  0.5442 0.090322
## 12 1.72        0 0.8029  -0.8029   -1.852 -0.3440 0.063819
## 31 1.77        0 0.7936  -0.7936   -1.821 -0.3267 0.055946
## 97 5.59        0 0.5044  -0.5044   -1.239 -0.3947 0.049267
## 73 5.48        1 0.4993   0.5007    1.224  0.3561 0.039962
## 70 5.48        0 0.4993  -0.4993   -1.221 -0.3554 0.039735
## 58 5.43        1 0.4972   0.5028    1.224  0.3418 0.036908
## 13 1.96        0 0.7577  -0.7577   -1.713 -0.2696 0.034590
## 80 5.04        0 0.4853  -0.4853   -1.174 -0.2378 0.017543
## 1  4.96        1 0.4840   0.5160    1.225  0.2327 0.017368
## 33 4.88        1 0.4830   0.5170    1.225  0.2184 0.015319
## 61 2.52        0 0.6572  -0.6572   -1.476 -0.1788 0.012429
## 74 2.56        0 0.6506  -0.6506   -1.462 -0.1758 0.011886
## 75 2.57        0 0.6490  -0.6490   -1.459 -0.1751 0.011761
## 84 4.66        1 0.4823   0.5177    1.221  0.1854 0.011051
## 63 4.65        1 0.4823   0.5177    1.221  0.1842 0.010899
## 67 2.65        0 0.6363  -0.6363   -1.433 -0.1701 0.010877
## 96 2.82        0 0.6108  -0.6108   -1.384 -0.1623 0.009530
## 21 4.64        0 0.4824  -0.4824   -1.159 -0.1737 0.009334
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 84 4.66        1       0 0.4823 0.08526
## 63 4.65        1       0 0.4823 0.08471
## 21 4.64        0       0 0.4824 0.08416
## 53 4.63        0       0 0.4824 0.08363
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS Turnover predict fitted std.err
## 70 5.48        0       0 0.4993 0.15604
## 73 5.48        1       0 0.4993 0.15604
## 55 3.97        1       1 0.5005 0.06559
## 16 3.95        0       1 0.5015 0.06547
## 8  3.92        1       1 0.5031 0.06531
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS Turnover predict fitted std.err
## 66 1.19        1       1 0.8945 0.08584
## 48 1.05        1       1 0.9147 0.08191
## 88 0.67        1       1 0.9582 0.06150
## 24 0.23        1       1 0.9874 0.02972
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       39  39.8       14     25     35.9 
## Turnover   1       59  60.2        9     50     84.7 
## ---------------------------------------------------
##          Total     98                           65.3 
## 
## Accuracy: 65.31 
## Recall: 84.75 
## Precision: 66.67</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1016-1.png" width="672" /></p>
<p>Because the interaction term (<code>JS:log(JS)</code>) regression coefficient of 1.1696 in the <em>Model Coefficients</em> table is nonsignificant (<em>p</em> = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is <em>non</em>linear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (“Curvilinear Effects in Logistic Regression”) from Osborne <span class="citation">(<a href="#ref-osborne2015" role="doc-biblioref">2015</a>)</span>. Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well.</p>
<p><strong>Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the <code>log</code> function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the <em>p</em>-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis.</strong></p>
<p>Just for the sake of demonstration, let’s transform the <code>JS</code> variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (<code>JS_0</code>) that is equal to <code>JS</code> minus the minimum value of <code>JS</code>.</p>
<div class="sourceCode" id="cb1816"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1816-1"><a href="logistic.html#cb1816-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero</span></span>
<span id="cb1816-2"><a href="logistic.html#cb1816-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>JS_0 <span class="ot">&lt;-</span> td<span class="sc">$</span>JS <span class="sc">-</span> <span class="fu">min</span>(td<span class="sc">$</span>JS, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>Now that we have a variable called <code>JS_0</code> with at least one score equal to zero, let’s try the try the Box-Tidwell test. I’m going to add the argument <code>brief=TRUE</code> to reduce the amount of output generated by the function.</p>
<div class="sourceCode" id="cb1817"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1817-1"><a href="logistic.html#cb1817-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1817-2"><a href="logistic.html#cb1817-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with predictor containing zero value(s)]</span></span>
<span id="cb1817-3"><a href="logistic.html#cb1817-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS_0 <span class="sc">+</span> JS_0<span class="sc">:</span><span class="fu">log</span>(JS_0), <span class="at">data=</span>td, <span class="at">brief=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>If you ran the script above, you likely got an error message that looked like this:</p>
<p><span class="math inline">\(\color{red}{\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\)</span></p>
<p>The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on <code>JS_0</code>, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above).</p>
<p>Using the first approach, create a new variable in which the <code>JS_0</code> variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the <code>JS_0</code> variable, which results in the lowest score on the new variable (<code>JS_1</code>) being 1. As verification, I include the <code>min</code> function from base R.</p>
<div class="sourceCode" id="cb1818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1818-1"><a href="logistic.html#cb1818-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation that results in lowest score being 1</span></span>
<span id="cb1818-2"><a href="logistic.html#cb1818-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>JS_1 <span class="ot">&lt;-</span> td<span class="sc">$</span>JS_0 <span class="sc">+</span> <span class="fu">abs</span>(<span class="fu">min</span>(td<span class="sc">$</span>JS_0, <span class="at">na.rm=</span><span class="cn">TRUE</span>)) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1818-3"><a href="logistic.html#cb1818-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1818-4"><a href="logistic.html#cb1818-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that new lowest score is 1</span></span>
<span id="cb1818-5"><a href="logistic.html#cb1818-5" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(td<span class="sc">$</span>JS_1, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test.</p>
<div class="sourceCode" id="cb1820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1820-1"><a href="logistic.html#cb1820-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1820-2"><a href="logistic.html#cb1820-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with transformed predictor = 1]</span></span>
<span id="cb1820-3"><a href="logistic.html#cb1820-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS_1 <span class="sc">+</span> JS_1<span class="sc">:</span><span class="fu">log</span>(JS_1), <span class="at">data=</span>td, <span class="at">brief=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS_1
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    8.0985     5.0081    1.617    0.106     -1.7171     17.9141 
##        JS_1   -4.0594     2.9774   -1.363    0.173     -9.8950      1.7762 
## JS_1:log(JS_1)    1.5097     1.2264    1.231    0.218     -0.8939      3.9133 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)   3289.6100      0.1796   60256846.1183 
##        JS_1      0.0173      0.0001      5.9072 
## JS_1:log(JS_1)      4.5254      0.4090     50.0661 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 124.575 on 95 degrees of freedom
## 
## AIC: 130.5747 
## 
## Number of iterations to convergence: 4</code></pre>
<p>There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (<code>JS_1:log(JS_1)</code>) is nonsignificant (<em>b</em> = 1.510, <em>p</em> = .218). Thus, we don’t see evidence that the assumption of linearity has been violated.</p>
<p>We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the <code>rows=</code> argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that we’re back to using the variable called <code>JS_0</code> that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial).</p>
<div class="sourceCode" id="cb1822"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1822-1"><a href="logistic.html#cb1822-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1822-2"><a href="logistic.html#cb1822-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with only cases with scores greater than zero]</span></span>
<span id="cb1822-3"><a href="logistic.html#cb1822-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS_0 <span class="sc">+</span> JS_0<span class="sc">:</span><span class="fu">log</span>(JS_0), <span class="at">data=</span>td, <span class="at">rows=</span>(JS_0 <span class="sc">&gt;</span> <span class="dv">0</span>), <span class="at">brief=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS_0
## 
## Number of cases (rows) of data:  97 
## Number of cases retained for analysis:  97 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    4.6652     2.8216    1.653    0.098     -0.8650     10.1953 
##        JS_0   -2.6157     1.9936   -1.312    0.189     -6.5230      1.2916 
## JS_0:log(JS_0)    1.0392     0.9280    1.120    0.263     -0.7796      2.8579 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)    106.1854      0.4211   26777.8281 
##        JS_0      0.0731      0.0015      3.6387 
## JS_0:log(JS_0)      2.8269      0.4586     17.4256 
## 
## 
## Model Fit
## 
##     Null deviance: 130.725 on 96 degrees of freedom
## Residual deviance: 124.616 on 94 degrees of freedom
## 
## AIC: 130.6162 
## 
## Number of iterations to convergence: 4</code></pre>
<p>We lost one case because that person had a score of zero on the <code>JS_0</code> continuous predictor variable, and we see that the interaction term (<code>JS_0:log(JS_0)</code>) is non significant (<em>b</em> = 1.0392, <em>p</em> = .263).</p>
<p><strong>To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your <em>continuous</em> predictor variable has scores that are equal to or less than zero.</strong> Now that we’ve met the assumption of linearity, we’re finally ready to interpret the model results!</p>
</div>
<div id="interpret_simple_logistic" class="section level4" number="49.2.4.2">
<h4><span class="header-section-number">49.2.4.2</span> Interpret Model Results</h4>
<p><strong>Basic Analysis:</strong> The <em>Basic Analysis</em> section of the original output first displays a table called the <em>Model Coefficients</em>, which includes the regression coefficients (slopes, weights) and their standard errors, <em>z</em>-values, <em>p</em>-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (<code>JS</code>) in relation to the outcome variable (<code>Turnover</code>) is often of substantive interest. Here, we see that the unstandardized regression coefficient for <code>JS</code> is -.438, and its associated <em>p</em>-value is less than .05 (<em>b</em> = -.438, <em>p</em> = .025). Given that the <em>p</em>-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. Further, the 95% confidence interval ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: <em>For every one unit increase in the predictor variable (<code>JS</code>), the logistic function decreases by .438 units – or that for every one unit increase in the predictor variable (<code>JS</code>) the logit transformation of the outcome variable decreases by .438.</em> Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(\ln(\frac{p}{1-p}) = 1.855 -.438 \times JS_{observed}\)</span></p>
<p>where <span class="math inline">\(p\)</span> refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to <span class="math inline">\(\log(odds)\)</span>.</p>
<p><span class="math inline">\(\log(odds) = \ln(\frac{p}{1-p}) = 1.855 -.438 \times JS_{observed}\)</span></p>
<p>To that end, to aid our interpretation of the significant finding, we can move our attention to the table called <em>Odds ratios and confidence intervals</em>. To convert our regression coefficient to an odds ratio, the <code>Logit</code> function has already exponentiated it. Behind the scenes, this is what happened:</p>
<p><span class="math inline">\(e^{-.438} = .646\)</span></p>
<p>We could also do this manually by using the <code>exp</code> function from base R. Any difference between the <code>Logit</code> output and the output below is attributable to rounding.</p>
<div class="sourceCode" id="cb1824"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1824-1"><a href="logistic.html#cb1824-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For the sake of demonstration:</span></span>
<span id="cb1824-2"><a href="logistic.html#cb1824-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1824-3"><a href="logistic.html#cb1824-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the Logit function already does this for us</span></span>
<span id="cb1824-4"><a href="logistic.html#cb1824-4" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="sc">-</span>.<span class="dv">438</span>)</span></code></pre></div>
<pre><code>## [1] 0.6453258</code></pre>
<p>In the <em>Odds ratios and confidence intervals</em> table, we see that indeed the odds ratio is approximately .646. Because the odds ratio is <em>less than</em> 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (<span class="math inline">\(100 \times .354\)</span>) for every one unit increase in job satisfaction (<code>JS</code>). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of <em>not</em> quitting (i.e., staying): The odds of <em>not</em> quitting are 1.548 times as likely for every one unit increase in job satisfaction (<code>JS</code>). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1.</p>
<table>
<thead>
<tr class="header">
<th align="center">Odds Ratio &gt; 1</th>
<th align="center">Odds Ratio &lt; 1</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.2</td>
<td align="center">.8</td>
<td align="center">Small</td>
</tr>
<tr class="even">
<td align="center">2.5</td>
<td align="center">.4</td>
<td align="center">Medium</td>
</tr>
<tr class="odd">
<td align="center">4.3</td>
<td align="center">.2</td>
<td align="center">Large</td>
</tr>
</tbody>
</table>
<p>In the <em>Model Fit</em> table, note that we don’t have an estimate of <em>R</em>-squared (<em>R</em><sup>2</sup>) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-<em>R</em>-squared (<em>R</em><sup>2</sup>) values, but for now let’s focus on what is produced in the <code>Logit</code> function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data by looking at the <em>Specified Confusion Matrices</em> table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 39 (39.8%) and 59 (60.2%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%). Overall, we tend to be most interested in the overall percentage of correct classifications, which is 58.2% – so not a monumental amount of prediction accuracy when using just <code>JS</code> (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent.</p>
<p><strong>Forecasts:</strong> In the output section called <em>Forecasts</em>, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true <em>predictive analytics</em> and <em>machine learning</em>; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one <em>training</em> dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a <em>test</em> dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a <em>validation</em> dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results.</p>
<p><strong>Nagelkerke pseudo-<em>R</em><sup>2</sup>:</strong> To compute Nagelkerke’s pseudo-<em>R</em><sup>2</sup>, we will need to install and access the <code>DescTools</code> package so that we can use its <code>PseudoR2</code> function.</p>
<div class="sourceCode" id="cb1826"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1826-1"><a href="logistic.html#cb1826-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1826-2"><a href="logistic.html#cb1826-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;DescTools&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1827"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1827-1"><a href="logistic.html#cb1827-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1827-2"><a href="logistic.html#cb1827-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span></code></pre></div>
<p>To use the function, we’ll need to re-estimate our simple logistic regression model using the <code>glm</code> function from base R. To request a logistic regression model as a specific type of generalized linear model, we’ll add the <code>family=binomial</code> argument. Using the <code>&lt;-</code> assignment operator, we will assign the resulting estimated model to an object that we’ll arbitrarily call <code>model1</code>.</p>
<div class="sourceCode" id="cb1828"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1828-1"><a href="logistic.html#cb1828-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model and assign to object</span></span>
<span id="cb1828-2"><a href="logistic.html#cb1828-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span></code></pre></div>
<p>In the <code>PseudoR2</code> function, we will specify the name of the model object (<code>model1</code>) as the first argument. As the second argument, type <code>"Nagel"</code> to request pseudo-<em>R</em><sup>2</sup> calculated using Nagelkerke’s formula.</p>
<div class="sourceCode" id="cb1829"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1829-1"><a href="logistic.html#cb1829-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Nagelkerke pseudo-R-squared</span></span>
<span id="cb1829-2"><a href="logistic.html#cb1829-2" aria-hidden="true" tabindex="-1"></a><span class="fu">PseudoR2</span>(model1, <span class="st">&quot;Nagel&quot;</span>)</span></code></pre></div>
<pre><code>## Nagelkerke 
## 0.07258382</code></pre>
<p>The estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> is .073. Remember, a pseudo-<em>R</em><sup>2</sup> is not the exact same thing as a true <em>R</em><sup>2</sup>, so we should interpret it with caution. With caution, we can conclude that <code>JS</code> explains 7.3% of the variance in <code>Turnover</code>.</p>
<p>Because the <code>DescTools</code> package also has a function called <code>Logit</code>, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the <code>Logit</code> function from <code>DescTools</code> as opposed to the one from <code>lessR</code>.</p>
<div class="sourceCode" id="cb1831"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1831-1"><a href="logistic.html#cb1831-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detach package</span></span>
<span id="cb1831-2"><a href="logistic.html#cb1831-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:DescTools&quot;</span>, <span class="at">character.only=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>Technical Write-Up of Results:</strong> A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (<code>Turnover</code>) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (<code>JS</code>) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (<em>b</em> = -.438, <em>p</em> = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover.</p>
<p><strong>Dealing with Bivariate Outliers:</strong> If you recall above, we found that the cases associated with row numbers <em>66</em> and <em>67</em> in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cook’s distance value greater than 1.0). If you <em>were to</em> decide to remove cases <em>66</em> and <em>67</em>, here’s what you would do. First, look at the data frame (using the <code>View</code> function) and determine which cases row numbers <em>66</em> and <em>67</em> are associated with; because we have a unique identifier variable (<code>ID</code>) in our data frame, we can see that they are associated with <code>ID</code> equal to <em>EMP861</em> and <em>EMP862</em>, respectively. Next, with respect to estimating the logistic regression model, the model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: <code>rows=(!ID %in% c("EMP861","EMP862"))</code>; the <code>rows</code> argument subsets the data frame within the <code>Logit</code> function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which <code>ID</code> is <em>not</em> (<code>!</code>) within the vector containing <em>EMP861</em> and <em>EMP862</em>. Please consider revisiting the <a href="filter.html#filter">chapter on filtering (subsetting) data</a> if you would like to see the full list of logical operators or to review how to filter out cases from a data frame <em>before</em> specifying the model.</p>
<div class="sourceCode" id="cb1832"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1832-1"><a href="logistic.html#cb1832-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with outlier/influential cases removed</span></span>
<span id="cb1832-2"><a href="logistic.html#cb1832-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">rows=</span>(<span class="sc">!</span>ID <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;EMP861&quot;</span>,<span class="st">&quot;EMP862&quot;</span>)))</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  97 
## Number of cases retained for analysis:  96 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    1.8799     0.7067    2.660    0.008      0.4948      3.2649 
##          JS   -0.4388     0.1997   -2.198    0.028     -0.8301     -0.0475 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      6.5528      1.6402     26.1785 
##          JS      0.6448      0.4360      0.9536 
## 
## 
## Model Fit
## 
##     Null deviance: 128.887 on 95 degrees of freedom
## Residual deviance: 123.664 on 94 degrees of freedom
## 
## AIC: 127.6641 
## 
## Number of iterations to convergence: 4 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 96 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits   cooks
## 69 6.00        1 0.3202   0.6798   1.5612  0.3758 0.08581
## 7  1.38        0 0.7815  -0.7815  -1.7807 -0.2965 0.06696
## 73 5.48        1 0.3717   0.6283   1.4394  0.2963 0.04900
## 12 1.72        0 0.7549  -0.7549  -1.7039 -0.2564 0.04677
## 58 5.43        1 0.3769   0.6231   1.4281  0.2889 0.04624
## 31 1.77        0 0.7509  -0.7509  -1.6928 -0.2507 0.04425
## 13 1.96        0 0.7349  -0.7349  -1.6508 -0.2291 0.03562
## 1  4.96        1 0.4264   0.5736   1.3247  0.2239 0.02592
## 33 4.88        1 0.4350   0.5650   1.3076  0.2136 0.02334
## 61 2.52        0 0.6844  -0.6844  -1.5305 -0.1720 0.01814
## 97 5.59        0 0.3605  -0.3605  -0.9631 -0.2060 0.01765
## 84 4.66        1 0.4589   0.5411   1.2617  0.1870 0.01736
## 74 2.56        0 0.6806  -0.6806  -1.5221 -0.1685 0.01729
## 70 5.48        0 0.3717  -0.3717  -0.9809 -0.2021 0.01715
## 63 4.65        1 0.4600   0.5400   1.2596  0.1858 0.01713
## 75 2.57        0 0.6797  -0.6797  -1.5200 -0.1676 0.01708
## 80 5.04        0 0.4178  -0.4178  -1.0538 -0.1840 0.01480
## 77 4.46        1 0.4807   0.5193   1.2209  0.1649 0.01316
## 96 2.82        0 0.6553  -0.6553  -1.4682 -0.1483 0.01283
## 39 4.43        1 0.4840   0.5160   1.2149  0.1618 0.01262
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 69 6.00        1       0 0.3202  0.1234
## 97 5.59        0       0 0.3605  0.1134
## 70 5.48        0       0 0.3717  0.1103
## 73 5.48        1       0 0.3717  0.1103
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS Turnover predict fitted std.err
## 39 4.43        1       0 0.4840 0.07517
## 83 4.41        0       0 0.4862 0.07449
## 64 4.26        1       1 0.5027 0.06955
## 27 4.15        0       1 0.5147 0.06614
## 14 4.14        0       1 0.5158 0.06584
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS Turnover predict fitted std.err
## 7  1.38        0       1 0.7815 0.07718
## 48 1.05        1       1 0.8052 0.08013
## 88 0.67        1       1 0.8300 0.08191
## 24 0.23        1       1 0.8556 0.08194
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       38  39.6        8     30     21.1 
## Turnover   1       58  60.4        9     49     84.5 
## ---------------------------------------------------
##          Total     96                           59.4 
## 
## Accuracy: 59.38 
## Recall: 84.48 
## Precision: 62.03</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1028-1.png" width="672" /></p>
</div>
<div id="predictingprobabilities_simple_logistic" class="section level4" number="49.2.4.3">
<h4><span class="header-section-number">49.2.4.3</span> Optional: Compute Predicted Probabilities Based on Sample Data</h4>
<p>The <code>Logit</code> function makes it easy to compute the probabilities of the even occurring based on the sample observations. In fact, all we have to do is add the <code>pred_all=TRUE</code> argument to the function.</p>
<div class="sourceCode" id="cb1834"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1834-1"><a href="logistic.html#cb1834-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with predicted probabilities</span></span>
<span id="cb1834-2"><a href="logistic.html#cb1834-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">pred_all=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    1.8554     0.6883    2.695    0.007      0.5063      3.2044 
##          JS   -0.4378     0.1958   -2.236    0.025     -0.8216     -0.0540 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      6.3939      1.6591     24.6415 
##          JS      0.6455      0.4397      0.9475 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 126.341 on 96 degrees of freedom
## 
## AIC: 130.3413 
## 
## Number of iterations to convergence: 4 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 98 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits   cooks
## 69 6.00        1 0.3162   0.6838   1.5688  0.3725 0.08496
## 7  1.38        0 0.7775  -0.7775  -1.7682 -0.2877 0.06241
## 73 5.48        1 0.3673   0.6327   1.4476  0.2949 0.04889
## 58 5.43        1 0.3724   0.6276   1.4363  0.2877 0.04618
## 12 1.72        0 0.7507  -0.7507  -1.6920 -0.2486 0.04353
## 31 1.77        0 0.7466  -0.7466  -1.6810 -0.2429 0.04117
## 13 1.96        0 0.7305  -0.7305  -1.6393 -0.2219 0.03314
## 1  4.96        1 0.4217   0.5783   1.3332  0.2239 0.02609
## 33 4.88        1 0.4302   0.5698   1.3162  0.2138 0.02353
## 84 4.66        1 0.4540   0.5460   1.2703  0.1875 0.01757
## 63 4.65        1 0.4551   0.5449   1.2682  0.1863 0.01733
## 61 2.52        0 0.6797  -0.6797  -1.5199 -0.1668 0.01693
## 97 5.59        0 0.3562  -0.3562  -0.9554 -0.2021 0.01693
## 70 5.48        0 0.3673  -0.3673  -0.9731 -0.1985 0.01648
## 74 2.56        0 0.6758  -0.6758  -1.5115 -0.1635 0.01615
## 75 2.57        0 0.6749  -0.6749  -1.5095 -0.1626 0.01596
## 67 2.65        0 0.6671  -0.6671  -1.4929 -0.1563 0.01454
## 80 5.04        0 0.4131  -0.4131  -1.0457 -0.1813 0.01431
## 77 4.46        1 0.4757   0.5243   1.2296  0.1656 0.01336
## 39 4.43        1 0.4790   0.5210   1.2235  0.1625 0.01282
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 69 6.00        1       0 0.3162 0.12145
## 97 5.59        0       0 0.3562 0.11198
## 70 5.48        0       0 0.3673 0.10900
## 73 5.48        1       0 0.3673 0.10900
## 58 5.43        1       0 0.3724 0.10758
## 80 5.04        0       0 0.4131 0.09555
## 1  4.96        1       0 0.4217 0.09291
## 33 4.88        1       0 0.4302 0.09023
## 84 4.66        1       0 0.4540 0.08274
## 63 4.65        1       0 0.4551 0.08240
## 21 4.64        0       0 0.4561 0.08206
## 53 4.63        0       0 0.4572 0.08172
## 45 4.59        0       0 0.4616 0.08036
## 47 4.57        0       0 0.4638 0.07968
## 77 4.46        1       0 0.4757 0.07597
## 39 4.43        1       0 0.4790 0.07497
## 83 4.41        0       0 0.4812 0.07431
## 64 4.26        1       0 0.4976 0.06946
## 27 4.15        0       1 0.5097 0.06609
## 14 4.14        0       1 0.5107 0.06579
## 29 4.11        0       1 0.5140 0.06491
## 23 4.10        0       1 0.5151 0.06462
## 89 4.07        0       1 0.5184 0.06376
## 99 4.06        0       1 0.5195 0.06348
## 94 4.03        0       1 0.5228 0.06265
## 55 3.97        1       1 0.5293 0.06105
## 16 3.95        0       1 0.5315 0.06054
## 8  3.92        1       1 0.5348 0.05978
## 42 3.90        0       1 0.5369 0.05929
## 57 3.87        1       1 0.5402 0.05858
## 40 3.83        0       1 0.5446 0.05767
## 6  3.81        1       1 0.5467 0.05724
## 50 3.79        0       1 0.5489 0.05681
## 98 3.69        1       1 0.5597 0.05489
## 86 3.63        1       1 0.5662 0.05390
## 11 3.62        1       1 0.5672 0.05375
## 76 3.50        0       1 0.5801 0.05222
## 54 3.49        1       1 0.5812 0.05211
## 93 3.45        0       1 0.5854 0.05174
## 56 3.44        0       1 0.5865 0.05166
## 87 3.42        1       1 0.5886 0.05151
## 28 3.37        1       1 0.5939 0.05119
## 18 3.35        1       1 0.5960 0.05109
## 82 3.34        0       1 0.5971 0.05105
## 92 3.33        1       1 0.5981 0.05101
## 46 3.32        0       1 0.5992 0.05097
## 22 3.27        1       1 0.6044 0.05085
## 59 3.27        1       1 0.6044 0.05085
## 36 3.26        0       1 0.6055 0.05084
## 60 3.25        0       1 0.6065 0.05083
## 71 3.23        0       1 0.6086 0.05082
## 78 3.22        1       1 0.6096 0.05082
## 91 3.21        1       1 0.6107 0.05083
## 35 3.19        1       1 0.6127 0.05085
## 32 3.15        0       1 0.6169 0.05094
## 37 3.15        1       1 0.6169 0.05094
## 43 3.05        1       1 0.6272 0.05141
## 5  3.04        1       1 0.6282 0.05147
## 4  3.01        1       1 0.6313 0.05168
## 79 3.01        1       1 0.6313 0.05168
## 20 2.98        0       1 0.6343 0.05192
## 30 2.96        0       1 0.6364 0.05209
## 25 2.94        1       1 0.6384 0.05228
## 81 2.93        1       1 0.6394 0.05237
## 96 2.82        0       1 0.6504 0.05359
## 51 2.79        1       1 0.6534 0.05397
## 26 2.73        1       1 0.6593 0.05478
## 34 2.67        1       1 0.6652 0.05565
## 67 2.65        0       1 0.6671 0.05595
## 65 2.62        1       1 0.6700 0.05642
## 75 2.57        0       1 0.6749 0.05721
## 74 2.56        0       1 0.6758 0.05738
## 61 2.52        0       1 0.6797 0.05804
## 95 2.48        1       1 0.6835 0.05871
## 17 2.46        1       1 0.6853 0.05905
## 41 2.38        1       1 0.6928 0.06044
## 9  2.35        1       1 0.6956 0.06096
## 19 2.33        1       1 0.6975 0.06131
## 68 2.28        1       1 0.7021 0.06220
## 72 2.08        1       1 0.7201 0.06571
## 15 2.03        1       1 0.7245 0.06657
## 90 2.00        1       1 0.7271 0.06708
## 13 1.96        0       1 0.7305 0.06775
## 38 1.96        1       1 0.7305 0.06775
## 62 1.92        1       1 0.7340 0.06842
## 49 1.84        1       1 0.7407 0.06971
## 31 1.77        0       1 0.7466 0.07079
## 85 1.76        1       1 0.7474 0.07095
## 2  1.72        1       1 0.7507 0.07154
## 12 1.72        0       1 0.7507 0.07154
## 10 1.69        1       1 0.7532 0.07198
## 3  1.64        1       1 0.7572 0.07270
## 44 1.43        1       1 0.7737 0.07541
## 7  1.38        0       1 0.7775 0.07598
## 66 1.19        1       1 0.7916 0.07790
## 48 1.05        1       1 0.8015 0.07904
## 88 0.67        1       1 0.8266 0.08096
## 24 0.23        1       1 0.8525 0.08116
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       39  39.8        8     31     20.5 
## Turnover   1       59  60.2       10     49     83.1 
## ---------------------------------------------------
##          Total     98                           58.2 
## 
## Accuracy: 58.16 
## Recall: 83.05 
## Precision: 61.25</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1029-1.png" width="672" /></p>
<p>In the output under the <em>Forecasts</em> section, you should see a section called <em>Data, Fitted Values, Standard Errors</em>. This section now contains the predicted probabilities and associated classifications for <em>all</em> cases in the sample. The column labeled <em>fitted</em> contains the predicted probabilities, and the column labeled <em>predict</em> contains the probabilities when applied to a default probability threshold of .50, such that 1 indicates that the probability was .50 or greater (i.e., event predicted to occur) 0 indicates the probability was less than .50 (i.e., event not predicted to occur).</p>
<p>If desired, we can also append the predicted probabilities (i.e., fitted values) to our existing data frame by referencing the row names (i.e., row numbers) of each object when we merge.</p>
<ol style="list-style-type: decimal">
<li>Let’s overwrite the existing <code>td</code> data frame object by using the <code>&lt;-</code> assignment operator.</li>
<li>Specify the name of the <code>merge</code> function from base R. For more information on this function, please refer to <a href="join.html#join_mergefunction">this chapter supplement</a> from the chapter on joining data frames.</li>
</ol>
<ul>
<li>As the first argument in the <code>merge</code> function, specify <code>x=</code> followed by the name of the <code>td</code> data frame object.</li>
<li>As the second argument in the <code>merge</code> function, specify <code>y=</code> followed by the <code>data.frame</code> function from base R. As the sole argument within the <code>data.frame</code> function specify a name for the new variable that will contain the predicted probabilities based on <code>JS</code> scores (<code>prob_JS</code>), followed by the <code>=</code> operator and our simple logistic regression model from above with <code>$fitted.values</code> to the end. This will extract just the fitted values from the output and then convert the vector to a data frame object.</li>
<li>As the third argument in the <code>merge</code> function, type <code>by="row.names"</code>, which will match rows from the <code>x</code> and <code>y</code> data frame objects based on their respective row names (i.e., row numbers).</li>
<li>As the fourth argument in the <code>merge</code> function, type <code>all=TRUE</code> to request a full merge, such that all rows with data will be retained from both data frame objects when merging.</li>
</ul>
<div class="sourceCode" id="cb1836"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1836-1"><a href="logistic.html#cb1836-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with predicted probabilities</span></span>
<span id="cb1836-2"><a href="logistic.html#cb1836-2" aria-hidden="true" tabindex="-1"></a><span class="co"># added as new variable in existing data frame object</span></span>
<span id="cb1836-3"><a href="logistic.html#cb1836-3" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">merge</span>(<span class="at">x=</span>td, </span>
<span id="cb1836-4"><a href="logistic.html#cb1836-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span><span class="fu">data.frame</span>(</span>
<span id="cb1836-5"><a href="logistic.html#cb1836-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">prob_JS =</span> <span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">pred_all=</span><span class="cn">TRUE</span>)<span class="sc">$</span>fitted.values</span>
<span id="cb1836-6"><a href="logistic.html#cb1836-6" aria-hidden="true" tabindex="-1"></a>              ),</span>
<span id="cb1836-7"><a href="logistic.html#cb1836-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">by=</span><span class="st">&quot;row.names&quot;</span>,</span>
<span id="cb1836-8"><a href="logistic.html#cb1836-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">all=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    1.8554     0.6883    2.695    0.007      0.5063      3.2044 
##          JS   -0.4378     0.1958   -2.236    0.025     -0.8216     -0.0540 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      6.3939      1.6591     24.6415 
##          JS      0.6455      0.4397      0.9475 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 126.341 on 96 degrees of freedom
## 
## AIC: 130.3413 
## 
## Number of iterations to convergence: 4 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 98 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits   cooks
## 69 6.00        1 0.3162   0.6838   1.5688  0.3725 0.08496
## 7  1.38        0 0.7775  -0.7775  -1.7682 -0.2877 0.06241
## 73 5.48        1 0.3673   0.6327   1.4476  0.2949 0.04889
## 58 5.43        1 0.3724   0.6276   1.4363  0.2877 0.04618
## 12 1.72        0 0.7507  -0.7507  -1.6920 -0.2486 0.04353
## 31 1.77        0 0.7466  -0.7466  -1.6810 -0.2429 0.04117
## 13 1.96        0 0.7305  -0.7305  -1.6393 -0.2219 0.03314
## 1  4.96        1 0.4217   0.5783   1.3332  0.2239 0.02609
## 33 4.88        1 0.4302   0.5698   1.3162  0.2138 0.02353
## 84 4.66        1 0.4540   0.5460   1.2703  0.1875 0.01757
## 63 4.65        1 0.4551   0.5449   1.2682  0.1863 0.01733
## 61 2.52        0 0.6797  -0.6797  -1.5199 -0.1668 0.01693
## 97 5.59        0 0.3562  -0.3562  -0.9554 -0.2021 0.01693
## 70 5.48        0 0.3673  -0.3673  -0.9731 -0.1985 0.01648
## 74 2.56        0 0.6758  -0.6758  -1.5115 -0.1635 0.01615
## 75 2.57        0 0.6749  -0.6749  -1.5095 -0.1626 0.01596
## 67 2.65        0 0.6671  -0.6671  -1.4929 -0.1563 0.01454
## 80 5.04        0 0.4131  -0.4131  -1.0457 -0.1813 0.01431
## 77 4.46        1 0.4757   0.5243   1.2296  0.1656 0.01336
## 39 4.43        1 0.4790   0.5210   1.2235  0.1625 0.01282
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 69 6.00        1       0 0.3162 0.12145
## 97 5.59        0       0 0.3562 0.11198
## 70 5.48        0       0 0.3673 0.10900
## 73 5.48        1       0 0.3673 0.10900
## 58 5.43        1       0 0.3724 0.10758
## 80 5.04        0       0 0.4131 0.09555
## 1  4.96        1       0 0.4217 0.09291
## 33 4.88        1       0 0.4302 0.09023
## 84 4.66        1       0 0.4540 0.08274
## 63 4.65        1       0 0.4551 0.08240
## 21 4.64        0       0 0.4561 0.08206
## 53 4.63        0       0 0.4572 0.08172
## 45 4.59        0       0 0.4616 0.08036
## 47 4.57        0       0 0.4638 0.07968
## 77 4.46        1       0 0.4757 0.07597
## 39 4.43        1       0 0.4790 0.07497
## 83 4.41        0       0 0.4812 0.07431
## 64 4.26        1       0 0.4976 0.06946
## 27 4.15        0       1 0.5097 0.06609
## 14 4.14        0       1 0.5107 0.06579
## 29 4.11        0       1 0.5140 0.06491
## 23 4.10        0       1 0.5151 0.06462
## 89 4.07        0       1 0.5184 0.06376
## 99 4.06        0       1 0.5195 0.06348
## 94 4.03        0       1 0.5228 0.06265
## 55 3.97        1       1 0.5293 0.06105
## 16 3.95        0       1 0.5315 0.06054
## 8  3.92        1       1 0.5348 0.05978
## 42 3.90        0       1 0.5369 0.05929
## 57 3.87        1       1 0.5402 0.05858
## 40 3.83        0       1 0.5446 0.05767
## 6  3.81        1       1 0.5467 0.05724
## 50 3.79        0       1 0.5489 0.05681
## 98 3.69        1       1 0.5597 0.05489
## 86 3.63        1       1 0.5662 0.05390
## 11 3.62        1       1 0.5672 0.05375
## 76 3.50        0       1 0.5801 0.05222
## 54 3.49        1       1 0.5812 0.05211
## 93 3.45        0       1 0.5854 0.05174
## 56 3.44        0       1 0.5865 0.05166
## 87 3.42        1       1 0.5886 0.05151
## 28 3.37        1       1 0.5939 0.05119
## 18 3.35        1       1 0.5960 0.05109
## 82 3.34        0       1 0.5971 0.05105
## 92 3.33        1       1 0.5981 0.05101
## 46 3.32        0       1 0.5992 0.05097
## 22 3.27        1       1 0.6044 0.05085
## 59 3.27        1       1 0.6044 0.05085
## 36 3.26        0       1 0.6055 0.05084
## 60 3.25        0       1 0.6065 0.05083
## 71 3.23        0       1 0.6086 0.05082
## 78 3.22        1       1 0.6096 0.05082
## 91 3.21        1       1 0.6107 0.05083
## 35 3.19        1       1 0.6127 0.05085
## 32 3.15        0       1 0.6169 0.05094
## 37 3.15        1       1 0.6169 0.05094
## 43 3.05        1       1 0.6272 0.05141
## 5  3.04        1       1 0.6282 0.05147
## 4  3.01        1       1 0.6313 0.05168
## 79 3.01        1       1 0.6313 0.05168
## 20 2.98        0       1 0.6343 0.05192
## 30 2.96        0       1 0.6364 0.05209
## 25 2.94        1       1 0.6384 0.05228
## 81 2.93        1       1 0.6394 0.05237
## 96 2.82        0       1 0.6504 0.05359
## 51 2.79        1       1 0.6534 0.05397
## 26 2.73        1       1 0.6593 0.05478
## 34 2.67        1       1 0.6652 0.05565
## 67 2.65        0       1 0.6671 0.05595
## 65 2.62        1       1 0.6700 0.05642
## 75 2.57        0       1 0.6749 0.05721
## 74 2.56        0       1 0.6758 0.05738
## 61 2.52        0       1 0.6797 0.05804
## 95 2.48        1       1 0.6835 0.05871
## 17 2.46        1       1 0.6853 0.05905
## 41 2.38        1       1 0.6928 0.06044
## 9  2.35        1       1 0.6956 0.06096
## 19 2.33        1       1 0.6975 0.06131
## 68 2.28        1       1 0.7021 0.06220
## 72 2.08        1       1 0.7201 0.06571
## 15 2.03        1       1 0.7245 0.06657
## 90 2.00        1       1 0.7271 0.06708
## 13 1.96        0       1 0.7305 0.06775
## 38 1.96        1       1 0.7305 0.06775
## 62 1.92        1       1 0.7340 0.06842
## 49 1.84        1       1 0.7407 0.06971
## 31 1.77        0       1 0.7466 0.07079
## 85 1.76        1       1 0.7474 0.07095
## 2  1.72        1       1 0.7507 0.07154
## 12 1.72        0       1 0.7507 0.07154
## 10 1.69        1       1 0.7532 0.07198
## 3  1.64        1       1 0.7572 0.07270
## 44 1.43        1       1 0.7737 0.07541
## 7  1.38        0       1 0.7775 0.07598
## 66 1.19        1       1 0.7916 0.07790
## 48 1.05        1       1 0.8015 0.07904
## 88 0.67        1       1 0.8266 0.08096
## 24 0.23        1       1 0.8525 0.08116
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       39  39.8        8     31     20.5 
## Turnover   1       59  60.2       10     49     83.1 
## ---------------------------------------------------
##          Total     98                           58.2 
## 
## Accuracy: 58.16 
## Recall: 83.05 
## Precision: 61.25</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1030-1.png" width="672" /></p>
<p>If we print the first six rows from the <code>td</code> data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model.</p>
<div class="sourceCode" id="cb1838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1838-1"><a href="logistic.html#cb1838-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 6 rows of data frame object</span></span>
<span id="cb1838-2"><a href="logistic.html#cb1838-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>##   Row.names     ID Turnover   JS   OC   TI NAff JS_0 JS_1   prob_JS
## 1         1 EMP559        1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566
## 2        10 EMP614        1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576
## 3        11 EMP617        1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481
## 4        12 EMP619        0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079
## 5        13 EMP634        0 1.96 3.02 3.41   NA 1.73 2.73 0.7305327
## 6        14 EMP636        0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466</code></pre>
<p>We can then take those predicted probabilities and apply a threshold by which scores higher than that threshold will indicate that the event is likely to happen. A common threshold for dichotomous outcomes is .50. We’ll use the <code>mutate</code> function from the <code>dplyr</code> package and the <code>ifelse</code> function base R to make this happen.</p>
<div class="sourceCode" id="cb1840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1840-1"><a href="logistic.html#cb1840-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1840-2"><a href="logistic.html#cb1840-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;dplyr&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1841"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1841-1"><a href="logistic.html#cb1841-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1841-2"><a href="logistic.html#cb1841-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code></pre></div>
<div class="sourceCode" id="cb1842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1842-1"><a href="logistic.html#cb1842-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classify probabilities as 1 (Quit) or 0 (Stay)</span></span>
<span id="cb1842-2"><a href="logistic.html#cb1842-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Using threshold of .50</span></span>
<span id="cb1842-3"><a href="logistic.html#cb1842-3" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">mutate</span>(td,</span>
<span id="cb1842-4"><a href="logistic.html#cb1842-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">Pred_Turnover =</span> <span class="fu">ifelse</span>(prob_JS <span class="sc">&gt;=</span> .<span class="dv">50</span>, <span class="co"># apply threshold</span></span>
<span id="cb1842-5"><a href="logistic.html#cb1842-5" aria-hidden="true" tabindex="-1"></a>                                    <span class="dv">1</span>, <span class="co"># if logical statement is true (&quot;Quit&quot;)</span></span>
<span id="cb1842-6"><a href="logistic.html#cb1842-6" aria-hidden="true" tabindex="-1"></a>                                    <span class="dv">0</span>) <span class="co"># if logical statement is false (&quot;Stay&quot;)</span></span>
<span id="cb1842-7"><a href="logistic.html#cb1842-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the <code>JS</code> predictor variable, which we could then use to plug into our model; to do this with a model estimated using the <code>Logit</code> model, setting up the model as follows may be the best course of action.</p>
<div class="sourceCode" id="cb1843"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1843-1"><a href="logistic.html#cb1843-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign simple logistic regression model to an object</span></span>
<span id="cb1843-2"><a href="logistic.html#cb1843-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  98 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    1.8554     0.6883    2.695    0.007      0.5063      3.2044 
##          JS   -0.4378     0.1958   -2.236    0.025     -0.8216     -0.0540 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      6.3939      1.6591     24.6415 
##          JS      0.6455      0.4397      0.9475 
## 
## 
## Model Fit
## 
##     Null deviance: 131.746 on 97 degrees of freedom
## Residual deviance: 126.341 on 96 degrees of freedom
## 
## AIC: 130.3413 
## 
## Number of iterations to convergence: 4 
## 
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 98 cases (rows) of data]
## --------------------------------------------------------------------
##      JS Turnover fitted residual rstudent  dffits   cooks
## 66 6.00        1 0.3162   0.6838   1.5688  0.3725 0.08496
## 67 1.38        0 0.7775  -0.7775  -1.7682 -0.2877 0.06241
## 71 5.48        1 0.3673   0.6327   1.4476  0.2949 0.04889
## 54 5.43        1 0.3724   0.6276   1.4363  0.2877 0.04618
## 4  1.72        0 0.7507  -0.7507  -1.6920 -0.2486 0.04353
## 25 1.77        0 0.7466  -0.7466  -1.6810 -0.2429 0.04117
## 5  1.96        0 0.7305  -0.7305  -1.6393 -0.2219 0.03314
## 1  4.96        1 0.4217   0.5783   1.3332  0.2239 0.02609
## 27 4.88        1 0.4302   0.5698   1.3162  0.2138 0.02353
## 83 4.66        1 0.4540   0.5460   1.2703  0.1875 0.01757
## 60 4.65        1 0.4551   0.5449   1.2682  0.1863 0.01733
## 58 2.52        0 0.6797  -0.6797  -1.5199 -0.1668 0.01693
## 97 5.59        0 0.3562  -0.3562  -0.9554 -0.2021 0.01693
## 68 5.48        0 0.3673  -0.3673  -0.9731 -0.1985 0.01648
## 72 2.56        0 0.6758  -0.6758  -1.5115 -0.1635 0.01615
## 73 2.57        0 0.6749  -0.6749  -1.5095 -0.1626 0.01596
## 64 2.65        0 0.6671  -0.6671  -1.4929 -0.1563 0.01454
## 79 5.04        0 0.4131  -0.4131  -1.0457 -0.1813 0.01431
## 75 4.46        1 0.4757   0.5243   1.2296  0.1656 0.01336
## 33 4.43        1 0.4790   0.5210   1.2235  0.1625 0.01282
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS Turnover predict fitted std.err
## 66 6.00        1       0 0.3162  0.1215
## 97 5.59        0       0 0.3562  0.1120
## 68 5.48        0       0 0.3673  0.1090
## 71 5.48        1       0 0.3673  0.1090
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS Turnover predict fitted std.err
## 33 4.43        1       0 0.4790 0.07497
## 82 4.41        0       0 0.4812 0.07431
## 61 4.26        1       0 0.4976 0.06946
## 20 4.15        0       1 0.5097 0.06609
## 6  4.14        0       1 0.5107 0.06579
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS Turnover predict fitted std.err
## 63 1.19        1       1 0.7916 0.07790
## 43 1.05        1       1 0.8015 0.07904
## 87 0.67        1       1 0.8266 0.08096
## 17 0.23        1       1 0.8525 0.08116
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       39  39.8        8     31     20.5 
## Turnover   1       59  60.2       10     49     83.1 
## ---------------------------------------------------
##          Total     98                           58.2 
## 
## Accuracy: 58.16 
## Recall: 83.05 
## Precision: 61.25</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1035-1.png" width="672" /></p>
<div class="sourceCode" id="cb1845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1845-1"><a href="logistic.html#cb1845-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract intercept and predictor coefficient</span></span>
<span id="cb1845-2"><a href="logistic.html#cb1845-2" aria-hidden="true" tabindex="-1"></a>b0 <span class="ot">=</span> <span class="fu">unname</span>(mod<span class="sc">$</span>coefficients[<span class="st">&quot;(Intercept)&quot;</span>]) <span class="co"># extract intercept</span></span>
<span id="cb1845-3"><a href="logistic.html#cb1845-3" aria-hidden="true" tabindex="-1"></a>b1 <span class="ot">=</span> <span class="fu">unname</span>(mod<span class="sc">$</span>coefficients[<span class="st">&quot;JS&quot;</span>]) <span class="co"># extract predictor coefficient</span></span>
<span id="cb1845-4"><a href="logistic.html#cb1845-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1845-5"><a href="logistic.html#cb1845-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify equation to estimate predicted probabilities</span></span>
<span id="cb1845-6"><a href="logistic.html#cb1845-6" aria-hidden="true" tabindex="-1"></a><span class="co"># using sample data on which model was estimated</span></span>
<span id="cb1845-7"><a href="logistic.html#cb1845-7" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>prob_JS_alt <span class="ot">&lt;-</span> <span class="fu">exp</span>(b0 <span class="sc">+</span> b1<span class="sc">*</span>td<span class="sc">$</span>JS) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(b0 <span class="sc">+</span> b1<span class="sc">*</span>td<span class="sc">$</span>JS))</span></code></pre></div>
<p>If we so desired, we could plug in new values for <code>JS</code> from another data frame object like we did in the <a href="predictingcriterionscores.html#predictingcriterionscores">chapter on predicting criterion scores using simple linear regression</a>. Or we could specify a vector of values for <code>JS</code> that we wish to compute the predicted probabilities of turnover.</p>
<div class="sourceCode" id="cb1846"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1846-1"><a href="logistic.html#cb1846-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create vector of plausible values for JS</span></span>
<span id="cb1846-2"><a href="logistic.html#cb1846-2" aria-hidden="true" tabindex="-1"></a>vec_JS <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)</span>
<span id="cb1846-3"><a href="logistic.html#cb1846-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1846-4"><a href="logistic.html#cb1846-4" aria-hidden="true" tabindex="-1"></a><span class="co"># &quot;Plug in&quot; vector of values in equation</span></span>
<span id="cb1846-5"><a href="logistic.html#cb1846-5" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(b0 <span class="sc">+</span> b1<span class="sc">*</span>vec_JS) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(b0 <span class="sc">+</span> b1<span class="sc">*</span>vec_JS))</span></code></pre></div>
<pre><code>## [1] 0.8647542 0.8049594 0.7270717 0.6322888 0.5260465 0.4173924 0.3162077</code></pre>
<p>The 7 predicted probabilities for turning over corresponding to the 7 values we listed in our vector have been printed in order to our Console.</p>
</div>
</div>
<div id="estimate_multiple_logistic" class="section level3" number="49.2.5">
<h3><span class="header-section-number">49.2.5</span> Estimate Multiple Logistic Regression Model</h3>
<p>Now we will expand our initial model by specifying a multiple logistic regression model, which means the model will include more than one predictor variable. Let’s begin by regressing <code>Turnover</code> on <code>JS</code>, <code>NAff</code>, and <code>TI</code> using the <code>Logit</code> function from the <code>lessR</code> package. If you haven’t already, install and access the <code>lessR</code> package.</p>
<div class="sourceCode" id="cb1848"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1848-1"><a href="logistic.html#cb1848-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1848-2"><a href="logistic.html#cb1848-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;lessR&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1849"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1849-1"><a href="logistic.html#cb1849-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1849-2"><a href="logistic.html#cb1849-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lessR)</span></code></pre></div>
<p>As the first argument in the <code>Logit</code> function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variables are typed to the right of the <code>~</code> symbol. Separate predictor variables with the <code>+</code> symbol. As the second argument, type <code>data=</code> followed by the name of the data frame to which both of the variables belong (<code>td</code>). Let’s estimate a model with <code>JS</code>, <code>NAff</code>, and <code>TI</code> as the predictor variables.</p>
<div class="sourceCode" id="cb1850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1850-1"><a href="logistic.html#cb1850-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple logistic regression model</span></span>
<span id="cb1850-2"><a href="logistic.html#cb1850-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> NAff <span class="sc">+</span> TI, <span class="at">data=</span>td)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## Predictor Variable 2:  NAff
## Predictor Variable 3:  TI
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  95 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)   -3.9286     1.8120   -2.168    0.030     -7.4800     -0.3772 
##          JS   -0.2332     0.2215   -1.053    0.293     -0.6674      0.2010 
##        NAff    1.1952     0.4996    2.392    0.017      0.2160      2.1743 
##          TI    0.8967     0.3166    2.832    0.005      0.2761      1.5172 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      0.0197      0.0006      0.6858 
##          JS      0.7920      0.5130      1.2227 
##        NAff      3.3041      1.2411      8.7963 
##          TI      2.4514      1.3180      4.5593 
## 
## 
## Model Fit
## 
##     Null deviance: 127.017 on 94 degrees of freedom
## Residual deviance: 105.229 on 91 degrees of freedom
## 
## AIC: 113.2285 
## 
## Number of iterations to convergence: 4 
## 
## 
## Collinearity
## 
##      Tolerance       VIF
## JS       0.885     1.129
## NAff     0.973     1.028
## TI       0.903     1.108
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 95 cases (rows) of data]
## --------------------------------------------------------------------
##      JS NAff   TI Turnover  fitted residual rstudent  dffits   cooks
## 6  4.14 3.07 4.33        0 0.93448  -0.9345  -2.4565 -0.4581 0.15185
## 1  4.96 1.87 0.51        1 0.08371   0.9163   2.3396  0.4680 0.13427
## 66 6.00 1.45 2.37        1 0.18699   0.8130   1.9301  0.5252 0.10092
## 42 4.57 2.01 4.27        0 0.77499  -0.7750  -1.8127 -0.5016 0.08228
## 96 2.82 2.05 4.25        0 0.84220  -0.8422  -1.9794 -0.3798 0.05870
## 74 3.50 3.28 1.37        0 0.59961  -0.5996  -1.4140 -0.4473 0.04692
## 27 4.88 1.76 1.81        1 0.20749   0.7925   1.8220  0.3639 0.04554
## 26 3.15 2.84 3.48        0 0.86430  -0.8643  -2.0397 -0.3080 0.04250
## 67 1.38 2.30 3.18        0 0.79411  -0.7941  -1.8204 -0.3391 0.03973
## 61 4.26 1.73 2.10        1 0.27461   0.7254   1.6390  0.3009 0.02635
## 68 5.48 2.27 2.78        0 0.49981  -0.4998  -1.2109 -0.3241 0.02182
## 4  1.72 2.11 1.78        0 0.44724  -0.4472  -1.1203 -0.3091 0.01882
## 25 1.77 2.24 1.16        0 0.34887  -0.3489  -0.9604 -0.3157 0.01803
## 52 3.44 2.35 3.37        0 0.75019  -0.7502  -1.6856 -0.2364 0.01720
## 54 5.43 3.01 2.43        1 0.64141   0.3586   0.9747  0.3071 0.01719
## 69 3.23 2.84 2.18        0 0.66088  -0.6609  -1.4931 -0.2582 0.01719
## 8  3.95 2.87 1.36        0 0.45014  -0.4501  -1.1216 -0.2899 0.01661
## 75 4.46 2.81 1.80        1 0.50095   0.4991   1.2014  0.2785 0.01611
## 77 3.01 1.53 3.01        1 0.47428   0.5257   1.2459  0.2729 0.01593
## 64 2.65 1.94 3.27        0 0.66911  -0.6691  -1.5074 -0.2437 0.01552
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS NAff   TI Turnover predict  fitted std.err
## 1  4.96 1.87 0.51        1       0 0.08371 0.05860
## 97 5.59 1.60 1.62        0       0 0.13385 0.07684
## 99 4.06 2.19 0.65        0       0 0.15774 0.08794
## 66 6.00 1.45 2.37        1       0 0.18699 0.10944
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS NAff   TI Turnover predict fitted std.err
## 77 3.01 1.53 3.01        1       0 0.4743 0.11354
## 53 3.87 1.72 3.05        1       0 0.4899 0.09955
## 68 5.48 2.27 2.78        0       0 0.4998 0.13673
## 75 4.46 2.81 1.80        1       1 0.5009 0.11987
## 78 3.92 2.80 1.70        1       1 0.5070 0.11250
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS NAff   TI Turnover predict fitted std.err
## 17 0.23 2.38 4.20        1       1 0.9327 0.04890
## 6  4.14 3.07 4.33        0       1 0.9345 0.04905
## 19 2.73 3.64 3.36        1       1 0.9426 0.04146
## 95 2.48 3.18 4.74        1       1 0.9719 0.02322
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       37  38.9       23     14     62.2 
## Turnover   1       58  61.1        6     52     89.7 
## ---------------------------------------------------
##          Total     95                           78.9 
## 
## Accuracy: 78.95 
## Recall: 89.66 
## Precision: 78.79</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1039-1.png" width="672" /></p>
<p><strong>Note: In some instances, you might receive the error message shown below. You can ignore this message, as it just indicates that you have a poor predictor variable in the model that results in fitted/predicted values that are all the same. If you get this message, proceed forward with your interpretation of the output.</strong></p>
<p><span class="math inline">\(\color{red}{\text{Error:}}\)</span>
<span class="math inline">\(\color{red}{\text{All predicted values are 0.}}\)</span>
<span class="math inline">\(\color{red}{\text{Something is wrong here.}}\)</span></p>
<p>The output generates the model coefficient estimates, the odds ratios and confidence intervals, model fit information (i.e., AIC), collinearity diagnostics, outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model, the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis (<em>N</em> = 95) after removing cases with missing data on any of the model variables.</p>
<div id="teststatisticalassumptions_multiple_logistic" class="section level4" number="49.2.5.1">
<h4><span class="header-section-number">49.2.5.1</span> Test Statistical Assumptions</h4>
<p>To determine whether it’s appropriate to interpret the results of a multiple logistic regression model, we need to first test the following <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a>.</p>
<p><strong>Cases Are Randomly Sampled from the Population:</strong> As mentioned in the <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a> section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied.</p>
<p><strong>Outcome Variable Is Dichotomous:</strong> We already know that the outcome variable called <code>Turnover</code> is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption.</p>
<p><strong>Data Are Free of Multivariate Outliers:</strong> To determine whether the data are free of multivariate outliers, let’s take a look at the text output section called <em>Analysis of Residuals and Influence</em>. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cook’s distance (cooks). The case associated with row number <em>14</em> has the highest Cook’s distance value (.152), followed by the cases associated with row numbers <em>1</em> (.134), <em>69</em> (.101), and <em>47</em> (.082). A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers <em>14</em>, <em>1</em>, <em>69</em> and <em>47</em> from our data frame; though, I wouldn’t necessarily recommend this, as these Cook’s distance values aren’t too concerning. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so.</p>
<p><strong>No (Multi)Collinearity Between Predictor Variables:</strong> To assess whether (multi)collinearity might be an issue, check out the table called <em>Collinearity</em> in the output. This table shows two indices of collinearity: tolerance and valence inflation factor (VIF). Because the VIF is just the reciprocal of the tolerance (1/tolerance), let’s focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - <em>R</em><sup>2</sup>), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are very lower levels of collinearity. In the table, we can see that the tolerance statistics are all closer to 1.00 and well above .20, thereby indicating that collinearity is not likely an issue.</p>
<p><strong>Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear:</strong> To investigate the assumption of linearity between <em>continuous</em> predictor variables and the logit transformation of the outcome variable, we can add the interaction between each continuous predictor variable and its logarithmic (i.e., natural log) transformation. We will use an approach that is commonly referred to as the Box-Tidwell approach <span class="citation">(<a href="#ref-hosmerlemeshow2000" role="doc-biblioref">Hosmer and Lemeshow 2000</a>)</span>. To apply this approach, we need to add the interaction term between each predictor variable (i.e., <code>JS</code>, <code>NAff</code>, <code>TI</code>) and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation. In our regression model formula, specify the dichotomous outcome variable <code>Turnover</code> to the left of the <code>~</code> operator. To the right of the <code>~</code> operator, type the name of each predictor variable <code>JS</code> followed by the <code>+</code> operator. After the <code>+</code> operator, type the name of the same predictor variable <code>JS</code>, followed by the <code>:</code> operator and the <code>log</code> function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all <em>continuous</em> predictor variables in the model.</p>
<div class="sourceCode" id="cb1852"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1852-1"><a href="logistic.html#cb1852-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variables and</span></span>
<span id="cb1852-2"><a href="logistic.html#cb1852-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable</span></span>
<span id="cb1852-3"><a href="logistic.html#cb1852-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Logit</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> JS<span class="sc">:</span><span class="fu">log</span>(JS) <span class="sc">+</span> NAff <span class="sc">+</span> NAff<span class="sc">:</span><span class="fu">log</span>(NAff) <span class="sc">+</span> TI <span class="sc">+</span> TI<span class="sc">:</span><span class="fu">log</span>(TI), <span class="at">data=</span>td)</span></code></pre></div>
<pre><code>## 
## Response Variable:   Turnover
## Predictor Variable 1:  JS
## Predictor Variable 2:  NAff
## Predictor Variable 3:  TI
## 
## Number of cases (rows) of data:  99 
## Number of cases retained for analysis:  95 
## 
## 
## 
##    BASIC ANALYSIS 
## 
## Model Coefficients
## 
##              Estimate    Std Err  z-value  p-value   Lower 95%   Upper 95%
## (Intercept)    0.9206     9.8331    0.094    0.925    -18.3519     20.1932 
##          JS   -4.5845     2.6176   -1.751    0.080     -9.7149      0.5459 
##        NAff   -0.4107     6.4047   -0.064    0.949    -12.9636     12.1422 
##          TI    3.2888     3.0158    1.091    0.275     -2.6221      9.1998 
##  JS:log(JS)    2.0053     1.1844    1.693    0.090     -0.3160      4.3266 
## NAff:log(NAff)    0.9559     3.5321    0.271    0.787     -5.9670      7.8788 
##  TI:log(TI)   -1.2101     1.5232   -0.794    0.427     -4.1956      1.7754 
## 
## 
## Odds ratios and confidence intervals
## 
##              Odds Ratio   Lower 95%   Upper 95%
## (Intercept)      2.5108      0.0000   588545072.9802 
##          JS      0.0102      0.0001      1.7261 
##        NAff      0.6632      0.0000   187626.1824 
##          TI     26.8110      0.0726   9894.6983 
##  JS:log(JS)      7.4284      0.7291     75.6872 
## NAff:log(NAff)      2.6010      0.0026   2640.6975 
##  TI:log(TI)      0.2982      0.0151      5.9024 
## 
## 
## Model Fit
## 
##     Null deviance: 127.017 on 94 degrees of freedom
## Residual deviance: 100.854 on 88 degrees of freedom
## 
## AIC: 114.8541 
## 
## Number of iterations to convergence: 5 
## 
## 
## Collinearity
## 
##      Tolerance       VIF
## JS       0.030    32.947
## NAff     0.012    82.947
## TI       0.073    13.775
## 
## 
## 
##    ANALYSIS OF RESIDUALS AND INFLUENCE 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance
##    [sorted by Cook&#39;s Distance]
##    [res_rows = 20 out of 95 cases (rows) of data]
## --------------------------------------------------------------------
##      JS NAff   TI Turnover  fitted residual rstudent  dffits   cooks
## 1  4.96 1.87 0.51        1 0.03086   0.9691   3.2985  0.9563 0.63056
## 6  4.14 3.07 4.33        0 0.91111  -0.9111  -2.3981 -0.6526 0.14155
## 67 1.38 2.30 3.18        0 0.91516  -0.9152  -2.3760 -0.5617 0.10829
## 68 5.48 2.27 2.78        0 0.73899  -0.7390  -1.7745 -0.6741 0.07686
## 66 6.00 1.45 2.37        1 0.55250   0.4475   1.2378  0.7960 0.07043
## 42 4.57 2.01 4.27        0 0.72166  -0.7217  -1.7245 -0.6507 0.06898
## 74 3.50 3.28 1.37        0 0.50709  -0.5071  -1.3129 -0.6965 0.05735
## 96 2.82 2.05 4.25        0 0.72223  -0.7222  -1.6950 -0.5521 0.04977
## 79 5.04 1.22 2.92        0 0.42774  -0.4277  -1.1658 -0.6487 0.04597
## 4  1.72 2.11 1.78        0 0.53924  -0.5392  -1.3118 -0.4774 0.02800
## 27 4.88 1.76 1.81        1 0.25761   0.7424   1.6982  0.3893 0.02590
## 25 1.77 2.24 1.16        0 0.31999  -0.3200  -0.9531 -0.5030 0.02531
## 26 3.15 2.84 3.48        0 0.83033  -0.8303  -1.9278 -0.3366 0.02494
## 81 3.34 1.27 2.96        0 0.33222  -0.3322  -0.9633 -0.4596 0.02133
## 77 3.01 1.53 3.01        1 0.41376   0.5862   1.3781  0.4014 0.02101
## 21 3.37 1.65 4.16        1 0.57404   0.4260   1.1068  0.4163 0.01894
## 61 4.26 1.73 2.10        1 0.26659   0.7334   1.6620  0.3239 0.01759
## 70 2.08 1.62 4.55        1 0.75821   0.2418   0.7970  0.3930 0.01463
## 52 3.44 2.35 3.37        0 0.68034  -0.6803  -1.5371 -0.2832 0.01210
## 58 2.52 1.30 2.13        0 0.24723  -0.2472  -0.7985 -0.3562 0.01207
## 
## 
##    FORECASTS 
## 
## Probability threshold for predicting : 0.5
## 
## 
## Data, Fitted Values, Standard Errors
##    [sorted by fitted value]
##    [to save space only some intervals printed,   pred_all=TRUE to see all]
## --------------------------------------------------------------------
##      JS NAff   TI Turnover predict  fitted std.err
## 1  4.96 1.87 0.51        1       0 0.03086 0.05764
## 99 4.06 2.19 0.65        0       0 0.04451 0.06863
## 88 4.07 2.21 1.21        0       0 0.14009 0.09587
## 46 3.79 1.45 1.83        0       0 0.15073 0.09141
## 
## ... for the rows of data where fitted is close to 0.5 ...
## 
##      JS NAff   TI Turnover predict fitted std.err
## 49 4.63 2.45 2.01        0       0 0.4820  0.1237
## 86 3.42 2.23 2.57        1       0 0.4962  0.1032
## 28 2.67 1.95 2.68        1       0 0.4997  0.1070
## 74 3.50 3.28 1.37        0       1 0.5071  0.2402
## 13 2.98 2.32 2.41        0       1 0.5145  0.1018
## 
## ... for the last 4 rows of sorted data ...
## 
##      JS NAff   TI Turnover predict fitted  std.err
## 95 2.48 3.18 4.74        1       1 0.9499 0.063267
## 19 2.73 3.64 3.36        1       1 0.9537 0.060780
## 87 0.67 2.56 3.08        1       1 0.9890 0.020304
## 17 0.23 2.38 4.20        1       1 0.9988 0.003638
## --------------------------------------------------------------------
## 
## 
## ----------------------------
## Specified confusion matrices
## ----------------------------
## 
## Probability threshold for predicting : 0.5
## 
##                  Baseline         Predicted 
## ---------------------------------------------------
##                 Total  %Tot        0      1  %Correct 
## ---------------------------------------------------
##            0       37  38.9       21     16     56.8 
## Turnover   1       58  61.1        8     50     86.2 
## ---------------------------------------------------
##          Total     95                           74.7 
## 
## Accuracy: 74.74 
## Recall: 86.21 
## Precision: 75.76</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1040-1.png" width="672" /></p>
<p>Because the <em>p</em>-values associated with each of the interaction terms and their regression coefficients (<code>JS:log(JS)</code>, <code>NAff:log(NAff)</code>, <code>TI:log(TI)</code>) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio).</p>
<p><strong>Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the <code>log</code> function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the <em>p</em>-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a <a href="logistic.html#teststatisticalassumptions_simple_logistic">previous section on simple logistic regression</a>.</strong></p>
</div>
<div id="interpret_multiple_logistic" class="section level4" number="49.2.5.2">
<h4><span class="header-section-number">49.2.5.2</span> Interpret Model Results</h4>
<p><strong>Basic Analysis:</strong> The <em>Basic Analysis</em> section of the original output first displays a table called the <em>Model Coefficients</em>, which includes the regression coefficients (slopes, weights) and their standard errors, <em>z</em>-values, <em>p</em>-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (<code>JS</code>) in relation to the logit transformation of the outcome variable (<code>Turnover</code>) is not statistically significant when controlling for the other predictor variables in the model because the associated <em>p</em>-value is equal to or greater than .05 (<em>b</em> = -.233, <em>p</em> = .293, 95% CI[-0.667, .201]). The regression coefficient for <code>NAff</code> is 1.195 and its associated <em>p</em>-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (<em>b</em> = 1.195, <em>p</em> = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for <code>TI</code> is .897 and its associated <em>p</em>-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (<em>b</em> = .897, <em>p</em> = .005, 95% CI[.276, 1.517]).</p>
<p>Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, in some contexts we might not write out the regression equation with that variable included; instead, we might re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(\ln(\frac{p}{1-p}) = -3.929 - .233 \times JS_{observed} + 1.195 \times NAff_{observed} + .897 \times TI_{observed}\)</span></p>
<p>where <span class="math inline">\(p\)</span> refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to <span class="math inline">\(\log(odds)\)</span>.</p>
<p><span class="math inline">\(\log(odds) = \ln(\frac{p}{1-p}) = -3.929 - .233 \times JS_{observed} + 1.195 \times NAff_{observed} + .897 \times TI_{observed}\)</span></p>
<p>To that end, to aid our interpretation of the significant finding, we can move our attention to the table called <em>Odds ratios and confidence intervals</em>. To convert our regression coefficients to odds ratios, the function is simply exponentiating them. Behind the scenes, this is what happened:</p>
<p><span class="math inline">\(e^{1.195} = 3.304\)</span></p>
<p>We can also do this manually using the <code>exp</code> function from base R. Any difference between the <code>Logit</code> output and the output below is attributable to rounding.</p>
<div class="sourceCode" id="cb1854"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1854-1"><a href="logistic.html#cb1854-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For the sake of demonstration:</span></span>
<span id="cb1854-2"><a href="logistic.html#cb1854-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1854-3"><a href="logistic.html#cb1854-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the Logit function already does this for us</span></span>
<span id="cb1854-4"><a href="logistic.html#cb1854-4" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">1.195</span>)</span></code></pre></div>
<pre><code>## [1] 3.303558</code></pre>
<p>In the <em>Odds ratios and confidence intervals</em> table, we see that the odds ratios are automatically computed for us. We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belong to <code>NAff</code> and <code>TI</code>. Regarding <code>NAff</code>, its odds ratio of 3.304 is <em>greater than</em> 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times greater for every one unit increase in <code>NAff</code> when controlling for other predictor variables in the model. Regarding <code>TI</code>, its odds ratio of 2.451 is also <em>greater than</em> 1, and thus, the odds of quitting are 2.451 times greater for every one unit increase in <code>TI</code> when controlling for other predictor variables in the model. Note that the odds ratio (OR) can be conceptualized as a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude.</p>
<table>
<thead>
<tr class="header">
<th align="center">Odds Ratio &gt; 1</th>
<th align="center">Odds Ratio &lt; 1</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.2</td>
<td align="center">.8</td>
<td align="center">Small</td>
</tr>
<tr class="even">
<td align="center">2.5</td>
<td align="center">.4</td>
<td align="center">Medium</td>
</tr>
<tr class="odd">
<td align="center">4.3</td>
<td align="center">.2</td>
<td align="center">Large</td>
</tr>
</tbody>
</table>
<p>In the <em>Model Fit</em> table, note that we don’t have an estimate of <em>R</em>-squared (<em>R</em><sup>2</sup>) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-<em>R</em>-squared (<em>R</em><sup>2</sup>) values, but for now let’s focus on what is produced in the <code>Logit</code> function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data by looking at the <em>Specified Confusion Matrices</em> table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 37 (38.9%) and 58 (61.1%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 89.7% accuracy. Overall, we tend to be most interested in the overall percentage of correct classifications, which is 78.9%. That is, based on the logistic regression model that included the predictor variables of <code>JS</code>, <code>NAff</code>, and <code>TI</code>, we were able to predict actual turnover behavior from the same sample 78.9% of the time, which was a big improvement over the simple logistic regression model in which we had just <code>JS</code> as a predictor variable.</p>
<p><strong>Forecasts:</strong> In the output section called <em>Forecasts</em>, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true <em>predictive analytics</em> and <em>machine learning</em>; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one <em>training</em> dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a <em>test</em> dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a <em>validation</em> dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results.</p>
<p><strong>Nagelkerke pseudo-<em>R</em><sup>2</sup>:</strong> To compute Nagelkerke’s pseudo-<em>R</em><sup>2</sup>, we will need to install and access the <code>DescTools</code> package (if we haven’t already) so that we can use its <code>PseudoR2</code> function.</p>
<div class="sourceCode" id="cb1856"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1856-1"><a href="logistic.html#cb1856-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1856-2"><a href="logistic.html#cb1856-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;DescTools&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1857"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1857-1"><a href="logistic.html#cb1857-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1857-2"><a href="logistic.html#cb1857-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span></code></pre></div>
<p>To use the function, we’ll need to re-estimate our multiple logistic regression model using the <code>glm</code> function from base R. To request a logistic regression model as a specific type of generalized linear model, we’ll add the <code>family=binomial</code> argument. Using the <code>&lt;-</code> assignment operator, we will assign the resulting estimated model to an object that we’ll arbitrarily call <code>model2</code>.</p>
<div class="sourceCode" id="cb1858"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1858-1"><a href="logistic.html#cb1858-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model and assign to object</span></span>
<span id="cb1858-2"><a href="logistic.html#cb1858-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> NAff <span class="sc">+</span> TI, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span></code></pre></div>
<p>In the <code>PseudoR2</code> function, we will specify the name of the model object (<code>model2</code>) as the first argument. As the second argument, type <code>"Nagel"</code> to request pseudo-<em>R</em><sup>2</sup> calculated using Nagelkerke’s formula.</p>
<div class="sourceCode" id="cb1859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1859-1"><a href="logistic.html#cb1859-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Nagelkerke pseudo-R-squared</span></span>
<span id="cb1859-2"><a href="logistic.html#cb1859-2" aria-hidden="true" tabindex="-1"></a><span class="fu">PseudoR2</span>(model2, <span class="st">&quot;Nagel&quot;</span>)</span></code></pre></div>
<pre><code>## Nagelkerke 
##  0.2779516</code></pre>
<p>The estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-<em>R</em><sup>2</sup> was just .073. Remember, a pseudo-<em>R</em><sup>2</sup> is not the exact same thing as a true <em>R</em><sup>2</sup>, so we should interpret it with caution. With caution, we can conclude that <code>JS</code>, <code>NAff</code>, and <code>TI</code> explain 27.8% of the variance in <code>Turnover</code>.</p>
<p>Because the <code>DescTools</code> package also has a function called <code>Logit</code>, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the <code>Logit</code> function from <code>DescTools</code> as opposed to the one from <code>lessR</code>.</p>
<div class="sourceCode" id="cb1861"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1861-1"><a href="logistic.html#cb1861-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detach package</span></span>
<span id="cb1861-2"><a href="logistic.html#cb1861-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:DescTools&quot;</span>, <span class="at">character.only=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>Technical Write-Up of Results:</strong> A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (<code>Turnover</code>) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was <em>not</em> associated with turnover behavior to a statistically significant extent (<em>b</em> = -.233, <em>p</em> = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (<em>b</em> = 1.195, <em>p</em> = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (<em>b</em> = .897, <em>p</em> = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover.</p>
</div>
</div>
<div id="summary_logistic" class="section level3" number="49.2.6">
<h3><span class="header-section-number">49.2.6</span> Summary</h3>
<p>In this chapter, we learned how to estimate simple and multiple logistic regression models using the <code>Logit</code> function from <code>lessR</code>.</p>
</div>
</div>
<div id="logistic_supplement" class="section level2" number="49.3">
<h2><span class="header-section-number">49.3</span> Chapter Supplement</h2>
<p>In addition to the <code>Logit</code> function from the <code>lessR</code> package covered <a href="logistic.html#estimate_multiple_logistic">above</a>, we can use the <code>glm</code> function from base R to estimate a simple and multiple logistic regression models and the <code>predict</code> function from base R to predict probabilities of the event in question occurring. Because these functions come from base R, we do not need to install and access an additional package.</p>
<div id="logistic_supplement_functions" class="section level3" number="49.3.1">
<h3><span class="header-section-number">49.3.1</span> Functions &amp; Packages Introduced</h3>
<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>glm</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>log</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>exp</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>plot</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>cooks.distance</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>summary</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>confint</code></td>
<td>base R / <code>MASS</code></td>
</tr>
<tr class="even">
<td><code>coef</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>subset</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>na.omit</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>xtabs</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>print</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>prop.table</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>predict</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>sum</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>scale</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>vif</code></td>
<td><code>car</code></td>
</tr>
<tr class="even">
<td><code>PseudoR2</code></td>
<td><code>DescTools</code></td>
</tr>
<tr class="odd">
<td><code>c</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>merge</code></td>
<td>base R</td>
</tr>
<tr class="odd">
<td><code>data.frame</code></td>
<td>base R</td>
</tr>
<tr class="even">
<td><code>mutate</code></td>
<td><code>dplyr</code></td>
</tr>
<tr class="odd">
<td><code>ifelse</code></td>
<td>base R</td>
</tr>
</tbody>
</table>
</div>
<div id="logistic_initsteps_supplement" class="section level3" number="49.3.2">
<h3><span class="header-section-number">49.3.2</span> Initial Steps</h3>
<p>If required, please refer to the <a href="logistic.html#initsteps_logistic">Initial Steps</a> section from this chapter for more information on these initial steps.</p>
<div class="sourceCode" id="cb1862"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1862-1"><a href="logistic.html#cb1862-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set your working directory</span></span>
<span id="cb1862-2"><a href="logistic.html#cb1862-2" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(<span class="st">&quot;H:/RWorkshop&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1863"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1863-1"><a href="logistic.html#cb1863-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install readr package if you haven&#39;t already</span></span>
<span id="cb1863-2"><a href="logistic.html#cb1863-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [Note: You don&#39;t need to install a package every </span></span>
<span id="cb1863-3"><a href="logistic.html#cb1863-3" aria-hidden="true" tabindex="-1"></a><span class="co"># time you wish to access it]</span></span>
<span id="cb1863-4"><a href="logistic.html#cb1863-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;readr&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1864"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1864-1"><a href="logistic.html#cb1864-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access readr package</span></span>
<span id="cb1864-2"><a href="logistic.html#cb1864-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb1864-3"><a href="logistic.html#cb1864-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1864-4"><a href="logistic.html#cb1864-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data and name data frame (tibble) object</span></span>
<span id="cb1864-5"><a href="logistic.html#cb1864-5" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;Turnover.csv&quot;</span>)</span></code></pre></div>
<pre><code>## Rows: 99 Columns: 6</code></pre>
<pre><code>## -- Column specification ------------------------------------------------------------------------------------------------------------------------
## Delimiter: &quot;,&quot;
## chr (1): ID
## dbl (5): Turnover, JS, OC, TI, NAff</code></pre>
<pre><code>## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb1868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1868-1"><a href="logistic.html#cb1868-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the names of the variables in the data frame (tibble) object</span></span>
<span id="cb1868-2"><a href="logistic.html#cb1868-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(td)</span></code></pre></div>
<pre><code>## [1] &quot;ID&quot;       &quot;Turnover&quot; &quot;JS&quot;       &quot;OC&quot;       &quot;TI&quot;       &quot;NAff&quot;</code></pre>
<div class="sourceCode" id="cb1870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1870-1"><a href="logistic.html#cb1870-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View variable type for each variable in data frame (tibble) object</span></span>
<span id="cb1870-2"><a href="logistic.html#cb1870-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(td)</span></code></pre></div>
<pre><code>## spec_tbl_df [99 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ ID      : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ...
##  $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ...
##  $ JS      : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ...
##  $ OC      : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ...
##  $ TI      : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ...
##  $ NAff    : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ...
##  - attr(*, &quot;spec&quot;)=
##   .. cols(
##   ..   ID = col_character(),
##   ..   Turnover = col_double(),
##   ..   JS = col_double(),
##   ..   OC = col_double(),
##   ..   TI = col_double(),
##   ..   NAff = col_double()
##   .. )
##  - attr(*, &quot;problems&quot;)=&lt;externalptr&gt;</code></pre>
<div class="sourceCode" id="cb1872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1872-1"><a href="logistic.html#cb1872-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View first 6 rows of data frame (tibble) object</span></span>
<span id="cb1872-2"><a href="logistic.html#cb1872-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   ID     Turnover    JS    OC    TI  NAff
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 EMP559        1  4.96  5.32  0.51  1.87
## 2 EMP561        1  1.72  1.47  4.08  2.48
## 3 EMP571        1  1.64  0.87  2.65  2.84
## 4 EMP589        1  3.01  2.15  4.17  2.43
## 5 EMP592        1  3.04  1.94  3.27  2.76
## 6 EMP601        1  3.81  3.81  3.01  3.67</code></pre>
</div>
<div id="glm_function_simple_logistic" class="section level3" number="49.3.3">
<h3><span class="header-section-number">49.3.3</span> Simple Logistic Regression Model Using <code>glm</code> Function from Base R</h3>
<p>The <code>glm</code> function from base R stands for “generalized linear model (GLM),” making it an appropriate function for estimating logistic regression models and other models in the GLM family; logistic regression model is a GLM with a logit link function. Let’s begin by specifying <code>JS</code> as a predictor variable for <code>Turnover</code>.</p>
<ol style="list-style-type: decimal">
<li>Using the <code>&lt;-</code> assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., <code>model1</code>).</li>
<li>Type the name of the <code>glm</code> function.</li>
</ol>
<ul>
<li>As the function’s first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variable is typed to the right of the <code>~</code> symbol.</li>
<li>As the second argument, type <code>data=</code> followed by the name of the data frame (<code>td</code>) to which both of the variables belong.</li>
<li>As the third argument, type <code>family=binomial</code>, which by default estimates a logit (logistic) model.</li>
</ul>
<div class="sourceCode" id="cb1874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1874-1"><a href="logistic.html#cb1874-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model</span></span>
<span id="cb1874-2"><a href="logistic.html#cb1874-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span></code></pre></div>
<p><em>Note:</em> You won’t see a summary of the results in the Console, as we have not requested those. As a next step, we will use the <code>model1</code> object to determine whether we have satisfied the statistical assumptions of a simple logistic regression model.</p>
<div id="glm_teststatisticalassumptions_simple_logistic" class="section level4" number="49.3.3.1">
<h4><span class="header-section-number">49.3.3.1</span> Test Statistical Assumptions</h4>
<p>To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a>.</p>
<p><strong>Cases Are Randomly Sampled from the Population:</strong> As mentioned in the <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a> section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied.</p>
<p><strong>Outcome Variable Is Dichotomous:</strong> We already know that the outcome variable called <code>Turnover</code> is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption.</p>
<p><strong>Data Are Free of Bivariate Outliers:</strong> To determine whether the data are free of bivariate outliers, let’s look at Cook’s distance (D) values across cases. Using the <code>plot</code> function from base R, type the name of our model object (<code>model1</code>) as the first argument, and as the second argument, enter the numeral <em>4</em> to request the fourth diagnostic plot, which is the Cook’s distance plot.</p>
<div class="sourceCode" id="cb1875"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1875-1"><a href="logistic.html#cb1875-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics plot: Cook&#39;s Distance plot</span></span>
<span id="cb1875-2"><a href="logistic.html#cb1875-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model1, <span class="dv">4</span>)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1053-1.png" width="672" /></p>
<p>The case associated with row number <em>66</em> has the highest Cook’s distance value, followed by the cases associated with row numbers <em>67</em> and <em>71</em> – although <em>66</em> and <em>67</em> seem to be clearest potential outliers of the three.</p>
<p>We can also print the cases with the highest Cook’s distances. To do so, let’s create an object called <code>cooksD</code> to which we will assign a vector of Cook’s distance values using the <code>cooks.distance</code> function from base R. Just type the name of the logistic regression model object (<code>model1</code>) as the sole parenthetical argument. Next, update the object we called <code>cooksD</code> by applying the <code>sort</code> function from base R and typing the <code>cooksD</code> object name as the first object and <code>decreasing=TRUE</code> as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the <code>head</code> function from base R, and as the first argument enter the name of the <code>cooksD</code> object that we just sorted; as the second argument, type <code>n=20</code> to show the top 20 rows, as opposed to the default top 6 rows.</p>
<div class="sourceCode" id="cb1876"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1876-1"><a href="logistic.html#cb1876-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate Cook&#39;s distance values</span></span>
<span id="cb1876-2"><a href="logistic.html#cb1876-2" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model1)</span>
<span id="cb1876-3"><a href="logistic.html#cb1876-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1876-4"><a href="logistic.html#cb1876-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort Cook&#39;s distance values in descending order               </span></span>
<span id="cb1876-5"><a href="logistic.html#cb1876-5" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">sort</span>(cooksD, <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb1876-6"><a href="logistic.html#cb1876-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1876-7"><a href="logistic.html#cb1876-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View top 20 Cook&#39;s distance values</span></span>
<span id="cb1876-8"><a href="logistic.html#cb1876-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(cooksD, <span class="at">n=</span><span class="dv">20</span>)</span></code></pre></div>
<pre><code>##         69          7         73         58         12         31         13          1         33         84         63         61         97 
## 0.08495602 0.06240864 0.04888996 0.04617863 0.04352935 0.04117397 0.03313842 0.02608930 0.02352896 0.01756755 0.01733141 0.01693066 0.01692984 
##         70         74         75         67         80         77         39 
## 0.01648318 0.01614505 0.01595550 0.01453516 0.01431210 0.01336026 0.01281733</code></pre>
<p>Again, we see cases associated with row numbers <em>66</em> (.085), <em>67</em> (.062), and <em>71</em> (.049) having the highest Cook’s distance values, with <em>66</em> and <em>67</em> still looking like the most influential cases of the lot. As a sensitivity analysis, we <em>could</em> estimate our model once more after removing the cases associated with row numbers <em>66</em> and <em>67</em> from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cook’s distance values are all well-below the more liberal threshold, so I would say we’re safe to leave the associated cases in the data.</p>
<p><strong>Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear:</strong> To test the assumption of linearity between a <em>continuous</em> predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. <strong>[Note: We do not perform the following test/approach for <em>categorical</em> predictor variables.]</strong> We will use an approach that is commonly referred to as the Box-Tidwell approach <span class="citation">(<a href="#ref-hosmerlemeshow2000" role="doc-biblioref">Hosmer and Lemeshow 2000</a>)</span>. To apply this approach, we need to add the interaction term between our predictor variable <code>JS</code> and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of <code>JS</code>. In our regression model formula, specify the dichotomous outcome variable <code>Turnover</code> to the left of the <code>~</code> operator. To the right of the <code>~</code> operator, type the name of the predictor variable <code>JS</code> followed by the <code>+</code> operator. After the <code>+</code> operator, type the name of the predictor variable <code>JS</code>, followed by the <code>:</code> operator and the <code>log</code> function from base R with the predictor variable <code>JS</code> as its sole parenthetical argument. Finally, we will use the <code>summary</code> function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (<code>boxtidwell</code>).</p>
<div class="sourceCode" id="cb1878"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1878-1"><a href="logistic.html#cb1878-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1878-2"><a href="logistic.html#cb1878-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable</span></span>
<span id="cb1878-3"><a href="logistic.html#cb1878-3" aria-hidden="true" tabindex="-1"></a>boxtidwell <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> JS<span class="sc">:</span><span class="fu">log</span>(JS), <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1878-4"><a href="logistic.html#cb1878-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1878-5"><a href="logistic.html#cb1878-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1878-6"><a href="logistic.html#cb1878-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boxtidwell)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS + JS:log(JS), family = binomial, 
##     data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9972  -1.1790   0.7179   1.0762   1.2077  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   5.4461     3.2062   1.699   0.0894 .
## JS           -2.9840     2.1693  -1.376   0.1690  
## JS:log(JS)    1.1696     0.9778   1.196   0.2316  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 131.75  on 97  degrees of freedom
## Residual deviance: 124.62  on 95  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 130.62
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Because the interaction term (<code>JS:log(JS)</code>) regression coefficient of 1.1696 in the <em>Coefficients</em> table is nonsignificant (<em>p</em> = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is <em>non</em>linear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (“Curvilinear Effects in Logistic Regression”) from Osborne <span class="citation">(<a href="#ref-osborne2015" role="doc-biblioref">2015</a>)</span>. Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well.</p>
<p><strong>Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the <code>log</code> function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the <em>p</em>-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis.</strong></p>
<p>Just for the sake of demonstration, let’s transform the <code>JS</code> variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (<code>JS_0</code>) that is equal to <code>JS</code> minus the minimum value of <code>JS</code>.</p>
<div class="sourceCode" id="cb1880"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1880-1"><a href="logistic.html#cb1880-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero</span></span>
<span id="cb1880-2"><a href="logistic.html#cb1880-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>JS_0 <span class="ot">&lt;-</span> td<span class="sc">$</span>JS <span class="sc">-</span> <span class="fu">min</span>(td<span class="sc">$</span>JS, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>Now that we have a variable called <code>JS_0</code> with at least one score equal to zero, let’s try the try the Box-Tidwell test. I’m going to add the argument <code>brief=TRUE</code> to reduce the amount of output generated by the function.</p>
<div class="sourceCode" id="cb1881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1881-1"><a href="logistic.html#cb1881-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1881-2"><a href="logistic.html#cb1881-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with predictor containing zero value(s)]</span></span>
<span id="cb1881-3"><a href="logistic.html#cb1881-3" aria-hidden="true" tabindex="-1"></a>boxtidwell <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS_0 <span class="sc">+</span> JS_0<span class="sc">:</span><span class="fu">log</span>(JS_0), <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1881-4"><a href="logistic.html#cb1881-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1881-5"><a href="logistic.html#cb1881-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1881-6"><a href="logistic.html#cb1881-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boxtidwell)</span></code></pre></div>
<p>If you ran the script above, you likely got an error message that looked like this:</p>
<p><span class="math inline">\(\color{red}{\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\)</span></p>
<p>The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on <code>JS_0</code>, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above).</p>
<p>Using the first approach, create a new variable in which the <code>JS_0</code> variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the <code>JS_0</code> variable, which results in the lowest score on the new variable (<code>JS_1</code>) being 1. As verification, I include the <code>min</code> function from base R.</p>
<div class="sourceCode" id="cb1882"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1882-1"><a href="logistic.html#cb1882-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear transformation that results in lowest score being 1</span></span>
<span id="cb1882-2"><a href="logistic.html#cb1882-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>JS_1 <span class="ot">&lt;-</span> td<span class="sc">$</span>JS_0 <span class="sc">+</span> <span class="fu">abs</span>(<span class="fu">min</span>(td<span class="sc">$</span>JS_0, <span class="at">na.rm=</span><span class="cn">TRUE</span>)) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1882-3"><a href="logistic.html#cb1882-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1882-4"><a href="logistic.html#cb1882-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Verify that new lowest score is 1</span></span>
<span id="cb1882-5"><a href="logistic.html#cb1882-5" aria-hidden="true" tabindex="-1"></a><span class="fu">min</span>(td<span class="sc">$</span>JS_1, <span class="at">na.rm=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test.</p>
<div class="sourceCode" id="cb1884"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1884-1"><a href="logistic.html#cb1884-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1884-2"><a href="logistic.html#cb1884-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with transformed predictor = 1]</span></span>
<span id="cb1884-3"><a href="logistic.html#cb1884-3" aria-hidden="true" tabindex="-1"></a>boxtidwell <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS_1 <span class="sc">+</span> JS_1<span class="sc">:</span><span class="fu">log</span>(JS_1), <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1884-4"><a href="logistic.html#cb1884-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1884-5"><a href="logistic.html#cb1884-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1884-6"><a href="logistic.html#cb1884-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boxtidwell)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS_1 + JS_1:log(JS_1), family = binomial, 
##     data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0004  -1.1792   0.7129   1.0780   1.2090  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)       8.099      5.008   1.617    0.106
## JS_1             -4.059      2.977  -1.363    0.173
## JS_1:log(JS_1)    1.510      1.226   1.231    0.218
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 131.75  on 97  degrees of freedom
## Residual deviance: 124.57  on 95  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 130.57
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (<code>JS_1:log(JS_1)</code>) is nonsignificant (<em>b</em> = 1.510, <em>p</em> = .218). Thus, we don’t see evidence that the assumption of linearity has been violated.</p>
<p>We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the <code>rows=</code> argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that we’re back to using the variable called <code>JS_0</code> that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial).</p>
<div class="sourceCode" id="cb1886"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1886-1"><a href="logistic.html#cb1886-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variable and</span></span>
<span id="cb1886-2"><a href="logistic.html#cb1886-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable [with only cases with scores greater than zero]</span></span>
<span id="cb1886-3"><a href="logistic.html#cb1886-3" aria-hidden="true" tabindex="-1"></a>boxtidwell <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS_0 <span class="sc">+</span> JS_0<span class="sc">:</span><span class="fu">log</span>(JS_0), <span class="at">data=</span>td, <span class="at">family=</span>binomial, <span class="at">subset=</span>(JS_0 <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb1886-4"><a href="logistic.html#cb1886-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1886-5"><a href="logistic.html#cb1886-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1886-6"><a href="logistic.html#cb1886-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boxtidwell)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS_0 + JS_0:log(JS_0), family = binomial, 
##     data = td, subset = (JS_0 &gt; 0))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9868  -1.1763   0.7365   1.0743   1.2079  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)       4.665      2.822   1.653   0.0982 .
## JS_0             -2.616      1.994  -1.312   0.1895  
## JS_0:log(JS_0)    1.039      0.928   1.120   0.2628  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 130.72  on 96  degrees of freedom
## Residual deviance: 124.62  on 94  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 130.62
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We lost one case because that person had a score of zero on the <code>JS_0</code> continuous predictor variable, and we see that the interaction term (<code>JS_0:log(JS_0)</code>) is non significant (<em>b</em> = 1.039, <em>p</em> = .263).</p>
<p><strong>To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your <em>continuous</em> predictor variable has scores that are equal to or less than zero.</strong> Now that we’ve met the assumption of linearity, we’re finally ready to interpret the model results!</p>
</div>
<div id="glm_interpret_simple_logistic" class="section level4" number="49.3.3.2">
<h4><span class="header-section-number">49.3.3.2</span> Interpret Model Results</h4>
<p>Now we’re ready to interpret the results of the simple logistic regression model. For clarity, let’s re-specify our original simple logistic regression model from above. To apply the <code>glm</code> function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variable is typed to the right of the <code>~</code> symbol. As the second argument, type <code>data=</code> followed by the name of the data frame (<code>td</code>) to which both of the variables belong. Third, type the argument <code>family=binomial</code>, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the <code>&lt;-</code> assignment operator; here, I again arbitrarily named the object <code>model1</code>. Finally, use the <code>summary</code> function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (<code>model1</code>).</p>
<div class="sourceCode" id="cb1888"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1888-1"><a href="logistic.html#cb1888-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model</span></span>
<span id="cb1888-2"><a href="logistic.html#cb1888-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1888-3"><a href="logistic.html#cb1888-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1888-4"><a href="logistic.html#cb1888-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1888-5"><a href="logistic.html#cb1888-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS, family = binomial, data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7337  -1.2149   0.7806   0.9945   1.5175  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   1.8554     0.6883   2.695  0.00703 **
## JS           -0.4378     0.1958  -2.236  0.02538 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 131.75  on 97  degrees of freedom
## Residual deviance: 126.34  on 96  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 130.34
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The output generates the model coefficient estimates and model fit (i.e., AIC) information.</p>
<p><strong>Coefficients:</strong> The <em>Coefficients</em> section of the output displays the regression coefficients (slopes, weights) and their standard errors, <em>z</em>-values, and <em>p</em>-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (<code>JS</code>) in relation to the outcome variable (<code>Turnover</code>) is often of substantive interest. Here, we see that the unstandardized regression coefficient for <code>JS</code> is -.438, and its associated <em>p</em>-value is less than .05 (<em>b</em> = -.438, <em>p</em> = .025). Given that the <em>p</em>-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: <em>For every one unit increase in the predictor variable (<code>JS</code>), the logit transformation of the outcome variable (<code>Turnover</code>) decreases by .438 units.</em> Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(\ln(\frac{p}{1-p}) = 1.855 -.438 \times JS_{observed}\)</span></p>
<p>where <span class="math inline">\(p\)</span> refers to, in this example, as the probability of quitting.</p>
<p><strong>Confidence Intervals:</strong> To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the <code>confint</code> function from base R with the name of our model as the sole argument (<code>model1</code>).</p>
<div class="sourceCode" id="cb1890"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1890-1"><a href="logistic.html#cb1890-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate 95% confidence intervals</span></span>
<span id="cb1890-2"><a href="logistic.html#cb1890-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model1)</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                  2.5 %      97.5 %
## (Intercept)  0.5644186  3.28485115
## JS          -0.8412956 -0.06705288</code></pre>
<p>The 95% confidence interval for <code>JS</code> ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values.</p>
<p><strong>Odds Ratios:</strong> If you recall from earlier in the tutorial, we can also interpret our findings with respect to <span class="math inline">\(\log(odds)\)</span>.</p>
<p><span class="math inline">\(\log(odds) = \ln(\frac{p}{1-p}) = 1.855 -.438 \times JS_{observed}\)</span></p>
<p>To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it - for example:</p>
<p><span class="math inline">\(e^{-.438} = .646\)</span></p>
<p>We can also do this using R by applying the <code>exp</code> function from base R. Specifically, within the <code>exp</code> function parentheses, type the name of the <code>coef</code> function from base R, which extracts the regression coefficients from the model object. As the sole argument within the <code>coef</code> function parentheses, enter the name of the model we previously specified (<code>model1</code>).</p>
<div class="sourceCode" id="cb1893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1893-1"><a href="logistic.html#cb1893-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1893-2"><a href="logistic.html#cb1893-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model1))</span></code></pre></div>
<pre><code>## (Intercept)          JS 
##   6.3939475   0.6454756</code></pre>
<p>In the output, we see that indeed the odds ratio is approximately .646. Because the odds ratio is <em>less than</em> 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (100 <span class="math inline">\(\times\)</span> .354) for every one unit increase in job satisfaction (<code>JS</code>). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of <em>not</em> quitting (i.e., staying): The odds of <em>not</em> quitting are 1.548 times as likely for every one unit increase in job satisfaction (<code>JS</code>). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1.</p>
<table>
<thead>
<tr class="header">
<th align="center">Odds Ratio &gt; 1</th>
<th align="center">Odds Ratio &lt; 1</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.2</td>
<td align="center">.8</td>
<td align="center">Small</td>
</tr>
<tr class="even">
<td align="center">2.5</td>
<td align="center">.4</td>
<td align="center">Medium</td>
</tr>
<tr class="odd">
<td align="center">4.3</td>
<td align="center">.2</td>
<td align="center">Large</td>
</tr>
</tbody>
</table>
<p>To get the 95% confidence intervals for the odds ratio, we can nest the <code>confint</code> function within the <code>exp</code> function.</p>
<div class="sourceCode" id="cb1895"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1895-1"><a href="logistic.html#cb1895-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1895-2"><a href="logistic.html#cb1895-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(model1))</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 1.7584252 26.7050090
## JS          0.4311515  0.9351458</code></pre>
<p><strong>Model Fit &amp; Performance:</strong> Returning to the original output of the simple logistic regression model, let’s examine the section that contains information about model fit. Note that we don’t have an estimate of <em>R</em>-squared (<em>R</em><sup>2</sup>) like we would with a traditional linear regression model. Later on, we will compute the pseudo-<em>R</em>-squared (<em>R</em><sup>2</sup>) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (<em>N</em> = 98). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial.</p>
<p>For our purposes, we will assess the model’s fit to the data initially by creating a confusion matrix in which we display the model’s accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 98 for the analysis. Thus, we should drop the case with missing data prior to estimating our baseline data and confusion matrix. We’ll start by using the <code>subset</code> function from base R to select just the two variables from our data frame that we used in our logistic regression model: <code>Turnover</code> and <code>JS</code>. As the first argument, type the name of the data frame (<code>td</code>). As the second argument, type <code>select=</code> followed by the <code>c</code> (combine) function from base R. Within the <code>c</code> function parentheses, enter the names of the two variables we wish to retain (<code>Turnover</code>, <code>JS</code>). Using the <code>&lt;-</code> assignment operator, create a new data frame object, and here I arbitrarily call this object <code>td_short</code>.</p>
<div class="sourceCode" id="cb1898"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1898-1"><a href="logistic.html#cb1898-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new short data frame with just the retained variables</span></span>
<span id="cb1898-2"><a href="logistic.html#cb1898-2" aria-hidden="true" tabindex="-1"></a>td_short <span class="ot">&lt;-</span> <span class="fu">subset</span>(td, <span class="at">select=</span><span class="fu">c</span>(Turnover, JS))</span></code></pre></div>
<p>Next, apply the <code>na.omit</code> function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (<code>td_short</code>). Using the <code>&lt;-</code> assignment operator, overwrite the <code>td_short</code> data frame by entering its name to the left of the <code>&lt;-</code> assignment operator.</p>
<div class="sourceCode" id="cb1899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1899-1"><a href="logistic.html#cb1899-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop cases with missing data in the short data frame</span></span>
<span id="cb1899-2"><a href="logistic.html#cb1899-2" aria-hidden="true" tabindex="-1"></a>td_short <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(td_short)</span></code></pre></div>
<p>There are different ways we can estimate our baseline, but let’s keep it simple and use the <code>xtabs</code> function from base R. As the first argument, type the <code>~</code> followed the name of the short data frame <code>td_short</code>, followed by the <code>$</code> and the outcome variable (<code>Turnover</code>). Use the <code>&lt;-</code> operator to create and name a new table object, where here I arbitrarily call it <code>table1</code>. Use the <code>print</code> function to view the table object (<code>table1</code>). Remember that for the <code>Turnover</code> variable quit = 1 and stay = 0.</p>
<div class="sourceCode" id="cb1900"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1900-1"><a href="logistic.html#cb1900-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of cases at each level of outcome variable</span></span>
<span id="cb1900-2"><a href="logistic.html#cb1900-2" aria-hidden="true" tabindex="-1"></a>table1 <span class="ot">&lt;-</span> <span class="fu">xtabs</span>(<span class="sc">~</span> td_short<span class="sc">$</span>Turnover)</span>
<span id="cb1900-3"><a href="logistic.html#cb1900-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1900-4"><a href="logistic.html#cb1900-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the table</span></span>
<span id="cb1900-5"><a href="logistic.html#cb1900-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(table1)</span></code></pre></div>
<pre><code>## td_short$Turnover
##  0  1 
## 39 59</code></pre>
<p>As you can see, 39 people actually stayed and 59 people actually quit. To convert these to proportions, apply the <code>prop.table</code> function from base R to the table object we created in the previous step (<code>table1</code>).</p>
<div class="sourceCode" id="cb1902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1902-1"><a href="logistic.html#cb1902-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of cases at each level of outcome variable</span></span>
<span id="cb1902-2"><a href="logistic.html#cb1902-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(table1)</span></code></pre></div>
<pre><code>## td_short$Turnover
##         0         1 
## 0.3979592 0.6020408</code></pre>
<p>Based on this output, we see that 39.8% of people stayed and 60.2% quit.</p>
<p>Now we’re ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (<code>pred.prob</code>), using the <code>&lt;-</code> assignment operator. Next, type the name of the <code>predict</code> function from base R. As the first argument, type the name of our logistic regression model (<code>model1</code>). As the second argument, enter the argument <code>type="response"</code> to indicate that you want to predict the outcome (response) variable.</p>
<div class="sourceCode" id="cb1904"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1904-1"><a href="logistic.html#cb1904-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate predicted probabilities </span></span>
<span id="cb1904-2"><a href="logistic.html#cb1904-2" aria-hidden="true" tabindex="-1"></a>pred.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<p>We now need to dichotomize the vector of predicted probabilities (<code>pred.prob</code>), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new vector called <code>dich.pred.prob</code> (or whatever you would like to call it) based on the values from the <code>pred.prob</code> vector you created in the previous step. Next, for the <code>dich.pred.prob</code>, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out <a href="clean.html#clean">chapter on cleaning data</a>.</p>
<div class="sourceCode" id="cb1905"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1905-1"><a href="logistic.html#cb1905-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay)</span></span>
<span id="cb1905-2"><a href="logistic.html#cb1905-2" aria-hidden="true" tabindex="-1"></a>dich.pred.prob <span class="ot">&lt;-</span> pred.prob</span>
<span id="cb1905-3"><a href="logistic.html#cb1905-3" aria-hidden="true" tabindex="-1"></a>dich.pred.prob[dich.pred.prob <span class="sc">&gt;=</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1905-4"><a href="logistic.html#cb1905-4" aria-hidden="true" tabindex="-1"></a>dich.pred.prob[dich.pred.prob <span class="sc">&lt;</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<p>Building on the <code>xtabs</code> function from above, use the <code>+</code> symbol to add the new <code>dich.pred.prob</code> vector as the column variable in the table. Also, create a new table object using the <code>&lt;-</code> operator called <code>table2</code> (or whatever you would like to call it). Use the <code>print</code> function to print the <code>table2</code> object.</p>
<div class="sourceCode" id="cb1906"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1906-1"><a href="logistic.html#cb1906-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of cases at each level of outcome variable</span></span>
<span id="cb1906-2"><a href="logistic.html#cb1906-2" aria-hidden="true" tabindex="-1"></a>table2 <span class="ot">&lt;-</span> <span class="fu">xtabs</span>(<span class="sc">~</span> td_short<span class="sc">$</span>Turnover <span class="sc">+</span> dich.pred.prob)</span>
<span id="cb1906-3"><a href="logistic.html#cb1906-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(table2)</span></code></pre></div>
<pre><code>##                  dich.pred.prob
## td_short$Turnover  0  1
##                 0  8 31
##                 1 10 49</code></pre>
<p>This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data.</p>
<p>By applying the <code>prop.table</code> function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (<code>table2</code>), and as the second argument, enter the numeral 1 to indicate that we want the row proportions.</p>
<div class="sourceCode" id="cb1908"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1908-1"><a href="logistic.html#cb1908-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Row proportions</span></span>
<span id="cb1908-2"><a href="logistic.html#cb1908-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(table2, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                  dich.pred.prob
## td_short$Turnover         0         1
##                 0 0.2051282 0.7948718
##                 1 0.1694915 0.8305085</code></pre>
<p>Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%).</p>
<p>Finally, let’s determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, we’re interested in the cells that correspond to 0 on the <code>Turnover</code> variable and 0 on the <code>dich.pred.prob</code> vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (<code>table2</code>), and within brackets beside it (<code>[ ]</code>), type the name of the row label (“0”), followed by a comma and the name of the column label (“0”). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (<code>table2</code>), and within brackets beside it (<code>[ ]</code>), type the name of the row label (“1”), followed by a comma and the name of the column label (“1”). Now add those together to form the numerator. In the denominator, apply the <code>sum</code> function from base R to the table object (<code>table2</code>) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (<code>JS</code>) and the outcome variable (<code>Turnover</code>) in our original model.</p>
<div class="sourceCode" id="cb1910"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1910-1"><a href="logistic.html#cb1910-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate overall percentage of correct classifications</span></span>
<span id="cb1910-2"><a href="logistic.html#cb1910-2" aria-hidden="true" tabindex="-1"></a>(table2[<span class="st">&quot;0&quot;</span>,<span class="st">&quot;0&quot;</span>] <span class="sc">+</span> table2[<span class="st">&quot;1&quot;</span>,<span class="st">&quot;1&quot;</span>]) <span class="sc">/</span> <span class="fu">sum</span>(table2)</span></code></pre></div>
<pre><code>## [1] 0.5816327</code></pre>
<p>The overall percentage of correct classifications 58.2%, which is not a monumental amount of prediction accuracy when using just <code>JS</code> (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent.</p>
<p><strong>Nagelkerke pseudo-<em>R</em><sup>2</sup>:</strong> To compute Nagelkerke’s pseudo-<em>R</em><sup>2</sup>, we will need to install and access the <code>DescTools</code> package so that we can use its <code>PseudoR2</code> function.</p>
<div class="sourceCode" id="cb1912"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1912-1"><a href="logistic.html#cb1912-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1912-2"><a href="logistic.html#cb1912-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;DescTools&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1913"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1913-1"><a href="logistic.html#cb1913-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1913-2"><a href="logistic.html#cb1913-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span></code></pre></div>
<p>In the <code>PseudoR2</code> function, we will specify the name of the model object (<code>model1</code>) as the first argument. As the second argument, type <code>"Nagel"</code> to request pseudo-<em>R</em><sup>2</sup> calculated using Nagelkerke’s formula.</p>
<div class="sourceCode" id="cb1914"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1914-1"><a href="logistic.html#cb1914-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Nagelkerke pseudo-R-squared</span></span>
<span id="cb1914-2"><a href="logistic.html#cb1914-2" aria-hidden="true" tabindex="-1"></a><span class="fu">PseudoR2</span>(model1, <span class="st">&quot;Nagel&quot;</span>)</span></code></pre></div>
<pre><code>## Nagelkerke 
## 0.07258382</code></pre>
<p>The estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> is .073. Remember, a pseudo-<em>R</em><sup>2</sup> is not the exact same thing as a true <em>R</em><sup>2</sup>, so we should interpret it with caution. With caution, we can conclude that <code>JS</code> explains 7.3% of the variance in <code>Turnover</code>.</p>
<p>Because the <code>DescTools</code> package also has a function called <code>Logit</code>, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the <code>Logit</code> function from <code>DescTools</code> as opposed to the one from <code>lessR</code>.</p>
<div class="sourceCode" id="cb1916"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1916-1"><a href="logistic.html#cb1916-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detach package</span></span>
<span id="cb1916-2"><a href="logistic.html#cb1916-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:DescTools&quot;</span>, <span class="at">character.only=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>Technical Write-Up of Results:</strong> A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (<code>Turnover</code>) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (<code>JS</code>) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (<em>b</em> = -.438, <em>p</em> = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover.</p>
<p><strong>Dealing with Bivariate Outliers:</strong> If you recall above, we found that the cases associated with row numbers <em>66</em> and <em>67</em> in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cook’s distance value greater than 1.0). If you <em>were to</em> decide to remove cases <em>66</em> and <em>67</em>, here’s what you would do. First, look at the data frame (using the <code>View</code> function) and determine which cases row numbers <em>66</em> and <em>67</em> are associated with; because we have a unique identifier variable (<code>ID</code>) in our data frame, we can see that they are associated with <code>ID</code> equal to <em>EMP861</em> and <em>EMP862</em>, respectively. Next, with respect to estimating the logistic regression model, I suggest naming the unstandardized regression model something different, and here I name it <code>model1_b</code>. The model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: <code>subset=(!ID %in% c("EMP861","EMP862"))</code>; the <code>subset</code> argument subsets the data frame within the <code>glm</code> function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which <code>ID</code> is <em>not</em> (<code>!</code>) within the vector containing <em>EMP861</em> and <em>EMP862</em>. Please consider revisiting the <a href="filter.html#filter">chapter on filtering (subsetting) data</a> if you would like to see the full list of logical operators or to review how to filter out cases from a data frame <em>before</em> specifying the model.</p>
<div class="sourceCode" id="cb1917"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1917-1"><a href="logistic.html#cb1917-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with outlier/influential cases removed</span></span>
<span id="cb1917-2"><a href="logistic.html#cb1917-2" aria-hidden="true" tabindex="-1"></a>model1_b <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS, <span class="at">data=</span>td, <span class="at">family=</span>binomial, </span>
<span id="cb1917-3"><a href="logistic.html#cb1917-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">subset=</span>(<span class="sc">!</span>ID <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;EMP861&quot;</span>,<span class="st">&quot;EMP862&quot;</span>)))</span>
<span id="cb1917-4"><a href="logistic.html#cb1917-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1917-5"><a href="logistic.html#cb1917-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1917-6"><a href="logistic.html#cb1917-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1_b)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS, family = binomial, data = td, subset = (!ID %in% 
##     c(&quot;EMP861&quot;, &quot;EMP862&quot;)))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7441  -1.2209   0.7819   0.9887   1.5092  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   1.8799     0.7067   2.660  0.00781 **
## JS           -0.4388     0.1997  -2.198  0.02797 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 128.89  on 95  degrees of freedom
## Residual deviance: 123.66  on 94  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 127.66
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p><strong>Standardizing Continuous Predictor Variable:</strong> Optionally, sometimes we may decide to standardize the continuous predictor variable in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the <code>scale</code> function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the <a href="center.html#center">chapter on centering and standardizing variables</a>.</p>
<div class="sourceCode" id="cb1919"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1919-1"><a href="logistic.html#cb1919-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model </span></span>
<span id="cb1919-2"><a href="logistic.html#cb1919-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with standardized continuous predictor variable</span></span>
<span id="cb1919-3"><a href="logistic.html#cb1919-3" aria-hidden="true" tabindex="-1"></a>model1_c <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> <span class="fu">scale</span>(JS), <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1919-4"><a href="logistic.html#cb1919-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1919-5"><a href="logistic.html#cb1919-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1919-6"><a href="logistic.html#cb1919-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1_c)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ scale(JS), family = binomial, data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7337  -1.2149   0.7806   0.9945   1.5175  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   0.4385     0.2132   2.057   0.0397 *
## scale(JS)    -0.5022     0.2247  -2.236   0.0254 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 131.75  on 97  degrees of freedom
## Residual deviance: 126.34  on 96  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 130.34
## 
## Number of Fisher Scoring iterations: 4</code></pre>
</div>
<div id="glm_predictingprobabilities_simple_logistic" class="section level4" number="49.3.3.3">
<h4><span class="header-section-number">49.3.3.3</span> Compute Predicted Probabilities</h4>
<p>When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didn’t append this new vector to of values to our data frame object (<code>td</code>). For the sake of clarity, we will re-do this step using the <code>predict</code> function from base R once more. We’ll begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (<code>pred.prob</code>), using the <code>&lt;-</code> assignment operator. Next, type the name of the <code>predict</code> function from base R. As the first argument, type the name of our logistic regression model (<code>model1</code>). As the second argument, enter the argument <code>type="response"</code> to indicate that you want to predict the outcome (response) variable.</p>
<div class="sourceCode" id="cb1921"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1921-1"><a href="logistic.html#cb1921-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate predicted probabilities based on</span></span>
<span id="cb1921-2"><a href="logistic.html#cb1921-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sample used to estimate the logistic regression model</span></span>
<span id="cb1921-3"><a href="logistic.html#cb1921-3" aria-hidden="true" tabindex="-1"></a>pred.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<p>Now let’s add the <code>pred.prob</code> vector as a new variable called <code>pred.prod.JS</code> in the <code>td</code> data frame object.</p>
<ol style="list-style-type: decimal">
<li>Let’s overwrite the existing <code>td</code> data frame object by using the <code>&lt;-</code> assignment operator.</li>
<li>Specify the name of the <code>merge</code> function from base R. For more information on this function, please refer to <a href="join.html#join_mergefunction">this chapter supplement</a> from the chapter on joining data frames.</li>
</ol>
<ul>
<li>As the first argument in the <code>merge</code> function, specify <code>x=</code> followed by the name of the <code>td</code> data frame object.</li>
<li>As the second argument in the <code>merge</code> function, specify <code>y=</code> followed by the <code>data.frame</code> function from base R. As the sole argument within the <code>data.frame</code> function specify a name for the new variable that will contain the predicted probabilities based on <code>JS</code> scores (<code>pred.prod.JS</code>), followed by the <code>=</code> operator and the name of the vector object containing the predicted probabilities (<code>pred.prod</code>).</li>
<li>As the third argument in the <code>merge</code> function, type <code>by="row.names"</code>, which will match rows from the <code>x</code> and <code>y</code> data frame objects based on their respective row names (i.e., row numbers).</li>
<li>As the fourth argument in the <code>merge</code> function, type <code>all=TRUE</code> to request a full merge, such that all rows with data will be retained from both data frame objects when merging.</li>
</ul>
<div class="sourceCode" id="cb1922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1922-1"><a href="logistic.html#cb1922-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with predicted probabilities</span></span>
<span id="cb1922-2"><a href="logistic.html#cb1922-2" aria-hidden="true" tabindex="-1"></a><span class="co"># added as new variable in existing data frame object</span></span>
<span id="cb1922-3"><a href="logistic.html#cb1922-3" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">merge</span>(<span class="at">x=</span>td, </span>
<span id="cb1922-4"><a href="logistic.html#cb1922-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span><span class="fu">data.frame</span>(</span>
<span id="cb1922-5"><a href="logistic.html#cb1922-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">pred.prob.JS =</span> pred.prob),</span>
<span id="cb1922-6"><a href="logistic.html#cb1922-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">by=</span><span class="st">&quot;row.names&quot;</span>,</span>
<span id="cb1922-7"><a href="logistic.html#cb1922-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">all=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>If we print the first six rows from the <code>td</code> data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model.</p>
<div class="sourceCode" id="cb1923"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1923-1"><a href="logistic.html#cb1923-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 6 rows of data frame object</span></span>
<span id="cb1923-2"><a href="logistic.html#cb1923-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>##   Row.names     ID Turnover   JS   OC   TI NAff JS_0 JS_1 pred.prob.JS
## 1         1 EMP559        1 4.96 5.32 0.51 1.87 4.73 5.73    0.4216566
## 2        10 EMP614        1 1.69 2.82 2.58 2.07 1.46 2.46    0.7531576
## 3        11 EMP617        1 3.62 1.08 3.53 2.74 3.39 4.39    0.5672481
## 4        12 EMP619        0 1.72 3.88 1.78 2.11 1.49 2.49    0.7507079
## 5        13 EMP634        0 1.96 3.02 3.41   NA 1.73 2.73    0.7305327
## 6        14 EMP636        0 4.14 2.69 4.33 3.07 3.91 4.91    0.5107466</code></pre>
<p>We can now dichotomize the predicted probabilities (<code>pred.prob.JS</code>), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new variable called <code>dich.pred.prob.JS</code> (or whatever we would like to call it) based on the values from the <code>pred.prob.JS</code> variable we just created. Next, for the <code>dich.pred.prob.JS</code>, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out <a href="clean.html#clean">chapter on cleaning data</a>.</p>
<div class="sourceCode" id="cb1925"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1925-1"><a href="logistic.html#cb1925-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay)</span></span>
<span id="cb1925-2"><a href="logistic.html#cb1925-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prob.JS <span class="ot">&lt;-</span> td<span class="sc">$</span>pred.prob.JS</span>
<span id="cb1925-3"><a href="logistic.html#cb1925-3" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prob.JS[td<span class="sc">$</span>dich.pred.prob.JS <span class="sc">&gt;=</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1925-4"><a href="logistic.html#cb1925-4" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prob.JS[td<span class="sc">$</span>dich.pred.prob.JS <span class="sc">&lt;</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<div class="sourceCode" id="cb1926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1926-1"><a href="logistic.html#cb1926-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 6 rows of data frame object</span></span>
<span id="cb1926-2"><a href="logistic.html#cb1926-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>##   Row.names     ID Turnover   JS   OC   TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS
## 1         1 EMP559        1 4.96 5.32 0.51 1.87 4.73 5.73    0.4216566                 0
## 2        10 EMP614        1 1.69 2.82 2.58 2.07 1.46 2.46    0.7531576                 1
## 3        11 EMP617        1 3.62 1.08 3.53 2.74 3.39 4.39    0.5672481                 1
## 4        12 EMP619        0 1.72 3.88 1.78 2.11 1.49 2.49    0.7507079                 1
## 5        13 EMP634        0 1.96 3.02 3.41   NA 1.73 2.73    0.7305327                 1
## 6        14 EMP636        0 4.14 2.69 4.33 3.07 3.91 4.91    0.5107466                 1</code></pre>
<p>It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the <code>JS</code> predictor variable, which we could then use to plug into our model; to do this with a model estimated using the <code>glm</code> model, will create a toy data frame with “fresh” <code>JS</code> values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object – just like we did in the <a href="predictingcriterionscores.html#predictingcriterionscores">chapter on predicting criterion scores using simple linear regression</a>.</p>
<p>In this toy data frame that we’re naming <code>fresh_td</code>, we use the <code>data.frame</code> function from base R. As the first argument, we’ll create an <code>ID</code> variable with some fictional employee ID numbers, and as the second argument, we’ll create a <code>JS</code> variable with some fictional job satisfaction scores.</p>
<div class="sourceCode" id="cb1928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1928-1"><a href="logistic.html#cb1928-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create toy data frame object for illustration purposes</span></span>
<span id="cb1928-2"><a href="logistic.html#cb1928-2" aria-hidden="true" tabindex="-1"></a>fresh_td <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">ID=</span><span class="fu">c</span>(<span class="st">&quot;EMP1000&quot;</span>,<span class="st">&quot;EMP1001&quot;</span>,<span class="st">&quot;EMP1002&quot;</span>,</span>
<span id="cb1928-3"><a href="logistic.html#cb1928-3" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;EMP1003&quot;</span>,<span class="st">&quot;EMP1004&quot;</span>,<span class="st">&quot;EMP1005&quot;</span>),</span>
<span id="cb1928-4"><a href="logistic.html#cb1928-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">JS=</span><span class="fu">c</span>(<span class="fl">4.5</span>, <span class="fl">1.6</span>, <span class="fl">0.7</span>,</span>
<span id="cb1928-5"><a href="logistic.html#cb1928-5" aria-hidden="true" tabindex="-1"></a>                            <span class="fl">5.9</span>, <span class="fl">2.1</span>, <span class="fl">3.0</span>))</span></code></pre></div>
<p>When estimating the predicted probabilities of turnover based on these new <code>JS</code> scores, we’ll do the following:</p>
<ol style="list-style-type: decimal">
<li>Using the <code>&lt;-</code> assignment operator, we will create a new variable called <code>prob_JS</code> that we will append to the <code>fresh_td</code> toy data frame object (using the <code>$</code> operator).</li>
<li>To the right of the <code>&lt;-</code> assignment operator, type the name of the <code>predict</code> function from base R.</li>
</ol>
<ul>
<li>As the first argument, type the name of the logistic regression model object we estimated using the <code>td</code> (original) data frame.</li>
<li>As the second argument, type <code>newdata=</code> followed by the name of the data frame object containing “fresh” data (<code>fresh_td</code>). It’s important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated.</li>
<li>As the third argument, insert <code>type="response"</code>, which will request the predicted probabilities.</li>
</ul>
<div class="sourceCode" id="cb1929"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1929-1"><a href="logistic.html#cb1929-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use simple logistic regression model to estimate </span></span>
<span id="cb1929-2"><a href="logistic.html#cb1929-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted probabilities of turnover for &quot;fresh&quot; data</span></span>
<span id="cb1929-3"><a href="logistic.html#cb1929-3" aria-hidden="true" tabindex="-1"></a>fresh_td<span class="sc">$</span>prob_JS <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="co"># name of logistic regression model</span></span>
<span id="cb1929-4"><a href="logistic.html#cb1929-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">newdata=</span>fresh_td, <span class="co"># name of &quot;fresh&quot; data frame</span></span>
<span id="cb1929-5"><a href="logistic.html#cb1929-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1930"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1930-1"><a href="logistic.html#cb1930-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data frame object</span></span>
<span id="cb1930-2"><a href="logistic.html#cb1930-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fresh_td)</span></code></pre></div>
<pre><code>##        ID  JS   prob_JS
## 1 EMP1000 4.5 0.4713805
## 2 EMP1001 1.6 0.7604090
## 3 EMP1002 0.7 0.8247569
## 4 EMP1003 5.9 0.3257483
## 5 EMP1004 2.1 0.7182989
## 6 EMP1005 3.0 0.6322888</code></pre>
</div>
</div>
<div id="glm_function_multiple_logistic" class="section level3" number="49.3.4">
<h3><span class="header-section-number">49.3.4</span> Multiple Logistic Regression Using <code>glm</code> Function from Base R</h3>
<p>Just as we did for simple logistic regression, we can estimate a multiple logistic regression model using the <code>glm</code> function from base R. Let’s specify a multiple logistic regression model with <code>JS</code>, <code>NAff</code>, and <code>TI</code> as predictor variables and <code>Turnover</code> as the dichotomous outcome variable.</p>
<ol style="list-style-type: decimal">
<li>Using the <code>&lt;-</code> assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., <code>model2</code>).</li>
<li>Type the name of the <code>glm</code> function.</li>
</ol>
<ul>
<li>As the function’s first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variables, separated by the <code>+</code> operator, are typed to the right of the <code>~</code> symbol.</li>
<li>As the second argument, type <code>data=</code> followed by the name of the data frame (<code>td</code>) to which both of the variables belong.</li>
<li>As the third argument, type <code>family=binomial</code>, which by default estimates a logit (logistic) model.</li>
</ul>
<div class="sourceCode" id="cb1932"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1932-1"><a href="logistic.html#cb1932-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple logistic regression model</span></span>
<span id="cb1932-2"><a href="logistic.html#cb1932-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> NAff <span class="sc">+</span> TI, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span></code></pre></div>
<p><em>Note:</em> You won’t see a summary of the results in the Console, as we have not requested those. As a next step, we will use the <code>model1</code> object to determine whether we have satisfied the statistical assumptions of a multiple logistic regression model.</p>
<div id="glm_teststatisticalassumptions_multiple_logistic" class="section level4" number="49.3.4.1">
<h4><span class="header-section-number">49.3.4.1</span> Test Statistical Assumptions</h4>
<p>To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a>.</p>
<p><strong>Cases Are Randomly Sampled from the Population:</strong> As mentioned in the <a href="logistic.html#statisticalassumptions_logistic">statistical assumptions</a> section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied.</p>
<p><strong>Outcome Variable Is Dichotomous:</strong> We already know that the outcome variable called <code>Turnover</code> is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption.</p>
<p><strong>Data Are Free of Multivariate Outliers:</strong> To determine whether the data are free of multivariate outliers, let’s look at Cook’s distance (D) values across cases. Using the <code>plot</code> function from base R, type the name of our model object (<code>model1</code>) as the first argument, and as the second argument, enter the numeral <em>4</em> to request the fourth diagnostic plot, which is the Cook’s distance plot.</p>
<div class="sourceCode" id="cb1933"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1933-1"><a href="logistic.html#cb1933-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics plot: Cook&#39;s Distance plot</span></span>
<span id="cb1933-2"><a href="logistic.html#cb1933-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(model2, <span class="dv">4</span>)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-1089-1.png" width="672" /></p>
<p>The case associated with row number <em>6</em> has the highest Cook’s distance value, followed by the cases associated with row numbers <em>1</em> and <em>66</em>. The cases associated with row numbers <em>6</em> and <em>1</em> seem to be clearest potential outliers of the three.</p>
<p>We can also print the cases with the highest Cook’s distances. Let’s create an object called <code>cooksD</code> that we will assign a vector of Cook’s distance values to using the <code>cooks.distance</code> function from base R. Just enter the name of the logistic regression model object (<code>model1</code>) as the sole parenthetical argument. Next, update the object we called <code>cooksD</code> by applying the <code>sort</code> function from base R and entering the <code>cooksD</code> object as the first object and <code>decreasing=TRUE</code> as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the <code>head</code> function from base R, and as the first argument enter the name of the <code>cooksD</code> object that we just sorted; as the second argument, type <code>n=20</code> to show the top 20 rows, as opposed to the default top 6 rows.</p>
<div class="sourceCode" id="cb1934"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1934-1"><a href="logistic.html#cb1934-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate Cook&#39;s distance values</span></span>
<span id="cb1934-2"><a href="logistic.html#cb1934-2" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(model2)</span>
<span id="cb1934-3"><a href="logistic.html#cb1934-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1934-4"><a href="logistic.html#cb1934-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort Cook&#39;s distance values in descending order               </span></span>
<span id="cb1934-5"><a href="logistic.html#cb1934-5" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">sort</span>(cooksD, <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb1934-6"><a href="logistic.html#cb1934-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1934-7"><a href="logistic.html#cb1934-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View top 20 Cook&#39;s distance values</span></span>
<span id="cb1934-8"><a href="logistic.html#cb1934-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(cooksD, <span class="at">n=</span><span class="dv">20</span>)</span></code></pre></div>
<pre><code>##          6          1         66         42         96         74         27         26         67         61         68          4         25 
## 0.15184792 0.13426790 0.10091817 0.08227834 0.05869871 0.04691783 0.04554332 0.04249992 0.03973226 0.02634920 0.02182251 0.01882168 0.01802907 
##         52         54         69          8         75         77         64 
## 0.01720366 0.01718997 0.01718628 0.01661234 0.01611474 0.01593173 0.01552323</code></pre>
<p>Again, we see cases associated with row numbers <em>6</em> (.152), <em>1</em> (.134), and <em>66</em> (.101) having the highest Cook’s distance values. As a sensitivity analysis, we <em>could</em> estimate our model once more after removing the cases associated with row numbers <em>6</em> and <em>1</em> (and maybe even <em>66</em>) from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cook’s distance values are all well-below the more liberal threshold, so I would say we’re safe to leave the associated cases in the data.</p>
<p><strong>No (Multi)Collinearity Between Predictor Variables:</strong> To assess whether (multi)collinearity might be an issue, let’s estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you haven’t already, install the <code>car</code> package which contains the <code>vif</code> function we will use to compute tolerance.</p>
<div class="sourceCode" id="cb1936"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1936-1"><a href="logistic.html#cb1936-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install car package if you haven&#39;t already</span></span>
<span id="cb1936-2"><a href="logistic.html#cb1936-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;car&quot;</span>)</span></code></pre></div>
<p>Now, access the <code>car</code> package using the <code>library</code> function.</p>
<div class="sourceCode" id="cb1937"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1937-1"><a href="logistic.html#cb1937-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access car package</span></span>
<span id="cb1937-2"><a href="logistic.html#cb1937-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code></pre></div>
<p>Apply the <code>vif</code> function to the model object (<code>model2</code>).</p>
<div class="sourceCode" id="cb1938"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1938-1"><a href="logistic.html#cb1938-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute VIF statistic</span></span>
<span id="cb1938-2"><a href="logistic.html#cb1938-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(model2)</span></code></pre></div>
<pre><code>##       JS     NAff       TI 
## 1.068428 1.020933 1.067496</code></pre>
<p>Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - <em>R</em><sup>2</sup>), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approach .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide <em>1</em> by the VIF.</p>
<div class="sourceCode" id="cb1940"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1940-1"><a href="logistic.html#cb1940-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute tolerance statistic</span></span>
<span id="cb1940-2"><a href="logistic.html#cb1940-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> <span class="fu">vif</span>(model2)</span></code></pre></div>
<pre><code>##        JS      NAff        TI 
## 0.9359549 0.9794966 0.9367719</code></pre>
<p>This table indicates that there are low levels of collinearity. Specifically, we can see that the tolerance statistics are all closer to 1.00 and definitely above .20, thereby indicating that collinearity is not likely an issue.</p>
<p><strong>Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear:</strong> To test the assumption of linearity between a <em>continuous</em> predictor variable and the logit transformation of the outcome variable, we can add to the model the interaction term between any continuous predictor variable and its logarithmic (i.e., natural log) transformation. <strong>[Note: We do not perform the following test/approach for <em>categorical</em> predictor variables.]</strong> We will use an approach that is commonly referred to as the Box-Tidwell approach <span class="citation">(<a href="#ref-hosmerlemeshow2000" role="doc-biblioref">Hosmer and Lemeshow 2000</a>)</span>. To apply this approach, we need to add the interaction term between each predictor variable and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of the predictor variable. In our regression model formula, specify the dichotomous outcome variable <code>Turnover</code> to the left of the <code>~</code> symbol. To the right of the <code>~</code> symbol, type the name of each predictor variable <code>JS</code> followed by the <code>+</code> symbol. After the <code>+</code> symbol, type the name of the same predictor variable <code>JS</code>, followed by the <code>:</code> symbol and the <code>log</code> function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all continuous predictor variables in the model. Be sure to name the model object, and here I name it arbitrarily <code>boxtidwell</code>. Finally, use the <code>summary</code> function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (<code>boxtidwell</code>).</p>
<div class="sourceCode" id="cb1942"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1942-1"><a href="logistic.html#cb1942-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box-Tidwell diagnostic test of linearity between continuous predictor variables and</span></span>
<span id="cb1942-2"><a href="logistic.html#cb1942-2" aria-hidden="true" tabindex="-1"></a><span class="co"># logit transformation of outcome variable</span></span>
<span id="cb1942-3"><a href="logistic.html#cb1942-3" aria-hidden="true" tabindex="-1"></a>boxtidwell <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> JS<span class="sc">:</span><span class="fu">log</span>(JS) <span class="sc">+</span> NAff <span class="sc">+</span> NAff<span class="sc">:</span><span class="fu">log</span>(NAff) <span class="sc">+</span> TI <span class="sc">+</span> TI<span class="sc">:</span><span class="fu">log</span>(TI), </span>
<span id="cb1942-4"><a href="logistic.html#cb1942-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1942-5"><a href="logistic.html#cb1942-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1942-6"><a href="logistic.html#cb1942-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1942-7"><a href="logistic.html#cb1942-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(boxtidwell)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + 
##     TI + TI:log(TI), family = binomial, data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2213  -0.8788   0.4455   0.8257   2.6375  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)      0.9206     9.8331   0.094   0.9254  
## JS              -4.5845     2.6176  -1.751   0.0799 .
## NAff            -0.4107     6.4047  -0.064   0.9489  
## TI               3.2888     3.0158   1.091   0.2755  
## JS:log(JS)       2.0053     1.1844   1.693   0.0904 .
## NAff:log(NAff)   0.9559     3.5321   0.271   0.7867  
## TI:log(TI)      -1.2101     1.5232  -0.794   0.4269  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 127.02  on 94  degrees of freedom
## Residual deviance: 100.85  on 88  degrees of freedom
##   (4 observations deleted due to missingness)
## AIC: 114.85
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Because the <em>p</em>-values associated with each of the interaction terms and their regression coefficients (<code>JS:log(JS)</code>, <code>NAff:log(NAff)</code>, <code>TI:log(TI)</code>) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio).</p>
<p><strong>Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the <code>log</code> function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the <em>p</em>-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a <a href="logistic.html#glm_teststatisticalassumptions_simple_logistic">previous section on simple logistic regression</a>.</strong></p>
</div>
<div id="glm_interpret_multiple_logistic" class="section level4" number="49.3.4.2">
<h4><span class="header-section-number">49.3.4.2</span> Interpret Model Results</h4>
<p>Now we’re ready to interpret the results of the multiple logistic regression model. For clarity, let’s re-specify our original simple logistic regression model from above. To apply the <code>glm</code> function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the <code>~</code> symbol, and the predictor variables are typed to the right of the <code>~</code> symbol, with each predictor variable separated by the <code>+</code> operator. As the second argument, type <code>data=</code> followed by the name of the data frame (<code>td</code>) to which both of the variables belong. Third, type the argument <code>family=binomial</code>, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the <code>&lt;-</code> assignment operator; here, I again arbitrarily named the object <code>model2</code>. Finally, use the <code>summary</code> function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (<code>model2</code>).</p>
<div class="sourceCode" id="cb1944"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1944-1"><a href="logistic.html#cb1944-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple logistic regression model</span></span>
<span id="cb1944-2"><a href="logistic.html#cb1944-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> JS <span class="sc">+</span> NAff <span class="sc">+</span> TI, <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1944-3"><a href="logistic.html#cb1944-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1944-4"><a href="logistic.html#cb1944-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1944-5"><a href="logistic.html#cb1944-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ JS + NAff + TI, family = binomial, data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3347  -0.9248   0.5881   0.8403   2.2273  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  -3.9286     1.8120  -2.168  0.03015 * 
## JS           -0.2332     0.2215  -1.053  0.29253   
## NAff          1.1952     0.4996   2.392  0.01674 * 
## TI            0.8967     0.3166   2.832  0.00462 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 127.02  on 94  degrees of freedom
## Residual deviance: 105.23  on 91  degrees of freedom
##   (4 observations deleted due to missingness)
## AIC: 113.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The output generates the model coefficient estimates and model fit (i.e., AIC) information.</p>
<p>To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the <code>confint</code> function from base R with the name of our model as the sole argument (<code>model2</code>).</p>
<div class="sourceCode" id="cb1946"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1946-1"><a href="logistic.html#cb1946-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate 95% confidence intervals</span></span>
<span id="cb1946-2"><a href="logistic.html#cb1946-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(model2)</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) -7.6905274 -0.5144811
## JS          -0.6793832  0.1980706
## NAff         0.2606377  2.2375600
## TI           0.3091687  1.5633542</code></pre>
<p><strong>Coefficients:</strong> The <em>Coefficients</em> section of the output from the summary of the <code>glm</code> function generates the model coefficient estimates and model fit (i.e., AIC) information, and the output from the <code>confint</code> provides the confidence intervals. To begin, the <em>Coefficients</em> section displays the regression coefficients (slopes, weights) and their standard errors, <em>z</em>-values, and <em>p</em>-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (<code>JS</code>) in relation to the logit transformation of the outcome variable (<code>Turnover</code>) is not statistically significant when controlling for the other predictor variables in the model because the associated <em>p</em>-value is equal to or greater than .05 (<em>b</em> = -.233, <em>p</em> = .293, 95% CI[-0.667, .201]). The regression coefficient for <code>NAff</code> is 1.195 and its associated <em>p</em>-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (<em>b</em> = 1.195, <em>p</em> = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for <code>TI</code> is .897 and its associated <em>p</em>-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (<em>b</em> = .897, <em>p</em> = .005, 95% CI[.276, 1.517]).</p>
<p>Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, we typically would not write out the regression equation with that variable included. Instead, we would typically re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(\ln(\frac{p}{1-p}) = -3.929 - .233 \times JS_{observed} + 1.195 \times NAff_{observed} + .897 \times TI_{observed}\)</span></p>
<p>where <span class="math inline">\(p\)</span> refers to, in this example, as the probability of quitting.</p>
<p>If you recall from earlier in the tutorial, we can also interpret our findings with respect to <span class="math inline">\(\log(odds)\)</span>.</p>
<p><span class="math inline">\(\log(odds) = \ln(\frac{p}{1-p}) = -3.929 - .233 \times JS_{observed} + 1.195 \times NAff_{observed} + .897 \times TI_{observed}\)</span></p>
<p>To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it – for example:</p>
<p><span class="math inline">\(e^{1.195} = 3.304\)</span></p>
<p>We can also do this using R by applying the <code>exp</code> function from base R. Specifically, within the <code>exp</code> function parentheses, type the name of the <code>coef</code> function from base R, which extracts the regression coefficients from the model object. As the sole argument within the <code>coef</code> function parentheses, enter the name of the model we previously specified (<code>model2</code>).</p>
<div class="sourceCode" id="cb1949"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1949-1"><a href="logistic.html#cb1949-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1949-2"><a href="logistic.html#cb1949-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model2))</span></code></pre></div>
<pre><code>## (Intercept)          JS        NAff          TI 
##   0.0196714   0.7919969   3.3041473   2.4513941</code></pre>
<p>We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belonging to <code>NAff</code> and <code>TI</code>. Regarding <code>NAff</code>, its odds ratio of 3.304 is <em>greater than</em> 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times as likely for every one unit increase in <code>NAff</code> when controlling for other predictor variables in the model. Regarding <code>TI</code>, its odds ratio of 2.451 is also <em>greater than</em> 1, and thus, the odds of quitting are 2.451 times as likely for every one unit increase in <code>TI</code> when controlling for other predictor variables in the model. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude.</p>
<table>
<thead>
<tr class="header">
<th align="center">Odds Ratio &gt; 1</th>
<th align="center">Odds Ratio &lt; 1</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1.2</td>
<td align="center">.8</td>
<td align="center">Small</td>
</tr>
<tr class="even">
<td align="center">2.5</td>
<td align="center">.4</td>
<td align="center">Medium</td>
</tr>
<tr class="odd">
<td align="center">4.3</td>
<td align="center">.2</td>
<td align="center">Large</td>
</tr>
</tbody>
</table>
<p>To get the 95% confidence intervals for the odds ratio, we can nest the <code>confint</code> function within the <code>exp</code> function.</p>
<div class="sourceCode" id="cb1951"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1951-1"><a href="logistic.html#cb1951-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Exponentiate logistic regression coefficient to convert to odds ratio</span></span>
<span id="cb1951-2"><a href="logistic.html#cb1951-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(model2))</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                   2.5 %    97.5 %
## (Intercept) 0.000457137 0.5978107
## JS          0.506929579 1.2190484
## NAff        1.297757439 9.3704394
## TI          1.362292195 4.7748100</code></pre>
<p><strong>Model Fit &amp; Performance:</strong> Returning to the original output of the multiple logistic regression model, let’s examine the section that contains information about model fit. Note that we don’t have an estimate of <em>R</em>-squared (<em>R</em><sup>2</sup>) like we would with a traditional linear regression model. Later on, we will compute the pseudo-<em>R</em>-squared (<em>R</em><sup>2</sup>) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (<em>N</em> = 95). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial.</p>
<p>For our purposes, we will assess the model’s fit to the data initially by creating a confusion matrix in which we display the model’s accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 95 for the analysis. Thus, we should drop the cases with missing data prior to estimating our baseline data and confusion matrix. We’ll start by using the <code>subset</code> function from base R to select just the variables from our data frame that we used in our logistic regression model: <code>Turnover</code>, <code>JS</code>, <code>NAff</code>, and <code>TI</code>. As the first argument, type the name of the data frame (<code>td</code>). As the second argument, type <code>select=</code> followed by the <code>c</code> (combine) function from base R. Within the <code>c</code> function parentheses, enter the names of the two variables we wish to retain (<code>Turnover</code>, <code>JS</code>). Using the <code>&lt;-</code> assignment operator, create a new data frame object, and here I arbitrarily call this object <code>td_short</code>.</p>
<div class="sourceCode" id="cb1954"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1954-1"><a href="logistic.html#cb1954-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new short data frame with just the retained variables</span></span>
<span id="cb1954-2"><a href="logistic.html#cb1954-2" aria-hidden="true" tabindex="-1"></a>td_short <span class="ot">&lt;-</span> <span class="fu">subset</span>(td, <span class="at">select=</span><span class="fu">c</span>(Turnover, JS, NAff, TI))</span></code></pre></div>
<p>Next, apply the <code>na.omit</code> function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (<code>td_short</code>). Using the <code>&lt;-</code> assignment operator, overwrite the <code>td_short</code> data frame by entering its name to the left of the <code>&lt;-</code> assignment operator.</p>
<div class="sourceCode" id="cb1955"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1955-1"><a href="logistic.html#cb1955-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop cases with missing data in the short data frame</span></span>
<span id="cb1955-2"><a href="logistic.html#cb1955-2" aria-hidden="true" tabindex="-1"></a>td_short <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(td_short)</span></code></pre></div>
<p>There are different ways we can estimate our baseline, but let’s keep it simple and use the <code>xtabs</code> function from base R. As the first argument, type the <code>~</code> followed the name of the short data frame <code>td_short</code>, followed by the <code>$</code> and the outcome variable (<code>Turnover</code>). Use the <code>&lt;-</code> operator to create and name a new table object, where here I arbitrarily call it <code>table1</code>. Use the <code>print</code> function to view the table object (<code>table1</code>). Remember that for the <code>Turnover</code> variable quit = 1 and stay = 0.</p>
<div class="sourceCode" id="cb1956"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1956-1"><a href="logistic.html#cb1956-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of cases at each level of outcome variable</span></span>
<span id="cb1956-2"><a href="logistic.html#cb1956-2" aria-hidden="true" tabindex="-1"></a>table1 <span class="ot">&lt;-</span> <span class="fu">xtabs</span>(<span class="sc">~</span> td_short<span class="sc">$</span>Turnover)</span>
<span id="cb1956-3"><a href="logistic.html#cb1956-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1956-4"><a href="logistic.html#cb1956-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the table</span></span>
<span id="cb1956-5"><a href="logistic.html#cb1956-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(table1)</span></code></pre></div>
<pre><code>## td_short$Turnover
##  0  1 
## 37 58</code></pre>
<p>As you can see, 37 people actually stayed and 58 people actually quit. To convert these to proportions, apply the <code>prop.table</code> function from base R to the table object we created in the previous step (<code>table1</code>).</p>
<div class="sourceCode" id="cb1958"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1958-1"><a href="logistic.html#cb1958-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of cases at each level of outcome variable</span></span>
<span id="cb1958-2"><a href="logistic.html#cb1958-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(table1)</span></code></pre></div>
<pre><code>## td_short$Turnover
##         0         1 
## 0.3894737 0.6105263</code></pre>
<p>Based on this output, we see that 38.9% of people stayed and 61.1% quit.</p>
<p>Now we’re ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (<code>pred.prob</code>), using the <code>&lt;-</code> assignment operator. Next, type the name of the <code>predict</code> function from base R. As the first argument, type the name of our logistic regression model (<code>model2</code>). As the second argument, enter the argument <code>type="response"</code> to indicate that you want to predict the outcome (response) variable.</p>
<div class="sourceCode" id="cb1960"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1960-1"><a href="logistic.html#cb1960-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate predicted probabilities </span></span>
<span id="cb1960-2"><a href="logistic.html#cb1960-2" aria-hidden="true" tabindex="-1"></a>pred.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<p>We now need to dichotomize the vector of predicted probabilities (<code>pred.prob</code>), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new vector called <code>dich.pred.prob</code> (or whatever you would like to call it) based on the values from the <code>pred.prob</code> vector you created in the previous step. Next, for the <code>dich.pred.prob</code>, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out <a href="clean.html#clean">chapter on cleaning data</a>.</p>
<div class="sourceCode" id="cb1961"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1961-1"><a href="logistic.html#cb1961-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay)</span></span>
<span id="cb1961-2"><a href="logistic.html#cb1961-2" aria-hidden="true" tabindex="-1"></a>dich.pred.prob <span class="ot">&lt;-</span> pred.prob</span>
<span id="cb1961-3"><a href="logistic.html#cb1961-3" aria-hidden="true" tabindex="-1"></a>dich.pred.prob[dich.pred.prob <span class="sc">&gt;=</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1961-4"><a href="logistic.html#cb1961-4" aria-hidden="true" tabindex="-1"></a>dich.pred.prob[dich.pred.prob <span class="sc">&lt;</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<p>Building on the <code>xtabs</code> function from above, use the <code>+</code> symbol to add the new <code>dich.pred.prob</code> vector as the column variable in the table. Also, create a new table object using the <code>&lt;-</code> operator called <code>table2</code> (or whatever you would like to call it). Use the <code>print</code> function to print the <code>table2</code> object.</p>
<div class="sourceCode" id="cb1962"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1962-1"><a href="logistic.html#cb1962-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of cases at each level of outcome variable</span></span>
<span id="cb1962-2"><a href="logistic.html#cb1962-2" aria-hidden="true" tabindex="-1"></a>table2 <span class="ot">&lt;-</span> <span class="fu">xtabs</span>(<span class="sc">~</span> td_short<span class="sc">$</span>Turnover <span class="sc">+</span> dich.pred.prob)</span>
<span id="cb1962-3"><a href="logistic.html#cb1962-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(table2)</span></code></pre></div>
<pre><code>##                  dich.pred.prob
## td_short$Turnover  0  1
##                 0 23 14
##                 1  6 52</code></pre>
<p>This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data.</p>
<p>By applying the <code>prop.table</code> function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (<code>table2</code>), and as the second argument, enter the numeral 1 to indicate that we want the row proportions.</p>
<div class="sourceCode" id="cb1964"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1964-1"><a href="logistic.html#cb1964-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Row proportions</span></span>
<span id="cb1964-2"><a href="logistic.html#cb1964-2" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(table2, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                  dich.pred.prob
## td_short$Turnover         0         1
##                 0 0.6216216 0.3783784
##                 1 0.1034483 0.8965517</code></pre>
<p>Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model even better, as we were able to predict that outcome with 89.7% accuracy. Not too shabby.</p>
<p>Finally, let’s determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, we’re interested in the cells that correspond to 0 on the <code>Turnover</code> variable and 0 on the <code>dich.pred.prob</code> vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (<code>table2</code>), and within brackets beside it (<code>[ ]</code>), type the name of the row label (“0”), followed by a comma and the name of the column label (“0”). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (<code>table2</code>), and within brackets beside it (<code>[ ]</code>), type the name of the row label (“1”), followed by a comma and the name of the column label (“1”). Now add those together to form the numerator. In the denominator, apply the <code>sum</code> function from base R to the table object (<code>table2</code>) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (<code>JS</code>) and the outcome variable (<code>Turnover</code>) in our original model.</p>
<div class="sourceCode" id="cb1966"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1966-1"><a href="logistic.html#cb1966-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate overall percentage of correct classifications</span></span>
<span id="cb1966-2"><a href="logistic.html#cb1966-2" aria-hidden="true" tabindex="-1"></a>(table2[<span class="st">&quot;0&quot;</span>,<span class="st">&quot;0&quot;</span>] <span class="sc">+</span> table2[<span class="st">&quot;1&quot;</span>,<span class="st">&quot;1&quot;</span>]) <span class="sc">/</span> <span class="fu">sum</span>(table2)</span></code></pre></div>
<pre><code>## [1] 0.7894737</code></pre>
<p>The overall percentage of correct classifications 78.9%, which is a pretty good accuracy in prediction when using the three predictor variables in our model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent.</p>
<p><strong>Nagelkerke pseudo-<em>R</em><sup>2</sup>:</strong> To compute Nagelkerke’s pseudo-<em>R</em><sup>2</sup>, we will need to install and access the <code>DescTools</code> package so that we can use its <code>PseudoR2</code> function.</p>
<div class="sourceCode" id="cb1968"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1968-1"><a href="logistic.html#cb1968-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1968-2"><a href="logistic.html#cb1968-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;DescTools&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1969"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1969-1"><a href="logistic.html#cb1969-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1969-2"><a href="logistic.html#cb1969-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span></code></pre></div>
<p>In the <code>PseudoR2</code> function, we will specify the name of the model object (<code>model2</code>) as the first argument. As the second argument, type <code>"Nagel"</code> to request pseudo-<em>R</em><sup>2</sup> calculated using Nagelkerke’s formula.</p>
<div class="sourceCode" id="cb1970"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1970-1"><a href="logistic.html#cb1970-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Nagelkerke pseudo-R-squared</span></span>
<span id="cb1970-2"><a href="logistic.html#cb1970-2" aria-hidden="true" tabindex="-1"></a><span class="fu">PseudoR2</span>(model2, <span class="st">&quot;Nagel&quot;</span>)</span></code></pre></div>
<pre><code>## Nagelkerke 
##  0.2779516</code></pre>
<p>The estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-<em>R</em><sup>2</sup> was just .073. Remember, a pseudo-<em>R</em><sup>2</sup> is not the exact same thing as a true <em>R</em><sup>2</sup>, so we should interpret it with caution. With caution, we can conclude that <code>JS</code>, <code>NAff</code>, and <code>TI</code> explain 27.8% of the variance in <code>Turnover</code>.</p>
<p>Because the <code>DescTools</code> package also has a function called <code>Logit</code>, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the <code>Logit</code> function from <code>DescTools</code> as opposed to the one from <code>lessR</code>.</p>
<div class="sourceCode" id="cb1972"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1972-1"><a href="logistic.html#cb1972-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detach package</span></span>
<span id="cb1972-2"><a href="logistic.html#cb1972-2" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(<span class="st">&quot;package:DescTools&quot;</span>, <span class="at">character.only=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p><strong>Technical Write-Up of Results:</strong> A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (<code>Turnover</code>) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was <em>not</em> associated with turnover behavior to a statistically significant extent (<em>b</em> = -.233, <em>p</em> = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (<em>b</em> = 1.195, <em>p</em> = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (<em>b</em> = .897, <em>p</em> = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-<em>R</em><sup>2</sup> was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover.</p>
<p><strong>Standardizing Continuous Predictor Variables:</strong> Optionally, sometimes we may decide to standardize the continuous predictor variables in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the <code>scale</code> function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the <a href="center.html#center">chapter on centering and standardizing variables</a>.</p>
<div class="sourceCode" id="cb1973"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1973-1"><a href="logistic.html#cb1973-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate simple logistic regression model </span></span>
<span id="cb1973-2"><a href="logistic.html#cb1973-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with standardized continuous predictor variable</span></span>
<span id="cb1973-3"><a href="logistic.html#cb1973-3" aria-hidden="true" tabindex="-1"></a>model2_b <span class="ot">&lt;-</span> <span class="fu">glm</span>(Turnover <span class="sc">~</span> <span class="fu">scale</span>(JS) <span class="sc">+</span> <span class="fu">scale</span>(NAff) <span class="sc">+</span> <span class="fu">scale</span>(TI), </span>
<span id="cb1973-4"><a href="logistic.html#cb1973-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">data=</span>td, <span class="at">family=</span>binomial)</span>
<span id="cb1973-5"><a href="logistic.html#cb1973-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1973-6"><a href="logistic.html#cb1973-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print summary of results</span></span>
<span id="cb1973-7"><a href="logistic.html#cb1973-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2_b)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Turnover ~ scale(JS) + scale(NAff) + scale(TI), 
##     family = binomial, data = td)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3347  -0.9248   0.5881   0.8403   2.2273  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   0.5669     0.2415   2.348  0.01888 * 
## scale(JS)    -0.2675     0.2542  -1.053  0.29253   
## scale(NAff)   0.6190     0.2588   2.392  0.01674 * 
## scale(TI)     0.7685     0.2713   2.832  0.00462 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 127.02  on 94  degrees of freedom
## Residual deviance: 105.23  on 91  degrees of freedom
##   (4 observations deleted due to missingness)
## AIC: 113.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
</div>
<div id="glm_predictingprobabilities_multiple_logistic" class="section level4" number="49.3.4.3">
<h4><span class="header-section-number">49.3.4.3</span> Compute Predicted Probabilities</h4>
<p>When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didn’t append this new vector to of values to our data frame object (<code>td</code>). For the sake of clarity, we will re-do this step using the <code>predict</code> function from base R once more. We’ll begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (<code>pred.prob</code>), using the <code>&lt;-</code> assignment operator. Next, type the name of the <code>predict</code> function from base R. As the first argument, type the name of our logistic regression model (<code>model1</code>). As the second argument, enter the argument <code>type="response"</code> to indicate that you want to predict the outcome (response) variable.</p>
<div class="sourceCode" id="cb1975"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1975-1"><a href="logistic.html#cb1975-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate predicted probabilities based on</span></span>
<span id="cb1975-2"><a href="logistic.html#cb1975-2" aria-hidden="true" tabindex="-1"></a><span class="co"># sample used to estimate the logistic regression model</span></span>
<span id="cb1975-3"><a href="logistic.html#cb1975-3" aria-hidden="true" tabindex="-1"></a>pred.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<p>Now let’s add the <code>pred.prob</code> vector as a new variable called <code>pred.prod.allpred</code> in the <code>td</code> data frame object.</p>
<ol style="list-style-type: decimal">
<li>Let’s overwrite the existing <code>td</code> data frame object by using the <code>&lt;-</code> assignment operator.</li>
<li>Specify the name of the <code>merge</code> function from base R. For more information on this function, please refer to <a href="join.html#join_mergefunction">this chapter supplement</a> from the chapter on joining data frames.</li>
</ol>
<ul>
<li>As the first argument in the <code>merge</code> function, specify <code>x=</code> followed by the name of the <code>td</code> data frame object.</li>
<li>As the second argument in the <code>merge</code> function, specify <code>y=</code> followed by the <code>data.frame</code> function from base R. As the sole argument within the <code>data.frame</code> function specify a name for the new variable that will contain the predicted probabilities based on <code>JS</code> scores (<code>pred.prod.allpred</code>), followed by the <code>=</code> operator and the name of the vector object containing the predicted probabilities (<code>pred.prod</code>).</li>
<li>As the third argument in the <code>merge</code> function, type <code>by="row.names"</code>, which will match rows from the <code>x</code> and <code>y</code> data frame objects based on their respective row names (i.e., row numbers).</li>
<li>As the fourth argument in the <code>merge</code> function, type <code>all=TRUE</code> to request a full merge, such that all rows with data will be retained from both data frame objects when merging.</li>
</ul>
<div class="sourceCode" id="cb1976"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1976-1"><a href="logistic.html#cb1976-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple logistic regression model with predicted probabilities</span></span>
<span id="cb1976-2"><a href="logistic.html#cb1976-2" aria-hidden="true" tabindex="-1"></a><span class="co"># added as new variable in existing data frame object</span></span>
<span id="cb1976-3"><a href="logistic.html#cb1976-3" aria-hidden="true" tabindex="-1"></a>td <span class="ot">&lt;-</span> <span class="fu">merge</span>(<span class="at">x=</span>td, </span>
<span id="cb1976-4"><a href="logistic.html#cb1976-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">y=</span><span class="fu">data.frame</span>(</span>
<span id="cb1976-5"><a href="logistic.html#cb1976-5" aria-hidden="true" tabindex="-1"></a>              <span class="at">pred.prod.allpred =</span> pred.prob),</span>
<span id="cb1976-6"><a href="logistic.html#cb1976-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">by=</span><span class="st">&quot;row.names&quot;</span>,</span>
<span id="cb1976-7"><a href="logistic.html#cb1976-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">all=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>If we print the first six rows from the <code>td</code> data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model.</p>
<div class="sourceCode" id="cb1977"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1977-1"><a href="logistic.html#cb1977-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 6 rows of data frame object</span></span>
<span id="cb1977-2"><a href="logistic.html#cb1977-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>##   Row.names Row.names     ID Turnover   JS   OC   TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS pred.prod.allpred
## 1         1         1 EMP559        1 4.96 5.32 0.51 1.87 4.73 5.73    0.4216566                 0        0.08371016
## 2        10        18 EMP643        1 3.35 2.94 3.60 2.08 3.12 4.12    0.5960009                 1        0.73187048
## 3        11        19 EMP644        1 2.33 0.80 3.22 2.45 2.10 3.10    0.6974856                 1        0.79306191
## 4        12         2 EMP561        1 1.72 1.47 4.08 2.48 1.49 2.49    0.7507079                 1        0.90827169
## 5        13        20 EMP647        0 2.98 2.16 2.41 2.32 2.75 3.75    0.6343220                 1        0.57694329
## 6        14        21 EMP677        0 4.64 2.92 1.48 2.43 4.41 5.41    0.4561403                 0        0.31447250</code></pre>
<p>We can now dichotomize the predicted probabilities (<code>pred.prod.allpred</code>), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new variable called <code>dich.pred.prod.allpred</code> (or whatever we would like to call it) based on the values from the <code>pred.prod.allpred</code> variable we just created. Next, for the <code>dich.pred.prod.allpred</code>, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out <a href="clean.html#clean">chapter on cleaning data</a>.</p>
<div class="sourceCode" id="cb1979"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1979-1"><a href="logistic.html#cb1979-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay)</span></span>
<span id="cb1979-2"><a href="logistic.html#cb1979-2" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prod.allpred <span class="ot">&lt;-</span> td<span class="sc">$</span>pred.prod.allpred</span>
<span id="cb1979-3"><a href="logistic.html#cb1979-3" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prod.allpred[td<span class="sc">$</span>dich.pred.prod.allpred <span class="sc">&gt;=</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1979-4"><a href="logistic.html#cb1979-4" aria-hidden="true" tabindex="-1"></a>td<span class="sc">$</span>dich.pred.prod.allpred[td<span class="sc">$</span>dich.pred.prod.allpred <span class="sc">&lt;</span> .<span class="dv">50</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<div class="sourceCode" id="cb1980"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1980-1"><a href="logistic.html#cb1980-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print first 6 rows of data frame object</span></span>
<span id="cb1980-2"><a href="logistic.html#cb1980-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(td)</span></code></pre></div>
<pre><code>##   Row.names Row.names     ID Turnover   JS   OC   TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS pred.prod.allpred dich.pred.prod.allpred
## 1         1         1 EMP559        1 4.96 5.32 0.51 1.87 4.73 5.73    0.4216566                 0        0.08371016                      0
## 2        10        18 EMP643        1 3.35 2.94 3.60 2.08 3.12 4.12    0.5960009                 1        0.73187048                      1
## 3        11        19 EMP644        1 2.33 0.80 3.22 2.45 2.10 3.10    0.6974856                 1        0.79306191                      1
## 4        12         2 EMP561        1 1.72 1.47 4.08 2.48 1.49 2.49    0.7507079                 1        0.90827169                      1
## 5        13        20 EMP647        0 2.98 2.16 2.41 2.32 2.75 3.75    0.6343220                 1        0.57694329                      1
## 6        14        21 EMP677        0 4.64 2.92 1.48 2.43 4.41 5.41    0.4561403                 0        0.31447250                      0</code></pre>
<p>It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the predictor variables, which we could then use to plug into our model; to do this with a model estimated using the <code>glm</code> model, will create a toy data frame with “fresh” predictor variable values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object – just like we did in the <a href="compensatory.html#compensatory">chapter on applying a compensatory approach to selection decisions using multiple linear regression</a>.</p>
<p>In this toy data frame that we’re naming <code>fresh_td</code>, we use the <code>data.frame</code> function from base R. As the first argument, we’ll create an <code>ID</code> variable with some fictional employee ID numbers. As the second argument, we’ll create a <code>JS</code> variable with some fictional job satisfaction scores. As the third argument, we’ll create a <code>NAff</code> variable with some fictional negative affectivity scores. As the fourth argument, we’ll create a <code>TI</code> variable with some fictional turnover intentions scores.</p>
<div class="sourceCode" id="cb1982"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1982-1"><a href="logistic.html#cb1982-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create toy data frame object for illustration purposes</span></span>
<span id="cb1982-2"><a href="logistic.html#cb1982-2" aria-hidden="true" tabindex="-1"></a>fresh_td <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">ID=</span><span class="fu">c</span>(<span class="st">&quot;EMP1000&quot;</span>,<span class="st">&quot;EMP1001&quot;</span>,<span class="st">&quot;EMP1002&quot;</span>,</span>
<span id="cb1982-3"><a href="logistic.html#cb1982-3" aria-hidden="true" tabindex="-1"></a>                            <span class="st">&quot;EMP1003&quot;</span>,<span class="st">&quot;EMP1004&quot;</span>,<span class="st">&quot;EMP1005&quot;</span>),</span>
<span id="cb1982-4"><a href="logistic.html#cb1982-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">JS=</span><span class="fu">c</span>(<span class="fl">4.5</span>, <span class="fl">1.6</span>, <span class="fl">0.7</span>,</span>
<span id="cb1982-5"><a href="logistic.html#cb1982-5" aria-hidden="true" tabindex="-1"></a>                            <span class="fl">5.9</span>, <span class="fl">2.1</span>, <span class="fl">3.0</span>),</span>
<span id="cb1982-6"><a href="logistic.html#cb1982-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">NAff=</span><span class="fu">c</span>(<span class="fl">0.9</span>, <span class="fl">3.3</span>, <span class="fl">6.0</span>,</span>
<span id="cb1982-7"><a href="logistic.html#cb1982-7" aria-hidden="true" tabindex="-1"></a>                            <span class="fl">4.2</span>, <span class="fl">4.0</span>, <span class="fl">1.9</span>),</span>
<span id="cb1982-8"><a href="logistic.html#cb1982-8" aria-hidden="true" tabindex="-1"></a>                       <span class="at">TI=</span><span class="fu">c</span>(<span class="fl">6.0</span>, <span class="fl">5.5</span>, <span class="fl">4.8</span>,</span>
<span id="cb1982-9"><a href="logistic.html#cb1982-9" aria-hidden="true" tabindex="-1"></a>                            <span class="fl">3.0</span>, <span class="fl">0.1</span>, <span class="fl">1.1</span>))</span></code></pre></div>
<p>When estimating the predicted probabilities of turnover based on these new predictor variable scores, we’ll do the following:</p>
<ol style="list-style-type: decimal">
<li>Using the <code>&lt;-</code> assignment operator, we will create a new variable called <code>prob_allpred</code> that we will append to the <code>fresh_td</code> toy data frame object (using the <code>$</code> operator).</li>
<li>To the right of the <code>&lt;-</code> assignment operator, type the name of the <code>predict</code> function from base R.</li>
</ol>
<ul>
<li>As the first argument, type the name of the logistic regression model object we estimated using the <code>td</code> (original) data frame.</li>
<li>As the second argument, type <code>newdata=</code> followed by the name of the data frame object containing “fresh” data (<code>fresh_td</code>). It’s important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated.</li>
<li>As the third argument, insert <code>type="response"</code>, which will request the predicted probabilities.</li>
</ul>
<div class="sourceCode" id="cb1983"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1983-1"><a href="logistic.html#cb1983-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use multiple logistic regression model to estimate </span></span>
<span id="cb1983-2"><a href="logistic.html#cb1983-2" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted probabilities of turnover for &quot;fresh&quot; data</span></span>
<span id="cb1983-3"><a href="logistic.html#cb1983-3" aria-hidden="true" tabindex="-1"></a>fresh_td<span class="sc">$</span>prob_allpred <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, <span class="co"># name of logistic regression model</span></span>
<span id="cb1983-4"><a href="logistic.html#cb1983-4" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">newdata=</span>fresh_td, <span class="co"># name of &quot;fresh&quot; data frame</span></span>
<span id="cb1983-5"><a href="logistic.html#cb1983-5" aria-hidden="true" tabindex="-1"></a>                                 <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1984"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1984-1"><a href="logistic.html#cb1984-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print data frame object</span></span>
<span id="cb1984-2"><a href="logistic.html#cb1984-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fresh_td)</span></code></pre></div>
<pre><code>##        ID  JS NAff  TI prob_allpred
## 1 EMP1000 4.5  0.9 6.0    0.8142131
## 2 EMP1001 1.6  3.3 5.5    0.9897887
## 3 EMP1002 0.7  6.0 4.8    0.9993788
## 4 EMP1003 5.9  4.2 3.0    0.9172278
## 5 EMP1004 2.1  4.0 0.1    0.6111323
## 6 EMP1005 3.0  1.9 1.1    0.2024548</code></pre>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hosmerlemeshow2000" class="csl-entry">
Hosmer, D W, and S Lemeshow. 2000. <em>Applied Logistic Regression</em>. New York, NY: John Wiley &amp; Sons.
</div>
<div id="ref-nagelkerke1991note" class="csl-entry">
Nagelkerke, N J D. 1991. <span>“A Note on a General Definition of the Coefficient of Determination.”</span> <em>Biometrika</em> 78 (3): 691–92.
</div>
<div id="ref-osborne2015" class="csl-entry">
Osborne, J W. 2015. <em>Best Practices in Logistic Regression</em>. Thousand Oaks, CA: Sage.
</div>
<div id="ref-R-readr" class="csl-entry">
Wickham, Hadley, Jim Hester, and Jennifer Bryan. 2021. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr">https://CRAN.R-project.org/package=readr</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="turnoverchisquare.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kfold.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
