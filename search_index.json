[["index.html", "Preface 0.1 Growth of HR Analytics 0.2 Skills Gap 0.3 Project Life Cycle Perspective 0.4 Overview of HRIS &amp; HR Analytics 0.5 My Philosophy for This Book 0.6 Structure 0.7 About the Author 0.8 Contacting the Author 0.9 Acknowledgements", " R for HR: An Introduction to Human Resource Analytics Using R David E. Caughlin Version 0.1.5: 2024-05-02 Preface This book is free to read and is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The contents of this book may not be used for commercial purposes. This is Version 0.1.5 of this book, which means that it is in a nearly complete form and will undergo further editing and expansion. 0.1 Growth of HR Analytics The term human resource analytics can mean different things to different people and to different organizations. Further, human resource analytics sometimes goes by other names like people analytics, talent analytics, workforce analytics, and human capital analytics. While some may argue for distinctions between these different names, for this book, I will treat them as interchangeable labels. Moreover, for the purposes of this book, human resource (HR) analytics is defined as the “process of collecting, analyzing, interpreting, and reporting people-related data for the purpose of improving decision making, achieving strategic objectives, and sustaining a competitive advantage” (Bauer et al. 2025, 42). The foundation of HR analytics formed over a century ago with the emergence of disciplines like industrial and organizational (I/O) psychology. In recent decades, advances in information technology and systems have reduced the time HR professionals spend on transactional and administrative activities, thereby creating more time and opportunity for transformational activities supporting the realization of strategic organizational objectives. HR analytics has the potential to play an integral role in such transformational activities, as it can inform HR system design (e.g., choosing selection tools, validating selection tools) and high-stakes decision making involving people-related data from the organization. A 2023 survey of companies highlighted the lack of readiness to adopt and integrate HR analytics (Institute 2023). 0.2 Skills Gap Although many organizations regard HR analytics as strategically important for organizational success, today many of those same organizations face an HR analytics talent shortage. To some extent, the talent shortage can be attributed to data literacy – or the lack thereof. Historically, academic and professional HR training and development opportunities did not emphasize data-literacy skills, and this omission has left organizations today scrambling to hire external talent or to close the skills gap of existing HR professionals. To address the HR analytics talent shortage and skills gap, organizations have, broadly speaking, two options. First, for some organizations, closing the skills gap may be as straightforward as hiring a “quant” (e.g., data scientist, statistician), provided the individual works closely with HR professionals when working with data associated with HR systems, policies, and procedures, and identifying HR-specific legal and ethical issues. Second, I would argue that for most organizations perhaps a better alternative is to close the skills gap among current HR professionals, as their HR-specific knowledge, skills, abilities, and other characteristics (KSAOs) offer tremendous value when deriving insights from HR data as well as a solid domain-specific foundation for subsequently layering on data-literacy KSAOs. Importantly, those with existing HR domain expertise presumably have working knowledge of prevailing employment and labor laws and experience with anticipating and uncovering ethical issues, both of which are necessary when acquiring, managing, analyzing, visualizing, and reporting HR data. At the most basic level, proficiency in HR analytics involves the integration of knowledge, skills, abilities, and other characteristics (KSAOs) associated with HR expertise and data literacy. 0.3 Project Life Cycle Perspective When building efficacy in HR analytics, I have found that it’s helpful to envision where and how contributions can be made at the project level and which specific KSAOs are required at each phase. To that end, I developed the HR Analytics Project Life Cycle (HRAPLC) as a way to conceptualize the prototypical phases of a generic project life cycle. These phases include: Question Formulation, Data Acquisition, Data Management, Data Analysis, Data Interpretation and Storytelling, and Deployment and Implementation. I dedicate Part 1 of this book to providing a conceptual overview of the HRPLC in Chapters 1-7. The Human Resource Analytics Project Life Cycle (HRAPLC) offers a way to conceptualize the prototypical phases of a generic HR analytics project life cycle. 0.4 Overview of HRIS &amp; HR Analytics If you are just looking for a basic overview of HR information systems (HRIS) and HR analytics, consider checking out the following introductory video. Link to conceptual video: https://youtu.be/3X7qmb1M39A And if you are looking for an introduction to human resource management with supplementary Excel-based tutorials and data exercises, I recommend checking out one of the following textbooks I co-authored: Bauer, T. N., Erdogan, B., Caughlin, D. E., &amp; Truxillo, D. M. (2025). Human resource management: People, data, and analytics (2nd ed.). Thousand Oaks, CA: Sage Bauer, T. N., Erdogan, B., Caughlin, D. E., &amp; Truxillo, D. M. (2020). Fundamentals of human resource management: People, data, and analytics. Thousand Oaks, CA: Sage. 0.5 My Philosophy for This Book Working with data does not need to be scary or intimidating; yet, over the years, I have interacted with students and professionals who carry with them what I refer to as a numerical phobia or quantitative trauma. Unfortunately, at some point in their lives, some people begin to believe that they are not suited for mathematics, statistics, and/or generally working with data. Given these psychological barriers, a primary objective of this book is to make data analytics – and HR analytics specifically – relevant, accessible, and maybe even a little fun. In early chapters, my intention is to ease the reader into foundational concepts, applications, and tools in order to build self-efficacy in HR analytics incrementally. The tutorials in each chapter are grounded in common and (hopefully) meaningful HR contexts (e.g., validating employee selection tools). As the book progresses, I introduce more challenging statistical concepts and data-analytic techniques. Reading this book and following along with the in-chapter tutorials will not lead to expert-level knowledge and skill; however, my hope is that working through this book will do the following: Build excitement for working with data to inform decision making. Instill a sense of intellectual curiosity about data and a hunger to expand boundaries of expertise. Inspire further in-depth training, education, and learning in areas and topics introduced in this book. Enhance data literacy, including knowledge and skills related to (a) critical thinking and logic, (b) mathematics, statistics, and data analysis, and (c) data visualization and storytelling with data. 0.5.1 Rationale for Using R Today, we have the potential to access and use a remarkable number of statistical and data-analytic tools. Examples of such tools include (in no particular order) R, Python, SPSS, SAS, Stata, MatLab, Mplus, Alteryx, Tableau, PowerBI, and Microsoft Excel. Notably, some of these programs can be quite expensive when it comes to user licensing or subscription costs, which can be a barrier to access for many. Programming languages like R and Python have several desirable qualities when it comes to managing, analyzing, and visualizing data. Namely, both are free to use, and both have an ever-growing number of free (add-on) packages with domain- or area-specific functions (e.g., data visualizations). It is beyond the scope of this Preface to provide an exhaustive comparison of the relative merits of R versus Python; however, when it comes to the statistical analysis of data, specifically, I argue that R provides a more user-friendly entry point for beginners as well as more advanced capabilities desired by expert users, especially for ad-hoc analyses. Moreover, the integrated development environment program called RStudio (which “sits on top of” base R) offers useful workflow tools and generally makes for an inviting environment. That said, Python has been catching up in these regards, and I wouldn’t be surprised if Python closes these gaps relative to R in the next few years. I would be remiss if I didn’t mention that the Python language is powerful and has capabilities that extend far beyond the management, analysis, and visualization of data. Fortunately, learning R makes learning Python easier (and vice versa), which means that this book can serve as a springboard for learning Python or other programming languages; in fact, RStudio allows users to create and run Python code. Finally, I believe it to be unlikely that one tool (e.g., program, language) will emerge that is ideal for every task, and thus, I encourage you to build familiarity with multiple tools so that you develop a “toolbox” of sorts, thereby allowing you to choose the best (or at least better) tool for each task. 0.5.2 Audience I have written this book with current or soon-to-be HR professionals in mind, particularly those who have an interest in upskilling their data-analytic knowledge and skills.This book can provide a meaningful context for learning key data-analytic concepts, applications, and tools that are applicable beyond the HR context. Relatedly, this book may serve as a user-friendly gateway and introduction to the programming language called R for those who are interested in other non-HR domains. 0.6 Structure This book consists of the following parts and associated chapters: HR Analytics Project Life Cycle: Overview of HR Analytics Project Life Cycle Question Formulation Data Acquisition Data Management Data Analysis Data Intrepretation &amp; Storytelling Deployment &amp; Implementation Introduction to R: Overview of R &amp; RStudio Installing R &amp; RStudio Getting Started with R &amp; RStudio Basic Features &amp; Operations of the R Language Setting a Working Directory Data Acquisition &amp; Management: Reading Data into R Removing, Adding, &amp; Changing Variable Names Writing Data from R Arranging (Sorting) Data Joining (Merging) Data Filtering (Subsetting) Data Cleaning Data Manipulating &amp; Restructuring Data Centering &amp; Standardizing Variables Removing Objects from the R Environment Employee Demographics: Introduction to Employee Demographics Describing Employee Demographics Using Descriptive Statistics Summarizing Two or More Categorical Variables Using Cross-Tabulations Applying Pivot Tables to Explore Employee Demographic Data Employee Surveys: Introduction to Employee Surveys Aggregating &amp; Segmenting Employee Survey Data Estimating Internal Consistency Reliability Using Cronbach’s alpha Creating a Composite Variable Based on a Multi-Item Measure Employee Training: Introduction to Employee Training Evaluating a Post-Test/Post-Test without Control Group Design Using Paired-Samples t-test Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA Employee Selection: Introduction to Employee Selection Investigating Disparate Impact Estimating Criterion-Related Validity of a Selection Tool Using Correlation Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method Testing for Differential Prediction Using Moderated Multiple Linear Regression Statistically &amp; Empirically Cross-Validating a Selection Tool Employee Separation &amp; Retention: Introduction to Employee Separation &amp; Retention Computing Monthly &amp; Annual Turnover Rates Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence Identifying Predictors of Turnover Using Logistic Regression Applying k-Fold Cross-Validation to Logistic Regression Understanding Length of Service Using Survival Analysis Employee Performance Management: Introduction to Employee Performance Management Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations Investigating Nonlinear Associations Using Polynomial Regression Supervised Statistical Learning Using Lasso Regression Investigating Processes Using Path Analysis Estimating a Mediation Model Using Path Analysis Employee Compensation &amp; Reward Systems: Introduction to Employee Compensation &amp; Reward Systems Preparing Market Survey Data Estimating a Market Pay Line Using Linear &amp; Polynomial Regression Identifying Pay Determinants Using Hierarchical Linear Regression Computing Compa Ratios &amp; Investigating Pay Compression Odds &amp; Ends: Primer on Data Legal &amp; Ethical Issues Judgment, Decision Making, &amp; Bias Language Considerations Creating a Data Analytics Portfolio Conducting a Literature Search &amp; Review Statistical &amp; Practical Significance Missing Data Power Analysis Evaluating Measurement Models Using Confirmatory Factor Analysis Estimating Change Using Latent Growth Modeling Evaluating a Pre-Test/Post-Test with Control Group Design Using an Independent-Samples t-test 0.7 About the Author David Caughlin is an Assistant Professor of Organizational Behavior in the Orfalea College of Business at Cal Poly (California Polytechnic State University) in San Luis Obispo, where he teaches and researches topics related to organizational behavior, human resource management, and data analytics. David received his B.S. in psychology and B.A. in Spanish from Indiana University, M.S. in industrial and organizational psychology from Indiana University - Purdue University at Indianapolis, and Ph.D. in industrial and organizational psychology from Portland State University with concentrations in quantitative methodology and occupational health psychology. His research interests are generally focused on supervisor support, work motivation, and occupational safety and health. David has co-authored research published in peer-reviewed outlets such as Journal of Applied Psychology, Journal of Management, Human Resource Management, Journal of Business and Psychology, Journal of Vocational Behavior, Journal of Occupational Health Psychology, and Psychology, Public Policy, and the Law. In addition, he has co-authored the textbooks Human Resource Management: People, Data, and Analytics and Fundamentals of Human Resource Management. David teaches undergraduate and graduate courses on topics related to organizational behavior, human resource management, information systems, and data analytics. In his HR analytics courses, David teaches students how to apply the statistical programming language R to manage, analyze, and visualize HR data to improve strategic decision making; in the process, students build data-literacy, critical-thinking, and logical-reasoning skills. He received the following teaching awards from Portland State University’s School of Business: Teaching Innovation Award (2018), “Extra Mile” Teaching Excellence Award (2019), Teaching Innovation Award (2020), and the Brenda Eichelberger “Extra Mile” Teaching Excellence Award (2022). In his free time, David enjoys outdoor activities like running, cycling, hiking, paddle boarding, and skiing. 0.8 Contacting the Author I created this free resource as a side project and outside of my day job. Thus, if you reach out to me, I may not have time to respond in a timely manner – or at all. That being said, if you would like to attempt to reach me, please email: davidcaughlin@rforhr.com. 0.9 Acknowledgements My inspiration for writing and compiling the contents of this book stems from interactions with countless colleagues, professional acquaintances, and undergraduate and graduate students. A broad “thank you” is in order for anyone with whom I have taught or had a conversation about data analytics and data science. Further, I want to thank Liz Harman for lending her copy-editing skills to this book, David Gerbing for the many conversations we have shared about teaching R using his lessR package (Gerbing, Business, and University 2021) and his introductory R data analysis textbook, and Sam Caughlin for creating the cover art for this book. Finally, I created this book using the following programs and packages: R (R Core Team 2024), RStudio (RStudio Team 2020), rmarkdown (Xie, Allaire, and Grolemund 2018; Allaire et al. 2023), knitr (Xie 2015a, 2014, 2023b), and bookdown (Xie 2016, 2023a). References "],["overviewhraplc.html", "Chapter 1 Overview of HR Analytics Project Life Cycle", " Chapter 1 Overview of HR Analytics Project Life Cycle Link to conceptual video: https://youtu.be/Ht18NwAKKcM The Human Resource Analytics Project Life Cycle (HRAPLC) offers a framework for conceptualizing the prototypical phases of a generic project life cycle. When learning how to apply HR analytics, I’ve found that it’s helpful to envision where and how contributions can be made at the project level and which specific knowledge, skills, abilities, and other characteristics (KSAOs) are required at each phase of a project. The HRAPLC phases are as follows. Question Formulation: process of identifying and posing strategy-inspired and -aligned problems and/or questions that can be solved or answered using data. Data Acquisition: process of collecting, retrieving, gathering, and sourcing data that can be used to solve problems and answer questions. Data Management: process of wrangling, cleaning, manipulating, and structuring data. Data Analysis: process of applying mathematical, statistical, and/or computational analyses to data to identify associations, differences, changes, or classes, as well as to predict the likelihood of future events, values, differences, or changes. Data Interpretation and Storytelling: process of making sense of data-analysis findings in the context of the focal problem and/or question, and of disseminating and communicating the findings to different stakeholders. Deployment and Implementation: process of prescribing or taking action based on interpretation of data-analysis findings. The HR Analytics Project Life Cycle (HRAPLC) offers a way to conceptualize the prototypical phases of a generic HR analytics project life cycle. Notably, the phases of the HRAPLC generally align with the steps of the classic scientific process, which include formulating a hypothesis, designing a study, collecting data, analyzing data, and reporting findings. This similarity underscores how HR analytics is consistent with a scientific approach to HR management. and, more generally, to an empirical approach aimed at uncovering truth based on data. Like the scientific process, the HRAPLC is predicated on empiricism, which means truth comes from data. That is, the engine of the HRAPLC runs on data, and these data serve as evidence on which we build knowledge and glean insights. The phases of the HR Analytics Project Life Cycle (HRAPLC) generally align with the classic steps of the scientific process. In the following six chapters, I provide a detailed conceptual overview of each HRAPLC phase, beginning with Question Formulation. "],["questionformulation.html", "Chapter 2 Question Formulation 2.1 Adopting a Strategic Mindset 2.2 Defining Problems &amp; Formulating Questions 2.3 Summary", " Chapter 2 Question Formulation Link to conceptual video: https://youtu.be/3TxiVitYYqQ The first phase of the HR Analytics Project Life Cycle (HRAPLC) is Question Formulation. Question formulation refers to the process of posing strategy-inspired and -aligned questions or hypotheses (that can be answered using data) in order to investigate why or how a problem occurs, what a problem might lead to or be associated with, or who is affected by the problem. Thoughtful question formulation results in (a) more effective and efficient data acquisition, management, and analysis, and (b) answers that are more useful to stakeholders. Question formulation is closely associated with problem definition, which refers to the process of framing and diagnosing a problem (e.g., challenge, opportunity, threat) for which finding a solution will bring value. The Question Formulation phase of the HR Analytics Project Life Cycle (HRAPLC) involves defining problems and formulating questions. 2.1 Adopting a Strategic Mindset When our goal is to improve the organization and the employee experience, adopting a strategic mindset sets us up for defining the right problems and formulating the right questions. A strategic mindset involves: Familiarity with strategic goals and roles, which requires recognizing and understanding strategic objects of the department and the organization, and the roles that key decision makers and stakeholders play. Understanding of organization systems, which requires the application of systems thinking when defining problems and formulating questions. Focus on opportunities for innovation, which requires thinking broadly and openly prior to narrowing focus. Application of (intellectual) curiosity, which entails asking “why”, “why not”, and “how” type questions. Before defining a problem and formulating a question, it is important to adopt a strategic mindset and to engage in divergent and convergent thinking processes. 2.1.1 Strategy Because adopting a strategy necessitates an understanding of strategy, let’s take a moment to review the concept of a strategy. Simply put, a strategy refers to “a well-devised and thoughtful plan for achieving an objective” (Bauer et al. 2025, 35). Notably, a strategy is future oriented and provides a “roadmap” towards completion of a desired objective. Moreover, realizing a strategy requires coordinating business activities to achieve both short- and long-term objectives. When creating a new strategy, we often focus on two distinguishable phases: (a) strategy formulation and (b) strategy implementation. 2.1.2 Strategy Formulation Strategy formulation involves “planning what to do to achieve organizational objectives – or in other words, the development and/or refinement of a strategy” (Bauer et al. 2025, 35–36). The prototypical strategy formulation often includes the following steps: Creating a mission statement, articulating a vision, and defining core values. The mission, vision, and values can serve as a compass by guiding organizational actions in the direction of a desired future state and by providing parameters and guidelines for reaching that future state. Analyzing the internal and external environments. Decision-making tools like a SWOT (strengths, weaknesses, opportunities, threats) analysis provide frameworks for understanding and describing the characteristics that are internal to the organization (e.g., employees) and characteristics that are external to the organization (e.g., labor market). Selecting a broad type of strategy. The type of strategy refers to the general approach an organization takes when bringing the mission, vision, and values to life, such as differentiation or cost leadership. Defining specific strategic objectives aimed at satisfying relevant stakeholders. This involves considering the needs of key stakeholders (e.g., customers, investors, shareholders, employees, broader communities) and considering what will result in a sustainable competitive advantage. Finalizing and temporarily “crystallizing” the strategy prior to strategy implementation. This step often results in a clear strategic plan that summarizes the previous four steps. 2.1.3 Strategy Implementation Once we’ve completed the strategy formulation phase, we’re ready to move onto strategy implementation, where strategy implementation refers to the process of following through on the strategic plan developed and/or refined during the strategy formulation phase (Bauer et al. 2025). This phase involves building and leveraging the organization’s human capital capabilities in order to enact and realize the strategic plan, and aligning HR policies and practices with the strategic plan. For example, if a strategic objective outlined in the strategic plan requires acquiring, managing, or retaining the best software engineers, then the organization should focus on how HR systems like recruitment, selection, performance management, and reward systems can help support that requirement. 2.1.4 Strategic Human Resource Management When HR management aligns with and supports the realization of organizational strategic objects, strategic human resource management emerges. In other words, strategic human resource management involves a strategy-oriented approach to human resource management. While most specific HR policies and practices will vary across organizations, there are practices that are generally strategically relevant for all organizations, and examples include selectively hiring new employees to ensure sufficiently high qualifications and good fit, and training employees to enhance job- and work-relevant knowledge, skills, abilities, and other characteristics (KSAOs) (Pfeffer 1998). 2.1.4.1 Video Lecture Link to video lecture: https://youtu.be/08whYkgFiQI 2.2 Defining Problems &amp; Formulating Questions The overarching purpose of the Question Formulation phase of the HRAPLC is to define problems and formulate questions for which solutions and answers, respectively, can improve the organization and push it towards its goals. In the sections that follow, we will focus on the processes of problem definition and question formulation, and ultimately learn the value of applying both divergent and convergent thinking during both of these processes. 2.2.1 Defining a Problem With a department’s and/or organization’s strategy in mind, we’re ready to define an organizationally relevant problem in need of a solution. While we typically know a problem when we see one, I found it’s helpful to take a step back and think about what a problem actually is. For our purposes, we can think of a problem as a gap between the current state of affairs and a desired state of affairs. Given that, problem definition refers to the process of framing and diagnosing a problem (e.g., challenge, opportunity, threat) for which finding a solution will be of value. As shown below, key problem-definition components often include articulating the problem statement, decision makers and stakeholders, scope, and goals (Chevallier 2016; Conn and McLean 2018). A fully formed problem definition often involves articulating key problem definition components, such as articulating the problem statement, decision makers and stakeholders, scope, and goals. With respect to the HR context, in the figure below, I provide an example focused on the problem of voluntary turnover among new customer service representatives, and provide hypothetical examples of key problem-definition components. Articulating the problem definition components is appropriate for most problems, including those specific to HR, such as the one in this example. 2.2.2 Formulating a Question Once a general problem has been defined, we’re ready to begin question formulation, which refers to the process of posing a question or hypothesis to investigate why or how a problem occurs, what a problem might lead to or be associated with, or who is affected by the problem. Question formulation can even help us flesh out the typical problem definition components outlined in the previous section. Just as we did with problem definition, we should continue to apply our strategic mindset when formulating questions. Moreover, we should focus on posing questions for which finding answers hold value for the organization, as doing so can contribute to the following (for example): a better understanding of the focal problem, the identification of potential solutions to the focal problem, solutions that meet the needs of key decision makers and stakeholders, more efficient and targeted literature searches and reviews, and more effective and focused storytelling. So what is a question in this context? A question can be described as a general line of inquiry regarding a problem or phenomenon of interest. Examples of questions are as follows. What factors influence voluntary turnover? Does engagement predict voluntary turnover? How does engagement relate to voluntary turnover? Which work characteristics predict voluntary turnover? Why do new employees turn over in the first 6 months? If, based on prior research or theory, an informed prediction can be made, we might choose to restate a question as a hypothesis, where a hypothesis can be thought of as a statement about an expected association, difference, change, or classification. Examples of hypotheses are as follows. Engagement is negatively related to voluntary turnover. Turnover intention mediates the relationship between engagement and voluntary turnover. Autonomy and task significance negatively predict voluntary turnover. New employees who participate in the formal onboarding program are less likely to quit during their first 6 months. Regardless of whether we pose a question or state a hypothesis, it’s important to remember that question formulation is often an iterative process, which means that answering one question (or testing one hypothesis) often leads to additional questions (or hypotheses). Question formulation can be an iterative process. Often, finding an answer to one question (or testing one hypothesis) leads to further questions (or hypotheses). Further, we can draw upon existing theory and research to inform the types of questions we pose (or hypotheses we state). A theory provides a way to understand or explain a phenomenon of interest in a parsimonious manner. As an example, the theory of planned behavior (Ajzen 1991) is based on a voluminous body of research, and some of the basic tenets of the theory are as follows. First, an individual’s attitude towards a behavior, perception of the norms associated with the behavior, and sense of control over enacting a behavior contribute to their intention to perform a behavior. Second, an individual’s intention to perform a behavior often leads them to perform a behavior. This theory can be applied to any number of behaviors. Given that HR professionals (and organizational leadership) are often concerned with voluntary turnover (i.e., an employee-initiated organizational separation), let’s focus on voluntary turnover. Using the theoretical tenets, we can reason that an individual’s voluntary turnover behavior might be explained by their attitudes, norms, and sense of control related to voluntary turnover, and ultimately to their intentions to turn over voluntarily. Thus, based on the theory of planned behavior, we might first focus on the proposed link between attitudes and behavioral intention, and formulate the following question: Are more positive attitudes towards leaving the organization associated with a stronger intention to voluntarily turn over? Further, by focusing on the proposed link between behavioral intention and actually enacting the behavior, we might hypothesize the following: Stronger turnover intention is associated with a higher probability of quitting. Existing theories, such as the theory of planned behavior (Azjen, 1991) can be used to inform and direct the types of questions or hypotheses that are posed during question formulation. 2.2.3 Thinking Divergently &amp; Convergently Throughout the processes of defining a problem and formulating a question, it is advisable to apply both divergent and convergent thinking. On one hand, divergent thinking refers to the process of adopting a broadened and imaginative mindset in which many ideas, possibilities, and alternatives are considered, which can lead to a large quantity of creative or novel ideas; on the other hand, convergent thinking refers to the process of adopting a critical and evaluative mindset in which various alternatives are judged, which can lead to a more refined set of high-quality ideas (Basadur, Graen, and Scandura 1986; Basadur, Runco, and Vega 2000; Reiter-Palmon and Illies 2004). When used effectively, these two ways of thinking can improve problem definition and question formulation. More specifically, once we have adopted a strategic mindset, the identification, framing, and defining of problems should involve: (a) a phase of ideation with divergent thinking, such that many potential ideas, possibilities, and ultimately problems are entertained and considered –- with the primary focus being on novelty, creativity, and quantity; and (b) a phase with deliberate convergent thinking, such that the list of potential problems generated during the previous phase is winnowed to those that will contribute the most to the organization’s strategic goals, ends, or initiatives. A similar two-phase process can be applied when formulating questions. 2.2.3.1 Video Lecture Link to video lecture: https://youtu.be/w1104yS-zJU 2.3 Summary In this chapter, we did a conceptual deep dive into the Question Formulation phase of the HR Analytics Project Life Cycle. In doing so, we reviewed the importance of adopting a strategic mindset, and then engaging in thoughtful problem definition and question formulation. Finally, we learned how a two-phase process of deliberate divergent and convergent thinking can help us identify the most important problems and questions for which solutions and answers, respectively, would provide the most value to the organization. References "],["dataacquisition.html", "Chapter 3 Data Acquisition 3.1 Employee Surveys 3.2 Rating Forms 3.3 Surveillance &amp; Monitoring 3.4 Database Queries 3.5 Scraping 3.6 Summary", " Chapter 3 Data Acquisition Link to conceptual video: https://youtu.be/osxe5Za_-74 Data acquisition refers to the process of collecting, retrieving, gathering, and sourcing data that can be used to solve problems, answer questions, and test hypotheses that were identified during the Question Formulation phase of the HR Analytics Project Life Cycle. Various tools can be used for data acquisition, such as employee surveys, (performance) rating forms, surveillance and monitoring, database queries, and scraping or crawling. In some instances, the required data may already reside in an HR information system (HRIS) or enterprise resource planning (ERP) platform, and such data are often referred to as archival. From ethical, legal, and practical perspectives, my general advice is to acquire data with a purpose. That is, if we don’t have a compelling and well-thought-out rationale to collect certain data – especially data about people – then we should probably should resist collecting such data. The Data Acquisition phase of the Human Resource Analytics Project Life Cycle (HRAPLC) involves gathering the data necessary for solving the problem or answering the question from the Question Formulation phase. 3.1 Employee Surveys When it comes to acquiring data about employee attitudes, behaviors, and feelings, the employee survey is perhaps one of the most common (if not the most common) tools. If you’re unfamiliar with employee surveys, simply put, they consist of some number of items (e.g., questions) to which employees are asked to respond. Survey items can be open-ended (e.g., “Please describe our onboarding experience.”) or close-ended with fixed response options (e.g., “I am satisfied with my job.” [1 = Strongly Disagree, 5 = Strongly Agree]), and they can vary in length, ranging from shorter yet more frequent pulse surveys to longer yet less frequent annual engagement surveys. Further, surveys can be used to deploy multi-item measures of multi-faceted and nuanced concepts (i.e., constructs) such as engagement and organizational citizenship behaviors. The quality of the data acquired using an employee survey depends largely on the quality of the survey content (e.g., quality of item writing), the appropriateness of the survey for the target population, and respondents’ motivation (or lack thereof) for taking the survey. To learn more about writing high-quality items, avoiding common pitfalls, and other design and administration considerations, I recommend checking out the Google re:Work guide for developing and administering employee surveys, as it distills many best practices into a user-friendly and efficient format. Below, I list some potential advantages and disadvantages of using employee surveys for data acquisition. Advantages: If designed well, they can be efficient and effective tools for acquiring self- or observer-report data on employee personality, attitudes, individual differences, and behaviors as well as perceptions of work, working environment, work-family interface, supervisor behavior, coworker behavior, and client behavior. They tend to be relatively affordable to administer and a variety of platforms exist today to facilitate this process (e.g., Qualtrics, SurveyMonkey). Employees are typically familiar with the concept of a survey and can exert more control over the information that is collected. Disadvantages: Some may argue that the date acquired may be more subjective in nature than the data acquired by other tools, as respondents may succumb to perceived social desirability pressures and/or fake or distort their responses. They can be time-consuming and resource-intensive to respond to and to develop. If surveyed too frequently, employees may experience “survey fatigue.” 3.2 Rating Forms Rating forms often share some of the same characteristics as employee surveys (e.g, multiple close-ended items) but tend to be more focused on measuring work-related behaviors and job performance. Examples of common types of ratings forms include the behavioral observation scale and the behavioral-anchored rating scale. Given the breadth of the performance domain for most jobs, when targeting performance, ratings forms tend to consist of multiple items or dimensions. For example, the performance domain for the prototypical customer service representative job will involve interacting with customers but will likely also involve administrative tasks, for example, involving the documentation of customer complaints. Below, I offer some advantages and disadvantages of using ratings forms for data acquisition. Advantages: If designed well, they allow raters to produce data efficiently. They can offer a standardized and consistent format that ultimately results in “cleaner” and more structured data than using no rating form at all to collect the same data. Disadvantages: Achieving sufficiently high reliability across raters can be challenging, even when they are using the same rating form. Some types of ratings forms likely the behavioral-anchored rating scale can be very time-consuming and resource-intensive to develop. Ratings may be influenced by office politics and idiosyncratic rater motivations (e.g., “I scored this person lower than they deserved to send a message.”) 3.3 Surveillance &amp; Monitoring Surveillance and monitoring offer a more discrete and less obtrusive approach to data acquisition. Examples include tracking system login information (e.g., dates, times), recording video or audio of employees, examining email correspondence, and deploying sensors and other wearable technologies (e.g., sociometric badges). Below, I offer some advantages and disadvantages of using surveillance and monitoring to acquire data. Advantages: They tend to be nonintrusive and operate “behind the scenes” which can lead to the acquisition of more realistic and authentic employee behavior. Technological advances continue to expand surveillance and monitoring capabilities, such as those designed to measure geolocation, tone of voice, interactions, heart rate, sleep quantity and quality, and exposure to noxious chemicals. Disadvantages: Without clear and transparent communication regarding the use of such tools, employees may perceive a violation of trust. The technologies behind many surveillance and monitoring tools can produce truly big data (e.g., high velocity, massive amounts, unstructured) which can make wrangling and managing the data challenging and time-consuming. Employees may have ethical and privacy concerns about these tools, including about how they data are going to be used by the organization and how they are going to be protected. 3.4 Database Queries When acquiring data that already reside in an information system or enterprise resource planning platform, a database query is often the tool of choice, where a database query refers to an action in which a request is made to access, acquire, restructure, and/or manipulate data housed in a database. Structured query language (SQL) is an example of a language that is commonly used to access data from a relational database. In many instances, the data retrieved from a database via a query meet the definition of archival data. Below, I offer some potential advantages and disadvantages of using database queries to acquire data. Advantages: They can be an efficient way to gather archival data already residing in an information system or enterprise resource planning platform. They provide an opportunity to leverage data that an organization acquired previously. Disadvantages: Just because data reside in a database does not necessarily mean they are of high quality or are trustworthy. Unless carefully documented, important characteristics and definitions regarding the data residing in a database may be challenging to locate, which means even when queried, the definition and purpose of certain fields (i.e., variables) may remain unclear. 3.5 Scraping Scraping refers to the process of extracting data from websites, documents, and other sources of information. In many cases, we use scraping to gather data that were not originally intended to be used in the way that we plan to use them. For example, to predict changes in the stock market, an analyst might scraped tweets from Twitter over some period of time, use text analysis to code their sentiment, and then correlation tweet sentiment with market performance indicators. Scraping might also be applied to emails, internal company chat applications, and even electronic documents like applicant resumes. Below, I suggest some potential advantages and disadvantages of using scraping as a data-acquisition tool. Advantages: New scraping tools and R packages have made it easier than ever to scrape data. Scraping tools can offer new insights based on previously difficult-to-reach or difficult-to-acquire text data that are rich with contextual information. Disadvantages: Scraping data that aren’t public or that weren’t intended to be used in the manner we plan to use them, can raise ethical and privacy concerns. Once scraped, the data often need to be structured into a format for subsequent, which can be a labor-intensive and exhausting process in terms of effort and time. 3.6 Summary In this chapter, we reviewed the Data Acquisition phase of the HR Analytics Project Life Cycle, which included overviews of common data-acquisition tools or techniques like employee surveys, rating forms, surveillance and monitoring, database queries, and scraping. "],["datamanagement.html", "Chapter 4 Data Management 4.1 Data Cleaning 4.2 Data Manipulation &amp; Structuring 4.3 Common Data-Management Tools 4.4 Summary", " Chapter 4 Data Management Link to conceptual video: https://youtu.be/aY5sU0bOsjA Data management refers to the process of wrangling, cleaning, manipulating, and structuring the data attained during the Data Acquisition phase of the HR Analytics Project Life Cycle. In many instances, the overarching goal of data management is to prepare the data for subsequent analysis during the Data Analysis phase. In general, you can expect to spend 80% of your time managing data and about 20% of your time analyzing data; however, upon reflection of my own personal experiences, I generally spend about 90% of my time managing data for a project and only 5% of my time actually analyzing the data. My point is that we shouldn’t underestimate how long the data manage process will take. The Data Management phase of the Human Resource Analytics Project Life Cycle (HRAPLC) involves wrangling, cleaning, manipulating, and structuring the data gathered during the Data Acquisition phase. Data analysts and database administrators manage data to complete tasks like: Defining the characteristics of the data (e.g., quantitative vs. qualitative; structured vs. unstructured); Organizing the data in a manner that promotes integration, data quality, and accessibility, particularly when data are longitudinal or multisource; Cleaning the data by addressing data-validation issues, missing data, and untrustworthy data or variables; Manipulating and joining (i.e., merging) data, and adding structure if needed; Maintaining the data security and privacy, including restricting access to only authorized individuals. 4.1 Data Cleaning Data cleaning is an essential part of data management and is discussed in greater detail in the Data Cleaning chapter. Broadly speaking, when we clean data, we attempt to identify, correct, or remove problematic observations, scores, or variables. More specifically, data cleaning often entails (but is not limited to): Identifying and correcting data-entry errors and inconsistent coding; Evaluating missing data and determining how to handle them; Flagging and correcting out-of-bounds scores for variables; Addressing open-ended and/or “other” responses from employee surveys; Flagging and potentially removing untrustworthy variables. By default, different spellings and cases (e.g., Beaverton, beverton, beaverton) for what is supposed to be the same category (e.g., Beaverton) will be treated as separate categories by many programs; therefore, it is important to clean the data by creating consistent category labels. 4.2 Data Manipulation &amp; Structuring Often, data require manipulation or (re)structuring prior to analysis or visualization. A common type of data manipulation is to pivot data from “wide” to “long” format or from “long to wide” format. When data are in wide format, each row contains all of the variable values for a single case (i.e., entity). For example, imagine a data table containing data related to each employee’s performance on the exact same tests administered at two points in time (e.g., pre-test, post-test) as well as their employee unique identifier number (e.g., employee ID). If this table is in wide format, then each employee’s data will be contained within a single row, such that their scores one each test belong to a separate variable (i.e., column, field). In contrast, when data are in long format, a single case’s data may be spread across multiple rows of data, where commonly each row belonging to a given case represents the a different observation for that case. Separation observations for a single case may have to do with the time (e.g., Time 1 vs. Time 2) or source (e.g., employee rating vs. supervisor rating) of measurement for common target. For example, the same imaginary data described above could be manipulated into long format, such that each employee would have up to two rows of data: one for each test (i.e., pre-test vs. post-test). Again, our decision regarding whether to manipulate data to wide or long format often has to do with what type of analysis or visualization we plan to apply to the data subsequently. As a prerequisite for applying many common types of analyses and visualizations, we need to apply structure to the data, which often means putting the data into rows and columns (e.g., table). Some data-acquisition tools (e.g., survey platforms) make this process relatively easy for us, as they typically export data in the form of a spreadsheet. Other data-acquisition tools (e.g., data scraping), however, will require that the data be structured prior to analysis. For example, imagine we wish to scrape data from employee emails (with their consent). In their raw form, the data contained within the typical email does not inherently fall within rows and columns, where columns represent variables and rows represent cases or observations. When data are acquired, they don’t always arrive in a structure that is amenable for subsequent analysis or visualization. For example, the data from these four emails are not yet structured in neat rows and columns. We might restructure the email data by creating separate variables, such as an email unique identifier number, sender, receiver, and subject line. Each row might then contain a single email’s data for those variables. By structuring the data into a table, we are now one step closer to applying perhaps some sort of text analysis or to construct a social network analysis. Structure can be applied to the same unstructured email data shown in the previous figure by organizing the data into columns (e.g., variables, fields) and rows (e.g., cases) – or in other words, organizing the data into a table. 4.3 Common Data-Management Tools A variety of tools can facilitate data management, including database management systems and information systems, data analysis software programs, and programming languages. Even widely available spreadsheet tools like Microsoft Excel and Google Sheets offer some data management capabilities. Enterprise database management systems and information system platforms often feature functions that enable joining and arranging data stored in tables. For example, Microsoft Access is perhaps one of the simplest relational database management tools, and it is not uncommon for instructors to teach beginners fundamental database management concepts using the program given its easy-to-understand graphic interface. Other examples of database management systems and vendors include MySQL, Microsoft SQL Server, SQLite, MongoDB, Oracle, and SAP. A number of common database management platforms can be accessed using Structured Query Language (SQL). Finally, programming languages like R and Python can be used to manage data. 4.4 Summary In this chapter, we reviewed the Data Management phase of the HR Analytics Project Life Cycle, which included discussions of data cleaning, manipulating and structuring data, and common data-management tools. "],["dataanalysis.html", "Chapter 5 Data Analysis 5.1 Tools &amp; Techniques 5.2 Continuum of Data Analytics 5.3 Summary", " Chapter 5 Data Analysis Link to conceptual video: https://youtu.be/BB53j47ZX-M Data analysis refers to the process of applying mathematical, statistical, and/or computational techniques to data to identify associations, differences or changes, or classes (categories), as well as to predict the likelihood of future events, values, or differences or changes. This phase draws upon the data that was acquired and managed during the Data Acquisition and Data Management phases, respectively, of the HR Analytics Project Life Cycle. In this chapter, we will review different data-analysis tools and techniques and the continuum of data analytics. The Data Analysis phase of the Human Resource Analytics Project Life Cycle (HRAPLC) refers to the process of applying mathematical, statistical, and/or computational techniques to data to identify associations, differences or changes, or classes (categories), as well as to predict the likelihood of future events, values, or differences or changes. 5.1 Tools &amp; Techniques A variety of tools and techniques can be used for data analysis, including: Mathematics, Statistics, Machine learning, Computational modeling and simulations, and Text analyses and qualitative analyses. 5.1.1 Mathematics With the exception of some forms of qualitative analysis, mathematics acts as the foundation for most data-analysis tools and techniques. Mathematics encompasses a broad array of topics ranging from basic arithmetic to algebra, geometry, and calculus. Even the application of arithmetic and simple counting to data can yield important and valuable insights. For example, a colleague of mine works as an executive coach and shared the following anecdote about a client who was the CEO of a small business. This CEO expressed concern about spending an inordinate amount of time in meetings. In response, my colleague asked how the CEO: “How much time do you spend in meetings each week?” The CEO responded: “I don’t know. How can figure that out?” Finding the answer to the question ended up being relatively straightforward. My colleague asked if the CEO to share their work calendar for the past four weeks, and together they counted up the total number of hours the CEO spent in meetings over the four-week period and then divided by four to compute the weekly average. The issue wasn’t that the CEO was unfamiliar with basic counting and arithmetic; rather, the CEO had never considered that useful data might be acquired from a work calendar and then analyzed using simple math. To me, this anecdote serves as a reminder that even simple forms of data analysis that we learned in primary school can yield valuable insights. Of course, the world of mathematics extends well beyond basic arithmetic to other more-advanced areas like geometry, algebra, and calculus. 5.1.2 Statistics When we think of data analysis in the context of HR analytics, we often focus on statistics, and thus we’ll spend a bit more time addressing this type of data analysis. Statistics is a branch of applied mathematics used to describe or infer something about a population, where a population could be, for example, all of the employees in an organization. In general, we can differentiate between descriptive and inferential statistics. Descriptive statistics are used to describe the characteristics of a sample drawn from a population; often, when dealing with data about human beings in organizations, it’s not feasible to attain data for the entire population, so instead we settle for what is hopefully a representative sample of individuals from the focal population. Common types of descriptive statistics include counts (i.e., frequencies), measures of central tendency (e.g., mean, median, mode), and measures of dispersion (e.g., variance, standard deviation, interquartile range). When we analyze employee demographic data, for example, we often compute descriptive statistics like the number of employees who identify with each race/ethnicity category or the average employee age and standard deviation. It’s important to remember that descriptive statistics are, well, descriptive. That is, they help us describe characteristics of a sample, but when computed with data from a sample, they don’t allow us to make inferences about the underlying population from which the sample was drawn. In contrast, we apply inferential statistics when we wish to make inferences about a particular population based on a sample of data we have collected from that population. A common approach to making these inferences is the application of null hypothesis significance testing (NHST). If you’re familiar with p-values, then you may also be familiar with NHST. For example, in the HR context, we commonly pilot new training programs with a sample of employees and collect outcome data, and then apply an inferential statistical analysis (e.g., independent-samples t-test) to infer whether the new training program will result in better outcomes than the older training program if applied to the overall population of employees. Two specific types of inferential statistics include parametric and nonparametric statistics. Simply put, parametric statistics make certain assumptions about population parameters and distributions (e.g., normally distributed), whereas nonparametric statistics don’t make these assumptions. Thus, when determining which inferential statistical analysis to apply to a sample of data, we often begin by investigating whether the assumptions for a parametric statistical analysis have been met, and if we don’t find that the data satisfy those assumptions, we then move on to selecting an appropriate nonparametric statistical analysis. Classic examples of parametric statistics include: t-tests, analyses of variance, Pearson product-moment correlations, and ordinary least squares linear regression. Examples of nonparametric statistics include: Mann-Whitney U-test, Kruskal-Wallis one-way analysis, and McNemar’s test. 5.1.3 Machine Learning Machine learning allows us to extend mathematical and statistical analysis of data - often to very large amounts of data. Specifically, machine learning is form of artificial intelligence in which data are statistically analyzed in ways that allow the “machine” to “learn” when presented with new data (for a brief yet broad overview, see Jordan and Mitchell 2015). Machine-learning algorithms (i.e., models) are often developed for the purposes of making more accurate predictions or classifications – or for finding patterns. For example, an HR data analyst might train, test, and validate a model aimed at accurately predicting who is at-risk for leaving the organization within a given time period. These algorithms can also be applied more broadly to automate processes and systems, with a famous example being how spam emails are detected and assigned to a spam folder (Guzella and Caminhas 2009; Gan 2018). Let’s distinguish between two types of machine learning: supervised and unsupervised (Oswald and Putka 2016). In supervised learning, the model is trained, tested, and validated on data with labeled variables and with particular types of patterns, predictions, or classifications specified (e.g., decision tree, Lasso regression). In contrast, in unsupervised learning, variables are unlabeled, and structures, patterns, or clusters are inferred from the data, making this type of machine learning more exploratory in nature (e.g., K-means). 5.1.4 Computational Modeling &amp; Simulations To understand and predict how complex systems behave, we can apply computational modeling and simulations. These tools are particularly useful in contexts in which it might be impractical, difficult (e.g., expensive, resource-intensive), or just not feasible to acquire and analyze real data (for a brief yet broad overview, see Calder et al. 2018). For example, employee interactions are complex and dynamic, and although technologies like sociometric badges enable us to monitor and surveil employee interactions in a mostly unobtrusive manner, such technologies may not be able to capture the specific type of interaction we’re interested in – and they may pose ethical and legal issues. Further, some types of interactions and relationship building may take months – or even years – to emerge, and it may not be practical to wait that long. To address these limitations and concerns, a tool like agent-based modeling (Bonabeau 2002; Smith and Conrey 2007) would allow us to specify evidence-informed characteristics and interaction decision rules (i.e., parameters) for each employee (i.e., agent) within a multi-employee system and to then observe that simulated system over a period of time. About a decade ago, I developed an agent-based model to simulate how employee emotional contagion and aggressive behavior propagate through a social network, which allowed me to understand how different social-network structures might affect the spread of emotion and aggressive behavior. In some sense, these types of simulations allow use to ask and answer “what if” questions. Other prominent examples of computational modeling and simulations include dynamical systems modeling and Monte Carlo methods, and the data generated by computational models can also be analyzed using the aforementioned data-analysis tools and techniques. Like any modeling technique, computational models are not without their limitations. For starters, the omission of relevant parameters will influence how closely the model approximates reality, and such models may fail to account for real-world unanticipated shocks to a system. Still, computation modeling can be a useful tool for helping us to test theory and to project what can or will happen in the future in complex systems. 5.1.5 Text Analyses &amp; Qualitative Analyses Text analysis is a large umbrella term for a vast array of analyses aimed at analyzing text data, including quantitative and computer-algorithmic approaches and qualitative-analysis approaches. Let’s start by describing quantitative and computer-algorithmic approaches. These approaches are computer driven and tend to efficient and scalable for large (or very large) amounts of text, and include popular toots and techniques like text mining, sentiment analysis, natural language processing, latent semantic analysis, and (broadly) computational linguistics (see Banks et al. 2018 for a review). As the label implies, quantitative and computer-algorithmic approaches often focus on translating rich sources of qualitative data into quantitative data and metrics. These approaches continue to evolve, which has led to improvements in classification and meaning-meaning extraction accuracy. Compared to a human being, quantitative and computer-algorithmic approaches can struggle to incorporate context and to detect and interpret tone and idiomatic expressions appropriately and accurately. Qualitative-analysis approaches differ substantially from quantitative and computer-algorithmic approaches in that the former rely primarily on human beings to analyze and interpret text data. The human brain still offers one of the best tools for detecting subtle contextual influences and interpreting tone and idiomatic expressions. With their reliance on the human brain, qualitative-analysis approaches can lead to a richer and more robust understanding of a phenomenon of interest captured in text data. In terms of human work hours, rigorous qualitative analysis can be quite resource-intensive, as it requires iterative coding and sensemaking by (typically) multiple independent human coders. This can make qualitative-analysis approaches less practical and feasible for large amounts of text data. Examples of qualitative analysis include content analysis, thematic analysis, narrative analysis, discourse analysis, and grounded theory (Strauss and Corbin 1997). 5.2 Continuum of Data Analytics Link to conceptual video: https://youtu.be/lxCaYg1G9fE When discussing the Data Analysis phase of the HRAPLC, I would be remiss if I failed to introduce and review the continuum of data analytics. The extent to which an organization does (or does not) engage in the full continuum of data analytics can be an indicator of that organization’s “data-analytics maturity.” In the following sections, we’ll work from descriptive analytics to predict-ish analytics to predictive analytics to prescriptive analytics. The continuum of data analytics classically includes descriptive, predictive, and prescriptive analytics; though, I like to place predict-ish analytics between descriptive and predictive analytics. 5.2.1 Descriptive Analytics Let’s begin with descriptive analytics, which falls at the beginning of the data-analytics continuum. We apply descriptive analytics when we wish to apply analyses that describe what has already happened, and thus, descriptive analytics offer a “snapshot” of the past. Common analyses include descriptive statistics (e.g., mean, standard deviation), HR metrics (e.g., turnover rate, time-to-fill), and various types of inferential statistics that we might use to infer whether differences or associations exist in a population (e.g., correlation, t-test). Some hypothetical examples of descriptive analytics include: Last year, Unit X had a 34% annual turnover rate while Unit Y had a 5% annual turnover rate. A recent survey indicated that 20% of respondents were strongly considering leaving the organization in the next 6 months. On average, employees used 15.3 days of paid time off (PTO) last year. Higher employee engage was strongly and significantly associated with higher task performance (r = .53, p = .01). Descriptive analytics involves analyzing past data and describing what has happened in the past. 5.2.2 Predict-ish Analytics Most discussions of the continuum of data analytics move straight from descriptive analytics to predictive analytics. Let’s, however, take the road less traveled by addressing the space between descriptive and predictive analytics: predict-ish analytics. You probably haven’t heard of predict-ish analytics before because, well, it’s a term I made up, as many ad-hoc data-analyses in HR analytics end up in this space. I define predict-ish analytics as the estimation of models that can predict what will happen in the future based on a snapshot of data from the past – but that don’t actually verify what will happen in the future and how accurate the original model’s predictions were. In other words, predict-ish analytics stops a bit shy of proper predictive analytics. Often predict-ish analytics take the form of inferential statistical analyses without cross-validation of any kind, and a major shortcoming is that this approach is prone to model overfitting on past data, making it unlikely that the model will fit future data well. As a hypothetical example of predict-ish analytics, imagine the following. An HR analytics team collected data from job incumbents as part of a criterion-related validation study. The team estimated a simple linear regression model by regressing scores for a job performance rating tool on scores on a structured interview selection tool. The results indicated that higher scores on the structured interview were positively and significantly associated with higher job performance ratings (b = 2.98, p = .02), such that for every 1-point increase in structured interview scores is associated with increase of 2.98 points in job performance ratings. The team has not, however, evaluated the accuracy of their model by applying it to a new sample of data from the intended population. Predict-ish analytics involves analyzing past data and building a predictive model but stopping short of testing or validating that model with a new sample of data. 5.2.3 Predictive Analytics Unlike descriptive analytics, predictive analytics focuses on the future. That being said, predictive analytics initially looks to the past. More precisely, predictive analytics refers to the process of estimating a model based on past data in order to predict what will happen in the future, and then testing and validating how accurate the model’s predictions are when the model is applied to new data. One of the classic examples of predictive analytics is the cross-validation of an inferential statistical model (e.g., linear regression model). Other more-advanced forms of predictive analytics involve applying machine-learning procedures or computational modeling. As a hypothetical example of predictive analytics, imagine the following scenario. Using available data from the previous year, an HR analyst estimated a logistic regression model with voluntary turnover (i.e., stayed vs. quit) as the outcome of interest and employee engagement and task performance as predictors. The results indicated that engagement and task performance both significant predictors of who would quit over the subsequent 6 months based on the initial sample of data, and more specifically, both engagement and task performance were negatively associated with turnover, such that those with lower engagement and task performance were more likely to quit. At the end of the following year, the analyst applied the model to new data that had been acquired for employee engagement, task performance, and voluntary turnover, and the analyst found that the model predicted who would quit and who would stay with 85% accuracy. Predictive analytics involves estimating a model based on past data, and then applying that model to new data in order to evaluate the accuracy of the model’s predictions. 5.2.4 Prescriptive Analytics At the far end of the continuum of data analytics sits prescriptive analytics. Prescriptive analytics takes predictive analytics to the next level by prescribing action and making targeted recommendations based on predictive-analytics findings. That is, in addition to interpreting the results of the predictive-analytics process, prescriptive analytics involves taking those results and coming up with evidence-informed interventions, recommendations, and actions. To illustrate how prescriptive analytics might work in practice, let’s continue with the hypothetical example that I provided for predictive analytics (see above). As a reminder, the model was focused on predicting who would voluntarily turnover based on engagement and task performance scores. Because the model predicted who would quit and how would stay with 85% accuracy, the HR analyst prescribed the following actions: The model should be applied at the beginning of each year to identify those individuals who have a higher probability of voluntarily turning over, allowing managers to check in with at-risk employees with greater frequency. Where feasible, the organization should redesign jobs to improve employee engagement, as lower engagement was found to be one of the drivers of voluntary turnover. In order to improve employee task performance, the organization should train managers how to deliver constructive feedback, as lower task performance was found to be one of the drivers of voluntary turnover. Prescriptive analytics expands upon predictive analytics by taking the results of the predictive-analytics process and recommending evidence-informed interventions and actions. 5.3 Summary In this chapter, we took a tour of the Data Analysis phase of the HR Analytics Project Life Cycle. Along the way, we learned about common tools and techniques for data analysis, including mathematics, statistics, machine learning, computational modeling and simulations, and text analyses and qualitative analyses. We concluded our tour by reviewing the continuum of data analytics, ranging from descriptive analytics to predict-ish analytics to predictive analytics to prescriptive analytics. References "],["datainterpretationstorytelling.html", "Chapter 6 Data Interpretation &amp; Storytelling 6.1 Data Interpretation 6.2 Storytelling 6.3 Data Visualization 6.4 Summary", " Chapter 6 Data Interpretation &amp; Storytelling Link to conceptual video: https://youtu.be/pwLg6SM35tQ Data interpretation and storytelling refers to the process of making sense of data analysis findings and evaluating questions and hypotheses, as well as disseminating the findings to different stakeholders. To support data interpretation and storytelling with data, data visualizations are commonly used (e.g., graphs, charts, plots, dashboards). This phase draws upon the results of the Data Analysis phase of the HR Analytics Project Life Cycle. In this chapter, we will review different data-analysis tools and techniques and the continuum of data analytics. The Data Interpretation &amp; Storytelling phase of the Human Resource Analytics Project Life Cycle (HRAPLC) refers to the process of making sense of data analysis findings and evaluating questions and hypotheses, as well as disseminating the findings to different stakeholders. Effective data interpretation and storytelling relies on the following: Recognizing the role of human decision making and judgment (including biases) when interpreting data-analytic findings; Understanding the context in which the data used for analysis were acquired and the context in which the story about the data will be told; Connecting the data-analytic findings back to the original problem or question to evaluate what (if any) solutions or answers may be informed by the findings; Acknowledging and honoring the needs and knowledge of the stakeholders to whom the story will be told; Focusing on effective communication, connection, and clarity with stakeholders when telling a story about data-analytic findings and interpretations. 6.1 Data Interpretation At this moment in time, data interpretation is still mostly a human endeavor. That is, human beings are still one of the best resources we have when it comes to interpreting the results of data analyses. In particular, humans are capable using their potentially rich knowledge of the context (e.g., population from which data were sampled; organizational history) and theory with data-analytic findings. With that being said, two people who come from different backgrounds and who have different knowledge, skills, and abilities may interpret the same findings differently, especially when it comes to the perceived practical significance of the findings. A difference in interpretation need not necessarily be a bad thing, though, as it can lead to meaningful conversations and potentially new problems that need solutions and questions that need answers. Although I treat Data Analysis and Data Interpretation &amp; Storytelling as distinct phases in the HR Analytics Project Life Cycle, analysis and interpretation often occur iteratively, such that an analyst jumps back and forth between the two phases. For example, an analyst might analyze the association between employees’ levels of job satisfaction and their level of supervisor-rated performance, and then interpret the statistical and practical significance of that finding; based on their interpretation, the analyst may decide to then analyze whether that association between job satisfaction and job performance is the same or different across organizational units, followed by interpretation of the results. 6.2 Storytelling Storytelling is an innate part of the human experience and is often used to facilitate shared learning and knowledge acquisition. Humans often construct stories to make sense of and explain their experiences – even when they’re not instructed to do so. For example, the story model from the juror decision making literature posits that jurors construct verbal narratives and mental representations of trial-related events to make sense of the information they’re presented with and to arrive at a verdict (Bennett and Feldman 1981; Devine 2012; Denning 1986; Pennington and Hastie 1993). In fact, jurors’ own life events and perceptions shape the story they construct. We can define a story as an account of a connected succession of events that is intended to instruct, inform, spur interest, and/or entertain (LiteraryTerms.net n.d.; Merriam-Webster.com n.d.; Dictionary.com n.d.). And by extension, storytelling is the process of communicating a story ourselves or to others. Telling a story can be done used any mode of communication, such as written words, spoken words, imagery, body language, music, and physical performance. Within HR analytics, we often create data visualizations (e.g., charts, graphs, plots) to support a broader story we are attempting to tell, which can be referred to more specifically as storytelling with data. In the business context, storytelling may take on a strategic purpose, which can be called strategic storytelling. To that end, an Organizational Performance Consultant named Seth Kahan described strategic storytelling as follows (Denning 2006). “In my work at NASA, I coached leaders on how to tell stories that accelerate positive change. The first thing these leaders needed to realize is that their primary goal is to make change happen, not to be seen as a good storyteller. It’s immaterial whether or not listeners are aware that a story is being told, much less that they admire the story. However, the listeners’ reactions in response to the story are critical. We are looking for changes in behavior that align with the leaders’ objectives. When that happens you have a powerful change agent equipped with a powerful tool.” — Seth Kahan, Organizational Performance Consultant 6.2.1 Structure Whether intentional or not, a story unfolds with a narrative structure, which includes the key parts or components of the story and the order in which they are presented. The linear structure is perhaps the simplest, as it partitions the story into a beginning, middle, and end – in that order. That being said, often even the simplest linear structures deploy tension in some manner, and storytellers often create tension that is in need of a resolution, leading to a climax of some kind (e.g., Freytag’s Dramatic Structure). Using tension effectively has the potential to foster greater audience engagement and interest. The mountain structure is a common storytelling structure (Knaflic 2015, 2020). This structure begins with the introduction of the plot. The plot includes providing needed context for the story and describing a gap between the current state of affairs and a desired state of affairs is introduced to the audience. To draw the audience into the story, it’s often helpful to provide “eye-catching,” provocative, or “shocking” information, as this can signal to the audience why they should care about the story you’re beginning to tell. After introducing the plot, the story builds tension through rising action to ultimately arrive at the climax, where the tension between the current and desired states of affairs reaches its peak. Arriving at the climax can be an opportunity to articulate the problem that needs a solution or the question that needs an answer. After the climax, the story resolves the tension until the ending – and in the case of strategic storytelling, falling action leads to a call to action wherein the story attempts to persuade the audience that some action is needed to achieve the desired state of affairs. If a problem (or question) was set up earlier in the story, the ending is where solutions to the problem can be offered (or answers to the question). The mountain structure builds tension (rising action) and resolves tension (falling tension). This is an example of the mountain structure applied to the HR context. 6.2.2 Clarity &amp; Parsimony Particularly in the case of strategic storytelling, it’s advisable to tell a story that is both clear and parsimonious. That is, avoid convoluted and complicated narratives that distract from the central message that the story is intended to communicate. Generally, I recommend identifying 1-3 points that you would really like for the audience to remember and/or act on – and telling a parsimonious story with deliberate and thoughtful repetition can facilitate this process. When telling stories using visual a visual medium (e.g., data visualization, PowerPoint presentation), sometimes there is a tendency to fill every available space with words or images. Often, however, it is more effective to leave unused visual space and to avoid unnecessary visual clutter. Similarly, in written and oral presentations, I recommend prioritizing brevity over verbosity and using silence before or after key ideas to create a sense of dramatic importance. To help achieve clarity and parsimony, I recommend focusing on a particular problem that needs a solution or a question that needs an answer. Doing so can help to ensure that your story will stay focused and on message. Further, towards the end of a story, sometimes it is helpful to remind the audience of the focal problem or question – and then tie the key takeaway points (e.g., call to action) back to the problem or question. 6.2.3 Influence &amp; Persuasion In the case of strategy storytelling, our goal is often to influence and persuade the audience to feel, think, or do something, and thus it’s worthwhile to unpack what influence and persuasion entail. Influence – or social influence – refers to “change in preferences or behavior that one individual or group causes in another,” and as a specific form of influence, persuasion is the “active attempt by an individual, group, or social entity (e.g., government, political party, business) to change a person’s beliefs, attitudes or behaviors by conveying information, feelings, or reasoning” (Cacioppo, Cacioppo, and Petty 2018). For example, in HR we might try to persuade: decision makers to invest in a data-informed employee retention initiative; HR professionals to build data-literacy skills; decision makers to support the development or acquisition of selection methods that leverage artificial intelligence (AI); front-line managers to check in and provide feedback regularly with their employees regarding their performance. When used effectively, influence and persuasion tactics can help HR analytics teams move from the Data Interpretation and Storytelling phase of the HR Analytics Project Life Cycle to the Deployment and Implementation phase. More specifically, influence and persuasion can facilitate the leap from knowledge acquired through data interpretation and storytelling to prescribed action that can be deployed and implemented in the organization – a process that is sometimes referred to as closing the “knowing-doing gap” (Pfeffer and Sutton 2000). For persuasion to lead to meaningful change in the audience, it is important to (a) recognize the boundaries of your own expertise, (b) understand the audience, (c) craft the message, and (d) tell the story. 6.3 Data Visualization When storytelling with data, data visualizations (e.g., graphs, charts, dashboards) can serve as useful tools. In the conceptual video below, I review data visualization from the perspective of human cognition and decision making, as it’s important to consider how those who view data visualizations will perceive, think about, and make decisions using the information contained within the display. Link to conceptual video: https://youtu.be/er1JboU3YGI 6.4 Summary In this chapter, we learned about the role of the Data Interpretation and Storytelling phase of the HR analytics Project Life Cycle, including how to tell an effect story. References "],["deploymentimplementation.html", "Chapter 7 Deployment &amp; Implementation", " Chapter 7 Deployment &amp; Implementation Link to conceptual video: https://youtu.be/EYAEppDX2e4 Deployment and implementation refers to the process of prescribing or taking action based on interpretation of data-analysis findings. This phase requires an (a) understanding of stakeholder needs, (b) an understanding of the business context, and (c) knowledge of change management theories and practices. It is during this phase of the HR Analytics Project Life Cycle (HRAPLC) that prescriptive analytics are often fully realized. Further, when deploying and implementing data-informed practices or interventions, it is advisable to apply strong change management processes and practices, as failure to do so could lead to resistance to the prescribed actions and a reduced likelihood of success. Ultimately, this phase of the HRAPLC leads to new questions, resulting in a return to the Question Formulation phase and starting the HRAPLC once more. The Deployment &amp; Implementation phase of the Human Resource Analytics Project Life Cycle (HRAPLC) refers to the process of prescribing or taking action based on interpretation of data-analysis findings, and requires understanding of stakeholder needs, understanding of the business context, and knowledge of change management theories and practices. "],["overviewR.html", "Chapter 8 Overview of R &amp; RStudio 8.1 R Programming Language 8.2 RStudio 8.3 Packages 8.4 Summary", " Chapter 8 Overview of R &amp; RStudio Link to conceptual video: https://youtu.be/BFeccMtpttA Advances in technology have paved the way for increasingly powerful, sophisticated, and expansive data-analytic tools. Examples of such tools include enterprise solutions by IBM and SAP as well as alteryx, Microsoft Power BI, Tableau, SAS, and Google TensorFlow. At the same time, barriers to entry for using powerful programming languages like R and Python have fallen as the number and quality of educational programs aimed at teaching programming has grown and the amount of free online content has increased. As time marches forward, the power, functionality, and capabilities of our data-analytics technologies have increased rapidly, with prime examples including tools like Tableau, Python, R, and TensorFlow. Compared to working with “off-the-shelf” platforms (e.g., Tableau, SAP), by working directly with programming languages, analysts can design and implement operations, models, and other tools that meet their specific needs. Programming languages allow analysts to define or customize their own functions or apply functions developed by experts around the world. In fact, these languages can have the advantage of getting us closer to our data and understanding and “seeing” the myriad decisions we must make when acquiring, managing, analyzing, and visualizing data. In this book, we focus specifically on the R programming language (R Core Team 2024). In the following sections, we will learn about about the R programming language itself as well as RStudio (RStudio Team 2020), where the latter is a integrated development environment for R. 8.1 R Programming Language In the following sections, we will learn answers to the following questions about R: What is R? Why use R? Who uses R? 8.1.1 What Is R? R is an open-source and freely available statistical programming language and environment that can be used for data management, analysis, and visualization. R is similar to the S language, where the latter was developed at Bell laboratories. You can learn more about R on the R Project website. R software can be freely downloaded for Windows, MacOS, and Linux operating systems via a Comprehensive R Archive Network (CRAN) mirror. Each CRAN mirror is a server that acts as a repository and distribution site that allows users to download copies of R-related software. Currently, CRAN mirrors are hosted at institutions all over the world, such as the University of Science and Technology Houari Boumediene in Algeria, Univerisiti Putra Malaysia in Malaysia, University of Bergen in Norway, and Indiana University in the United States. Often, R is referred to as base R, which can be useful for distinguishing it from add-on software like RStudio. To learn how to install the base R software, please refer to the following chapter. 8.1.2 Why Use R? R is a popular tool for managing, analyzing, and visualizing data. Some characteristics that make R particularly attractive are: R and its packages are free! R allows users to define new functions. Due to its open-source nature, R is often fast to react to new advances in data analysis and visualization. R is a powerful and constantly evolving language that many employers and educational institutions value. R is especially well-suited for ad hoc statistical analysis of data and data visualization. 8.1.3 Who Uses R? Data analysts and data scientists all over the world use R, including at well-known organizations like Google, Facebook, NASA, and Janssen. In addition, many scientists at academic institutions use R to statistically analyze data from research projects. 8.2 RStudio In the following sections, we will learn answers to the following questions about RStudio: What is RStudio? Why use RStudio? Who uses RStudio? 8.2.1 What is RStudio? RStudio is an integrated development environment (IDE) for R. RStudio uses base R as its engine and layers on additional features. Although an IDE like RStudio is not required to use R, using R via RStudio has a number of benefits. Namely, RStudio provides a user-friendly interface as well as easy-access to and integration with RMarkdown (Xie, Allaire, and Grolemund 2018; Allaire et al. 2023) and Shiny web applications (Chang et al. 2021), for example. Further, by default, RStudio includes window panes designated for R scripts, the Console, the Environment, and Plots. You can learn more about RStudio at the official website. Free desktop and server versions of RStudio are available, and you can learn how to download and install the desktop version in the following chapter. 8.2.2 Why RStudio? RStudio is a popular tool for implementing the R language and environment. Some characteristics that make RStudio particularly attractive are: The open-source versions of RStudio Desktop and Server are free to download and use. RStudio makes working in R easier, especially for beginners. RStudio facilitates report generation, particularly when the the report format remains generally the same over time but the data are updated. The RStudio developers hold conferences regularly, which can be great venues to connect with other R users. 8.2.3 Who Uses RStudio? Many people who use base R choose to use RStudio as well. I’ve found that it’s relatively rare to find people who work directly from base R these days. 8.3 Packages Although a base R installation comes standard with a number of very useful standard functions, a major advantage of using R is the availability packages with more specialized functions. A package contains a collection of functions, generally with an overarching theme or purpose. For example, the psych package (Revelle 2023) includes a suite of functions that are well-suited to conducting the types of analyses that are common in psychology. As I type this sentence, there are currently over 17,000 available R packages, and you can view the current list of available packages sorted by name by clicking here. Examples of packages that I demonstrate in this book include: apaTables (Stanley 2021), dplyr (Wickham et al. 2023), ggplot2 (Wickham 2016), lavaan (Rosseel 2012), lessR (Gerbing, Business, and University 2021), psych (Revelle 2023), readr (Wickham, Hester, and Bryan 2024), and tidyr (Wickham, Vaughan, and Girlich 2023). 8.4 Summary In summary, R is a powerful and widely used programming language and environment, and RStudio is an integrated development environment that layers user-friendly features and helpful tools onto R. Together, they help data analysts and data scientists manage, analyze, visualize, and report data. References "],["install.html", "Chapter 9 Installing R &amp; RStudio 9.1 Video Tutorial 9.2 Downloading &amp; Installing R 9.3 Downloading &amp; Installing RStudio 9.4 Summary", " Chapter 9 Installing R &amp; RStudio If you have a Windows, Mac, or Linux operating system, you have several ways in which you can begin working in R. Commonly, users install R on their computer along with an integrated development environment (IDE) software application like RStudio, which is owned by Posit. Posit Cloud (https://posit.cloud/), which was formerly called RStudio Cloud, has emerged as an alternative to installing R and RStudio by allowing users to use R and RStudio via the cloud, which notably allows those using the Google Chrome operating system to access R and RStudio. 9.1 Video Tutorial When it comes to learning how to use and apply R I’ve found that some people prefer written tutorials, others prefer video tutorials, and some like to learn using a combination of written and video tutorials. When first starting out, you might find it easier to follow along with a video tutorial, and as you get more comfortable with R, you may begin to prefer the written tutorials that are integrated into each chapter. Generally, the written tutorials contain more information and often more functions and operations, whereas the video tutorial provides the “need to know” information, functions, and operations. Because this first chapter is just focused on downloading and installing the R and RStudio programs, the written tutorial provided below may suffice; however, if you get stuck, you might find it useful to check out the video tutorial. As a final note, I created the video tutorial embedded in this book using a Windows computer, and thus there might be some minor aesthetic differences in the RStudio interface – as well as differences in hot keys (e.g., Ctrl+C vs. Command+C). Link to video tutorial: https://youtu.be/b18IHQERT4A 9.2 Downloading &amp; Installing R In the following sections, you will learn how to download and install the R program for Windows and Mac operating systems. The base R program must be installed prior to installing the RStudio program. R is open-source software and free to download. 9.2.1 For Windows Operation Systems R can currently run under operating systems as old as Windows Vista (circa 2007). To download R for your Windows operating system for the first time, click on this link: https://cran.r-project.org/bin/windows/base/. Once you are on the R download page, click on the hyperlink to download the current version of R for Windows. Once the file has downloaded, follow the installation prompts. 9.2.2 For Mac Operating Systems The current version of R works with Mac OS X (release 10.6 and higher). To download R for Mac OS X operating system for the first time, click on this link: https://cran.r-project.org/bin/macosx/. If you have Mac OS X 10.11 or higher, click on the hyperlink (with .pkg extension) under the “Latest release” section to begin your download. If you have Mac OS X 10.10 or lower, click on the appropriate hyperlink (with .pkg extension) under the “Binaries for legacy OS X systems” section. Once the file has downloaded, follow the installation prompts. I don’t advise using a Mac operating system that is older than Mac OS X 10.6 (which came out in 2009), as you may run into issues when using certain R packages for data analysis and visualization. 9.3 Downloading &amp; Installing RStudio RStudio is not required to use R; however, RStudio offers a number of helpful features and a user-friendly interface. More specifically, RStudio is an integrated development environment (IDE) for R. The desktop version of RStudio is free to download from the Posit website. To do so, click on this link: https://posit.co/download/rstudio-desktop/. Make sure that you have already installed the R program to your computer (see above). Click on the button labeled “Download RStudio Desktop.” Again, this version is free. Once the file has downloaded, follow the installation prompts. 9.4 Summary In this chapter, we learned how to install R and RStudio for Windows or Mac operating systems. "],["gettingstarted.html", "Chapter 10 Getting Started with R &amp; RStudio 10.1 Orientation to RStudio 10.2 Creating &amp; Saving an R Script 10.3 Creating an RStudio Project 10.4 Orientation to Written Tutorials 10.5 Summary", " Chapter 10 Getting Started with R &amp; RStudio Like any program, there is bound to be a learning curve. Personally, one of the initial barriers I face when working with a new program is navigating the graphic user interface (GUI). Given that, in this chapter we will begin with a brief orientation to the RStudio program. Subsequent sections will focus on creating and saving R scripts, creating and working with RStudio projects, and how to follow along with written tutorials like the ones presented throughout this book. 10.1 Orientation to RStudio Rather than describe a series of screenshots, I believe one of the best ways to learn how to navigate the RStudio interface is to watch someone else. In the following video, I walk through key (but not all) features of the RStudio interface. This is one of the rare occasions in this book in which only a video tutorial is provided. Please note that in the video I walk through the Windows version of RStudio Desktop; the MacOS version may have slight differences in layout. Link to video tutorial: https://youtu.be/cq4wixfCuhQ 10.2 Creating &amp; Saving an R Script An R Script is a text editor file in which you can create, edit, and save your R code for a particular task or project. An R Script file has the .R file extension. It is advisable that you type code directly into an R Script file if you wish to use the code again in the future or if you wish to save the code for another session. In general, try to avoid writing code directly into the Console using the command line if you wish to later reproduce your work. An R Script also allows you to make and save annotations (using the # symbol) to explain your code and decision making. Once you typed code (and annotations) into an R Script, you can highlight all of it (or chunks of it) and then click the Run button (or CTRL+Enter for Windows users or Command+Enter for Mac users), which is located in the upper right hand corner of the R Script editor window. In essence, an R Script allows you to save your code and to tell a story about what you have done. As much as you believe you’ll never forget what you were doing in a particular R session, you will likely forget important details as time passes. Or, imagine a scenario in which someone else inherits your data project; a well-written and -documented R Script file will help them retrace your footsteps and onboard them onto the project. If you prefer to follow along with screenshots when learning how to navigate a program’s interface, feel free to follow along with the written instructions for creating and saving an R script in this section. If, however, you prefer a voiceover of me walking through the interface menus in a video, then by all means follow along with the video below. Link to video tutorial: https://youtu.be/6_CFx5-KmMI 10.2.1 Creating a New R Script To create a new R Script in RStudio, in the drop-down menu, select File &gt; New File &gt; R Script (as shown below). 10.2.2 Using an R Script To use an R Script, simply type into the script interface. To illustrate how to do this, let’s type # Adding 2 plus 3 on the first line; note that I began the line with the # symbol, which tells R that any text written to the right is annotation and thus won’t be interpreted by R when you select it and click Run. On the next line, let’s type 2 + 3. Highlight both lines of script you typed and click the Run button (or CTRL+Enter for Windows users or Command+Enter for Mac users) (as shown below). # Adding 2 plus 3 2 + 3 ## [1] 5 Your Console window should show your output (as shown above). 10.2.3 Saving an R Script Always remember to save your R Script, and do so frequently. To save an R Script in RStudio, in the drop-down menu, select File &gt; Save As (as shown below). After that, a window will open, and you can save the R Script file in a location of your choosing and with a name of your choosing. 10.2.4 Opening a Saved R Script To open a saved R Script in RStudio, in the drop-down menu, select File &gt; Open File… (as shown below). After that, a window will open, and you can select the R Script file to open. 10.3 Creating an RStudio Project An RStudio project (or R project) file (.Rproj) is specific to RStudio and allows one to cluster associated scripts and data files into into a single workflow. For example, if you were evaluating a new onboarding program for your company, you could create an RStudio project with a common working directory that ties together any data files and R scripts that are relevant for evaluating the program. Creating an R project is not required for data management, analysis, and visualization work in RStudio, but it can be helpful. For more information on the value of RStudio projects, check out Wickham and Grolemund’s (2017) section on RStudio projects. To learn how to create an RStudio project, you have the choice between following along with screenshots and written explanations or the voiceover video tutorial below. Link to video tutorial: https://youtu.be/WyrJmJWgPiU 10.3.1 Creating a New RStudio Project First, to create a new project in RStudio, in the drop-down menu, select File &gt; New Project…. Second, when the “Create Project” window pops up, select the “New Directory” option if you have not yet created a working directory that can be used for your project (see Figure 2). [Alternatively, select the “Existing Directory” option if already have a working directory in place that can be used for your project.] Third, in the “Project Type” window, select “New Project”. Fourth, in the “Create New Project” window, input what you would like to name the new project (in the field under “Directory name”) and select the location of your working directory. Finally, click the “Create Project” button. 10.3.2 Opening an Existing RStudio Project To open an existing RStudio project, in the drop-down menu, select File &gt; Open Project…. 10.4 Orientation to Written Tutorials Throughout this book, I have included sample R code embedded in chapter tutorials, which I created using RMarkdown. This approach to demonstrating R tools and techniques is common, and thus it’s good to orient yourself to written tutorials in this format (which can be displayed in HTML or PDF formats). The following video provides an orientation to the in-chapter written tutorials involving R code that you will have the opportunity to follow along with in subsequent chapters. To learn how to create an RStudio project, you have the choice between following along with screenshots and written explanations or the voiceover video tutorial below. Link to video tutorial: https://youtu.be/1Wh6eUYAoZc 10.5 Summary In this chapter, you learned how to set a working directory, create an R script, create an RStudio project, and orient yourself to written R tutorials. First, setting the working directory is often an important step when reading (importing) and writing (exporting objects) in R. You can use the getwd function to check where your current working directory is, whereas the setwd can be used to set a new working directory. Second, writing and saving your R code in an R Script file (.R) is an important step towards reproducible data management, analysis, and visualization. Third, creating an RStudio project can streamline data-analytic projects and provides some user-friendly features. Finally, written R tutorials are commonly presented in either printed (PDF) and web-based (HTML) formats, and thus it’s worthwhile to familiarize yourself with how to follow along with these tutorial formats. References "],["gentleintro.html", "Chapter 11 Basic Features and Operations of the R Language 11.1 Video Tutorial 11.2 Functions &amp; Packages Introduced 11.3 R as a Calculator 11.4 Functions 11.5 Packages 11.6 Variable Assignment 11.7 Types of Data 11.8 Vectors 11.9 Lists 11.10 Data Frames 11.11 Annotations 11.12 Summary", " Chapter 11 Basic Features and Operations of the R Language In this chapter, you will learn about basic features of the R language along with key bits of terminology. Think of this chapter as the “gentle introduction to R” that nearly every book on R includes. Also, it is completely fine if you don’t fully grasp certain concepts and functions upon completing this chapter. We will revisit many of these concepts and functions in the HR context in subsequent chapters. Until then, use this chapter as an opportunity to practice writing R code. 11.1 Video Tutorial When exploring the basic features, operations, and functions feel free to follow along with the written tutorial below or to check out the video. In the video, I offer an abbreviated version of what’s covered in the written tutorial and focus on what I think most beginners need to know and understand early on about R. In the written tutorial, I get into some functions and operations that probably won’t become relevant until further along in your learning of using R as tool for HR analytics. Link to video tutorial: https://youtu.be/yHbVbHEjhLQ 11.2 Functions &amp; Packages Introduced Function Package print base R class base R str base R install.packages base R library base R is.numeric base R is.integer base R is.character base R is.logical base R as.Date base R as.POSIXct base R c base R data.frame base R names base R 11.3 R as a Calculator In its simplest form, R is a calculator. You can use R to carry out basic arithmetic, algebra, and other mathematical operations. The arithmetic operators in R are + (addition), - (subtraction), * (multiplication), / (division), ^ (exponent), and sqrt (square root). Below, you will find an example of these different arithmetic operators in action. In this book, lines of output are preceded by double hashtags (##); however, in your own R Console, you will not see the double hashtags before your output – unless, that is, you use double hashtags before your lines of script annotations. 3 + 2 ## [1] 5 3 - 2 ## [1] 1 3 * 2 ## [1] 6 3 / 2 ## [1] 1.5 3 ^ 2 ## [1] 9 sqrt(3) ## [1] 1.732051 Note how the six lines of output we generated (see above) appear in the same order in your Console; relatedly, remember that in R (like many other languages) the order of operations is important. In R it doesn’t matter whether there are spaces between the numeric values and the arithmetic operators. As such, we can write our code as follows and arrive at the same output. 3+2 ## [1] 5 3-2 ## [1] 1 3*2 ## [1] 6 3/2 ## [1] 1.5 3^2 ## [1] 9 sqrt(3) ## [1] 1.732051 11.4 Functions A function refers to an integrated set of instructions that can be applied consistently. Some functions also accept arguments, where an argument is used to further refine the instructions and resulting operations of the function. In R we can use functions that come standard from base R or functions that come from downloadable packages. Let’s take a look at the print function that comes standard with base R, which means that we don’t need a special package to access the function. This won’t be terribly exciting, but we can enter 3 as an argument within the print function parentheses; in general, arguments will appear within the inclusive parentheses. print(3) ## [1] 3 Note how the print function simply “printed” the numeric value 3 that we entered. We can also do the classic - yet super cliche - “Hello world!” example to illustrate how R and the print function handle text/character/string data; except, let’s change it to \"Hello HR Analytics!\". print(&quot;Hello HR Analytics!&quot;) ## [1] &quot;Hello HR Analytics!&quot; Note how we have to put text/character/string data in quotation marks. We can use double (\" \") or single quotes (' '). Some people prefer double quotes and some prefer single quotes. I happen to prefer double quotes. Now, let’s play around with the class function. The class function is used for determining the data type represented by a datum or by multiple data that are contained in a vector or variable. By entering 3 as an argument in the class variable, we find that the data type is numeric. class(3) ## [1] &quot;numeric&quot; If you would like to learn more about a function and the types of arguments that can be used within the function, you can access the help feature in R to access documentation on the function. The easiest way to do this is to enter ? before the name of the function. Upon doing so, a help window will open; if you’re using RStudio, a specific window pane dedicated to Help will open. ?class 11.5 Packages A package is a collection of functions with a common theme or that can be applied to address a similar set of problems. R packages go through a rigorous and laborious development and vetting process before being posted on the CRAN website (https://cran.r-project.org/). There are two functions that are important when it comes to installing and using packages. First, the install.packages function is used to install a package. The name of the package you wish to install should be surrounded with quotation marks (\" \" or ' ') and entered as an argument in the function. For example, if we wish to install the lessR package (Gerbing, Business, and University 2021), we type install.packages(\"lessR\"), as shown below. Please note that the names of packages (and functions, arguments, and objects) are case sensitive in R. install.packages(&quot;lessR&quot;) Once you have installed a package, you use the library function to “check out” the package from your “library” of functions. To use the function, enter the exact name of the function without quotation marks. library(lessR) ## ## lessR 4.3.0 feedback: gerbing@pdx.edu ## -------------------------------------------------------------- ## &gt; d &lt;- Read(&quot;&quot;) Read text, Excel, SPSS, SAS, or R data file ## d is default data frame, data= in analysis routines optional ## ## Learn about reading, writing, and manipulating data, graphics, ## testing means and proportions, regression, factor analysis, ## customization, and descriptive statistics from pivot tables ## Enter: browseVignettes(&quot;lessR&quot;) ## ## View changes in this and recent versions of lessR ## Enter: news(package=&quot;lessR&quot;) ## ## Interactive data analysis ## Enter: interact() We can also access a function from an installed package without using the library function. To do so, we can use the :: operator to append the function name to the package name. For illustration purposes, I use precede lessR::BoxPlot() (which would allow us to access the BoxPlot function from the lessR package) with ? to pull up the function documentation. ?lessR::BoxPlot() 11.6 Variable Assignment Variable assignment is the process of assigning a value or multiple values to a variable. There are two assignment operators that can be used for variable assignment as well as for (re)naming objects such as tables and data frames: &lt;- and =. Both work the same way. I prefer to use &lt;-, but others prefer =. In the example below, we assign the value 3 to a variable (i.e., object) we are naming x. x &lt;- 3 x = 3 Both functions achieved the same end, and the function that was run most recently overrides the previous attempt at assigning 3 to x. Using the print function we check with this worked. print(x) ## [1] 3 Or, instead of using the print function , we can simply run x by itself. x ## [1] 3 11.7 Types of Data In general, there are four different types of data in R: numeric, character, Date, and logical. 11.7.1 numeric Data numeric data are numbers or numeric values. This data type is ready-made for quantitative analysis. We can apply the is.numeric function to determine whether a value or variable is numeric; if the value or variable entered as an argument is numeric, R will return TRUE, and if it is not numeric, R will return FALSE. [Note that TRUE and FALSE statements don’t require quotation marks like text/character/string data, as they are handled differently in R.] Finally, let’s see if that \"Hello data science!\" phrase is numeric. is.numeric(3) ## [1] TRUE is.numeric(TRUE) ## [1] FALSE is.numeric(&quot;Hello data science!&quot;) ## [1] FALSE An integer is a special type of numeric data. An integer does not have any decimals, and thus is a whole number. To specify that numeric data are of type integer, L must be appended to the value. For example, to specify that 3 is an integer, it should be written as 3L. To verify that a value is in fact of type integer, we can apply the as.integer function. is.integer(3L) ## [1] TRUE is.integer(3) ## [1] FALSE Alternatively, we can use the class or str functions to determine whether a value or variable is integer or numeric. The function str is used to identify the structure of an object (e.g., data frame, variable, value). class(3L) ## [1] &quot;integer&quot; str(3L) ## int 3 class(3) ## [1] &quot;numeric&quot; str(3) ## num 3 Finally, if we assign a numeric or integer value to a variable, the resulting variable will take on the numeric or integer data type (respectively). x &lt;- 3 class(x) ## [1] &quot;numeric&quot; x &lt;- 3L class(x) ## [1] &quot;integer&quot; 11.7.2 character Data Data of type character do not explicitly or innately have quantitative properties. Sometimes this type of data is called “string” or “text” data. Data of type factor is similar to character but handled differently by R; this distinction becomes more important when working with vectors and analyses. That said, many analysis functions automatically convert character to factor for analyses, but when it comes to working with and manipulating data frames, this character versus factor distinction becomes more important. When data are of type character, we place quotation marks (\" \" or ' ') around the text. For example, if the character of interest is old, then we place quotation marks around text like this \"old\". Also note that character data are case sensitive, which means that \"old\" is not the same as \"Old\". Using the function is.character, we can determine whether data are in fact of type character. is.character(&quot;old&quot;) ## [1] TRUE Note how omitting the \" \" results in an error message. is.character(old) ## Error in eval(expr, envir, enclos): object &#39;old&#39; not found Finally, if we assign a numeric or integer value to a variable, the resulting variable will take on the numeric or integer data types. y &lt;- &quot;old&quot; class(y) ## [1] &quot;character&quot; 11.7.3 Date Data When working with dates in R, there are two different types: Date and POSIXct. Date captures just the date, whereas POSIXct captures the date and time. Behind the scenes, R treats Date numerically as the number of days since January 1, 1970, and POSIXct as the number of seconds since January 1, 1970. To specify a value as a date, we can use the as.Date function. z &lt;- as.Date(&quot;1970-03-01&quot;) class(z) ## [1] &quot;Date&quot; If we convert a variable of type Date to numeric using the as.numeric function, the result is the number of days since January 1, 1970. z &lt;- as.Date(&quot;1970-03-01&quot;) as.numeric(z) ## [1] 59 Now we can use the as.POSIXct function to specify a value as a date and time. Note the very specific format in which the data and time are to be written. z &lt;- as.POSIXct(&quot;1970-03-01 13:10&quot;) class(z) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; If we convert a variable of type POSIXct to numeric using the as.numeric function, the result is the number of seconds since January 1, 1970. z &lt;- as.POSIXct(&quot;1970-03-01 13:10&quot;) as.numeric(z) ## [1] 5173800 11.7.4 logical Data Data that are of type logical can take on values of either TRUE or FALSE, which correspond to the integers 1 and 0, respectively. As mentioned above, although TRUE and FALSE appear to be character or factor data, they are actually logical data, which means they do not require quotation marks (\" \" or ' '). w &lt;- FALSE class(w) ## [1] &quot;logical&quot; is.logical(w) ## [1] TRUE 11.8 Vectors A vector is a group of data elements in a particular order that are all the same data type. To create a vector, we can use the c function, which stands for “combine.” Within the c function parentheses, we can list the data elements and separate them by commas, as commas separate arguments within a function’s parentheses. We can also assign a vector to a variable using either the &lt;- or = operator. We can create vectors for all of the data types: numeric, character, Date, and logical. As an example, let’s create a vector of numeric values, and let’s call it a. a &lt;- c(1, 4, 7, 11, 19) Using the class and print functions, we can determine the class of our new a object and print its values, respectively. class(a) ## [1] &quot;numeric&quot; print(a) ## [1] 1 4 7 11 19 Let’s repeat this process by creating vectors containing integer, character, Date, and logical values. b &lt;- c(3L, 10L, 2L, 5L, 5L) class(b) ## [1] &quot;integer&quot; print(b) ## [1] 3 10 2 5 5 c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) class(c) ## [1] &quot;character&quot; print(c) ## [1] &quot;old&quot; &quot;young&quot; &quot;young&quot; &quot;old&quot; &quot;young&quot; d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) class(d) ## [1] &quot;Date&quot; print(d) ## [1] &quot;2018-06-01&quot; &quot;2018-06-01&quot; &quot;2018-10-31&quot; &quot;2018-01-01&quot; &quot;2018-06-01&quot; e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) class(e) ## [1] &quot;logical&quot; print(e) ## [1] TRUE TRUE TRUE FALSE FALSE We can also perform mathematical operations on vectors. For instance, we can multiply vector a (which we created above) by a numeric value, and as a result each vector value will be multiplied by that value. This is an important type of operation to remember when it comes time to transform a variable. a * 11 ## [1] 11 44 77 121 209 Note that performing mathematical operations on a vector does not automatically change the properties of the vector itself. If you inspect the a vector, you will see that the original data (e.g., 1, 4, 7, 11, 19) remain. print(a) ## [1] 1 4 7 11 19 If we want to overwrite a vector with new values based on our operations, we can use &lt;- or = to name the new vector (which, if named the same thing as the old vector, will override the old vector) and, ultimately, to create a vector with the operations applied to the original values. a &lt;- a * 11 print(a) ## [1] 11 44 77 121 209 To revert back to the original vector values for object a, we can simply specify the original values using the c function once more. a &lt;- c(1, 4, 7, 11, 19) Let’s now apply subtraction, addition, and division operators to the vector. Note that R adheres to the standard mathematical orders of operation. (3 + a) / 2 - 1 ## [1] 1.0 2.5 4.0 6.0 10.0 We can also perform mathematical operations on vectors of the same length (i.e., with the same number of data elements). In order, the mathematical operator will be applied to each pair of vector values from the respective vectors. Let’s begin by creating a new vector called f. f &lt;- c(3, 1, 3, 5, 3) Both a and f are the same length, which means we can multiply, add, divide, subtract, and exponentiate a * f ## [1] 3 4 21 55 57 a + f ## [1] 4 5 10 16 22 a / f ## [1] 0.3333333 4.0000000 2.3333333 2.2000000 6.3333333 a - f ## [1] -2 3 4 6 16 a ^ f ## [1] 1 4 343 161051 6859 11.9 Lists If we wish to combine data elements into a single list that with different data types, we can use the list function. The list function orders each data element and retains its value. g &lt;- list(1, &quot;dog&quot;, TRUE, &quot;2018-05-30&quot;) print(g) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;dog&quot; ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] &quot;2018-05-30&quot; class(g) ## [1] &quot;list&quot; 11.10 Data Frames A data frame is a specific type of table in which columns represent variables (i.e., fields) and rows represent cases (i.e., observations). We can create a simple data frame object by combining vectors of the same length. Let’s begin by creating six vector objects, which we will label a through f. a &lt;- c(1, 4, 7, 11, 19) b &lt;- c(3L, 10L, 2L, 5L, 5L) c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) f &lt;- c(3, 1, 3, 5, 3) Using the data.frame function from base R we can combine the six vectors to create a data frame object. All we need to do is enter the names of the six vectors as separate arguments in the function parentheses. Just as we did with the vectors, we can name the data frame object using the &lt;- operator (or = operator). Let’s name this data frame object R. r &lt;- data.frame(a, b, c, d, e, f) Using the print function, we can view the contents of our new data frame object called R. print(r) ## a b c d e f ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 We can also rename the columns (i.e., variables) of the data frame object by using the names function from base R along with the c function from base R. names(r) &lt;- c(&quot;TenureSup&quot;, &quot;TenureOrg&quot;, &quot;Age&quot;, &quot;HireDate&quot;, &quot;FTE&quot;, &quot;NumEmp&quot;) To view the changes to our data frame object, use the print function once more. print(r) ## TenureSup TenureOrg Age HireDate FTE NumEmp ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 Finally, we can use the class function to verify that the object is in fact a data frame. class(r) ## [1] &quot;data.frame&quot; 11.11 Annotations Part of the value of using a code/script-based program like R is that you can leave notes and explain your decisions and operations. When preceding text, the # symbol indicates that all text that follows on that line is a comment or annotation; as a result, R knows not to interpret or analyze the text that follows. To illustrate annotations, let’s repeat the steps from the previous section; however, this time, let’s include annotations. # Create six vectors a &lt;- c(1, 4, 7, 11, 19) # Vector a b &lt;- c(3L, 10L, 2L, 5L, 5L) # Vector b c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) # Vector c d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) # Vector d e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) # Vector e f &lt;- c(3, 1, 3, 5, 3) # Vector f # Combine vectors into data frame r &lt;- data.frame(a, b, c, d, e, f) # Print data frame print(r) ## a b c d e f ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 # Rename columns in data frame names(r) &lt;- c(&quot;TenureSup&quot;, &quot;TenureOrg&quot;, &quot;Age&quot;, &quot;HireDate&quot;, &quot;FTE&quot;, &quot;NumEmp&quot;) # Print data frame print(r) ## TenureSup TenureOrg Age HireDate FTE NumEmp ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 # Determine class of object class(r) ## [1] &quot;data.frame&quot; Can you start to envision how annotated code might help to tell a story about data-related decision-making processes? 11.12 Summary In this chapter, we learned the basics of working with the R statistical programming language. This chapter is by no means comprehensive, and there were probably some concepts and functions that still don’t quite make sense to you. Nonetheless, hopefully, this chapter provided you with a basic understanding of the basic operations and building blocks of R. We’ll practice applying many of the operations and functions from this chapter in subsequent chapters, which means you’ll have many more opportunities to learn and practice. References "],["setwd.html", "Chapter 12 Setting a Working Directory 12.1 Video Tutorial 12.2 Functions &amp; Packages Introduced 12.3 Identify the Current Working Directory 12.4 Set a New Working Directory 12.5 Summary", " Chapter 12 Setting a Working Directory A working directory refers to the location of a folder within a hierarchical file system. For our purposes, a working directory contains data files associated with a particular task or project. Ideally, a single working directory contains all of the data files you need for a task or project, but in some instances, it might make sense to have multiple working directories for a single project. From our designated working directory, we can read in data files (i.e., import files) to the R environment without adding long paths as prefixes in front of the variable names. Further, anytime you save a plot, data frame, or other object created in R, the default will be to save it to the folder you have set as your working directory (i.e., export files). 12.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial demonstrate how to identify what your current working directory is and how to set a new working directory. Link to video tutorial: https://youtu.be/oSqOqvMkhSE 12.2 Functions &amp; Packages Introduced Function Package getwd base R setwd base R 12.3 Identify the Current Working Directory To determine if a working directory has already been set, and if so, what that working directory is, use the getwd (get working directory) function from base R. Because this function comes standard with our R download, we don’t need to install an additional package to access it. For this function, you don’t need any arguments within the parentheses; in other words, leave the function parentheses empty. Alternatively, if you are using RStudio, you will see your current working directory next to the word “Console” in your Console window. # Find your current working directory getwd() 12.4 Set a New Working Directory Let’s assume that the current working directory is not what we want; meaning, we need to set a new or different working directory. If you need to set a new working directory, you can use the setwd function from base R. Within the parentheses, your only argument will be the working directory in quotation marks. I recommend typing your setwd function into an R Script (.R) file so that it can be saved for future sessions. I also recommend using the # to annotate your script so that you can remind yourself (and others) what you are doing. When it comes to working directories, R likes the forward slash (/) (as opposed to backslash). Remember, the working directory is the location of the data files you wish to access and bring into the R environment. You can access any folder you would like and set it as your working directory. For example, in the code below, I set my working directory to H:/RWorkshop, as that folder at the end of that path contains the data files I would like to work with. The folder (and associated path) you set as your working directory will almost certainly be different than the one I set below. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Alternatively, you may use the drop-down menus to select a working directory folder. To do so, go to Session &gt; Set Working Directory &gt; Choose Directory…, and select the folder where your files live. Upon doing so, your working directory will appear in the Console. You can copy and paste the working directory into your setwd function. Once you have set your working directory, you can verify that it was set to the correct folder by (a) typing getwd() into your console or (b) looking at the working directory listed next to the word “Console” in your Console window. 12.5 Summary In this chapter, you learned how to get and set a working directory using the getwd and setwd functions from base R. "],["read.html", "Chapter 13 Reading Data into R 13.1 Conceptual Overview 13.2 Tutorial 13.3 Chapter Supplement", " Chapter 13 Reading Data into R In this chapter, we will learn what “reading data” means in the context of the R language, and how to go about reading data into R so that we can begin managing, analyzing, and visualizing the data. 13.1 Conceptual Overview Reading data refers to the process of importing data from a (working) directory or website into R. When we read a data file into R, we often read it in as a data frame (df) object, where a data frame is a tabular display with columns representing variables and rows representing cases. For additional information on data frames, please refer to this section from a previous chapter. Many different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS). In this chapter, you will learn how to read .csv and .xlsx files into R; however, in the Chapter Supplement, you will have an opportunity to learn how to use the Read function from the lessR package, which can read in .sas7bdat (SAS) and .sav (SPSS) files. 13.2 Tutorial This chapter’s tutorial demonstrates how to read data files into R, such as those in .csv or .xlsx format. 13.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial demonstrate how to read a .csv file into R; however, in the video tutorial I demonstrate multiple functions that can read in .csv files (read.csv, read_csv, Read), whereas in the written tutorial, I demonstrate just the function I prefer to use (read_csv). In this written tutorial, I also demonstrate how to read in a .xlsx file using the read_excel function as well as some additional operations, and for time considerations, I don’t demonstrate those approaches in the video. Link to video tutorial: https://youtu.be/smWjqhaxHY8 13.2.2 Functions &amp; Packages Introduced Function Package read_csv readr excel_sheets readxl read_excel readxl View base R print base R head base R tail base R names base R colnames base R 13.2.3 Initial Steps Please note, that any function that appears in the Initial Steps section has been covered in a previous chapter. If you need a refresher, please view the relevant chapter. In addition, a previous chapter may show you how to perform the same action using different functions or packages. To get started, please save the following data files into a folder on your computer that you will set as your working directory: “PersData.csv” and “PersData_Excel.xlsx”. As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, set your working directory by using the setwd function (see below) or by doing it using drop-down menus. Your working directory folder will likely be different than the one shown below; “H:/RWorkshop” just happens to be the name of the folder that I save my data files to and that I set as my working directory. You can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. If you need a refresher on how to set a working directory, please refer to Setting a Working Directory. # Set your working directory to the folder containing your data file setwd(&quot;H:/RWorkshop&quot;) Finally, I highly recommend that you create a new R Script file (.R), which will allow you to edit and save your script and annotations. To learn more, please refer to Creating &amp; Saving an R Script. 13.2.4 Read a .csv File One of the easiest data file formats to work with when reading data into R is the .csv (comma-separated values) file format. Many HR analysts and other types of data analysts regularly work with .csv files, and .csv files can be created in Microsoft Excel and Google Sheets (as well as using many other programs). For example, many survey, data-analysis, and data-acquisition platforms allow data to be exported to .csv files. When getting started in R, the way in which the .csv file is formatted can make your life easier. Specifically, the most straightforward .csv file format to read is structured such that (a) the first row contains the names of the variables (i.e., columns, fields), and (b) the second, third, fourth, and fifth rows (and so on) contain the observed scores on the variables (i.e., data), where each row represents a case (i.e., observation, employee). In the chapter supplement section of this chapter, you will have an opportunity to read in .csv files in which the observed values do not begin until the third row or later. As part of the tidyverse of R packages (Wickham 2023; Wickham et al. 2019), the readr package (Wickham, Hester, and Bryan 2024) and its functions can be used to read in a few different data file formats (as long as they are rectangular), including .csv files. To read in .csv files, we will use the read_csv function from the readr package, as it tends to be faster than some of the other functions developed to read in data. There are several other R functions that can read in .csv files (e.g., read.csv, Read), and if you’re interested in learning two of those functions, feel free to check out the end-of-book supplement called Reading Data: Chapter Supplement. By default, the read_csv function reads data in as a data frame, where a data frame is a specific type of table in which columns contain variables and rows contain cases. Well, technically, the function reads data in as a tibble (as opposed to a data frame), where a tibble behaves a lot like a data frame. Thus, from here on out in the book, I’ll just use the term “data frame.” If you would like more information about tibbles, check out Wickham and Grolemund’s (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. To use the read_csv function, the readr package must be installed and accessed using the install.packages and library functions, respectively. Type \"readr\" (note the quotation marks) into the parentheses of the install.packages function, and run that line of code. # Install readr package install.packages(&quot;readr&quot;) Next, type readr (without quotation marks) into the parentheses of the library function. In other words, include readr as the library function’s sole parenthetical argument. Run that line of code. # Access readr package library(readr) Type the name of the read_csv function, and note that all of the letters in the function name are lowercase. As the sole argument within the function’s parentheses and within quotation marks (\" \"), type the exact name of the .csv data file as it is named in your working directory (“PersData.csv”), and be sure to follow it immediately with the .csv extension. Remember, R is a language where spaces matter in the context of file names; meaning, if there are spaces in your file name, there needs to be spaces when the file name appears in your R code. Remember, the file called “PersData.csv” should already be saved in your working directory folder (see Initial Steps). # Read .csv file into R as data frame read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see in your Console, the data frame that appears contains only a handful of rows and columns; nonetheless, this gives you an idea of how the read_csv function works. Often, you will want to assign a data frame to an object that will be stored in your (Global) Environment for subsequent use; once the data are assigned, the object becomes a data frame object. By creating a data frame object, you can manipulate and/or analyze the data within the object using a variety of functions (and without changing the data in the original .csv file). To assign the data frame to an object, we simply (a) use the same read_csv function and argument as above, (b) add either the &lt;- or = operator to the left of the read_csv function, and (c) create a name of our choosing for the data frame object by entering that name to the left of the &lt;- or = operator. You can name your data frame object whatever you would like as long as it doesn’t include spaces, doesn’t start with a numeral, and doesn’t include special characters like * or - (to name a few). I recommend choosing a name that is relatively short but descriptive, and that is not the same as another R function or variable name that you plan to use. Below, I name the new data frame object personaldata; note, however, that I could have just have easily called PersonalData, pd, df, or any other single-word name that doesn’t begin with a special character or a numeral. # Read .csv data file into R and name data frame object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Using the head function from base R, let’s print just the first 6 rows of our data frame object that we named personaldata. This will allow us to verify that everything worked as planned. # Print just the first 6 rows of the data frame object in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male If you are working in RStudio, you will see the data frame object appear in your Global Environment window panel, as shown below. If you click on the name of the data frame object in your Global Environment, a new tab will open up next to your R script editor tab, which will allow you to view the data. Alternatively, you can use the View function from base R with the exact name of the data frame object we just created as the sole parenthetical argument. Note that the View function begins with an uppercase letter. Remember, R is case and space sensitive when it comes to function names. Further, the name of the data frame object you enter into the parentheses of the function must be exactly the same as the name of the object you created. That is, R won’t recognize the data frame object if you type it as PersonalData, but R will recognize it if you type it as personaldata. Sometimes it helps to copy and paste the exact names of functions and variables into the function parentheses. # View data within data frame object View(personaldata) Instead of using the View function, you could just “run” the name of the data frame object by highlighting personaldata in your R Script and clicking “Run” (or you can enter the name of the data frame object directly into your Console command line and click Enter). To print an object to the Console, another option is to use the print function (from base R) with the name of the data frame object as the sole argument in the parentheses. Similarly, if you have many rows of data, you can use the head function from base R to print just the first 6 rows of data, or you can use the tail function from base R to print the last 6 rows of data. # Highlight the name of data frame object and run the code to view in Console personaldata ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male # Use print function with the name of the data frame object to view in Console print(personaldata) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male # Print just the first 6 rows of the data frame object in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male # Print just the last 6 rows of the data frame object in Console tail(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 125 Franklin Benjamin 1/5/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 201 Providence Cindy 1/9/2016 female ## 6 282 Legend John 1/9/2016 male If your data file resides in a folder other than your set working directory, then you can type the exact name of the path directory where the file resides followed by a forward slash (/) before the file name. Please note that your path directory will almost certainly be different than the one I show below. # Read data and name data frame object personaldata &lt;- read_csv(&quot;H:/RWorkshop/PersData.csv&quot;) Note that by assigning this data frame to an object called personaldata, we have overwritten the previous version of the object with that same name. In this case, this isn’t a big deal because we just read in the exact data using two different methods. If you don’t wish to overwrite an existing object, just name the object something unique. When naming objects, I suggest that you avoid the names of functions that you plan to use. When needed, you can also use the read_csv function to read in .csv data from a website. For example, rather than save the .csv file to a folder on your computer, you can read in the raw data directly from my GitHub site. Within the quotation marks (\" \"), simply paste in the following URL: https://raw.githubusercontent.com/davidcaughlin/R-Tutorial-Data-Files/master/PersData.csv. # Read .csv data file into R from a website personaldata &lt;- read_csv(&quot;https://raw.githubusercontent.com/davidcaughlin/R-Tutorial-Data-Files/master/PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 13.2.5 Read a .xlsx File Reading in Excel workbook files with more than one worksheet requires a bit more work. To read in a .xlsx file with multiple worksheets, we will use the excel_sheets and read_excel functions from the readxl package (Wickham and Bryan 2023). Be sure to install and access the read_xl package if you haven’t already. # Install readxl package install.packages(&quot;readxl&quot;) # Access readxl package library(readxl) To print the worksheet names within an Excel workbook file, simply type the name of the excel_sheets function, and as the sole parenthetical argument, type the exact name of the data file with the .xlsx extension – all within quotation marks (i.e., \"PersData_Excel.xlsx\"). # Print worksheet names contained within .xlsx file excel_sheets(&quot;PersData_Excel.xlsx&quot;) ## [1] &quot;Year1&quot; &quot;Year2&quot; Note that the .xlsx file contains two worksheets called “Year1” and “Year2”. We can now reference each of these worksheets when reading in the data from the Excel workbook file. To do so, we will use the read_excel function. As the first argument, enter the exact name of the data file (as named in your working directory), followed by .xlsx – and all within quotation marks (\" \"). As the second argument, type sheets= followed by the name of the worksheet containing the data you wish to read in; let’s read in the data from the worksheet called “Year1”. Finally, either the &lt;- or = operator can be used to name the data frame object. Below, I name the data frame object personaldata_year1 to avoid overwriting the data frame object we created above called personaldata. Remember to type a comma (,) before the second argument, as this is how we separate arguments from one another when there are more than one. # Read data from .xlsx sheet called &quot;Year1&quot; as data frame and assign to object personaldata_year1 &lt;- read_excel(&quot;PersData_Excel.xlsx&quot;, sheet=&quot;Year1&quot;) # Print data frame object in Console print(personaldata_year1) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 2016-01-01 00:00:00 male ## 2 154 McDonald Ronald 2016-01-09 00:00:00 male ## 3 155 Smith John 2016-01-09 00:00:00 male ## 4 165 Doe Jane 2016-01-04 00:00:00 female ## 5 125 Franklin Benjamin 2016-01-05 00:00:00 male ## 6 111 Newton Isaac 2016-01-09 00:00:00 male ## 7 198 Morales Linda 2016-01-07 00:00:00 female ## 8 201 Providence Cindy 2016-01-09 00:00:00 female ## 9 282 Legend John 2016-01-09 00:00:00 male Let’s repeat the process for the worksheet called “Year2” and assign these data to a new object. # Read data from .xlsx sheet called &quot;Year2&quot; as data frame and assign to object personaldata_year2 &lt;- read_excel(&quot;PersData_Excel.xlsx&quot;, sheet=&quot;Year2&quot;) # Print data frame object in Console print(personaldata_year2) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 2016-01-01 00:00:00 male ## 2 155 Smith John 2016-01-09 00:00:00 male ## 3 165 Doe Jane 2016-01-04 00:00:00 female ## 4 125 Franklin Benjamin 2016-01-05 00:00:00 male ## 5 111 Newton Isaac 2016-01-09 00:00:00 male ## 6 201 Providence Cindy 2016-01-09 00:00:00 female ## 7 282 Legend John 2016-01-09 00:00:00 male ## 8 312 Ramos Jorge 2017-03-01 00:00:00 male ## 9 395 Lucas Nadia 2017-03-04 00:00:00 female 13.2.6 Summary In this chapter, we learned how to read data into the R environment. Reading data into R is an important first step, and often, it is the step that causes the most problems for new R users. We practiced applying the read_csv function from the readr pack and the read_excel function from the read_xl package to read .csv and .xlsx files, respectively, into the R environment. 13.3 Chapter Supplement In this chapter supplement, I demonstrate additional functions that can be used to read in .csv files and demonstrate how to list the names of data files located in a (working directory) folder and how to skip rows of data when reading in a .csv file. 13.3.1 Functions &amp; Packages Introduced Function Package read.csv base R Read lessR list.files base R 13.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) 13.3.3 Additional Functions for Reading a .csv File In addition to the read_csv function from the readr package covered earlier in the chapter, we can read .csv files into R using the read.csv function from base R and the Read function from the lessR package (Gerbing, Business, and University 2021), which we will review in this chapter supplement. 13.3.3.1 read.csv Function from Base R The read.csv file comes standard with base R, which means that you don’t need to install a package to access the function. As the function name implies, this function is used when the source data file is in .csv format. To learn how to use the read.csv function, you have the choice to follow along with the video tutorial below or the subsequent written tutorial. Link to video tutorial: https://youtu.be/xsnOGUKtECo Typically, the read.csv function requires only a single argument within the parentheses, which will be the exact name of the data file enclosed with quotation marks; the file should be located your working directory folder. Remember, R is a language where case and space sensitivity matters when it comes to names; meaning, if there are spaces in your file name, there needs to be spaces when the file name appears in your R script, and if some letters are upper case in your file name, there needs to be corresponding upper-case letters in your R script. Let’s practice reading in a file called “PersData.csv” by entering the exact name of the file followed by the .csv extension, all within in quotation marks. Remember, the file called “PersData.csv” should already be saved in your working directory folder (see Initial Steps). # Read data from working directory read.csv(&quot;PersData.csv&quot;) ## id lastname firstname startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see, the data that appear in your Console contains only a handful of rows and columns; nonetheless, this gives you an idea of how the read.csv function works. Often, you will want to assign your data frame to an object that is stored in your Global Environment for subsequent use. By creating a data frame object, you can manipulate and/or analyze the data within the object using a variety of functions (and without changing the data in the source file). To create a data frame object, we simply (a) use the same read.csv function from above, (b) add either a &lt;- or = operator to the left of the read.csv function, and (c) create a name of our choosing for the data frame object by entering that name to the left of the &lt;- or = operator. You can name your data frame object whatever you would like as long as it doesn’t include spaces, doesn’t start with a numeral, and doesn’t include special characters like * or - (to name a few). I recommend choosing a name that is relatively short but descriptive, and that is not the same as another R function or variable name that you plan to use. Below, I name the new data frame object personaldata. # Read in data and name data frame object personaldata &lt;- read.csv(&quot;PersData.csv&quot;) 13.3.3.2 Read Function from lessR Package Just like the read.csv and read_csv functions, the Read function from the lessR package can read in .csv files; however, it can also read in other file formats like .xls/x, .sas7bdat (SAS), and .sav (SPSS). To use the Read function, the lessR package needs to be installed and accessed using the install.packages and library functions, respectively. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) When reading in a .csv file using the Read function, type the exact name of your data file from your working directory as an argument (followed by .csv and surrounded by quotation marks). Further, either the &lt;- or = operator can be used to name the data frame object. # Read data and assign to data frame object personaldata &lt;- Read(&quot;PersData.csv&quot;) ## ## &gt;&gt;&gt; Suggestions ## To read a csv or Excel file of variable labels, var_labels=TRUE ## Each row of the file: Variable Name, Variable Label ## Read into a data frame named l (the letter el) ## ## Details about your data, Enter: details() for d, or details(name) ## ## Data Types ## ------------------------------------------------------------ ## character: Non-numeric data values ## integer: Numeric data values, integers only ## ------------------------------------------------------------ ## ## Variable Missing Unique ## Name Type Values Values Values First and last values ## ------------------------------------------------------------------------------------------ ## 1 id integer 9 0 9 153 154 155 ... 198 201 282 ## 2 lastname character 9 0 9 Sanchez McDonald ... Providence Legend ## 3 firstname character 9 0 8 Alejandro Ronald ... Cindy John ## 4 startdate character 9 0 5 1/1/2016 1/9/2016 ... 1/9/2016 1/9/2016 ## 5 gender character 9 0 2 male male male ... female female male ## ------------------------------------------------------------------------------------------ ## ## ## For the column lastname, each row of data is unique. Are these values ## a unique ID for each row? To define as a row name, re-read the data file ## with the following setting added to your Read() statement: row_names=2 Let’s print just the first six rows of the personaldata data frame object to the Console to verify that everything worked as intended. # Print just the first 6 rows of the data frame object in Console head(personaldata) ## id lastname firstname startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male For more information on the Read function from the lessR package, check out David Gerbing’s website: http://www.lessrstats.com/videos.html. 13.3.4 Skip Rows of Data During Read Thus far, I have showcased some of the most common approaches to reading in data files, with an emphasis on reading in .csv files with the first row corresponding to the column (variable) names and the remaining rows containing the substantive data for cases. There are, however, other challenges and considerations you might encounter along the way. For example, some survey platforms like Qualtrics allow for data to be downloaded in .csv format; however, sometimes these platforms include variable name and label information in the second and even third rows of data as opposed to in just the first row. Fortunately, we can skip rows when reading in such data files. We’ll first learn how to skip rows with the read_csv function from the readr package, and then we’ll learn to do so using the read.csv function from base R and the Read function from the lessR package. Let’s pretend that the first row of the “PersData.csv” data file contains variable names, and the second and third rows contain variable label information and explanations. We can nest the read_csv function (from the readr package) within the names function, which will result in a vector of names from the first row of the data file. Using the &lt;- operator, let’s name this vector var_names so that we can reference it in the subsequent step. # Read variable names from first row of data var_names &lt;- names(read_csv(&quot;PersData.csv&quot;)) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Next, using the read_csv function, we will read in the data file, skip the variable names row and the first two rows of actual values (which adds to three rows), and add the variable names we pulled in the previous step. Notably, the read_csv function assumes that the first of data in your data file contain the variable names when you use the col_names argument, as we will do below. As usual, as the first argument of the read_csv function, type the exact name of the data file you wish to read in within quotation marks (\" \"). As the second argument, type skip=3 to indicate that you wish to skip the first three rows when reading in the data. As the third argument, type col_names= followed by the name of the var_names vector object we created in the previous step. Using the &lt;- operator, let’s name this data frame object test. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- read_csv(&quot;PersData.csv&quot;, skip=3, col_names=var_names) ## Rows: 7 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Finally, let’s see the fruits of our labor by printing the contents of the test data frame object to our Console. # Print data frame object in Console print(test) ## # A tibble: 7 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male The read.csv function from base R also allows for us to skip rows; however, to make the function operate like the read_csv function, we need to add the header=FALSE argument to pretend like the first row of data in the data file does not contain variable names. In doing so, we can keep the argument rows=3 the same as we did in the read_csv function above. Alternatively, if we were to set header=TRUE (which is the default setting for this function), then we would need to change the argument rows=3 to rows=2. It’s up to you which makes more intuitive sense to you. Finally, instead of col_names, the read.csv function equivalent argument is col.names. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- read.csv(&quot;PersData.csv&quot;, header=FALSE, skip=3, col.names=var_names) # Print data frame object in Console print(test) ## id lastname firstname startdate gender ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male Finally, if we take the code from above for the read.csv function and swap read.csv out with Read function (assuming we have already accessed the lessR package using the library function), then we can keep all of the arguments the same. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- Read(&quot;PersData.csv&quot;, header=FALSE, skip=3, col.names=var_names) ## ## &gt;&gt;&gt; Suggestions ## To read a csv or Excel file of variable labels, var_labels=TRUE ## Each row of the file: Variable Name, Variable Label ## Read into a data frame named l (the letter el) ## ## Details about your data, Enter: details() for d, or details(name) ## ## Data Types ## ------------------------------------------------------------ ## character: Non-numeric data values ## integer: Numeric data values, integers only ## ------------------------------------------------------------ ## ## Variable Missing Unique ## Name Type Values Values Values First and last values ## ------------------------------------------------------------------------------------------ ## 1 id integer 7 0 7 155 165 125 ... 198 201 282 ## 2 lastname character 7 0 7 Smith Doe ... Providence Legend ## 3 firstname character 7 0 6 John Jane ... Cindy John ## 4 startdate character 7 0 4 1/9/2016 1/4/2016 ... 1/9/2016 1/9/2016 ## 5 gender character 7 0 2 male female ... female male ## ------------------------------------------------------------------------------------------ ## ## ## For the column lastname, each row of data is unique. Are these values ## a unique ID for each row? To define as a row name, re-read the data file ## with the following setting added to your Read() statement: row_names=2 # Print data frame object in Console print(test) ## id lastname firstname startdate gender ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male 13.3.5 List Data File Names in Working Directory If you’re like me, and you save a lot of data files into a single folder, sometimes you find yourself flipping back and forth from RStudio to your file folder to see the exact names of the files when you’re attempting to read them into your R environment. If you would like to obtain the exact names of files located in a (working) directory, the list.files function from base R comes in handy. This function will return a list of all file names within a particular directory or file names that meet a particular pattern. For our purposes, let’s identify all of the .csv data file names contained within our current working directory. As the first argument, type path= followed by the path associated with your working directory. Second, because we are only pulling the file names associated with .csv files, enter the argument all.files=FALSE. Third, type the argument full.names=FALSE to indicate that we do not want the path to precede the file names. Finally, type the argument pattern=\".csv\" to request the names of only those file names that match the regular expression of “.csv” will be returned. # List data file names in working directory list.files(path=&quot;H:/RWorkshop&quot;, all.files=FALSE, full.names=FALSE, pattern=&quot;.csv&quot;) In your Console, you should see the list of file names you requested. You could then copy specific file names that you wish to read into R. References "],["addnames.html", "Chapter 14 Removing, Adding, &amp; Changing Variable Names 14.1 Conceptual Overview 14.2 Tutorial", " Chapter 14 Removing, Adding, &amp; Changing Variable Names In this chapter, we will learn how to remove, add, and change variable names in R. 14.1 Conceptual Overview After reading data into R as a data frame object, you may encounter situations in which it makes sense to remove the variable names (and not the data associated with the variable names), to add or replace variable names, or to just rename (change) certain variables. For example, perhaps the variable names from the original data file don’t adhere to your preferred naming conventions, and thus you wish to change the variable names. As another example, sometimes the variable names are divorced from the associated data, and thus as an initial data management step, we need to add the variable names to the associated data in R. In the following tutorial, you will learn some simple techniques to achieve these objectives. 14.2 Tutorial This chapter’s tutorial demonstrates how remove, add, and change variable names in a data frame object. 14.2.1 Video Tutorial Link to video tutorial: https://youtu.be/3m32O9f8gAI 14.2.2 Functions &amp; Packages Introduced Function Package names base R c base R head base R rename dplyr 14.2.3 Initial Steps If you haven’t already, save the file called “PersData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PersData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a “tibble” object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemund’s (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 14.2.4 Remove Variable Names from a Data Frame Object In some instances, you may wish to remove the variable names from a data frame. For example, I sometimes write (i.e., export) a data frame object I’ve been cleaning in R so that I may use the data file with the statistical software program called Mplus (Muthén and Muthén 1998-2018). Because Mplus doesn’t accept variable names within its data files, I may drop the variable names from the data frame object prior to writing to my working directory. To remove variable names, just apply the names function with the data frame name as the argument, and then use either the &lt;- operator with NULL to remove the variable names. # Remove variable names names(personaldata) &lt;- NULL # Print just the first 6 rows of the data frame object in Console head(personaldata) If you get the following error message (see below), then you likely need to convert your object from a tibble to a data frame object prior to removing the variable names. When we use the read_csv function from the readr package to read in data, we technically read in the data as a tibble as opposed to a standard data frame object. \\(\\color{red}{\\text{Error in names[old] &lt;- names(x)[j[old]] : replacement has length zero}}\\) To convert the object to a data frame object, we can use the as.data.frame object from base R as follows and then re-try the previous step. # If error message appears, convert object to data frame personaldata &lt;- as.data.frame(personaldata) # Remove variable names names(personaldata) &lt;- NULL # Print just the first 6 rows of the data frame object in Console head(personaldata) ## ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male As you can see, the variable names do not appear in the overwritten personaldata data frame object. 14.2.5 Add Variable Names to a Data Frame Object In other instances, you might find yourself with a dataset that lacks variable names (or has variable names that need to be replaced), which means that you will need to add those variable names to the data frame. Let’s work with the personaldata data frame object from the previous section for practice. To add variable names, we can use the names function from base R, and enter the name of the data frame as the argument. Using the &lt;- operator, we can specify the variable names using the c (combine) function that contains a vector of variable names in quotation marks (\" \") as the arguments. Remember to type a comma (,) between the function arguments, as commas are used to separate arguments from one another when there are more than one. Please note that the it’s important that the vector of variable names contains the same number of names as the data frame object has columns. # Add (or replace) variable names to data frame object names(personaldata) &lt;- c(&quot;id&quot;, &quot;lastname&quot;, &quot;firstname&quot;, &quot;startdate&quot;, &quot;gender&quot;) # Print just the first 6 rows of data in Console head(personaldata) ## id lastname firstname startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male Now the data frame object has variable names! 14.2.6 Change Specific Variable Names in a Data Frame Object Using the resulting data frame object from the previous section (personaldata), we can rename specific variables using the rename function from the dplyr package (Wickham et al. 2023). To get started, we’ll need to install the dplyr package so that we can access the rename function. If you haven’t already, install and access the dplyr package using the install.packages and library functions, respectively. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) We’ll begin by specifying the name of our data frame object personaldata, followed by the &lt;- operator so that we can overwrite the existing personaldata frame object with one that contains the renamed variables. Next, type the name of the rename function. As the first argument in the function, type the name of the data frame object (personaldata). As the second argument, let’s change the lastname variable to Last_Name by typing the name of our new variable followed by = and, in quotation marks (\" \"), the name of the original variable (Last_Name=\"lastname\"). As the third argument, let’s apply the same process as the second argument and change the firstname variable to First_Name by typing the name of our new variable followed by = and, in quotation marks (\" \"), the name of the original variable (First_Name=\"firstname\"). # Add (or replace) variable names to data frame object personaldata &lt;- rename(personaldata, Last_Name=&quot;lastname&quot;, First_Name=&quot;firstname&quot;) Using the head function from base R, let’s verify that we renamed the two variables successfully. # View just the first 6 rows of data in Console head(personaldata) ## id Last_Name First_Name startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male As you can see, the lastname and firstname variables are now named Last_Name and First_Name. It worked! 14.2.7 Summary In this chapter, we reviewed how to remove variable names from a data frame object; how to add variable names to a data frame object using the names, colnames, and c functions, which all come standard with your base R installation; and how to rename specific variables using the rename function from the dplyr package. References "],["write.html", "Chapter 15 Writing Data from R 15.1 Conceptual Overview 15.2 Tutorial", " Chapter 15 Writing Data from R In this chapter, we will learn what “writing data” means in the context of the R language, and how to go about writing data from R so that we share data with non-R users. 15.1 Conceptual Overview Writing data refers to the process of exporting data from the R Environment to a (working directory) folder. If you collaborate with others who do not work in R, writing data will allow them to use the data you cleaned, managed, or manipulated in the R Environment in other software programs. In the following tutorial, we will learn how to write a data frame object and a table object to our working directory folder as .csv files. 15.2 Tutorial This chapter’s tutorial demonstrates how to write data from R into a .csv file that can be opened in programs like Microsoft Excel or Google Sheets – along with many other analytical software programs. 15.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/ORTe8vE7nzU 15.2.2 Functions &amp; Packages Introduced Function Package write.csv base R write_csv readr write.table base R table base R 15.2.3 Initial Steps If you haven’t already, save the file called “PersData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PersData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a “tibble” object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemund’s (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 15.2.4 Write Data Frame to Working Directory The write.csv function from base R can be used to write a data frame object to your working directory or to a folder of your choosing. Let’s write the personaldata data frame (that we read in and named above) to our working directory. Before doing so, however, let’s make a minor change to the data frame to illustrate a scenario in which you clean your data in R and then write the data to a .csv file so that a colleague can work with the data in another program. Specifically, let’s remove the lastname variable from the data frame. To do so, type the name of the data frame (personaldata), followed by the $ symbol and then the name of the variable in question (lastname). Next, type the &lt;- operator followed by NULL. This code will remove the variable from the data frame. # Remove variable from data frame object personaldata$lastname &lt;- NULL # Print data frame object print(personaldata) ## # A tibble: 9 × 4 ## id firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Alejandro 1/1/2016 male ## 2 154 Ronald 1/9/2016 male ## 3 155 John 1/9/2016 male ## 4 165 Jane 1/4/2016 female ## 5 125 Benjamin 1/5/2016 male ## 6 111 Isaac 1/9/2016 male ## 7 198 Linda 1/7/2016 female ## 8 201 Cindy 1/9/2016 female ## 9 282 John 1/9/2016 male As you can see in your Console output, the variable called lastname is no longer present in the data frame object. To write our “cleaned”” data frame (personaldata) to our working directory, we use the write.csv function from base R. As the first argument in the parentheses, type the name of the data frame (personaldata). Remember to type a comma (,) before the second argument, as this is how we separate arguments from one another when there are more than one. As the second argument, let’s type what we want to name the file that we will create in our working directory. Make sure that the name of the new .csv file is in quotation marks (\" \"). Here, I name the new file “Cleaned PersData.csv”; it is important that you keep the .csv extension at the end of the name you provide. # Write data frame object to working directory write.csv(personaldata, &quot;Cleaned PersData.csv&quot;) If you go to your working directory folder, you will find the file called “Cleaned PersData.csv” saved there. We can also specify which folder that we want to write our data to using the full path extension and what we would like to name the new .csv file. # Write data frame object to folder write.csv(personaldata, &quot;H:/RWorkshop/Cleaned PersData2.csv&quot;) If you go to your working directory folder, you will find the file called “Cleaned PersData2.csv”. Alternatively, you can use the write_csv function from the readr package to write your data frame. One advantage of using the write_csv function over the write.csv is that the write_csv function doesn’t output a row number column in the .csv file like the write.csv function does. Assuming you have already installed the readr package, you’ll need to access the readr package if you haven’t already using the library function. Once you’ve done that, the write_csv function takes the same arguments as the write.csv function that we learned above. # Access readr package library(readr) # Write data frame object to working directory write_csv(personaldata, &quot;Cleaned PersData3.csv&quot;) # Write data frame object to folder write_csv(personaldata, &quot;H:/RWorkshop/Cleaned PersData4.csv&quot;) 15.2.5 Write Table to Working Directory Sometimes we work with table objects in R. If we wish to write a table to our working directory, we can use the write.table function from base R. Before doing so, we need to create a data table object as an example, which we can do using the table function from base R. To create a table, first, come up with a name for your new table object; in this example, I name the table table_example (because I’m so creative). Second, type the &lt;- operator to the right of your new table name to tell R that you are creating a new object. Third, type the name of the table-creation function, which is table. Fourth, in the function’s parentheses, as the first argument, enter the name of first variable you wish to use to make the table, and use the $ symbol to indicate that the variable (gender) belongs to the data frame in question (personaldata), which should look like this: personaldata$gender. Fifth, as the second argument, enter the name of the second variable you wish to use to make the table, and use the $ symbol to indicate that the variable (startdate) belongs to the data frame in question (personaldata), which should look like this: personaldata$startdate. # Create table from gender and startdate variables from personaldata data frame table_example &lt;- table(personaldata$gender, personaldata$startdate) # Print table object in Console print(table_example) ## ## 1/1/2016 1/4/2016 1/5/2016 1/7/2016 1/9/2016 ## female 0 1 0 1 1 ## male 1 0 1 0 4 The table above shows how how many female versus male employees started working on a given date. Now we are ready to write the table called table_example to our working directory using the write.table function. As the first argument, type the name of the table object (table_example). Second, type what we would like to call the file when it is saved in our working directory (**\"Practice Table.csv\"**); be sure to include the .csv extension in the name and wrap it all in quotation marks. Third, use the sep=\",\" argument to specify that the values in the table are separated by commas, as this will be a comma separated values file. Fourth, add the argument col.names=NA to format the table such that the column names will be aligned with their respective values. The reason for this fourth argument is that in our table the first column will contain the row names of one of the variables; if we don’t include this argument, the function will by default enter the name of the first column name associated with one of the levels of the variables in the first column, and because the first column actually contains the row names for the table, the row names will be off by one column. The col.names=NA argument simply leaves the first cell in the top row blank so that in the next column to the right, the first column name for one of the variables will appear. [To understand what the table would look like without this fourth argument, simply omit it, and open the resulting file in your working directory to see what happens.] # Write table object to working directory write.table(table_example, &quot;Practice Table.csv&quot;, sep=&quot;,&quot;, col.names=NA) If you go to your working directory, you will find the file called “Practice Table.csv”. 15.2.6 Summary Writing data from the R environment to your working directory or another folder can be useful, especially when collaborating with those who do not use R. The write.csv function writes a data frame object to a .csv file, whereas the write.table function writes a data table object to a .csv file. References "],["arrange.html", "Chapter 16 Arranging (Sorting) Data 16.1 Conceptual Overview 16.2 Tutorial 16.3 Chapter Supplement", " Chapter 16 Arranging (Sorting) Data In this chapter, we will learn how to arrange (sort) data within a data frame object, which can be useful for identifying high or low numeric values or to alphabetize character values. 16.1 Conceptual Overview Arranging (sorting) data refers to the process of ordering rows numerically or alphabetically in a data frame or table by the values of one or more variables. Sorting can make it easier to visually scan raw data, such as for the purposes of identifying extreme or outlier values. Sorting can also make facilitate decision making when rank ordering applicants’ scores, for example, on different selection tools. 16.2 Tutorial This chapter’s tutorial demonstrates how to arrange (sort) data in R. 16.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to arrange (sort) data with or without the pipe (%&gt;%) operator. If you’re unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Link to video tutorial: https://youtu.be/wVwJQsLNbmw 16.2.2 Functions &amp; Packages Introduced Function Package arrange dplyr desc dplyr 16.2.3 Initial Steps Please note, that any function that appears in the Initial Steps section has been covered in a previous chapter. If you need a refresher, please view the relevant chapter. In addition, a previous chapter may show you how to perform the same action using different functions or packages. If you haven’t already, save the file called “PersData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PersData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object personaldata ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a “tibble” object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemund’s (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 16.2.4 Arrange (Sort) Data There are different functions we could use to arrange (sort) the data in the data frame, and in this chapter, we will focus on the arrange function from the dplyr package (Wickham et al. 2023). Please note that there are other functions we could use to sort data, and if you’re interested, in the Arranging (Sorting) Data: Chapter Supplement, I demonstrate how to use the order function from base R to carry out the same operations we will cover below. Because the arrange function comes from the dplyr package, which is part of the tidyverse of R packages (Wickham 2023; Wickham et al. 2019). If you haven’t already, install and access the dplyr package using the install.packages and library functions, respectively. # Install dplyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) Before diving into arranging the data, as a disclaimer, I will demonstrate two techniques for arranging (sorting) data using the arrange function. The first technique uses a “pipe” which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2022), on which the dplyr is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemund’s (2017) chapter on pipes: https://r4ds.had.co.nz/pipes.html. This brings us to the second technique for arranging (sorting) data using the arrange function. The second technique uses a more traditional approach that some may argue lacks the efficiency and readability of the pipe. Conversely, others may argue against the use of pipes altogether. I’m not here to settle any “pipes versus no pipes” debate, and you’re welcome to use either technique. If you don’t want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 16.2.4.1 With Pipe To use the “with pipe” technique, first, type the name of our data frame object, which we previously named personaldata, followed by the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. Second, either on the same line or on the next line, type the name of the arrange function, and within the parentheses, enter the variable name startdate as the argument to indicate that we want to arrange (sort) the data by the start date of the employees. The default operation of the arrange function is to arrange (sort) the data in ascending order. If you’re wondering where I found the exact names of the variables in the data frame, revisit the use of the names function, which I demonstrated previously in this chapter in the Initial Steps section. # Arrange (sort) data by variable in ascending order (single line) (with pipe) personaldata %&gt;% arrange(startdate) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male Alternatively, we can write this script over two lines and achieve the same output in our Console. # Arrange (sort) data by variable in ascending order (two lines) (with pipe) personaldata %&gt;% arrange(startdate) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male Please note that the operations we have performed thus far have not changed anything in the personaldata data frame object itself; rather, the output in the Console simply shows what it looks like if the data are sorted by the variable in question. We can verify this by viewing the first six rows of data in our data frame object using the head function. As you can see below, nothing changed in the data frame itself. # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object (with pipe) personaldata &lt;- personaldata %&gt;% arrange(startdate) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male As you can see in the Console output, now the personaldata data frame object has been changed such that the data are arranged (sorted) by the startdate variable. To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object (with pipe) personaldata &lt;- personaldata %&gt;% arrange(desc(startdate)) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female To arrange (sort) data by values/levels of two variables, we simply enter the names of two variables as consecutive arguments. Let’s enter the gender variable first, followed by the startdate variable. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. As shown below, startdate is sorted within the sorted levels of the gender variable. As a reminder, the default operation of the arrange function is to arrange (sort) the data in ascending order. Remember, we use commas to separate arguments used in a function (if there are more than one arguments). # Arrange (sort) data by two variables in ascending order (with pipe) personaldata %&gt;% arrange(gender, startdate) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 201 Providence Cindy 1/9/2016 female ## 4 153 Sanchez Alejandro 1/1/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 154 McDonald Ronald 1/9/2016 male ## 7 155 Smith John 1/9/2016 male ## 8 111 Newton Isaac 1/9/2016 male ## 9 282 Legend John 1/9/2016 male Watch what happens when we switch the order of the two variables we are using to sort the data. # Arrange (sort) data by two variables in ascending order (with pipe) personaldata %&gt;% arrange(startdate, gender) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 201 Providence Cindy 1/9/2016 female ## 6 154 McDonald Ronald 1/9/2016 male ## 7 155 Smith John 1/9/2016 male ## 8 111 Newton Isaac 1/9/2016 male ## 9 282 Legend John 1/9/2016 male As you can see, the order of the two sorting variables matters. To arrange the data in descending order, just use the desc function from dplyr within the arrange function. # Arrange (sort) data by variable in descending order (with pipe) personaldata %&gt;% arrange(desc(gender), desc(startdate)) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 282 Legend John 1/9/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 153 Sanchez Alejandro 1/1/2016 male ## 7 201 Providence Cindy 1/9/2016 female ## 8 198 Morales Linda 1/7/2016 female ## 9 165 Doe Jane 1/4/2016 female Or, we can sort one variable in the default ascending order and the other in descending order. # Arrange (sort) data by two variables in ascending &amp; descending order (with pipe) personaldata %&gt;% arrange(gender, desc(startdate)) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201 Providence Cindy 1/9/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 165 Doe Jane 1/4/2016 female ## 4 154 McDonald Ronald 1/9/2016 male ## 5 155 Smith John 1/9/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 282 Legend John 1/9/2016 male ## 8 125 Franklin Benjamin 1/5/2016 male ## 9 153 Sanchez Alejandro 1/1/2016 male 16.2.4.2 Without Pipe We can achieve the same output without using the pipe (%&gt;%) operator as with the pipe operator; again, your choice of using or not using the pipe operator is up to you. To use the arrange function without the pipe operator, type the name of the arrange function, and within the parentheses, as the first argument, type the name of the personaldata data frame object, and as the second argument, type the startdate variable, where the latter indicates that we want to arrange (sort) the data frame object by the start date of the employees. The default operation of the arrange function is to arrange (sort) the data in ascending order. Remember, we use commas to separate arguments used in a function (if there are more than one arguments). If you’re wondering where I found the exact names of the variables in the data frame, revisit the use of the names function, which I demonstrated previously in this chapter in the Initial Steps section. # Arrange (sort) data by variable in ascending order without pipe arrange(personaldata, startdate) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object without pipe personaldata &lt;- arrange(personaldata, startdate) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. # Arrange (sort) data by variable in descending order and # overwrite existing data frame object without pipe personaldata &lt;- arrange(personaldata, desc(startdate)) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female To arrange (sort) data by values/levels of two variables, we simply enter the names of two variables as consecutive arguments (after the name of the data frame, which is the first argument). Let’s enter the gender variable first, followed by the startdate variable. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. # Arrange (sort) data by variable in ascending order without pipe personaldata &lt;- arrange(personaldata, gender, startdate) As shown in the output above, startdate is sorted within the sorted levels of the gender variable. This also verifies that the default operation of the arrange function is to arrange (sort) the data in ascending order. To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. You can use the desc function on one or both sorting variables. # Arrange (sort) data by one variable in ascending order and # the other in descending order without pipe personaldata &lt;- arrange(personaldata, gender, desc(startdate)) Or we can apply the desc function to both variables. # Arrange (sort) data by both variables descending order without pipe personaldata &lt;- arrange(personaldata, desc(gender), desc(startdate)) 16.2.5 Summary In this chapter, we learned how to arrange (sort) data by one or more variables using the arrange and desc functions from the dplyr package. This chapter also introduced the pipe (%&gt;%) operator, which can help make code easier to read in some contexts. 16.3 Chapter Supplement In addition to the arrange function from the dplyr package covered above, we can use the order function from base R to arrange (sort) data by values for one or more variable. Because this function comes from base R, we do not need to install and access an additional package like we do with the arrange functions, which some may find advantageous. 16.3.1 Functions &amp; Packages Introduced Function Package order base R c base R 16.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 16.3.3 order Function from Base R To sort a data frame object in ascending order based on a single variable, we will use the order function from base R to do the following: Type the name of the data frame object that you wish to arrange (sort) (personaldata). Insert brackets ([ ]), which allow us to reference rows or columns depending on how we format the brackets. If we type a function or value before the comma, we are indicating that we wish to apply operations to row(s), and if we type a function or value after the comma, we are indicating that we wish to apply operations to column(s). To sort the data frame into ascending rows by the startdate variable, type the name of the order function before the comma in the brackets. As the sole parenthetical argument of the order function, type the name of the personaldata data frame object, followed by the $ operator and the name of the variable by which we wish to sort the data frame, which to reiterate is the startdate variable. The $ operator signals to R that a variable belongs to a particular data frame object. By default, the order function sorts in ascending order. # Arrange (sort) data by variable in ascending order personaldata[order(personaldata$startdate),] ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order # and overwrite existing data frame object personaldata &lt;- personaldata[order(personaldata$startdate),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male To sort in descending order, add the argument decreasing=TRUE within the order function parentheses. Remember, we use commas to separate arguments used in a function (if there are two or more arguments). # Arrange (sort) data by variable in descending order personaldata &lt;- personaldata[order(personaldata$startdate, decreasing=TRUE),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female If we wish to sort a data frame object by two variables, as the second argument in the order function parentheses, simply add the name of the data frame object, followed by the $ operator and the name of the second second variable. We will sort the data frame in by gender and startdate. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. As shown below, startdate is sorted within the sorted levels of the gender variable. The default operation of the arrange function is to arrange (sort) the data in ascending order. # Arrange (sort) data by two variables in ascending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 201 Providence Cindy 1/9/2016 female ## 4 153 Sanchez Alejandro 1/1/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 154 McDonald Ronald 1/9/2016 male To sort by one of the variables in descending order and the other variable by the default ascending order, we need to add the decreasing= argument, but because we have two variables, we need to provide a vector containing logical values (TRUE, FALSE) to indicate which variable we wish to apply a descending order. If the logical value is TRUE for the decreasing= argument, then we sort in descending variable. Using the c (combine) function from base R, we create a vector of two logical values whose order corresponds to the order in which we listed the two variables in the order function. For example, if the argument is decreasing=c(FALSE, TRUE), then we sort the first variable in the default ascending order and the second variable in descending order, which is what we do below. Just be sure to add the following argument to the order function when attempting to sort two or more variables: method=\"radix\". # Arrange (sort) data by gender in ascending order and # startdate in descending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate, decreasing=c(FALSE, TRUE), method=&quot;radix&quot;),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201 Providence Cindy 1/9/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 165 Doe Jane 1/4/2016 female ## 4 154 McDonald Ronald 1/9/2016 male ## 5 155 Smith John 1/9/2016 male ## 6 111 Newton Isaac 1/9/2016 male Or, you could sort by both variables in descending order by change the argument to decreasing=c(TRUE, TRUE). # Arrange (sort) data by gender and startdate variables descending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate, decreasing=c(TRUE, TRUE), method=&quot;radix&quot;),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 282 Legend John 1/9/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 153 Sanchez Alejandro 1/1/2016 male References "],["join.html", "Chapter 17 Joining (Merging) Data 17.1 Conceptual Overview 17.2 Tutorial 17.3 Chapter Supplement", " Chapter 17 Joining (Merging) Data In this chapter, we will learn the fundamentals of joins (merges). Specifically, we will learn how to join (merge) data horizontally and vertically. 17.1 Conceptual Overview Joining (merging) refers to the process of matching two data frames by either one or more key variables (i.e., horizontal join) or by variable names or columns (i.e., vertical join). Sometimes a join is referred to as a merge and vice versa, and thus I will use these terms interchangeably throughout the chapter. Broadly speaking, there are two types of joins (merges): horizontal and vertical. 17.1.1 Review of Horizontal Joins (Merges) A horizontal join (merge) refers to the process of matching cases (i.e., rows, observations) between two data frames using a key variable (matching variable), which results in distinct sets of variables (i.e., fields, columns) being combined horizontally (laterally) across two data frames. The resulting joined data frame will be wider (in terms of the number of variables) than either of the original data frames in isolation. For example, imagine that we pull data from separate information systems, each with different variables (i.e., fields) but at least some employees (i.e., cases) in common; to combine these two data frames, we can perform a horizontal join. This is often a necessary step when creating a data frame that contains all of the variables we will need in subsequent data analyses. For instance, if we wish to estimate the criterion-related validities using the selection tool scores from one data frame with criterion (e.g., job performance) scores from another data frame, then we could perform a horizontal join, assuming we have a key variable with which to match the scores from the two data frames. In a horizontal join, cases (or observations) are matched between two data frames using one or more key variables. We will focus on four different types of horizontal joins: Inner join: All unmatched cases (or observations) are dropped, thereby retaining only those cases that are present in both the left (x, first) and right (y, second) data frames. In other words, a case is only included in the merged data frame if it appears in both of the original data data frames. In an inner join, all unmatched cases (or observations) are dropped, thereby retaining only those cases that are present in both the left (x, first) and right (y, second) data frames. Full join: All cases (or observations) are retained, including those cases that do not have a match in the other data data frame. In other words, a case is included in the merged data frame even if it only appears in one of the original data data frames. These type of join leads to the highest number of retained cases under conditions in which both data frames contain unique cases. In a full join, all cases (or observations) are retained, including those cases that do not have a match in the other data data frame. Left join: All cases (or observations) that appear in the left (x, first) data frame are retained, even if they lack a match in the right (y, second) data frame. Consequently, cases from the right data frame that lack a match in the left data frame are dropped in the merged data frame. In a left join, all cases (or observations) that appear in the left (x, first) data frame are retained, even if they lack a match in the right (y, second) data frame. Right join: All cases (or observations) that appear in the right (y, second) data frame are retained, even if they lack a match in the left (x, first) data frame. Consequently, cases from the left data frame that lack a match in the right data frame are dropped in the merged data frame. In a right join, only cases (or observations) that appear in the right (y, second) data frame are retained, even if they lack a match in the left (x, first) data frame. Please note that I have illustrated different types of horizontal joins using a single key variable. It is entirely possible to perform horizontal joins using two or more key variables. For example, imagine that each morning we administered a pulse survey to employees and each afternoon we afternoon we administered a different pulse survey to the same employees, and that we repeated this process for five consecutive workdays. In this instance, we would likely need to horizontally join the data frames using both a unique employee identifier variable and a unique day-of-week variable. 17.1.2 Review of Vertical Joins (Merges) A vertical join (merge) refers to the process of matching identical variables from two data frames, which results in distinct sets of cases or observations being combined vertically. The resulting joined data frame will be longer (in terms of the number of cases) than either of the original data frames in isolation. For example, imagine an organization administered the same survey to two facilities (i.e., independent groups) each with unique employees; we could combine the two resulting data frames by performing a vertical join. In a vertical join, identical variables are matched between two data frames, each with distinct sets of cases or observations. 17.2 Tutorial This chapter’s tutorial demonstrates how to join (merge) cases from two data frames. 17.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to join (merge) data with or without the pipe (%&gt;%) operator. If you’re unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Finally, please note that in the chapter supplement you have an opportunity to learn how to join (merge) cases from data frames using a function from base R, which you may find preferable or more intuitive. Link to video tutorial: https://youtu.be/38zsLj-fWo0 17.2.2 Functions &amp; Packages Introduced Function Package right_join dplyr left_join dplyr inner_join dplyr full_join dplyr data.frame base R c base R rep base R rbind base R 17.2.3 Initial Steps If you haven’t already, save the files called “PersData.csv” and “PerfData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data files for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called “PersData.csv” and “PerfData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. performancedata &lt;- read_csv(&quot;PerfData.csv&quot;) ## Rows: 6 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): id, perf_q1, perf_q2, perf_q3, perf_q4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male print(performancedata) ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 As you can see from the output generated in your Console, on the one hand, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. On the other hand, the performancedata data frame object contains the same id unique identifier variable as the personaldata data frame object, but instead of employee demographic information, this data frame object includes variables associated with quarterly employee performance: perf_q1, perf_q2, perf_q3, and perf_q4. In order to better illustrate certain join functions later on in this chapter, we’ll begin by removing the case (i.e., employee) associated with the id variable value of 153 (i.e., Alejandro Sanchez); in terms of a rationale for doing so, let’s imagine that Alejandro no longer works for the organization, and thus we would like to remove him from the personaldata data frame. If you don’t completely understand the following process for removing this individual from the data frame, no need to worry, as you will learn more in the subsequent chapter on filtering data. Type the name of the data frame object (personaldata) followed by the &lt;- operator to overwrite the existing data frame object. Type the name of the original data frame object (personaldata) followed by brackets ([ ]). Within the brackets ([ ]), type the name of the data frame object (personaldata) again, followed by the $ operator and the name of the variable we wish to use to select the case that will be removed, which in this instance is the id unique identifier variable. The $ operator indicates to R that the id variable belongs to the personaldata data frame. Type the “not equal to” operator, which is != (the ! means “not”), followed by the id variable value we wish to use to remove the case (i.e., 153). Type a comma (,) to indicate that we are removing a row, not a column. When referencing rows and columns in R, as we are doing in the brackets ([ ]), rows are entered first (before a comma), and columns are entered second (after a comma). In doing so, we are telling R to retain all rows of data in personaldata except for the one corresponding to id equal to 153. # Remove case with id variable equal to 153 personaldata &lt;- personaldata[personaldata$id != 153,] Check out the first 6 rows of the updated data frame for personaldata, and note that the data corresponding to the case associated with id equal to 153 is gone. # Print first 6 rows of first data frame object once more head(personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female 17.2.4 Horizontal Join (Merge) Recall that a horizontal join (merge) means that cases are matched using one more more key variables, and as a result, variables (i.e., columns, fields) are combined across two data frames. We will review two options for performing horizontal joins. To perform horizontal joins, we will learn how to use the join functions from the the dplyr package (Wickham et al. 2023), which include: right_join, left_join, inner_join, and full_join. Please note that there are other functions we could use to perform horizontal joins, and if you’re interested, in the Joining (Merging) Data: Chapter Supplement, I demonstrate how to use the merge function from base R to carry out the same operations we will cover below. Using the aforementioned join functions, we will match cases from the personaldata and performancedata data frames using the id unique identifier variable as a key variable. So how can we verify that id is an appropriate key variable? Well, let’s use the names function from base R to retrieve the list of variable names from the two data frames, which we already did above. Nevertheless, let’s call up those variable names once more. Simply enter the name of the data frame as a parenthetical argument in the names function. # Retrieve variable names from first data frame names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Retrieve variable names from second data frame names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; As you can see in the variable names listed above, the id variable is common to both data frames, and thus it will serve as our key variable. Now we are almost ready to begin joining the two data frames using the id unique identifier as a key variable. Before doing so, however, we should make sure that we have installed and accessed the dplyr package (if we haven’t already), as the join functions come from that package. # Install dplyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) I will demonstrate two techniques for applying the join function. The first technique uses the pipe operator (%&gt;%). The pipe operator comes from a package called magrittr (Bache and Wickham 2022), on which the dplyr is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemund’s (2017) chapter on pipes: https://r4ds.had.co.nz/pipes.html. The second technique for applying the join function takes a more traditional approach in that it involves nested functions being nested parenthetically. If you don’t want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 17.2.4.1 With Pipe Using the pipe (%&gt;%) operator technique, let’s begin with what is referred to as an inner join by doing the following: Use the &lt;- operator to name the joined (merged) data frame that we will create using the one of the dplyr join functions. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Make sure you put the name of the new data frame object to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the first data frame, which we named personaldata, followed by the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. On the same line or on the next line, type the inner_join function, and within the parentheses as the first argument, type the name of the second data frame, which we called performancedata. As the second argument, use the by= argument to indicate the name of the key variable, which in this example is id; make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join (with pipe) mergeddf &lt;- personaldata %&gt;% inner_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 5 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now, let’s revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female ## 7 201 Providence Cindy 1/9/2016 female ## 8 282 Legend John 1/9/2016 male # Print the second original data frame performancedata ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we simply swap out the inner_join function from our previous code with the full_join function. # Full join (with pipe) mergeddf &lt;- personaldata %&gt;% full_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 9 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 9 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the left_join function instead, while keeping the rest of the previous code the same. # Left join (with pipe) mergeddf &lt;- personaldata %&gt;% left_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 8 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how the left_join function retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we will use the right_join function instead, while keeping the rest of the previous code the same. # Right join (with pipe) mergeddf &lt;- personaldata %&gt;% right_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 6 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 6 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the right_join function retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. 17.2.4.2 Without Pipe In this section, I demonstrate the same dplyr join functions as above, except here I demonstrate how to specify the functions without the use of a pipe (%&gt;%) operator. Let’s begin with what is referred to as an inner join by doing the following: Use the &lt;- operator to name the joined (merged) data frame that we will create using the one of the dplyr join functions. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Make sure you put the name of the new data frame object to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the inner_join function. As the first argument within the parentheses, type the name of the first data frame, which we named personaldata. As the second argument, type the name of the second data frame we named performancedata. As the third argument, use the by= argument to indicate the name of the key variable, which in this example is id; make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join (without pipe) mergeddf &lt;- inner_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 5 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now, let’s revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female ## 7 201 Providence Cindy 1/9/2016 female ## 8 282 Legend John 1/9/2016 male # Print the second original data frame performancedata ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we simply swap out the inner_join function from our previous code with the full_join function. # Full join (without pipe) mergeddf &lt;- full_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 9 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 9 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the left_join function instead, while keeping the rest of the previous code the same. # Left join (without pipe) mergeddf &lt;- left_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 8 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how the left_join function retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we use the right_join function instead, while keeping the rest of the previous code the same. # Right join (without pipe) mergeddf &lt;- right_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 6 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 6 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the right_join function retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. 17.2.5 Vertical Join (Merge) To perform a vertical join (merge), we will use the rbind function from base R, which stands for “row bind.” As a reminder, with a horizontal join, our focus is on joining variables (i.e., columns, fields) from two data frames containing overlapping cases (i.e., rows). In contrast, with a vertical join, our focus is on joining cases from data frames with the same variables. To illustrate how to perform a vertical join, we take a slightly different approach than what we did with horizontal joins. Instead of reading in data files, we will create two “toy” employee demographic data frames with the exact same variables but different cases. We will use the data.frame function from base R to indicate that we wish to create a data frame object; we use the c (combine) function from base R to combine values into a vector; and we use the rep (replicate) function from base R to replicate the same value a specified number of times. Also note that the : operator, when used between two numbers, creates a vector of consecutive values, beginning with the first value and ending with the second. Please note, that using and understanding the data.frame, c, and rep functions is not consequential for understanding how to do a vertical merge; rather, I merely use these functions in this tutorial to create quick toy data frames that we can use to illustrate how to do a vertical join. For more information on the data.frame function and the c function, please refer to the chapter called Basic Features and Operations of the R Language. # Create data frames with same variables but arbitrary values df1 &lt;- data.frame(id=c(1:6), age=c(21:26), sex=c(rep(&quot;male&quot;, 6))) df2 &lt;- data.frame(id=c(7:10), age=c(27:30), sex=c(rep(&quot;female&quot;, 4))) # Print first data frame df1 ## id age sex ## 1 1 21 male ## 2 2 22 male ## 3 3 23 male ## 4 4 24 male ## 5 5 25 male ## 6 6 26 male # Print second data frame df2 ## id age sex ## 1 7 27 female ## 2 8 28 female ## 3 9 29 female ## 4 10 30 female Given that these two data frames (i.e., df1, df2) have the exact same variable names (id, age, and sex), we can easily perform a vertical join using the rbind function. To do so, enter the names of the two data frames as arguments, separated by a comma. Use the &lt;- operator to name the merged data frame something, which for this case, I arbitrarily named it mergeddf2. # Vertical merge mergeddf2 &lt;- rbind(df1, df2) # Print the merged data frame mergeddf2 ## id age sex ## 1 1 21 male ## 2 2 22 male ## 3 3 23 male ## 4 4 24 male ## 5 5 25 male ## 6 6 26 male ## 7 7 27 female ## 8 8 28 female ## 9 9 29 female ## 10 10 30 female Note how the two data frames are now “stacked” on one another. This was possible because they shared the same variables names and variables types (e.g., numeric and character). 17.2.6 Summary Joining (merging) data frames in R is a useful practice. In this chapter, we learned how to perform a horizontal join using the right_join, left_join, inner_join, and full_join functions from the dplyr package. We also learned how to perform a vertical join using the rbind function from base R. 17.3 Chapter Supplement In addition to the join functions from the dplyr package covered above, we can use the merge function from base R to perform a horizontal join. Because this function comes from base R, we do not need to install and access an additional package like we do with the join functions, which some may find advantageous. 17.3.1 Video Tutorial In addition to the written chapter supplement provided below, you can follow along with the following video tutorial to learn more about how to horizontally join data two data frames using the merge function from base R. Link to video tutorial: https://youtu.be/MLihEVEpJBg 17.3.2 Functions &amp; Packages Introduced Function Package names base R merge base R 17.3.3 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. Please note, however, that we are using two slightly different data files in this supplement, which will simply and clarify some of the different types of merges (joins) that we’ll go over. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects # Note that these data files are different than the # ones we used in the main part of the chapter personaldata &lt;- read_csv(&quot;PersonalData.csv&quot;) ## Rows: 8 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. performancedata &lt;- read_csv(&quot;PerformanceData.csv&quot;) ## Rows: 6 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): id, perf_q1, perf_q2, perf_q3, perf_q4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 8 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man print(performancedata) ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 17.3.4 merge Function from Base R We will use the merge function to horizontally match cases from the personaldata and performancedata data frames using id as a key variable. To identify what the key variable is, let’s use the names function from base R to print the list of variable names from the two data frames, which we already did above. Nevertheless, let’s call up those variable names once more. Simply enter the name of the data frame as a parenthetical argument in the names function. # Print variable names from first data frame names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print variable names from second data frame names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; As you can see in the variable names listed above, the id variable is common to both data frames, and thus it will serve as our key variable. Let’s begin with what is referred to as an inner join: Use the &lt;- operator to name the joined data frame that we create using the merge function. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Type the name of the new joined data frame to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the merge function. Within the merge function parentheses, we will provide the arguments needed to make this join a reality. First, enter the name of one of the data frames (e.g., personaldata), followed by a comma. Second, enter the name of of the other data frame (e.g., performancedata), followed by a comma. Third, use the by= argument to indicate the name of the key variable (e.g., id); make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 3 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 5 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Now, let’s revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man # Print the second original data frame performancedata ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we can add the all= argument to our previous code and specify the logical value TRUE. # Full join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 125 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2.1 1.9 2.1 2.3 ## 3 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 4 154 McDonald Ronald 1/9/2016 man NA NA NA NA ## 5 155 Smith John 1/9/2016 man NA NA NA NA ## 6 165 Doe Jane 1/4/2016 woman NA NA NA NA ## 7 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 8 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 9 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the all.x=TRUE argument instead. # Left join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all.x=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 3 154 McDonald Ronald 1/9/2016 man NA NA NA NA ## 4 155 Smith John 1/9/2016 man NA NA NA NA ## 5 165 Doe Jane 1/4/2016 woman NA NA NA NA ## 6 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 8 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the left join retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we use the all.y=TRUE argument instead. # Right join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all.y=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 125 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2.1 1.9 2.1 2.3 ## 3 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 4 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 5 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 6 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the right join retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. References "],["filter.html", "Chapter 18 Filtering (Subsetting) Data 18.1 Conceptual Overview 18.2 Tutorial 18.3 Chapter Supplement", " Chapter 18 Filtering (Subsetting) Data In this chapter, we will learn how to filter (subset) cases from a data frame and how to select or remove variables from a data frame. We’ll begin with a conceptual overview of filtering (subsetting) and variable selection/removal. In this chapter, we’ll use the terms “filter” and “subset” interchangeably. 18.1 Conceptual Overview Filtering data (i.e., subsetting data) is an important data-management process, as it allows us to: Select or remove a subset of cases from a data frame based on their scores on one or more variables; Select or remove a subset of variables from a data frame. In this section, we will review logical operators, as it is through the application of logical operators that we will ultimately filter (subset) cases from a data frame. 18.1.1 Review of Logical Operators When our goal is to select or remove a subset of cases (i.e., observations) from a data frame, we typically do so by applying logical operators. You may already be comfortable with the use of logical operators and expressions like “greater than” (\\(&gt;\\)), “less than” (\\(&lt;\\)), “equal to” (\\(=\\)), “greater than or equal to” (\\(\\ge\\)), and “less than or equal to” (\\(\\le\\)), but you may be less comfortable with the use of the logical “OR” and the logical “AND”. Thus, before we start working with data, let’s do a quick review. When we wish to apply a single logical statement, our job is relatively straightforward. To keep things simple, let’s focus on a single vector, and we’ll call that vector \\(X\\). For example, we might choose to select only those cases with scores on \\(X\\) that are “greater than or equal to” (\\(\\ge\\) or \\(&gt;\\)\\(=\\)) 3, thereby retaining scores that are equal to or greater than 3, as highlighted in blue below. In other words, we select only those cases for which their score on \\(X\\) would be true given the logical statement \\(X \\ge 3\\). The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we apply the logical statement that \\(X\\) is “greater than or equal to” (\\(\\ge\\)) 3, we select only those cases with scores that are equal to or greater than 3 and up to the maximum possible score of 7. As another example, we might choose to select only those cases with scores on \\(X\\) that are “less than or equal to” (\\(\\le\\) or \\(&lt;\\)\\(=\\)) 5, thereby retaining scores that are equal to or less than 5, as highlighted in red below. In other words, we select only those cases for which their score on \\(X\\) would be true given the logical statement \\(X \\le 5\\). The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we apply the logical statement that \\(X\\) is “greater than or equal to” (\\(\\ge\\)) 3, we select only those cases with scores that are equal to or greater than 3 and down to the minimum possible score of 1. If we apply the logical “OR” operator, things get a bit more interesting. The logical “OR” is used when our objective is to select cases based on two logical statements, and if either logical statement is true for a given case, then that case is selected. The logical “OR” is consistent with idea of a mathematical union (\\(\\cup\\)) from Set Theory. As an example, let’s combine the two logical statements from above with the logical “OR” operator. We retain those cases for which their score on \\(X\\) is “greater than or equal to” 3 or “less than or equal to” 5. Given that \\(X\\) has possible scores ranging from 1-7, below, we see that this logic retains all cases, as all scores (1-7) would satisfy one or both of the logical statements. The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we wish to retain those cases for which \\(X\\) is “greater than or equal to” 3 or “less than or equal to” 5, we select those cases that satisfy either (or both) logical statements. Using the same to logical statements above, let’s replace the logical “OR” with the logical “AND”. The logical “AND” is used when our objective is to select cases based on two logical statements, and if both logical statements are true for a given case, then that case is selected. The logical “AND” is consistent with idea of a mathematical intersection (\\(\\cap\\)) from Set Theory. In this example, we retain only those cases for which their score on \\(X\\) is “greater than or equal to” 3 and “less than or equal to” 5. Given that \\(X\\) has possible scores ranging from 1-7, below, we see that this logic retain only cases with scores within the range 3-5, as only scores of 3, 4, or 5 would satisfy both of the logical statements. The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we wish to retain those cases for which \\(X\\) is “greater than or equal to” 3 and “less than or equal to” 5, we select only those cases that satisfy both logical statements. 18.2 Tutorial This chapter’s tutorial demonstrates how to filter (subset) cases from a data frame and how to select or remove variables from a data frame. 18.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to filter (subset) data with or without the pipe (%&gt;%) operator. If you’re unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Finally, please note that in the chapter supplement you have an opportunity to learn how to filter (subset) cases and select/remove variables using a function from base R, which you may find more intuitive or preferable for other reasons. Link to video tutorial: https://youtu.be/izVcbPmu0D0 18.2.2 Functions &amp; Packages Introduced Function Package str base R filter dplyr c base R as.Date base R select dplyr subset base R 18.2.3 Initial Steps If you haven’t already, save the files called “PersData.csv” and “PerfData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called “PersData.csv” and “PerfData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## Rows: 9 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. performancedata &lt;- read_csv(&quot;PerfData.csv&quot;) ## Rows: 6 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): id, perf_q1, perf_q2, perf_q3, perf_q4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 9 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male print(performancedata) ## # A tibble: 6 × 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 As you can see from the output generated in your console, on the one hand, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. On the other hand, the personaldata data frame object contains the same id unique identifier variable as the personaldata data frame object, but instead of employee demographic information, this data frame object includes variables associated with quarterly employee performance: perf_q1, perf_q2, perf_q3, and perf_q4. To make this chapter more interesting (and for the sake of practice), let’s use the full_join function from dplyr (Wickham et al. 2023) to join (merge) the two data frames we just read in (personaldata, performancedata) using the id variable as the key variable. Let’s arbitrarily name the new joined (merged) data frame mergeddf using the &lt;- operator. For more information on joining data, check out the chapter called Joining (Merging) Data. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access package library(dplyr) # Full join (without pipe) mergeddf &lt;- full_join(personaldata, performancedata, by=&quot;id&quot;) # Print joined (merged) data frame object print(mergeddf) ## # A tibble: 9 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 8 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 9 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now we have a joined data frame called mergeddf! 18.2.4 Filter Cases from Data Frame Sometimes we want to select only a subset of cases from a data frame or table. There are different functions that can achieve this end. For example, the subset function filter from base R will do the trick. With that said, the dplyr package (Wickham et al. 2023) offers the filter function which has some advantages (e.g., faster with larger amounts of data), and thus, we will focus on the filter function in this chapter. If you would like to learn how to use the subset function from base R, check out the chapter supplement called Joining (Merging) Data. In order to properly filter data by cases, we need to know the respective types (classes) of the variables in the data frame. Perhaps the quickest way to find out the type (class) of each variable in the data frame is to use the str (structure) function from base R, and the function’s parentheses, just enter the name of the data frame (mergeddf). # Determine class of variables str(mergeddf) ## spc_tbl_ [9 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:9] 153 154 155 165 125 111 198 201 282 ## $ lastname : chr [1:9] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname: chr [1:9] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate: chr [1:9] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:9] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; ... ## $ perf_q1 : num [1:9] 3.9 NA NA NA 2.1 3.3 4.9 1.2 2.2 ## $ perf_q2 : num [1:9] 4.8 NA NA NA 1.9 3.3 4.5 1.1 2.3 ## $ perf_q3 : num [1:9] 4.9 NA NA NA 2.1 3.4 4.4 1 2.4 ## $ perf_q4 : num [1:9] 5 NA NA NA 2.3 3.3 4.8 1 2.5 ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Note that the id variable is of type integer; the lastname, firstname, startdate, and gender variables are of type character (string); and the perf_q4, perf_q4, perf_q4, and perf_q4 variables are of type numeric. The variable type will have important implications for how use use the filter function from dplyr. In R, we can apply any one of the following logical operators when filtering our data: Logical Operator Definition &lt; “less than” &gt; “greater than” &lt;= “less than or equal to” &gt;= “greater than or equal to” == “equal to” != “not equal to” | “or” &amp; “and” ! “not” To get started, install and access the dplyr package. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) I will demonstrate two approaches for applying the filter function from dplyr. The first option uses “pipe(s),” which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2022), on which the dplyr is partially dependent. In short, a pipe allows one to more efficiently code/script and to improve the readability of the code/script under certain conditions. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. The second option is more traditional and lacks the efficiency and readability of pipes. You can use either approach, and if don’t you want to use pipes, skip to the section below called Without Pipes. For more information on the pipe operator, check out this link: https://r4ds.had.co.nz/pipes.html. 18.2.4.1 With Pipes Using an approach with pipes, first, use the &lt;- operator to name the filtered data frame that we will create. For this example, I name the new joined data frame filterdf; you could name it whatever you would like. Second, type the name of the first data frame, which we named mergeddf (see above), followed by the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. Third, either on the same line or on the next line, type the filter function. Fourth, within the function parentheses, type the name of the variable we wish to filter the data frame by, which in this example is gender. Fourth, type a logical operator, which for this example is ==. Fifth, type a value for the filter variable, which in this example is “female”; because the gender variable is of type character, we need to put quotation marks (\" \") around the value of the variable that we wish to filter by. Remember, object names in R are case and space sensitive; for instance, gender is different from Gender, and “female” is different from “Female”. # Filter in by gender with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot;) # Print filtered data frame filterdf ## # A tibble: 3 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 165 Doe Jane 1/4/2016 female NA NA NA NA ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 3 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 Note how the data frame above contains only those cases with “female” as their gender variable designation. The filter worked as expected. Alternatively, we could filter out those cases in which gender is equal to “female” using the != (not equal to) logical operator. # Filter out by gender with pipe filterdf &lt;- mergeddf %&gt;% filter(gender!=&quot;female&quot;) # Print filtered data frame filterdf ## # A tibble: 6 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how cases with gender equal to “female” are no longer in the data frame, while every other case is retained. Let’s now filter by a variable of type numeric (or integer). Specifically, let’s select those cases in which the perf_q2 variable is greater than (&gt;) 4.0. Because the perf_q2 variable is of type numeric, we don’t use quotation marks (\" \") around the value we wish to filter by, which in this case is 4.0. # Filter by perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 2 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 If we wish to filter by two variables, we can apply the logical “or” (|) operator or “and” (&amp;) operator. First, let’s select those cases in which either gender is equal to “female” or perf_q2 is greater than 4.0 using the “or” (|) operator. # Filter by gender or perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot; | perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 4 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 165 Doe Jane 1/4/2016 female NA NA NA NA ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 Watch what happens if we apply the logical “and” (&amp;) operator with the same syntax as above. # Filter by gender and perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot; &amp; perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 1 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 We can also use the logical “or” (|) operator to select two values of the same variable. # Filter by two values of firstname with pipe filterdf &lt;- mergeddf %&gt;% filter(firstname==&quot;John&quot; | firstname==&quot;Jane&quot;) # Print filtered data frame filterdf ## # A tibble: 3 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 155 Smith John 1/9/2016 male NA NA NA NA ## 2 165 Doe Jane 1/4/2016 female NA NA NA NA ## 3 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Or we can select two ranges of values from the same variable using the logical “or” (|) operator, assuming the variable is of type numeric, integer, or date. # Filter by two ranges of values of perf_q1 with pipe filterdf &lt;- mergeddf %&gt;% filter(perf_q1&lt;=2.5 | perf_q1&gt;=4.0) # Print filtered data frame filterdf ## # A tibble: 4 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 3 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 4 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 The filter function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when you’ve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, let’s pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out id of 198 and 201 with pipe filterdf &lt;- mergeddf %&gt;% filter(!id %in% c(198,201)) # Print filtered data frame filterdf ## # A tibble: 7 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note that in the output above cases with id variable values equal to 198 and 201 are no longer present. If you remove the ! in front of the filter variable, only cases 198 and 201 are retained. # Filter in id of 198 and 201 with pipe filterdf &lt;- mergeddf %&gt;% filter(id %in% c(198,201)) # Print filtered data frame filterdf ## # A tibble: 2 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 2 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 And if you wanted to remove just a single case, you could use the unique identifier variable (id) and the following script/code. # Filter out id of 198 with pipe filterdf &lt;- mergeddf %&gt;% filter(id!=198) # Print filtered data frame filterdf ## # A tibble: 8 × 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. First, type the name of the data frame object (mergeddf), followed by the $ operator and the name of whatever you want to call the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Second, type the &lt;- operator. Third, type the name of the as.Date function. Fourth, in the function parentheses, as the first argument, enter the as.character function with the name of the data frame object (mergeddf), followed by the $ operator and the name the original variable (startdate) as the sole argument. Fifth, as the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. # Convert character startdate variable to the Date type startdate2 variable mergeddf$startdate2 &lt;- as.Date(as.character(mergeddf$startdate), format=&quot;%m/%d/%Y&quot;) To verify that the new startdate2 variable is of type date, use the str function from base R, and enter the name of the data frame object (mergeddf) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Verify that the startdate2 variable is now a variable of type Date str(mergeddf) ## spc_tbl_ [9 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:9] 153 154 155 165 125 111 198 201 282 ## $ lastname : chr [1:9] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname : chr [1:9] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate : chr [1:9] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:9] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; ... ## $ perf_q1 : num [1:9] 3.9 NA NA NA 2.1 3.3 4.9 1.2 2.2 ## $ perf_q2 : num [1:9] 4.8 NA NA NA 1.9 3.3 4.5 1.1 2.3 ## $ perf_q3 : num [1:9] 4.9 NA NA NA 2.1 3.4 4.4 1 2.4 ## $ perf_q4 : num [1:9] 5 NA NA NA 2.3 3.3 4.8 1 2.5 ## $ startdate2: Date[1:9], format: &quot;2016-01-01&quot; &quot;2016-01-09&quot; &quot;2016-01-09&quot; &quot;2016-01-04&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Now we are ready to filter using the new startdate2 variable. When specify the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I filter for those cases in which their startdate2 values are greater than 2016-01-07. # Filter by startdate2 with pipe filterdf &lt;- mergeddf %&gt;% filter(startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered data frame filterdf ## # A tibble: 5 × 10 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA 2016-01-09 ## 2 155 Smith John 1/9/2016 male NA NA NA NA 2016-01-09 ## 3 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 2016-01-09 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 2016-01-09 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 2016-01-09 18.2.4.2 Without Pipes We can also filter using the filter function from the dplyr package without using the pipe (%&gt;%) operator. Note how I simply move the name of the data frame object from before the pipe (%&gt;%) operator to the first argument in the filter function. Everything else remains the same. For simplicity, I don’t display the output below as it is the same as the output as above using pipes. Your decision whether to use a pipe operator is completely up to you. Let’s filter the mergeddf data frame object such that only those cases for which the gender variable is equal to “female” are retained. Note how we apply the equal to (==) logical operator. A table of logical operators is presented towards the beginning of this tutorial. # Filter in by gender without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot;) # Print filtered data frame filterdf Now let’s filter out those cases in which gender is not equal to “female”. We apply the not equal to (!=) logical operator to do so. # Filter in by gender without pipe filterdf &lt;- filter(mergeddf, gender!=&quot;female&quot;) # Print filtered data frame filterdf Filter the data frame such that we retain those cases for which the perf_q2 variable is greater than (&gt;) 4.0. Because the perf_q2 variable is numeric, we don’t put the value 4.0 in quotation marks. # Filter by perf_q2 without pipe filterdf &lt;- filter(mergeddf, perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical “or” operator (|), select those cases for which gender is equal to “female” or for which perf_q2 is greater than 4.0. # Filter by gender or perf_q2 without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot; | perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical “and” operator (&amp;), select those cases for which gender is equal to “female” and for which perf_q2 is greater than 4.0. Note the difference in the resulting filtered data frame. # Filter by gender and perf_q2 without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot; &amp; perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical “or” operator (|), select those cases for which firstname is equal to “John” or for which firstname is equal to “Jane”. In other words, select those individuals whose names are either “John” or “Jane”. # Filter by two values of firstname without pipe filterdf &lt;- filter(mergeddf, firstname==&quot;John&quot; | firstname==&quot;Jane&quot;) # Print filtered data frame filterdf Using the logical “or” operator (|), select the range of cases for which perf_q1 is less than equal to (&lt;=) 2.5 or for which perf_q1 is greater than or equal (&gt;=) to 4.0. # Filter by two ranges of values of perf_q1 without pipe filterdf &lt;- filter(mergeddf, perf_q1&lt;=2.5 | perf_q1&gt;=4.0) # Print filtered data frame filterdf The filter function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when you’ve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, let’s pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out id of 198 and 201 without pipe filterdf &lt;- filter(mergeddf, !id %in% c(198,201)) # Print filtered data frame filterdf Or if you wish to retain only those cases for which the id variable is equal to 198 and 201, drop the not operator (!) from the previous script. # Filter in id of 198 and 201 without pipe filterdf &lt;- filter(mergeddf, id %in% c(198,201)) # Print filtered data frame filterdf You can also drop specific cases one by one using the not equal to operator (!=) and the a unique identifier value associated with the case you wish to remove. We accomplish the same result as above but use two steps instead. Also, note that in the second step below, the new data frame object (filterdf) is used as the first argument because we want to retain the changes we made in the prior step (i.e., dropping case with id equal to 198). # Filter in id of 198 without pipe filterdf &lt;- filter(mergeddf, id!=198) # Filter in id of 201 without pipe filterdf &lt;- filter(filterdf, id!=201) # Print filtered data frame filterdf When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. First, type the name of the data frame object (mergeddf), followed by the $ operator and the name of whatever you want to call the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Second, type the &lt;- operator. Third, type the name of the as.Date function. Fourth, in the function parentheses, as the first argument, enter the as.character function with the name of the data frame object (mergeddf), followed by the $ operator and the name the original variable (startdate) as the sole argument. Fifth, as the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. To verify that the new startdate2 variable is of type date, on the next line, use the str function from base R, and enter the name of the data frame object (mergeddf) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Convert character startdate variable to the date type startdate2 variable mergeddf$startdate2 &lt;- as.Date(as.character(mergeddf$startdate), format=&quot;%m/%d/%Y&quot;) # Verify that the startdate2 variable is now a variable of type date str(mergeddf) Now we are ready to filter using the new startdate2 variable. When specify the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I filter for those cases in which their startdate2 values are greater than 2016-01-07. # Filter by startdate2 without pipe filterdf &lt;- filter(mergeddf, startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered data frame filterdf 18.2.5 Remove Single Variable from Data Frame If you just need to remove a single variable from a data frame, using the NULL object in R in conjunction with the &lt;- operator can designate which variable to drop. For example, if we wish to drop the startdate variable from the mergeddf data frame, we simply note that startdate belongs to mergeddf by joining them with $. Next, we set &lt;- NULL adjacent to mergeddf$startdate to indicate that we wish to remove that variable from that data frame. # Remove variable mergeddf$startdate &lt;- NULL # Print updated data frame mergeddf ## # A tibble: 9 × 9 ## id lastname firstname gender perf_q1 perf_q2 perf_q3 perf_q4 startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 153 Sanchez Alejandro male 3.9 4.8 4.9 5 2016-01-01 ## 2 154 McDonald Ronald male NA NA NA NA 2016-01-09 ## 3 155 Smith John male NA NA NA NA 2016-01-09 ## 4 165 Doe Jane female NA NA NA NA 2016-01-04 ## 5 125 Franklin Benjamin male 2.1 1.9 2.1 2.3 2016-01-05 ## 6 111 Newton Isaac male 3.3 3.3 3.4 3.3 2016-01-09 ## 7 198 Morales Linda female 4.9 4.5 4.4 4.8 2016-01-07 ## 8 201 Providence Cindy female 1.2 1.1 1 1 2016-01-09 ## 9 282 Legend John male 2.2 2.3 2.4 2.5 2016-01-09 18.2.6 Select Multiple Variables from Data Frame If you wish to select multiple variables from a data frame (and remove all others), the select function from the dplyr package is quite useful and intuitive. Below, I demonstrate how to select multiple variables with and without pipes. If you don’t want to use pipes, feel free to skip down to the section called Without Pipes. 18.2.6.1 With Pipe Using the pipe (%&gt;%) operator, first, decide whether you want to override an existing data frame or create a new data frame based on our selection; here, I override the mergeddf data frame using the &lt;- operator, which results in mergeddf &lt;-. Second, type the name of the original data frame (mergeddf), followed by the pipe (%&gt;%) operator. Third, type the name of the select function. Fourth, in the parentheses, list the names of the variables you wish to select as arguments; all variables that are not listed will be dropped. Here, we are selecting (to retain) the id, perf_q1, gender, lastname, and firstname variables. Note that the updated date frame includes the selected variables in the order in which you listed them. # Select multiple variables with pipe mergeddf &lt;- mergeddf %&gt;% select(id, perf_q1, gender, lastname, firstname) # Print updated data frame mergeddf ## # A tibble: 9 × 5 ## id perf_q1 gender lastname firstname ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 3.9 male Sanchez Alejandro ## 2 154 NA male McDonald Ronald ## 3 155 NA male Smith John ## 4 165 NA female Doe Jane ## 5 125 2.1 male Franklin Benjamin ## 6 111 3.3 male Newton Isaac ## 7 198 4.9 female Morales Linda ## 8 201 1.2 female Providence Cindy ## 9 282 2.2 male Legend John 18.2.6.2 Without Pipe If you decide not to use the pipe (%&gt;%) operator, the syntax remains almost the same except the name of the original data frame object (mergeddf) is moved from before the pipe (%&gt;%) operator to the first argument in the select function. Everything else remains the same. # Select multiple variables without pipe mergeddf &lt;- select(mergeddf, id, gender, lastname, firstname) # Print updated data frame mergeddf 18.2.7 Remove Multiple Variables from Data Frame If you wish to remove multiple variables from a data frame, the select function from dplyr will work just fine. I demonstrate how to use this function with and without pipes. If you don’t want to use pipes, feel free to skip down to the section called Without Pipes. 18.2.7.1 With Pipe Using the pipe (%&gt;%) operator, first, decide whether you want to override an existing data frame or create a new data frame from the subset; here, I override the mergeddf data frame using the &lt;- operator, which results in mergeddf &lt;-. Second, type the name of the original data frame (mergeddf), followed by the pipe (%&gt;%) operator. Third, enter the select function. Fourth, use the c (combine) function with - in front of it to note that you want to select all other variables except the ones listed in the c function. # Remove multiple variables with pipe mergeddf &lt;- mergeddf %&gt;% select(-c(lastname, firstname)) # Print updated data frame mergeddf ## # A tibble: 9 × 3 ## id perf_q1 gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 153 3.9 male ## 2 154 NA male ## 3 155 NA male ## 4 165 NA female ## 5 125 2.1 male ## 6 111 3.3 male ## 7 198 4.9 female ## 8 201 1.2 female ## 9 282 2.2 male Removing a single variable can also be done using the select function. To do so, just list a single variable with - in front of it (as the sole argument) to indicate that you wish to drop that variable. # Remove single variable with pipe mergeddf &lt;- mergeddf %&gt;% select(-gender) # Print updated data frame mergeddf ## # A tibble: 9 × 2 ## id perf_q1 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 ## 2 154 NA ## 3 155 NA ## 4 165 NA ## 5 125 2.1 ## 6 111 3.3 ## 7 198 4.9 ## 8 201 1.2 ## 9 282 2.2 18.2.7.2 Without Pipe If you decide not to use the pipe (%&gt;%) operator, the syntax remains mostly the same except the name of the original data frame object (mergeddf) is moved from before the pipe (%&gt;%) operator to the first argument in the select function. Everything else remains the same. # Remove multiple variables without pipe mergeddf &lt;- select(mergeddf, -c(lastname, firstname)) # Print updated data frame mergeddf And here’s the non-pipe equivalent to removing a single variable using this approach. # Remove single variable without pipe mergeddf &lt;- mergeddf %&gt;% select(-gender) # Print updated data frame mergeddf 18.2.8 Summary Applying filters and creating subsets of cases (rows) and variables (columns) from a data frame is an important part of data management. The dplyr package has two useful functions that can be used for these purposes: filter and select. 18.3 Chapter Supplement In addition to the filter function from the dplyr package covered above, we can use the subset function from base R to subset cases from a data frame and to select cases from a data frame. Because this function comes from base R, we do not need to install and access an additional package like we do with the filter function, which some may prefer or find advantageous. Further, we can also apply the str_detect function from the stringr package with either the subset or the filter function to filter by a text pattern contained within a string (e.g., character) variable. 18.3.1 Video Tutorials In addition to the written chapter supplement provided below, you can follow along with the following video tutorials to learn more about how to subset cases and select/remove variables using the subset function from base R. Link to video tutorial: https://youtu.be/iM1e0wxUMrs Link to video tutorial: https://youtu.be/kNpezEOx70g 18.3.2 Functions &amp; Packages Introduced Function Package subset base R str_detect stringr 18.3.3 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object # Note that we will only be reading in one # data frame object for this supplement # ones we used in the main part of the chapter personaldata &lt;- read_csv(&quot;PersonalData.csv&quot;) ## Rows: 8 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): lastname, firstname, startdate, gender ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 8 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man 18.3.4 subset Function from Base R As an alternative to the filter function from the dplyr package, we will learn how to use the subset function from base R to filter cases from a data frame and to select or remove variables from a data frame. 18.3.4.1 Filter (Subset) Cases from Data Frame We’ll begin by filtering cases from a data frame object. As a reminder, in R, we can apply any one of the following logical operators when filtering cases from a data frame or table object. Logical Operator Definition &lt; “less than” &gt; “greater than” &lt;= “less than or equal to” &gt;= “greater than or equal to” == “equal to” != “not equal to” | “or” &amp; “and” ! “not” To filter (subset) cases from a data frame object using the subset function from base R, we will take the following steps: We’ll use the &lt;- assignment operator to name the filtered data frame that we are about to create. For this example, I chose to create a new data frame object (sub_personaldata), which I specified to the left of the &lt;- operator; that being said, you could name the new data frame object whatever you would like. To the right of the &lt;- operator, type the name of subset function from base R. As the first argument in the function, type the name of the data frame we created above (personaldata). As the second argument, type the name of the variable we wish to filter the data frame by, which in this example is gender followed by a logical (conditional) argument. For this example, we wish to to retain only those cases in which gender is equal to “woman”, and we do so using this logical argument gender == \"woman\". Because the gender variable is of type character, we need to put quotation marks (\" \") around the variable value (i.e., text) that we wish to filter by. Remember, object names in R are case and space sensitive; for instance, gender is different from Gender, and “woman” is different from “Woman”. # Filter (subset) by gender sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 woman ## 2 198 Morales Linda 1/7/2016 woman ## 3 201 Providence Cindy 1/9/2016 woman Note how the data frame above contains only those cases with “woman” as their gender variable designation. The filter worked as expected. Alternatively, we could filter out (subset out) those cases in which gender is equal to “woman” using the != (not equal to) logical operator. Instead of overwriting the existing data frame object (personaldata), let’s assign the filtered data frame object to a new object that we’ll call sub_personaldata. # Filter (subset) by gender sub_personaldata &lt;- subset(personaldata, gender != &quot;woman&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 111 Newton Isaac 1/9/2016 man ## 5 282 Legend John 1/9/2016 man Note how cases with gender equal to “woman” are no longer in the data frame, while every other case is retained. Let’s now filter (subset) by a variable of type numeric/integer. Specifically, let’s select those cases in which the id variable is greater than (&gt;) 154. Because the id variable consists of type numeric/integer, we won’t use quotation marks (\" \") around the value we wish to filter by, which in this case is 154. As we did above, let’s assign the filtered data frame object to an object that we’ll call sub_personaldata; this will overwrite the existing object called sub_personaldata in our R Environment. # Filter (subset) by id sub_personaldata &lt;- subset(personaldata, id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 198 Morales Linda 1/7/2016 woman ## 4 201 Providence Cindy 1/9/2016 woman ## 5 282 Legend John 1/9/2016 man If we wish to filter (subset) by two variables, we can apply the logical “or” (|) operator or “and” (&amp;) operator. First, let’s select those cases in which either gender is equal to “woman” or id is greater than 154 using the logical “or” (|) operator. This will retain those cases for whom at least one logical statement is true. Once again, let’s assign the filtered data frame object to an object that we’ll call sub_personaldata; this will overwrite the existing object called sub_personaldata in our R Environment. # Filter (subset) by gender OR id (application of logical OR operator) sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot; | id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 198 Morales Linda 1/7/2016 woman ## 4 201 Providence Cindy 1/9/2016 woman ## 5 282 Legend John 1/9/2016 man Now watch what happens if we apply the logical “and” (&amp;) operator by keeping everything the same but swapping out the logical “or” (|) operator with the logical “and” (&amp;) operator. This will retain only those cases for whom both logical statements are true. # Filter (subset) by gender AND id (application of logical AND operator) sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot; &amp; id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 woman ## 2 198 Morales Linda 1/7/2016 woman ## 3 201 Providence Cindy 1/9/2016 woman We can also use the logical “or” (|) operator to select two values of the same variable. In this example, we will select cases for whom either firstname is equal to “John” or firstname is equal to “Jane”. # Filter (subset) by two values of firstname using logical OR sub_personaldata &lt;- subset(personaldata, firstname == &quot;John&quot; | firstname == &quot;Jane&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 282 Legend John 1/9/2016 man We can select two ranges of values from the same variable using the logical “or” (|) operator, assuming the variable is of type numeric, integer, or date. In this example, we will retain the cases for whom either logical statement is true: id is less than or equal to 154 or id is greater than or equal to 198. # Filter (subset) by two ranges of values for id using logical OR sub_personaldata &lt;- subset(personaldata, id &lt;= 154 | id &gt;= 198) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 111 Newton Isaac 1/9/2016 man ## 4 198 Morales Linda 1/7/2016 woman ## 5 201 Providence Cindy 1/9/2016 woman ## 6 282 Legend John 1/9/2016 man Alternatively, can select a single range of values within lower and upper bounds from the same variable by using the logical “and” (&amp;) operator, assuming the variable is of type numeric, integer, or date. In this example, we will retain the cases for whom both logical statements are true: id is greater than or equal to 154 and id is less than or equal to 198. # Filter (subset) by single range of values for id using logical AND sub_personaldata &lt;- subset(personaldata, id &gt;= 154 &amp; id &lt;= 198) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 4 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 man ## 2 155 Smith John 1/9/2016 man ## 3 165 Doe Jane 1/4/2016 woman ## 4 198 Morales Linda 1/7/2016 woman The subset function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when you’ve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, let’s pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out (subset out) id of 198 and 201 sub_personaldata &lt;- subset(personaldata, !id %in% c(198,201)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 282 Legend John 1/9/2016 man Note that in the output above cases with id variable values equal to 198 and 201 are no longer present. If we remove the ! in front of the filter (subset) variable, only cases 198 and 201 are retained. # Filter in (subset in, select) id of 198 and 201 sub_personaldata &lt;- subset(personaldata, id %in% c(198,201)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 2 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 198 Morales Linda 1/7/2016 woman ## 2 201 Providence Cindy 1/9/2016 woman We can also drop specific cases one by one using the not equal to logical operator (!=) and the a unique identifier value associated with the case you wish to remove. We accomplish the same result as above but use two steps instead. Also, note that in the second step below, the new data frame object (sub_personaldata) is used as the first argument in the subset function because we want to retain the changes we made in the prior step (i.e., dropping case with id equal to 198). # Filter out (subset out) id of 198 sub_personaldata &lt;- subset(personaldata, id != 198) # Filter out (subset out) id of 201 sub_personaldata &lt;- subset(sub_personaldata, id != 201) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 × 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 282 Legend John 1/9/2016 man When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. Begin by typing the name of the data frame object (personaldata), followed by the $ operator and the name of whatever you would like to name the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Type the &lt;- assignment operator. Type the name of the as.Date function. As the first argument in the function in the as.Date function, type the name of the as.character function with the name of the data frame object (personaldata), followed by the $ operator and the name the original variable (startdate) as the sole argument within the as.character function. Note that we are nesting the as.character function within the as.Date function, and due to order of operations, the as.character function will be run first, followed by the startdate function. As the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. # Convert character startdate variable to the Date type startdate2 variable personaldata$startdate2 &lt;- as.Date(as.character(personaldata$startdate), format=&quot;%m/%d/%Y&quot;) To verify that the new startdate2 variable is of type Date, use the str function from base R, and type the name of the data frame object (personaldata) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Verify that the startdate2 variable is now a variable of type Date str(personaldata) ## spc_tbl_ [8 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:8] 153 154 155 165 111 198 201 282 ## $ lastname : chr [1:8] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname : chr [1:8] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate : chr [1:8] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:8] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; ... ## $ startdate2: Date[1:8], format: &quot;2016-01-01&quot; &quot;2016-01-09&quot; &quot;2016-01-09&quot; &quot;2016-01-04&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Now we are ready to filter (subset) using the new startdate2 variable. When specifying the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I select those cases for whom their startdate2 values are greater than 2016-01-07 – or in other words, those cases who started after January 1, 2016. # Filter (subset) by startdate2 sub_personaldata &lt;- subset(personaldata, startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 × 6 ## id lastname firstname startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 154 McDonald Ronald 1/9/2016 man 2016-01-09 ## 2 155 Smith John 1/9/2016 man 2016-01-09 ## 3 111 Newton Isaac 1/9/2016 man 2016-01-09 ## 4 201 Providence Cindy 1/9/2016 woman 2016-01-09 ## 5 282 Legend John 1/9/2016 man 2016-01-09 18.3.4.2 Select Single Variable from Data Frame To display a single variable from a data frame in our Console, within the subset function, we can do the following: We’ll use the &lt;- assignment operator to name the data frame that we are about to create. For this example, I chose to create a new data frame object called tempdf, which I placed to the left of the &lt;- assignment operator; that being said, you could name the new data frame object whatever you would like – or you could overwrite the existing data frame object. To the right of the &lt;- operator, type the name of subset function from base R. As the first argument in the function, type the name of the data frame object from which we wish to select a single variable (personaldata) As the second argument, type select= followed by the name of a single variable (startdate) we wish to select. # Select only one variable from a data frame (startdate) tempdf &lt;- subset(personaldata, select=startdate) # Print data frame print(tempdf) ## # A tibble: 8 × 1 ## startdate ## &lt;chr&gt; ## 1 1/1/2016 ## 2 1/9/2016 ## 3 1/9/2016 ## 4 1/4/2016 ## 5 1/9/2016 ## 6 1/7/2016 ## 7 1/9/2016 ## 8 1/9/2016 18.3.4.3 Select Multiple Variables from Data Frame If our goal is to select multiple variables from a data frame (and remove all others), we can use the subset function as follows. As we did above, name new data frame object using the &lt;- operator, and we will overwrite the data frame object we created above called tempdf. As the first argument in the subset function, type the name of your original data frame object (mergeddf). As the second argument, type select= followed by a vector of variable name you wish to select/retain. The order in which you enter the variable names will correspond to the order in which they appear in the new data frame object. Use the c (combine) function from base R with each variable name you wish to select as arguments separated by commas. Here we select the lastname, firstname, and gender variables. # Select multiple variables (lastname, firstname, gender) tempdf &lt;- subset(personaldata, select=c(lastname, firstname, gender)) # Print data frame print(tempdf) ## # A tibble: 8 × 3 ## lastname firstname gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sanchez Alejandro man ## 2 McDonald Ronald man ## 3 Smith John man ## 4 Doe Jane woman ## 5 Newton Isaac man ## 6 Morales Linda woman ## 7 Providence Cindy woman ## 8 Legend John man 18.3.4.4 Remove Single Variable from Data Frame If you need to remove a single variable from a data frame, you can simply type the minus (-) operator before the name of the variable you wish to remove. Here, we remove the startdate variable. # Remove one variable from a data frame (startdate) tempdf &lt;- subset(personaldata, select=-startdate) # Print data frame print(tempdf) ## # A tibble: 8 × 5 ## id lastname firstname gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 153 Sanchez Alejandro man 2016-01-01 ## 2 154 McDonald Ronald man 2016-01-09 ## 3 155 Smith John man 2016-01-09 ## 4 165 Doe Jane woman 2016-01-04 ## 5 111 Newton Isaac man 2016-01-09 ## 6 198 Morales Linda woman 2016-01-07 ## 7 201 Providence Cindy woman 2016-01-09 ## 8 282 Legend John man 2016-01-09 18.3.4.5 Remove Multiple Variables from Data Frame If you wish to remove multiple variables from a data frame, you can apply the same syntax as you did when selecting multiple variables, except insert a minus (-) operator in from the of the c function. This tells the function to not select those variables. Here, we remove the lastname and firstname variables from the personaldata data frame object and assign the resulting data frame to an object called tempdf. # Remove multiple variables (lastname, firstname) tempdf &lt;- subset(personaldata, select= -c(lastname, firstname)) # Print data frame print(tempdf) ## # A tibble: 8 × 4 ## id startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 153 1/1/2016 man 2016-01-01 ## 2 154 1/9/2016 man 2016-01-09 ## 3 155 1/9/2016 man 2016-01-09 ## 4 165 1/4/2016 woman 2016-01-04 ## 5 111 1/9/2016 man 2016-01-09 ## 6 198 1/7/2016 woman 2016-01-07 ## 7 201 1/9/2016 woman 2016-01-09 ## 8 282 1/9/2016 man 2016-01-09 18.3.5 Filter by Pattern Contained within String In some cases, we may wish to filter cases from a data frame object based on a pattern contained within a string (i.e., text, characters). For example, using the personaldata data frame object we created above, perhaps we would like to select those cases for which their firstname string (i.e., value) contains a capital (“J”). To do so, we can use the str_detect function from the stringr package (Wickham 2019) within either the subset function from base R or the filter function from the dplyr package. To get started, make sure that you have installed and accessed the stringr package. # Install stringr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;stringr&quot;) # Access stringr package library(stringr) Given that this chapter supplement has focused thus far on the subset function from base R, let’s continue to use that function, but please note that you could just as easily use the filter function from the dplyr package. Using the &lt;- operator, we’ll assign the resulting subset data frame to an object that I’m calling sub_personaldata. To the right of the &lt;- operator, type the name of the subset function As the first argument in the subset function, type the name of the original data frame object that we’ve been working with called personaldata. As the second argument, type the name of the str_detect function. As the first argument within the str_detect function, type the name of the variable we wish to filter by, which in this example is firstname; as the second argument and within quotation marks (\" \"), type a pattern you would like to detect within text strings from the firstname variable. For this example, let’s detect any text string containing an uppercase “J” while noting that case sensitivity matters (e.g., “J” vs. “j”). In other words, we are filtering the data frame such that we will retain only those cases for which their firstname variable text strings (i.e., values) contain an uppercase “J”. # Select cases for which firstname variable contains a &quot;J&quot; # Note that case sensitivity matters (e.g., &quot;j&quot; vs. &quot;J&quot;) sub_personaldata &lt;- subset(personaldata, str_detect(firstname, &quot;J&quot;)) Now let’s print the new data frame object we created. # Print the data frame object print(sub_personaldata) ## # A tibble: 3 × 6 ## id lastname firstname startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 155 Smith John 1/9/2016 man 2016-01-09 ## 2 165 Doe Jane 1/4/2016 woman 2016-01-04 ## 3 282 Legend John 1/9/2016 man 2016-01-09 References "],["clean.html", "Chapter 19 Cleaning Data 19.1 Conceptual Overview 19.2 Tutorial", " Chapter 19 Cleaning Data In this chapter, we will learn how to clean data, such as correcting data-entry errors or removing out-of-bounds scores. 19.1 Conceptual Overview Link to conceptual video: https://youtu.be/f8KIDg2UUKQ Cleaning data is an essential part of the Data Management phase of the HR Analytics Project Life Cycle. When we clean data, broadly speaking, we identify, correct, or remove problematic observations, scores, or variables. More specifically, data cleaning often entails (but is not limited to): Identifying and correcting data-entry errors and inconsistent coding; Evaluating missing data and determining how to handle them; Flagging and correcting out-of-bounds scores for variables; Addressing open-ended and/or “other” responses from employee surveys; Flagging and potentially removing untrustworthy variables. With categorical (i.e., nominal, ordinal) variables containing text values, sometimes different spellings or formatting (e.g., uppercase, lowercase) are used (mistakenly) to represent the same category. Such issues broadly fall under data-entry errors and inconsistent coding reason for data cleaning. While the human eye can usually discern what the intended category is, many software programs and programming languages will be unable to automatically or correctly determine which text values represent which category. For example, in the table below, the facility variable is categorical and contains text values meant to represent different facilities at this hypothetical organization. Human eyes can quickly pick out that there are two facility locations represented in this table: Beaverton and Portland. With that said, for the R programming language, without direction, the different spellings and different cases (i.e., lowercase vs. uppercase “B”) for the Beaverton facility (i.e., Beaverton, beaverton, beverton) will be treated as unique categories (i.e., facilities in this context). Often this is the result of data-entry errors and/or the lack of data validation. To clean the Facility variable, we could convert all instances of “beaverton” and “beverton” to “Beaverton”. Data-entry errors and inconsistent coding: In this table, different spelling and letter cases (e.g., uppercase vs. lowercase) appear for what is supposed to be the same facility location: Beaverton. Missing data (i.e., missing scores) for certain observations (i.e., cases) should also be addressed during data cleaning. For the Facility variable in the table shown below, note how facility location data are missing for the employees with IDs EP9746 and EP9952. In this example, we could likely find other employee records or contact the employees (or their supervisors) in question to verify the facility location where these to employees work. Provided we find the facility locations for these two employees, we could then replace the missing values with the correct facility location information. In other instances, it may prove to be more difficult to replace missing data, such as when organization administers an anonymous employee survey and certain respondents have missing responses to certain questions or items. In such instances, we may decide to tolerate a small percentage of missing data (e.g., &lt; 10%) when we go to analyze the data; however, if the percentage of missing data is sufficiently large and if we plan to analyze the data to make inferences about underlying population from which the sample data were attained, we may begin to think more seriously about whether the data are missing completely at random, missing at random, or missing at not at random. A proper discussion of missing data theory is beyond the scope of this chapter. Missing data: In this table, data are missing for the employees with IDs EP9746 and EP9952. In some instances, we might encounter out-of-bounds scores, which refer to values that simply are too high or low, or that are just too unrealistic or implausible to be correct. In the table below, the Base Salary variable includes salaries that are all below 75,000 – with the notable exception of salary associated with employee ID EP0214. Let’s imagine that this table is only supposed to include data for a specific job category; knowing that, a base salary of 789,120,000 seems excessively high in a global sense and extraordinarily high in a local sense. It could be that someone entered the base salary data incorrectly for this employee, perhaps by adding four extra zeroes at the end of the actual base salary amount. In this case, we would try to find verify the correct base salary for this individual and then make the correction to the data. Out-of-bounds scores: In this table, the base salary for the individual with employee ID EP0214 seems extraordinarily high and is almost certainly a data entry error. If the example above seems a bit far-fetched to you, I’ll provide you with a personal example of just a single zero being mistakenly added to the end of a paycheck. After my fourth year of graduate school, I decided to teach a summer course as an adjunct faculty member, which happened to be an introductory human resource management course. During the week in which I was covering employee compensation, I received a paycheck with one extra zero added to my pay for that month, which of course increased my monthly pay 10-fold. As much as I would have enjoyed holding onto that extra money, I quickly reached out to the a representative from the university’s HR department, and the person addressed the error very quickly. At the end of the conversation, the HR representative said jokingly, “The irony is not lost on us that we made this payroll error for someone who is teaching a unit on employee compensation.” When an open-ended response or “other” response field is provided (as opposed to a close-ended response field with predetermined response options), the individual who enters the data can type in whatever they would like (in most instances) provided that they limit their response to the allotted space. This challenge crops up frequently in employee surveys, such as when employees may select an “other” response for one survey question that then branches them to a follow-up question that is open-ended. In the table below, survey respondents’ close-ended response options for the Number of Direct Reports variable include an “Other” option; respondents who responded with “Other” had an opportunity to indicate their number of direct reports using an open-ended survey question associated with the Number of Direct Reports (Other) variable. When cleaning such data, we often must determine what to do with the affect variables on a case-by-case basis. For example, the individual who responded to the survey associated with a unique identifier of 2 responded with “Not a supervisor”. If this survey was intended to acquire data from supervisors only, then we might decide to remove the row of data associated with that individual’s response, as they likely do not fit our definition of the target population. Open-ended and/or “other” responses: In this table, survey respondents’ close-ended response options for the Number of Direct Reports variable include an “Other” option; for respondents who responded with “Other”, they then had an opportunity to indicate their number of direct reports using an open-ended survey question associated with the Number of Direct Reports (Other) variable. Finally, sometimes scores for a variable (or even missing scores for a variable) seem “off,” incorrect, or implausible – or in other words, untrustworthy. For example, in the table below, a variable called Training Post Test is meant to include the scores on a post-training assessment; yet, we can see that the individual with employee ID EP1475 has a score of 99 even though the adjacent variable indicates that the individual did not complete the training. Now, it’s entirely possible that this person (and others) were part of a control group (i.e., comparison group) intended to be used as part of a training evaluation design, but that then begs the question why only one individual in this table has a training post-test score. At first glance, data associated with the Training Post Test variable seem untrustworthy, and they very well may be in reality. As a next step, we would want to do some sleuthing to figure out what errors or issues may be at work in these data, and whether we should remove the potentially untrustworthy variable in question. Untrustworthy variables: In this table, data are missing for all but employee ID EP1475 on the Training Post Test variable, and furthermore, the Completed Training variable indicates that none of the employees who appear in the table received training. 19.2 Tutorial This chapter’s tutorial demonstrates different techniques for cleaning data in R. 19.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/mGQvJ3FuNa8 19.2.2 Functions &amp; Packages Introduced Function Package View base R count dplyr str base R mutate dplyr replace base R match base R ifelse base R is.na base R toupper base R tolower base R names base R function base R clean_names janitor 19.2.3 Initial Steps If you haven’t already, save the file called “DataCleaningExample.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “DataCleaningExample.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) ## Rows: 10 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmpID, Facility, OnboardingCompleted ## dbl (2): JobLevel, Org_Tenure_Yrs ## date (1): StartDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;JobLevel&quot; &quot;StartDate&quot; &quot;Org_Tenure_Yrs&quot; &quot;OnboardingCompleted&quot; # Print data frame (tibble) objects df ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Note in the data frame that the EmpID field/variable is a unique identifier variable, which means that each case/observation has a unique value on this field/variable. This will become useful later on in this tutorial when we replace values on variables for specific cases. 19.2.4 Review Data There are different tools and techniques we can use to review the cleanliness or integrity of the available data. Often, it’s a good idea to give the data a once over with what I refer to as the “ocular test,” which simply means to scan the raw data using your eyes. The ocular test can give you an idea of the types of data cleaning issues you’ll need to address. The View function from base R is a great tool for this, as it allows you to look at the data frame object in a viewer tab, and using the arrows at the top of each column, you can sort each field (i.e., variable) manually. # View data frame object using View function View(df) Reviewing data using the View function from base R. In addition, applying the count function from the dplyr package (Wickham et al. 2023) can provide us with an understanding of the values (or lack thereof) associated with each variable in your data frame object. The count function groups and tallies the number of observations (i.e., frequencies) by value/level within a variable. Using this function we can (hopefully) identify any values that might be considered “out of bounds” or erroneous. Because the count function comes from the dplyr package, if you haven’t already, install and access the dplyr package using the install.packages and library functions, respectively. # Install dplyr package if you haven&#39;t already install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) We can achieve the same output with and without the use of the pipe (%&gt;%) operator because the dplyr package is built upon the magrittr (Bache and Wickham 2022) package. For more information on the pipe operator, check out this link: https://r4ds.had.co.nz/pipes.html. If you don’t want to use the pipe operator with the dplyr functions, just skip down below to see how to specify the function without using pipes. Using the pipe (%&gt;%) operator, first, type the name of the data frame object to which a variable belongs (e.g., df). Second, type the %&gt;% operator. Third, type the name of the count function, and within the parentheses, enter the exact name of the variable (e.g., Facility) as the sole argument. # Apply count function to Facility variable using pipe df %&gt;% count(Facility) ## # A tibble: 5 × 2 ## Facility n ## &lt;chr&gt; &lt;int&gt; ## 1 Beaverton 4 ## 2 Portland 3 ## 3 beaverton 1 ## 4 beverton 1 ## 5 &lt;NA&gt; 1 Note that the output yields a table object (or more specifically a “tibble”, which is used in the tidyverse collection of functions). There are two columns: (a) the variable name, below which appears all values/levels of that variable, and (b) the n column, which displays the number of cases/observations associated with each value/level of the variable in question. As you can see, there appear to be some errors. Perhaps the database used to gather these data lacked data validation rules for certain variables, which allowed users to enter different spellings or formatting of the same value/level of the variables. For example, the Beaverton facility is spelled/formatted in three ways: Beaverton, beaverton, and beverton. Because R is case sensitive, Beaverton and beaverton are treated as to distinct levels/values of the Facility variable. Further, beverton is a misspelling of Beaverton and lacks the capital B. Finally, note that there is one NA value, which indicates that someone forgot to enter the name of the facility where that employee works. Clearly, we need to correct these errors and clean up the data residing within this variable; later in the tutorial, you will learn how to correct/replace these values in the R environment. To perform the operation above without the pipe (%&gt;%) operator,type the name of the count function, and within the parentheses, type the name of the data frame object (df) as the first argument, followed by a comma. As the second argument, type the exact name of the variable (e.g., Facility). # Apply count function to Facility variable without using pipe count(df, Facility) ## # A tibble: 5 × 2 ## Facility n ## &lt;chr&gt; &lt;int&gt; ## 1 Beaverton 4 ## 2 Portland 3 ## 3 beaverton 1 ## 4 beverton 1 ## 5 &lt;NA&gt; 1 As you can see, the output is the same with and without the use of the pipe (%&gt;%) operator. Now let’s apply the count function to the JobLevel variable. To save space, I only show how to do this using the pipe operator (%&gt;%). All we need to do is replace Facility with JobLevel in the syntax/code we wrote previously. # Apply count function to JobLevel variable using pipe df %&gt;% count(JobLevel) ## # A tibble: 7 × 2 ## JobLevel n ## &lt;dbl&gt; &lt;int&gt; ## 1 -9999 1 ## 2 1 4 ## 3 2 1 ## 4 3 1 ## 5 4 1 ## 6 5 1 ## 7 11 1 For the sake of this example, let’s assume that the company has only five job levels (1 = lowest, 5 = highest). In the output, it is apparent that there are two out-of-bounds values: -9999 and 11. Rather than leave cell blank when entering data, some people prefer to use extreme negative values, such as -9999, to flag missing data, and let’s assume that is the case in this example. A potential issue with this approach to coding missing data is that R assumes -9999 is a real value, and in R, NA is often used to indicate missing data to avoid this issue. Regarding the 11 value, it is likely that someone made an error when entering the data value in the database by typing 1 twice by mistake; let’s assume that we verified that this was the case and that 11 should be replaced with 1. Lastly, let’s apply the count function to the OnboardingCompleted variable. # Apply count function to OnboardingCompleted variable using pipe df %&gt;% count(OnboardingCompleted) ## # A tibble: 1 × 2 ## OnboardingCompleted n ## &lt;chr&gt; &lt;int&gt; ## 1 No 10 In this example, employees either completed (Yes) or did not complete (No) the onboarding for new employees. As you can see in the output, there are no missing data (i.e., all 10 cases have a value); however, note that every single case/observation (i.e., employee) has the value No when in actuality (let’s assume) every single case/observation should have the value Yes because we learned that every single employee in this data frame completed the onboarding program. In the next section, you will learn how to clean the “dirty” data we identified within the Facility, JobLevel, and OnboardingCompleted variables. 19.2.5 Clean Data As is often the case in R (and in life), there are multiple approaches to cleaning data. I demonstrate two approaches in this tutorial: (a) replacing a value for specific cases/observations with the correct value and (b) replacing a specific value for all cases that have the same value on that variable. Both approaches can come in handy given the particular data-cleaning circumstance you are facing. 19.2.5.1 Replace a Specific Value for a Specific Case To begin, let’s replace a specific value for a specific case/observation. To do so, we’ll apply three functions: mutate from the dplyr package and replace and match from base R. If you haven’t already, be sure that you have installed and accessed the dplyr package using the install.packages and library functions, respectively. Using the pipe (%&gt;%) operator, we can replace a specific value for a specific case by: Type the name of a new data frame object, as it is generally a good practice to preserve the original data frame you read in earlier; here, I arbitrarily name the new data frame object newdf1. Type the &lt;- operator to the right of the new data frame object so that the results of the subsequent operations can be assigned to the new object. Type the name of the original data frame object (df), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. Within the mutate function parentheses, provide the name of an existing or new variable; in this case, we will overwrite the existing Facility variable by typing the same variable name. Next, enter the = operator to the right of the mutate function to specify how values for the Facility variable will be determined. Type the name of the replace function, which is a function that allows one to replace values in specific location within a vector or data frame. Within the replace function parentheses, as the first argument, type the name of the variable for which you wish to replace specific values (Facility). As the second argument, type the name of the match function, which is a function that allows us to find matched value(s) within a variable (i.e., vector); s the first argument within the match function, enter the value you wish to find a match for (“EP1202”), and as the second argument, enter the name of the variable (EmpID) to which that value belongs. Essentially, this match function and its two arguments will instruct R to identify the case in which EmpID (variable name) is equal to “EP1202”. Because the variable EmpID is of type character (chr), the value identified from that variable should be placed in quotation marks (\" \"); if the variable were of type numeric, you would not include the quotation marks (\" \"), and as shown below, the str (structure) function can be used to identify the variable type for the variables within a data frame object (e.g., df); as the final (third) argument within the replace function, enter the new value (e.g., “Beaverton”) that you would like to replace the existing value with for the particular case you identified. # Identify variable type str(df) ## spc_tbl_ [10 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : chr [1:10] &quot;EP1201&quot; &quot;EP1202&quot; &quot;EP1203&quot; &quot;EP1204&quot; ... ## $ Facility : chr [1:10] &quot;Beaverton&quot; &quot;beaverton&quot; &quot;Beaverton&quot; &quot;Portland&quot; ... ## $ JobLevel : num [1:10] 1 1 -9999 5 2 ... ## $ StartDate : Date[1:10], format: &quot;2010-05-05&quot; &quot;2008-01-31&quot; &quot;2017-02-05&quot; &quot;2018-09-19&quot; ... ## $ Org_Tenure_Yrs : num [1:10] 8.6 10.9 1.9 0.3 0.3 8.8 7.6 8.6 7.7 6.5 ## $ OnboardingCompleted: chr [1:10] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_character(), ## .. Facility = col_character(), ## .. JobLevel = col_double(), ## .. StartDate = col_date(format = &quot;&quot;), ## .. Org_Tenure_Yrs = col_double(), ## .. OnboardingCompleted = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # For EmpID equal to EP1202, replace &quot;beaverton&quot; with &quot;Beaverton&quot; using pipe newdf1 &lt;- df %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1202&quot;, EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No In the new data frame object (newdf1), you should now see that “beaverton” has been replaced with “Beaverton” for the case in which EmpID is equal to “EP1202”. Assuming you save your script as an R script file (.R), you will have a paper trail that shows that you replaced an existing value with a new (correct) value. To apply the mutate function without a pipe (%&gt;%) operator, we simply remove the pipe (%&gt;%) operator and enter the name of our original data frame object (df) as the first argument of the mutate function. # For EmpID equal to EP1202, replace &quot;beaverton&quot; with &quot;Beaverton&quot; without using pipe newdf1 &lt;- mutate(df, Facility = replace(Facility, match(&quot;EP1202&quot;,EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Now let’s take the newdf1 data frame object we just created and replace “beverton” with “Beaverton” for the case in which the EmpID variable value is equal to “EP1207”. All we need to do in this instance is to (a) swap out df with newdf1 as the original data frame name and (b) swap out “EP1202” with “EP1207” from the previous piped code we wrote. # For EmpID equal to EP1207, replace &quot;beverton&quot; with &quot;Beaverton&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1207&quot;, EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Let’s assume that after looking through other organizational records we found that the employee with EmpID equal to “EP1205” works at the “Portland” facility. Accordingly, we want to replace the NA value for the Facility variable for that employee with “Portland”. When adapting the previous script/code, all we need to do is replace “EP1207” with “EP1205” and “Beaverton” with “Portland”. # For EmpID equal to EP1205, replace NA with &quot;Portland&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1205&quot;, EmpID), &quot;Portland&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Now that the Facility variable has been cleaned, let’s move on to the JobLevel variable. If you recall, in the previous section, we determined that there were two problematic values for two cases: -9999 (associated with EmpID equal to “1203”) and 11 (associated with EmpID equal to “1210”). We decided that -9999 should be replace with NA because it represents a missing value, and 11 should be replaced with 1 because it represents a data entry error. Check out the script/code below to see how these replacements were handled. Also, note that because the JobLevel variable is of type numeric, we don’t put the replacement values of NA and 1 in direct quotations. # For EmpID equal to EP1203, replace -9999 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(JobLevel = replace(JobLevel, match(&quot;EP1203&quot;, EmpID), NA)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No # For EmpID equal to EP1210, replace 11 with 1 using pipe newdf1 &lt;- newdf1 %&gt;% mutate(JobLevel = replace(JobLevel, match(&quot;EP1210&quot;, EmpID), 1)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No Finally, let’s assume that for the OnboardingCompleted variable, two of the cases have values that should in fact be NA (missing). Specifically, because the hypothetical onboarding program takes 1.0 year to complete, the two employees (cases) with Org_Tenure_Yrs (organizational tenure in years) equal to 0.3 have not worked in the company long enough to have completed the 1-year-long onboarding program. Consequently, we decide that it is not appropriate to put “No” or “Yes” for these two employees, which means treating those values as missing (NA) would be most appropriate. The two employees in question have EmpID variable values of “EP1204” and “EP1205”. Note that we can enter NA as the final argument within the replace function parentheses, just as we would with an actual value. Check out the script/code below to see how we replace 0.3 with NA on the OnboardingCompleted variable for the cases with the EmpID unique identifier variable equal to “EP1204” and “EP1205”. # For EmpID equal to EP1204, replace 0.3 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = replace(OnboardingCompleted, match(&quot;EP1204&quot;, EmpID), NA)) # For EmpID equal to EP1205, replace 0.3 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = replace(OnboardingCompleted, match(&quot;EP1205&quot;, EmpID), NA)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 19.2.5.2 Replace a Specific Value for All Cases with a Particular Value If you need to systematically clean multiple values for a given variable, there is another approach that is more efficient. Note that this approach is only appropriate if you have identified that the values in question all need to be changed to the same value. For the sake of this example, let’s pretend that all of the “No” values for the OnboardingCompleted variable were entered incorrectly. Instead of “No”, the value should be “Yes” for each of these cases. First, you will learn how to replace these values in one fell swoop using the mutate function from the dplyr package and the ifelse function from base R. Second, you will learn how to do the same thing without the use of a specific function. Let’s start with the first approach, which involves the mutate and ifelse functions. Using the pipe (%&gt;%) operator, we can do the following: Type the name of a new data frame object; here, I use the same name as the original name to overwrite the existing data frame object called newdf1. Type the &lt;- operator to the right of the new data frame object so that the results of the subsequent operations can be assigned to the new object. Type the name of the original data frame object (newdf1), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. Within the mutate function parentheses, provide the name of an existing or a new variable; in this case, we will overwrite the existing OnboardingCompleted variable by typing the same name as the existing variable. To the right of the mutate function, type the = operator to specify how values for the OnboardingCompleted variable will be determined. Type the name of the ifelse function, which is a function that can be used for conditional value (element) selection (identification) and replacement. As the first argument, type a conditional statement for the variable whose values you wish to replace; because we wish to replace all “No” values with “Yes” for the OnboardingCompleted variable, we’ll specify the conditional statement as OnboardingCompleted==\"No\"; for a review of the different logical operators, please refer to the chapter called Filtering Data. As the second argument in the ifelse function, type the replacement value when the aforementioned conditional statement is true for a given value of the OnboardingCompleted variable, which is “Yes” in this example. Because the OnboardingCompleted variable is of type character, this value needs to be in quotation marks (\" \"). As the final argument function, instruct the ifelse function what the replacement value should be when the aforementioned conditional statement is false, which in this; in this example, we will replace all other values with the existing values of the OnboardingCompleted variable, and thus we’ll enter that variable name as the third argument. # For OnboardingCompleted variable, replace &quot;No&quot; values with &quot;Yes&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = ifelse(OnboardingCompleted==&quot;No&quot;, &quot;Yes&quot;, OnboardingCompleted)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 Yes ## 2 EP1202 Beaverton 1 2008-01-31 10.9 Yes ## 3 EP1203 Beaverton NA 2017-02-05 1.9 Yes ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 Yes ## 7 EP1207 Beaverton 1 2011-06-01 7.6 Yes ## 8 EP1208 Portland 4 2010-05-15 8.6 Yes ## 9 EP1209 Portland 3 2011-04-29 7.7 Yes ## 10 EP1210 Beaverton 1 2012-07-11 6.5 Yes Now let’s pretend that we wish to convert the “Yes” values for the OnboardingCompleted variable with the numeric value of 1. The format of the code remains virtually the same, except that we change the conditional statement to OnboardingCompleted==\"Yes\" as the first argument in the ifelse function, and in the second argument, we change the replacement value to 1. # For OnboardingCompleted variable, replace &quot;Yes&quot; values with 1 using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = ifelse(OnboardingCompleted==&quot;Yes&quot;, 1, OnboardingCompleted)) # Print new data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 1 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 1 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 1 ## 4 EP1204 Portland 5 2018-09-19 0.3 NA ## 5 EP1205 Portland 2 2018-09-19 0.3 NA ## 6 EP1206 Beaverton 1 2010-03-23 8.8 1 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 1 ## 8 EP1208 Portland 4 2010-05-15 8.6 1 ## 9 EP1209 Portland 3 2011-04-29 7.7 1 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 1 As an alternative approach, we can write the following code. First, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the variable in question (OnboardingCompleted). Second, type brackets ([ ]). Third, within the brackets, enter a conditional statement; for the sake of example, let’s say that we want to identify all instances in which the OnboardingCompleted variable is equal to 1 (newdf1$OnboardingCompleted==1). Fourth, type the &lt;- operator, followed by the value you wish to use to replace the existing values for which the conditional statement you previously specified is true; in this example, we enter 2. Because the two values are numeric, we do not use quotation marks (\" \"). # For OnboardingCompleted variable, replace 1 values with 2 newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==1] &lt;- 2 # Print data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 2 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 2 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 2 ## 4 EP1204 Portland 5 2018-09-19 0.3 NA ## 5 EP1205 Portland 2 2018-09-19 0.3 NA ## 6 EP1206 Beaverton 1 2010-03-23 8.8 2 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 2 ## 8 EP1208 Portland 4 2010-05-15 8.6 2 ## 9 EP1209 Portland 3 2011-04-29 7.7 2 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 2 If we wish to change numeric values to character values, be sure to put the character values in quotation marks (\" \"). In the following example, all instances in which the OnboardingCompleted variable is equal to 2 are changed to “Yes”. # For OnboardingCompleted variable, replace 2 values with &quot;Yes&quot; newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==2] &lt;- &quot;Yes&quot; # Print data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 Yes ## 2 EP1202 Beaverton 1 2008-01-31 10.9 Yes ## 3 EP1203 Beaverton NA 2017-02-05 1.9 Yes ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 Yes ## 7 EP1207 Beaverton 1 2011-06-01 7.6 Yes ## 8 EP1208 Portland 4 2010-05-15 8.6 Yes ## 9 EP1209 Portland 3 2011-04-29 7.7 Yes ## 10 EP1210 Beaverton 1 2012-07-11 6.5 Yes Just for fun, let’s change all of the “Yes” values for the OnboardingCompleted variable back to “No” using the code below. # For OnboardingCompleted variable, replace &quot;Yes&quot; values with &quot;No&quot; newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==&quot;Yes&quot;] &lt;- &quot;No&quot; # Print data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No Finally, imagine we wish to replace the NA values in the OnboardingCompleted variable with the value 2 using this approach. To do so, instead of making our own conditional statement, within the brackets ([ ]), apply the is.na function from base R, and type the name of the data frame, followed by the $ operator and the name of the variable containing the NAs. To the right of the bracket, use the &lt;- operator followed by the value with which you wish to replace the NAs. # For OnboardingCompleted variable, replace NA values with 2 newdf1$OnboardingCompleted[is.na(newdf1$OnboardingCompleted)] &lt;- 2 # Print data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 2 ## 5 EP1205 Portland 2 2018-09-19 0.3 2 ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 19.2.6 Rename Variables In some cases you may wish to rename an existing variable. One of the simplest ways to do this is to create a new variable and then delete the old variable. Let’s rename the Org_Tenure_Yrs variable as simply Tenure. First, specify the name of the data frame object (newdf1), followed by the $ operator and the new name of the variable (Tenure). Second, type the &lt;- operator. Third, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the old variable (Org_Tenure_Yrs). # Create new variable called Tenure based on Org_Tenure_Yrs variable newdf1$Tenure &lt;- newdf1$Org_Tenure_Yrs # Print data frame object print(newdf1) ## # A tibble: 10 × 7 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted Tenure ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No 8.6 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No 10.9 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No 1.9 ## 4 EP1204 Portland 5 2018-09-19 0.3 2 0.3 ## 5 EP1205 Portland 2 2018-09-19 0.3 2 0.3 ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No 8.8 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No 7.6 ## 8 EP1208 Portland 4 2010-05-15 8.6 No 8.6 ## 9 EP1209 Portland 3 2011-04-29 7.7 No 7.7 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 6.5 Note that there is now a new variable called Tenure and that the old variable called Org_Tenure_Yrs remains. To remove the old variable called Org_Tenure_Yrs, first, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the old variable (Org_Tenure_Yrs). Second, type the &lt;- operator. Third, enter NULL. # Remove Org_Tenure_Yrs variable from data frame newdf1$Org_Tenure_Yrs &lt;- NULL # Print data frame object print(newdf1) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate OnboardingCompleted Tenure ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 No 8.6 ## 2 EP1202 Beaverton 1 2008-01-31 No 10.9 ## 3 EP1203 Beaverton NA 2017-02-05 No 1.9 ## 4 EP1204 Portland 5 2018-09-19 2 0.3 ## 5 EP1205 Portland 2 2018-09-19 2 0.3 ## 6 EP1206 Beaverton 1 2010-03-23 No 8.8 ## 7 EP1207 Beaverton 1 2011-06-01 No 7.6 ## 8 EP1208 Portland 4 2010-05-15 No 8.6 ## 9 EP1209 Portland 3 2011-04-29 No 7.7 ## 10 EP1210 Beaverton 1 2012-07-11 No 6.5 As you can see, the Org_Tenure_Yrs variable is now gone. 19.2.7 Other Approaches to Cleaning Data In the following sections, we will consider some other approaches to cleaning data, that under certain conditions, may be more elegant and efficient solutions. 19.2.7.1 Changing the Case of Variable Names In some instances, we may wish to systematically change the case of all variable or column names. There are a few functions that can be quite handy in this regard. Let’s revert back the name of the data frame object we initially read in and names: df. # Access readr package library(readr) # Read in set of data as data frame df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) To change the case of variable or column names such that they are all lower case, we can use the tolower function from base R. Let’s change the variable names to be all lower case. We will need to use the names function from base R as well to signal that we are referencing the column names as opposed to the values for each variable. First, type the name of the names function with the name of the new data frame object you are creating and naming as the sole argument; here, we will put in the same name of the existing data frame to overwrite it. Second, enter the &lt;- operator to create and name the new data frame object. Third, enter the tolower function, and as the sole argument, include the names function again with the name of the focal data frame object as its sole argument. # Change case of variable names to all lower case names(df) &lt;- tolower(names(df)) # Print data frame object print(df) ## # A tibble: 10 × 6 ## empid facility joblevel startdate org_tenure_yrs onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No We can use the toupper function to change the variable names to all upper case. # Change case of variable names to all upper case names(df) &lt;- toupper(names(df)) # Print data frame object print(df) ## # A tibble: 10 × 6 ## EMPID FACILITY JOBLEVEL STARTDATE ORG_TENURE_YRS ONBOARDINGCOMPLETED ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No If you wanted to capitalize just the first letter in each variable name, we can create our function to do so. Let’s call this function firstletterupper and use the function called function from base R to program our own function. In this function, we will define what the function will do. Essentially, we are going to create two strings of text and then concatenate them using the paste function. As the first argument in the paste function and to select a string of text, we will pull just the first letter of each variable name using the substring function; the second and third numeric arguments in this function indicate the range of letters that we will pull out, and because we put 1 and 1, we are saying retain just the first letter in each name. We then enter this substring function as an argument in the toupper function so that we will just capitalize the first letter of each variable. We will then enter the second argument in the paste function, which is another substring function; this time, we will indicate that we simply wish to retain the second letter in each name followed by any remaining letters, and we do so by entering the numeral 2 as the second argument. We then enter this substring function as an argument in the tolower function to make all but the first letter in each variable name lower case. As the final argument in the paste function, we enter sep=\"\" to signify that we do not want any space between these letters when they are concatenated; note that there is no space within the quotation marks. We can now run this script to program our new function called firstletterupper. # Create function to change just first letter of variable name to upper case firstletterupper &lt;- function(x) {paste(toupper(substring(x, 1, 1)), tolower(substring(x, 2)), sep=&quot;&quot;)} With our new DIY function called firstletterupper we can apply it to the variable names using the same approach we did above with the toupper and tolower functions. # Change just first letter of variable name to upper case names(df) &lt;- firstletterupper(names(df)) # Print data frame object print(df) ## # A tibble: 10 × 6 ## Empid Facility Joblevel Startdate Org_tenure_yrs Onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Alternatively, we could use the clean_names function from the janitor package (Firke 2021). Be sure to install and access the package if you haven’t already. # Install janitor package if you haven&#39;t already install.packages(&quot;janitor&quot;) # Access janitor package library(janitor) When coupled with the case=upper_camel argument, the clean_names function will capitalize the first letter in variable names, and if there is an underscore within the variable name (_), the function will capitalize the letter that comes immediately after the underscore. # Change just first letter of variable name to upper case df &lt;- clean_names(df, case=&quot;upper_camel&quot;) # Print data frame object print(df) ## # A tibble: 10 × 6 ## Empid Facility Joblevel Startdate OrgTenureYrs Onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No There are many other case= arguments that you could use for the clean_name functions to different variations of capitalization. Just access the help menu for the function as shown below. # Access help information for clean_names function ?clean_names() 19.2.7.2 Changing the Case of Character Variable Values In other instances, we may wish to systematically change the case of values for a character variable. As before, let’s revert back the name of the data frame object we initially read in and names: df. # Access readr package library(readr) # Read in set of data as data frame df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) To change the case of variable or column names such that they are all lower case, we can use the tolower function from base R. Let’s change the variable names to be all lower case. We will need to use the names function from base R as well to signal that we are referencing the column names as opposed to the values for each variable. First, type the name of the names function with the name of the new data frame object you are creating and naming as the sole argument; here, we will put in the same name of the existing data frame to overwrite it. Second, enter the &lt;- operator to create and name the new data frame object. Third, enter the tolower function, and as the sole argument, include the names function again with the name of the focal data frame object as its sole argument. # Change case of Facility variable&#39;s values to all lower case df$Facility &lt;- tolower(df$Facility) # Print data frame object print(df) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 portland 4 2010-05-15 8.6 No ## 9 EP1209 portland 3 2011-04-29 7.7 No ## 10 EP1210 beaverton 11 2012-07-11 6.5 No We can similarly use the toupper function to change a character variable’s values to all upper case. # Change case of Facility variable&#39;s values to all upper case df$Facility &lt;- toupper(df$Facility) # Print data frame object print(df) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 BEAVERTON 1 2010-05-05 8.6 No ## 2 EP1202 BEAVERTON 1 2008-01-31 10.9 No ## 3 EP1203 BEAVERTON -9999 2017-02-05 1.9 No ## 4 EP1204 PORTLAND 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 BEAVERTON 1 2010-03-23 8.8 No ## 7 EP1207 BEVERTON 1 2011-06-01 7.6 No ## 8 EP1208 PORTLAND 4 2010-05-15 8.6 No ## 9 EP1209 PORTLAND 3 2011-04-29 7.7 No ## 10 EP1210 BEAVERTON 11 2012-07-11 6.5 No If you wanted to capitalize just the first letter in a character variable’s values, we can create our function to do so. Let’s call this function firstletterupper and use the function called function from base R to program our own function. In this function, we will define what the function will do. Essentially, we are going to create two strings of text and then concatenate them using the paste function. As the first argument in the paste function and to select a string of text, we will pull just the first letter of the character variable’s values using the substring function; the second and third numeric arguments in this function indicate the range of letters that we will pull out, and because we put 1 and 1, we are saying retain just the first letter in each name. We then enter this substring function as an argument in the toupper function so that we will just capitalize the first letter of the variable’s values. We will then enter the second argument in the paste function, which is another substring function; this time, we will indicate that we simply wish to retain the second letter in each value followed by any remaining letters, and we do so by entering the numeral 2 as the second argument. We then enter this substring function as an argument in the tolower function to make all but the first letter in each variable name lower case. As the final argument in the paste function, we enter sep=\"\" to signify that we do not want any space between these letters when they are concatenated; note that there is no space within the quotation marks. We can now run this script to program our new function called firstletterupper. Note that this is the same function we created above for changing the case of variable names, but we will take it to another level in this instance. Specifically, we are now ready to insert the paste function we just specified into the ifelse function from base R. Specifically, in case there are any missing values (NAs), we want to exclude those when running this paste function. As the first argument in the ifelse function enter !is.na(x) to indicate we want to select those values for that are not NA. As the second argument, we enter what we wrote for the paste function. As the third argument, we address the “else” part of the ifelse function; meaning, we need to indicate how values that are NA should be treated. By entering the name of the object x as this final argument, we are indicating that the original value (which in this case is actually missing as NA) will be retained. # Create function to change just first letter of character variable&#39;s values to upper case firstletterupper &lt;- function(x) {ifelse(!is.na(x), paste(toupper(substring(x, 1, 1)), tolower(substring(x, 2)), sep=&quot;&quot;), x)} With our new DIY function called firstletterupper we can apply it to the character variable we wish to change (Facility). # Change just first letter of character variable&#39;s values to upper case df$Facility &lt;- firstletterupper(df$Facility) # Print data frame object print(df) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No 19.2.8 Summary In this tutorial, using the View function from base R and the count function from the dplyr package, we learned how to review data to identify problematic values or variables. In addition, we learned how to clean data by change values using the mutate function from the dplyr package and the replace, match, and ifelse functions from base R. Next, we learned how to rename variables and remove existing variables. Finally, we learned some alternative approaches to changing the case of character variables and values. References "],["manipulate.html", "Chapter 20 Manipulating &amp; Restructuring Data 20.1 Conceptual Overview 20.2 Tutorial", " Chapter 20 Manipulating &amp; Restructuring Data In this chapter, we will learn how to manipulate and restructure data – and more specifically, convert a data frame object from wide to long format and from long to wide format. 20.1 Conceptual Overview Data manipulation and restructuring refers to the process in which data are restructured or formatted in a different manner. When we think about structured data, we often think in terms of wide versus long formats. Wide format data are structured such that there are more columns (i.e., variables) and fewer rows than with long format data. For example, most survey platforms allow you to download the survey responses in a wide format, such that each variable (that corresponds to a survey item/question) will have its own column. Long format data are structured such that that are (sometimes) fewer columns yet more rows than with wide-format data. For example, we can restructure survey data from wide format such that one variable (i.e., column) contains the names of the different survey items/questions, and another variable contains the responses/scores for each item/question. Often our decision to manipulate data into wide versus long formats has to do with the type of analysis or visualization we plan to perform. As such, understanding how to manipulate and restructure your data is an important data-science skill. Data can be manipulated into different structures to accomplish different goals. For example, a dataset in wide format might include employees’ scores on pre-test and post-test assessments as separate variables (i.e., columns), where one column includes the pre-test scores and one column includes the post-test scores. The same dataset could, alternatively, be manipulated or restructured into long format, such that the time of the test administration becomes a categorical variable and a separate variable contains the scores; In the wide-format dataset, each employee has a single row of data, whereas in the long-format dataset each employee has one row of data for their pre-test and one row of data for their post-test. 20.2 Tutorial This chapter’s tutorial demonstrates how to restructure data from wide to long format and from long to wide format. 20.2.1 Video Tutorial In the video tutorial below, I demonstrate how to use two data-manipulation functions that have since been deprecated by the tidyr package, which are called gather and spread. In this chapter, I review how to use the new data-manipulation functions from the tidyr package called pivot_longer and pivot_wider, respectively. I personally find the new functions to be more intuitive to use, but perhaps you’ll disagree. Regardless of which you choose, both will help you manipulate data from wide to long format and from long to wide format. Link to video tutorial: https://youtu.be/5DgEAKLRjhw 20.2.2 Functions &amp; Packages Introduced Function Package pivot_longer tidyr c base R pivot_wider tidyr 20.2.3 Initial Steps If you haven’t already, save the file called “ManipulatingData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “ManipulatingData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object datam &lt;- read_csv(&quot;ManipulatingData.csv&quot;) ## Rows: 20 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): SurveyID, JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(datam) ## [1] &quot;SurveyID&quot; &quot;JobSatisfaction&quot; &quot;TurnoverIntentions&quot; &quot;OrgCommitment&quot; &quot;JobInvolvement&quot; # Print data frame (tibble) object print(datam) ## # A tibble: 20 × 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 ## 13 13 22.3 57.2 70.2 30.6 ## 14 14 23.3 52.9 64.9 30.5 ## 15 15 25.9 51 62.2 30.5 ## 16 16 29.5 51 67.3 30.5 ## 17 17 32.8 51 40.6 30.5 ## 18 18 35.4 51.4 74.7 30.5 ## 19 19 40.3 51.4 71.8 50.1 ## 20 20 56.7 56 84.3 69.5 Note in the data frame that the SurveyID variable (i.e., column, field) is a unique identifier variable, which means that each case (i.e., observation) has a unique value on this variable. Each row represents a unique employee’s composite score on four measures (i.e., JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), where higher scores indicate higher levels of the concept (i.e., construct) being measured. # Determine number of rows in data frame nrow(datam) ## [1] 20 Note how the data frame currently has 20 rows (cases) of data, as shown by the output to the nrow function from base R. 20.2.4 Wide-to-Long Format Data Manipulation The data frame we read in called datam is in wide format, as each substantive variable has its own column. To restructure the data from wide to long format, we will use the pivot_longer function from the tidyr package (Wickham, Vaughan, and Girlich 2023). Along with readr and dplyr (as well as other useful packages), the tidyr package is part of the tidyverse of packages. Let’s begin by installing and accessing the tidyr package so that we can use the pivot_longer function. # Install tidyr package if you haven&#39;t already install.packages(&quot;tidyr&quot;) # Access tidyr package library(tidyr) If you received an error when attempting to access the tidyr package using the library function, you may need to install the following packages using the install.packages function: rlang and glue. Alternatively, you may try installing the entire tidyverse package. Now that we’ve accessed the tidyr package, I will demonstrate two techniques for applying the pivot_longer function. The first technique uses the pipe operator (%&gt;%). The pipe operator comes from a package called magrittr (Bache and Wickham 2022), on which the tidyr package is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemund’s (2017) chapter on pipes. The second technique for applying the pivot_longer function takes a more traditional approach in that it involves nested functions being nested parenthetically. If you don’t want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 20.2.4.1 With Pipe Using the pipe (%&gt;%) operator technique, let’s apply the pivot_longer function to manipulate the datam data frame object from wide format to long format. We can specify the wide-to-long manipulation as follows. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object datam_long. Use the &lt;- operator to assign the new long-form data frame object to the object named datam_long in the step above. Type the name of the original data frame object (datam), followed by the pipe (%&gt;%) operator. Type the name of the pivot_longer function. As the first argument in the pivot_longer function, type cols= followed by the c (combine) function. As the arguments within the c function, list the names of the variables that you wish to pivot from separate variables (wide) to levels or categories of a new variable, effectively stacking them vertically. In this example, let’s list the names of the four survey measures: JobSatisfaction, TurnoverIntentions, OrgCommitment, and JobInvolvement. As the second argument in the pivot_longer function, type names_to= followed by what you would like to name the new stacked variable (see previous) created from the four survey measure variables. Let’s call the new variable containing the names of the measures the following: \"Measure\". As the third argument in the pivot_longer function, type values_to= followed by what you would like to name the new variable that contains the scores for the four survey variables that are now stacked vertically for each case. Let’s call the new variable containing the scores on the four measures the following: \"Score\". # Apply pivot_longer function to restructure data in long format (using pipe) datam_long &lt;- datam %&gt;% pivot_longer(cols=c(JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), names_to=&quot;Measure&quot;, values_to=&quot;Score&quot;) # Print first 12 rows of new data frame head(datam_long, n=12) ## # A tibble: 12 × 3 ## SurveyID Measure Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 JobSatisfaction 55.4 ## 2 1 TurnoverIntentions 97.2 ## 3 1 OrgCommitment 32.3 ## 4 1 JobInvolvement 50.5 ## 5 2 JobSatisfaction 51.5 ## 6 2 TurnoverIntentions 96 ## 7 2 OrgCommitment 53.4 ## 8 2 JobInvolvement 50.3 ## 9 3 JobSatisfaction 46.2 ## 10 3 TurnoverIntentions 94.5 ## 11 3 OrgCommitment 63.9 ## 12 3 JobInvolvement 50.2 As you can see, in the output, each respondent now has four rows of data – one row for each measure and the associated score. The giveaway is that each respondent’s unique SurveyID value is repeated four times. # Print variable names names(datam_long) ## [1] &quot;SurveyID&quot; &quot;Measure&quot; &quot;Score&quot; Note how there are now just three variables: SurveyID, Measure, and Score. Further, by applying the nrow function to the new data frame (as shown below), we can see that the new data frame is now much longer in terms of the number of rows; specifically, the original datam data frame in wide format has 20 rows, and now the new datam_long data frame in long format has 80 rows. # Print number of rows in data frame nrow(datam_long) ## [1] 80 Now apply the View function from base R to view and scroll through the whole data frame. Note that the unique identifier variable (i.e., SurveyID) repeats multiple times to indicate that the same survey respondent has scores in multiple rows. # View entire data frame View(datam_long) 20.2.4.2 Without Pipe We can also apply the pivot_longer function without using the pipe (%&gt;%) operator. To do so, use the same code as above, except drop the pipe (%&gt;%) operator and type data= followed by the name of the original data frame (datam) as the first argument within the pivot_longer function parentheses. # Apply pivot_longer function to restructure data in long format (without using pipe) datam_long &lt;- pivot_longer(data=datam, cols=c(JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), names_to=&quot;Measure&quot;, values_to=&quot;Score&quot;) # Print first 12 rows of new data frame head(datam_long, n=12) ## # A tibble: 12 × 3 ## SurveyID Measure Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 JobSatisfaction 55.4 ## 2 1 TurnoverIntentions 97.2 ## 3 1 OrgCommitment 32.3 ## 4 1 JobInvolvement 50.5 ## 5 2 JobSatisfaction 51.5 ## 6 2 TurnoverIntentions 96 ## 7 2 OrgCommitment 53.4 ## 8 2 JobInvolvement 50.3 ## 9 3 JobSatisfaction 46.2 ## 10 3 TurnoverIntentions 94.5 ## 11 3 OrgCommitment 63.9 ## 12 3 JobInvolvement 50.2 Regardless of whether we use the pipe (%&gt;%) operator, we end up with the same wide-to-long format manipulation. 20.2.5 Long-to-Wide Format Data Manipulation Using the long-format data frame we just created called datam_long, let’s manipulate it back to wide format using the pivot_wider function from the tidyr package. As we did above, we’ll do the long-to-wide data manipulation with and without the pipe (%&gt;%) operator. 20.2.5.1 With Pipe Using the pipe (%&gt;%) operator technique, let’s apply the pivot_wider function to manipulate the datam_long data frame object from long format back to wide format. We can specify the long-to-wide manipulation as follows. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object datam_wide. Use the &lt;- operator to assign the new wide-form data frame object to the object named datam_wide in the step above. Type the name of the long-format data frame object (datam_long), followed by the pipe (%&gt;%) operator. Type the name of the pivot_wider function. As the first argument in the pivot_wider function, type names_from= followed by the name of the variable that contains the names of the different survey measures: Measure. The levels or categories of this Measure variable will become the names of separate columns (i.e., variables) when the long-format data frame object is converted to a wide-format data frame object. As the second argument in the pivot_wider function, type values_from= followed by the name of the variable that contains the values (i.e., scores) for each of the survey measures: Score. The values from the Score variable will provide the data for the new survey-measure variables when the data frame object is in wide format. # Apply pivot_wider function to restructure data in wide format (using pipe) datam_wide &lt;- datam_long %&gt;% pivot_wider(names_from=Measure, values_from=Score) # Print first 12 rows of new data frame head(datam_wide, n=12) ## # A tibble: 12 × 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 Now let’s print the variable names in the new datam_wide data frame object. # Print variable names names(datam_wide) ## [1] &quot;SurveyID&quot; &quot;JobSatisfaction&quot; &quot;TurnoverIntentions&quot; &quot;OrgCommitment&quot; &quot;JobInvolvement&quot; Note how we are now back to the same variables from our original wide-format data frame called datam: SurveyID, JobInvolvement, JobSatisfaction, OrgCommitment, and TurnoverIntentions. Further, by applying the nrow function to the new data frame (as shown below), we can see that the new wide-format data frame is now back to 20 rows. We have come full circle, as the pivot_wider function complements the pivot_longer function. # Print number of rows in data frame nrow(datam_wide) ## [1] 20 20.2.5.2 Without Pipe We can also apply the pivot_wider function without the pipe (%&gt;%) operator. Simply use the same code as above, except drop the pipe (%&gt;%) operator and type data= followed by the name of the long-format data frame object (datam_long) as the first argument within the pivot_wider function parentheses. # Apply pivot_wider function to restructure data in wide format (without using pipe) datam_wide &lt;- pivot_wider(data=datam_long, names_from=Measure, values_from=Score) # Print first 12 rows of new data frame head(datam_wide, n=12) ## # A tibble: 12 × 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 Regardless of whether we use the pipe (%&gt;%) operator, we end up with the same long-to-wide format manipulation. 20.2.6 Summary In this chapter, we learned how to manipulate a data frame from wide format to long format using the pivot_longer, and how to manipulate a data frame from long format to wide format using the pivot_wider. Both functions are from the tidyr package. Manipulating and restructuring data into different formats or structures is often an essential data-management step prior to running certain analyses or generating certain data visualizations. References "],["center.html", "Chapter 21 Centering &amp; Standardizing Variables 21.1 Conceptual Overview 21.2 Tutorial", " Chapter 21 Centering &amp; Standardizing Variables In this chapter, we will learn how to center and standardize variables. 21.1 Conceptual Overview Centering or standardizing variables can be a useful data preparation step. For example, we often center predictor variables prior to specifying a product term (i.e., interaction term) when estimating moderation effects in a multiple linear regression model. 21.1.1 Review of Centering Variables Centering is the process of subtracting the variable mean (average) from each of the values of that same variable; in other words, it’s a linear rescaling of a variable. Centering variables is sometimes completed prior to including those variables as predictors in a regression model, and it is generally done for one or both of the following purposes: (a) to make the intercept valuable more interpretable, and (b) to reduce collinearity between two or more predictor variables that are subsequently multiplied to create an interaction term (product term) when estimating a moderated multiple linear regression model or polynomial regression model, for example. Regarding the first purpose, centering to enhance the interpretability of the intercept (constant) value in a regression model is relevant to the extent that we wish to interpret the intercept value. In an ordinary least squares (OLS) regression model, the intercept value represents the mean of the outcome variable when all predictor variables are set to zero. If the scaling of any of the predictor variables in our model does not include zero (e.g., predictor variables are on a 1-10 scale), then considering the intercept value when the predictors are zero, doesn’t make much sense. Regarding the second purpose, it is important to center predictor variables prior to using them to create a multiplicative term (i.e., interaction variable), such as an interaction term (i.e., product term) or a polynomial term (e.g., quadratic term, cubic term). In the context of a moderated multiple linear regression model, centering does not affect the significance of the interaction term, but a lack of centering will affect the interpretation of the main effects. Thus far, I have been mostly referring to what is referred to as grand-mean centering. To grand-mean center a variable, we simply subtract the overall (grand) mean of the entire sample for that variable from each value of that variable, thereby creating a new variable in which the mean is zero and the standard deviation is the same as it was before centering. For example, let’s assume that we have a variable called Age for a sample of individuals, where Age is measured in years. In it’s raw format, the mean of Age is 36.2 years with a standard deviation of 8.1. If we grand-mean center Age, then for each individual in our sample, we create a new variable in which take their Age and subtract the mean Age of 36.2. If an individual has an Age of 40.0, then their centered Age would be 3.8 (40.0 - 36.2 = 3.8). By centering each individual’s Age relative to the grand-mean, we end up with a variable that has a mean of 0.0, but with a standard deviation that is equal to 8.1, which is equal to the standard deviation of the original Age variable. Why is this the case? Well, we have just performed a linear shift of Age, which affects only the mean and not the standard deviation. In the context of multilevel models, group-mean centering becomes relevant and important. In short, group-mean centering refers to the process of subtracting the respective group mean for a particular variable (based on another variable that acts as a grouping or clustering variable) for each case’s score on that same variable. For example, if Employee A belongs to Work Team A, and Work Team A consists of 10 other employees, we would first calculate the mean of Work Team A employees’ scores on a continuous variable of interest, and second, we would subtract that group mean from each Work Team A employee’s score on that continuous variable. We would then repeat this process for all employees relative to their respective work teams. In the context of multilevel modeling, both grand-mean centering and group-mean centering can have pronounced on the estimated coefficients and the interpretation of those coefficients. For instance, group-mean centering can be used in multilevel models to separate out the within-group effects and the between-groups effects, if that is of interest. A full discussion of grand-mean centering and group-mean centering in the context of multilevel modeling is beyond the scope of this tutorial; for a more complete overview, please check out Professor Jason Newsom’s handout: http://web.pdx.edu/~newsomj/mlrclass/ho_centering.pdf. 21.1.2 Review of Standardizing Variables Like centering variables, when standardizing (or scaling) variables, we center the variables around a mean of zero. However, when standardizing a variable, we are actually converting the variable to a z-score, which means we set the mean to 0 and the variance to 1; because the standard deviation is just the square root of the variance, then the standard deviation is also set to 1. So how do you interpret a variable that is standardized? Let’s assume that we standardized a variable called Age for a sample of individuals, where Age is measured in years. In it’s raw format, the mean of Age is 36.2 years with a standard deviation of 8.1, which would mean, for example, that a person who is 44.3 years old has an Age that is exactly 1 standard deviation higher than the mean (44.3 - 8.1 = 36.2). If we standardize the Age variable, then the mean becomes 0.0 and the standard deviation becomes 1.0. Accordingly, the standardized score for the person who has an Age of 44.3 years (which was 1 standard deviation above the mean) would become 1.0. If a person has an Age of 20.0, then that means they have a standardized score of -2.0, which represents 2.0 standard deviations below the mean (20.0 - 36.2 = -16.2 and -16.2 / 8.1 = -2.0). 21.2 Tutorial This chapter’s tutorial demonstrates how to center and standardize variables in R. 21.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Please note that the video shows how to grand-mean center and standardize variables – but not how to group-mean center variables. If your goal is to group-mean center variables, then check out the corresponding section below. Link to video tutorial: https://youtu.be/2_TxnvZGtV0 21.2.2 Functions &amp; Packages Introduced Function Package mean base R scale base R mutate dplyr 21.2.3 Initial Steps If you haven’t already, save the file called “DiffPred.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “DiffPred.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DiffPred.csv&quot;) ## Rows: 377 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): emp_id, gender, race ## dbl (3): perf_eval, interview, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;age&quot; &quot;gender&quot; &quot;race&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [377 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:377] &quot;MA322&quot; &quot;MA323&quot; &quot;MA324&quot; &quot;MA325&quot; ... ## $ perf_eval: num [1:377] 4.2 4.9 4.2 5.3 4.2 6.9 3.4 5.8 4.4 5.6 ... ## $ interview: num [1:377] 7.5 9.3 7.5 8 9.3 6.8 6.7 7 8.2 6.4 ... ## $ age : num [1:377] 29.7 31.7 29.4 37.9 30.9 46.2 43.9 47.8 31.7 44.6 ... ## $ gender : chr [1:377] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; ... ## $ race : chr [1:377] &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. age = col_double(), ## .. gender = col_character(), ## .. race = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 6 ## emp_id perf_eval interview age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 MA322 4.2 7.5 29.7 woman asian ## 2 MA323 4.9 9.3 31.7 man asian ## 3 MA324 4.2 7.5 29.4 woman asian ## 4 MA325 5.3 8 37.9 woman asian ## 5 MA326 4.2 9.3 30.9 man black ## 6 MA327 6.9 6.8 46.2 woman asian There are 6 variables and 377 cases (i.e., employees) in the DiffPred data frame: emp_id, perf_eval, interview, age, gender, and race. Per the output of the str (structure) function above, the variables perf_eval, interview, and age are of type numeric (continuous: interval/ratio), and the variables emp_id, gender, and race are of type character (nominal/categorical). The variable emp_id is the unique employee identifier. Imagine that these data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered a rated structured interview (interview) 90 days after entering the organization. The structured interview (interview) variable was designed to assess individuals’ interpersonal skills, and ratings can range from 1 (very weak interpersonal skills) to 10 (very strong interpersonal skills). The interviews were scored by untrained raters who were often the hiring managers but not always. The perf_eval variable is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, with possible ratings ranging from 1-7, with 7 indicating high performance. The age variable represents the job incumbents’ ages (in years). The gender variable represents the job incumbents’ tender identify and is defined by two levels/categories/values: man and woman. Finally, the race variable represents the job incumbents’ race/ethnicity and is defined by three levels/categories/values: asian, black, and white. 21.2.4 Grand-Mean Center Variables We only center variables that are of type numeric and that we conceptualize as having a continuous (interval/ratio) measurement scale. Further, if we’re centering variables prior to inclusion in a regression model, we often only center those variables that we plan on using as predictor variables (and not outcome variables). Thus, in our current data frame, we will grand-mean center just the interview and age variables; for more information on which variables to center, check out the chapter on estimating moderation effects in a multiple linear regression model. I will demonstrate three approaches, and you can try all three or just one, as any of the three will work. 21.2.4.1 Option 1: Basic Arithmetic and mean Function from Base R We’ll start with what is arguably the most intuitive approach for grand-mean centering. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variable’s names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each case’s value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction symbol (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. # Grand-mean centering: Using basic arithmetic and the mean function from base R df$c_interview &lt;- df$interview - mean(df$interview, na.rm=TRUE) To admire your work, take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 7 ## emp_id perf_eval interview age gender race c_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 21.2.4.2 Option 2: scale Function from Base R An alternative approach to grand-mean centering is to use the scale function from base R. For some, this function might be preferable to the approach described above, but again, it’s really a matter of preference. As an initial step, start by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. As I mentioned above, I typically like to name the centered variable by simply adding the c_ prefix to the existing variable’s names - for example: c_interview. Now, enter the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, begin by typing the name of the scale function. As the first argument, type the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). As the second argument, type center=TRUE which instructs the function to grand-mean center the values. As the third argument, type scale=FALSE to inform the function that you do not which to scale or standardize the variable you are centering. # Grand-mean centering: Using scale function from base R df$c_interview &lt;- scale(df$interview, center=TRUE, scale=FALSE) Take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 7 ## emp_id perf_eval interview age gender race c_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 21.2.4.3 Option 3: mutate Function from dplyr and mean Function from Base R This third approach to grand-mean centering variables can come in handy when we want to grand-mean center multiple variables in a single step. With that said, I will begin by showing how to grand-mean center a single variable using this approach, and then we will extend the code/script to involve two variables. We will be using the mutate function from the dplyr package (Wickham et al. 2023), so if you haven’t already, be sure to install and access the dplyr package using the functions below. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) In this example, I use the pipe (%&gt;%) operator from the dplyr package (and by extension, the magrittr package). For more information on using pipes with the mutate function, check out the chapter on cleaning data, and for more information on using pipes in general, check out this free eBook by one of the creator’s of the dplyr package and RStudio. To begin, type the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- assignment operator. Type the name of the original data frame to which the variable we wish to grand-mean center belongs (df), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. As the first and only argument, begin by typing what you would like to name the new grand-mean centered variable (c_interview). After that, type the = operator to assign values to this new variable. Finally, we will specify a formula to inform the function how the new values will be calculated. Specifically, we type the name of the original variable (interview), type the subtraction (-) operator, and finally type the name of the mean function from base R. As the first argument in the mean function, type the name of the variable (interview) for which you would like to calculate the mean, and as the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when computing this grand mean for the sample. # Grand-mean centering: Using mutate function from dplyr and mean function from base R df &lt;- df %&gt;% mutate(c_interview = interview - mean(interview, na.rm=TRUE)) Take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 7 ## emp_id perf_eval interview age gender race c_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 One of the advantages of using this approach is that we can center multiple variables in a single step. To do so, we simply specify which additional variable we would like to grand-mean center by adding an additional argument to the mutate function. # Grand-mean centering: Multiple variables df &lt;- df %&gt;% mutate(c_interview = interview - mean(interview, na.rm=TRUE), c_age = age - mean(age, na.rm=TRUE)) 21.2.5 Group-Mean Center Variables When estimating multilevel models, there are certain contexts in which group-mean centering should be applied. For more information on centering in general and group-mean centering specifically, please check out this handout created by my colleague Jason Newsom. Like we did above with Option 3 for grand-mean centering, for group-mean centering we will also use the mutate function from dplyr; however, we will also go a step further by applying the group_by function from dplyr to group the data by a values of a categorical (nominal, ordinal) grouping variable. For more information grouping data, check out the chapter on aggregation and segmentation. Let’s group-mean center interview scores by race variable categories. To begin, type the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- assignment operator. Type the name of the original data frame to which the variable we wish to grand-mean center belongs (df), followed by the pipe (%&gt;%) operator. Type the name of the group_by function, and as the function’s argument(s), specify the name(s) of the grouping variable(s); in this example, we will group by the race variable. Follow this function with the the pipe (%&gt;%) operator. Type the name of the mutate function. As the first and only argument, begin by typing what you would like to name the new group-mean centered variable (gpmc_interview). After that, type the = operator to assign values to this new variable. Finally, we will specify a formula to inform the function how the new values will be calculated. Specifically, we type the name of the original variable (interview), type the subtraction (-) operator, and finally type the name of the mean function from base R. As the first argument in the mean function, type the name of the variable (interview) for which you would like to calculate the mean, and as the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when computing this grand mean for the sample. Follow the mutate function with the pipe (%&gt;%) operator. Finally, type the name of the ungroup function, and don’t specify any arguments within the function’s parentheses. This last step makes sure that we ungroup the grouping that we initially applied to the data frame. # Group-mean centering by race variable df &lt;- df %&gt;% group_by(race) %&gt;% mutate(gpmc_interview = interview - mean(interview, na.rm=TRUE)) %&gt;% ungroup() Take a look at the first six rows of your data frame object to inspect the new group-mean centered variable called gpmc_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 9 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 21.2.6 Standardize Variables There are different approaches we can use to standardize a variable. I provide two options below. You can try both options or one. Both will get you to the same end. I suggest picking the one that is most intuitive for you. 21.2.6.1 Option 1: scale Function from Base R As our first approach to standardizing (or scaling) a variable, we will use the scale function from base R. In fact, we can even apply this function within the lm (linear regression) function from base R to get standardized regression coefficients; for more information on standardized regression coefficients, check out the chapters on predicting criterion scores using simple linear regression and estimating incremental validity using multiple linear regression. Using the scale function on its own is fairly straightforward when the goal is to standardize. Start by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the standardized variable by simply adding the st_ prefix to the existing variable’s names (e.g., st_interview). Now, enter the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (st_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To assign values to this new st_interview variable, begin by typing the name of the scale function. As the first and only argument, type the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). # Standardizing: Using scale function from base R df$st_interview &lt;- scale(df$interview) Take a look at your data frame object to inspect the new standardized variable called st_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 10 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview st_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 0.203 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 1.70 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 0.203 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 0.618 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 1.70 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 -0.378 21.2.6.2 Option 2: mutate Function from dplyr and scale Function from Base R This alternative approach to standardizing variables can come in handy when we want to standardize multiple variables in a single step. With that said, I will begin by showing how to standardize a single variable using this approach, and then we will try two variables. We will use the mutate function from the dplyr package, so if you haven’t already, be sure to install and access the dplyr package using the functions below. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) As the first step, enter the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- operator. Second, type the name of the original data frame to which the variable we wish to standardize belongs (df). Third, type the pipe (%&gt;%) operator. Fourth, type the name of the mutate function. As the first and only argument, begin by typing what you would like to call the new standardized variable, which in this case we will call the variable st_interview. After that, type the name of the scale function from base R. As the first and only argument in the scale function, type the name of the variable (interview) that you would like to standardize. # Standardizing: Using mutate function from dplyr and scale function from base R df &lt;- df %&gt;% mutate(st_interview = scale(interview)) Take a look at your data frame object to inspect the new standardized variable called st_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 × 10 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview st_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 0.203 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 1.70 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 0.203 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 0.618 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 1.70 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 -0.378 One of the advantages of using this approach is that we can standardize multiple variables in a single step. To do so, we simply specify which additional variable we would like to standardize by adding an additional argument to the mutate function. # Standardizing: Multiple variables df &lt;- df %&gt;% mutate(st_interview = scale(interview), st_age = scale(age)) 21.2.7 Summary In this chapter, we learned how to grand-mean center and standardize variables. For grand-mean centering variables, we used basic arithmetic and the mean function from base R, the scale function from base R, and a combination of the mutate function from dplyr and the mean function from base R. For standardizing variables, we used the scale function from base R and a combination of the mutate function from dplyr and the scale function from base R. References "],["removeobjects.html", "Chapter 22 Removing Objects from the R Environment 22.1 Conceptual Overview 22.2 Tutorial", " Chapter 22 Removing Objects from the R Environment In this chapter, we will learn how to remove objects that we’ve created from the R Environment. 22.1 Conceptual Overview When working in R, we often assign vectors, matrices, tables, data frames, models, and values to objects so that we can reference them in subsequent operations. Such objects end up in our R (Global) Environment. If you’re working in RStudio, you will see your objects in the Environment window, which is typically positioned in the upper right corner of the user interface. At times, we may wish to remove certain objects from our R Environment, which will be the focus of this chapter’s tutorial. 22.2 Tutorial This chapter’s tutorial demonstrates how to remove objects from the R Environment. 22.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/mUjSELUW-Ys 22.2.2 Functions &amp; Packages Introduced Function Package ls base R remove base R rm base R c base R 22.2.3 Initial Steps If you haven’t already, save the file called “DataCleaningExample.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “DataCleaningExample.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) ## Rows: 10 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmpID, Facility, OnboardingCompleted ## dbl (2): JobLevel, Org_Tenure_Yrs ## date (1): StartDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;JobLevel&quot; &quot;StartDate&quot; &quot;Org_Tenure_Yrs&quot; &quot;OnboardingCompleted&quot; # Print data frame (tibble) object print(df) ## # A tibble: 10 × 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No To demonstrate how to remove objects from the R Environment, we first need to add some more objects to our R Environment. Using the xtabs (cross-tabulation) function from base R, let’s create a two-way table called table1 from the JobLevel and Org_Tenure_Yrs variables from the df1 data frame object we just created and named. # Create table from JobLevel and Org_Tenure_Yrs variables from df data frame table1 &lt;- xtabs(~ JobLevel + Org_Tenure_Yrs, data=df) # Print table object print(table1) ## Org_Tenure_Yrs ## JobLevel 0.3 1.9 6.5 7.6 7.7 8.6 8.8 10.9 ## -9999 0 1 0 0 0 0 0 0 ## 1 0 0 0 1 0 1 1 1 ## 2 1 0 0 0 0 0 0 0 ## 3 0 0 0 0 1 0 0 0 ## 4 0 0 0 0 0 1 0 0 ## 5 1 0 0 0 0 0 0 0 ## 11 0 0 1 0 0 0 0 0 You should now see an object called table1 in your R Environment. Next, let’s assign an arbitrary value to an object. Specifically, let’s assign the value 3 to an object called a. # Assign the value 3 to the object a a &lt;- 3 # Print table object print(a) ## [1] 3 Finally, for good measure, let’s add one more object called B to our R Environment. # Assign the character value &quot;example&#39; to the object B B &lt;- &quot;example&quot; # Print table object print(B) ## [1] &quot;example&quot; At the very least, we should now have the following objects in our R Environment: df, table1, a, and B. 22.2.4 List Objects in R Environment In addition to viewing objects in our R (Global) Environment in the Environment window in RStudio, we can also use the ls function from base R to retrieve the names of objects that are currently in our Environment. The function is simple to use. Simply, type the name of the function (ls) without any parenthetical arguments. # Print names of objects in R Environment ls() ## [1] &quot;a&quot; &quot;B&quot; &quot;df&quot; &quot;table1&quot; In your R console, a list of the objects in your global environment should have printed to your Console. Next, we will learn how to remove some of these objects. 22.2.5 Remove Objects from R Environment To remove specific objects from our R Environment, we can use the remove function from base R, which can be abbreviated as rm. To remove a specific object, just type the name of the function (remove), and as the sole parenthetical argument, type the name of the object you wish to remove. In this example, let’s remove the data frame object called df. By default, this function first searches the R Environment that is currently active. # Remove df object from R Environment remove(df) Using the ls function, print the names of the objects that are currently in your R Environment. The df object should no longer be there. # Print names of objects in R Environment ls() ## [1] &quot;a&quot; &quot;B&quot; &quot;table1&quot; If our goal is to remove multiple objects from the R Environment, we could apply the remove function multiple times – one for each object we wish to remove. Alternatively, we could also use the list= argument from the remove function to specify a vector of objects we wish to remove. Using the c function from base R, which combines objects into a vector, we can specify a list of the objects we wish to remove. Let’s remove the objects called table1 and a from the R Environment. Make sure to put the variable names in quotation marks (\" \") when you list them as arguments within the c function. # Remove multiple objects from R Environment remove(list=c(&quot;table1&quot;,&quot;a&quot;)) Using the ls function, print the names of the objects that are currently in your R Environment. The table1 and a objects should no longer appear in the R Environment. # Print names of objects in R Environment ls() ## [1] &quot;B&quot; Finally, if you wish to remove all objects from the environment, use the remove function, and within the function parentheses, set the list argument equal to ls(). # Remove all objects from R Environment remove(list=ls()) If you use the ls function once more without arguments, you will receive the following output, which indicates that there are no objects in the environment: character(0). # Print names of objects in R Environment ls() ## character(0) 22.2.6 Summary In this chapter, we practiced applying the ls and remove functions from base R to print the names of objects in the R Environment and to remove objects from the R Environment, respectively. References "],["employeedemographics.html", "Chapter 23 Introduction to Employee Demographics 23.1 Chapters Included", " Chapter 23 Introduction to Employee Demographics Employee demographics include a broad array of variables that can be used to describe an organization’s work force. Examples include sex, gender identity, race, ethnicity, age, national origin, age, veteran status, and disability status, for example. In addition, we can describe who employees report to (e.g., direct supervisor), the facility where they work, or their unit (e.g., department, division). Although they can be used in more advanced analyses, employee-demographic variables are also amenable to descriptive statistics. By applying descriptive statistics like counts, measures of central tendency, and measures of dispersion to employee demographic variables, we can summarize the workers in an organization – or even across organizations. Doing so can elucidate an organization’s level of demographic diversity, and kept help an organization understand the extent to which its workers reflect the broader population of available workers in the labor pool. 23.1 Chapters Included This part of the book marks a transition into specific contexts within HR and HR analytics. In the chapters within this part, you will have an opportunity to learn how to work with employee-demographic data in the following ways. Describing Employee Demographics Using Descriptive Statistics Summarizing Two or More Categorical Variables Using Cross-Tabulations Applying Pivot Tables to Explore Employee Demographic Data "],["descriptives.html", "Chapter 24 Describing Employee Demographics Using Descriptive Statistics 24.1 Conceptual Overview 24.2 Tutorial 24.3 Chapter Supplement", " Chapter 24 Describing Employee Demographics Using Descriptive Statistics In this chapter, we will learn about how descriptive statistics can be used to describe employee-demographic variables. To determine which type of descriptive statistics is appropriate for a given variable, we will learn about measurement scales and how to distinguish from a construct and a measure. Finally, we’ll conclude with a tutorial. 24.1 Conceptual Overview In this section, we’ll review the four different types of measurement scales (i.e., nominal, ordinal, interval, ratio), the distinctions between constructs, measures, and measurement scales, and different types of descriptive statistics (e.g., counts, measures of central tendency, measures of dispersion). 24.1.1 Review of Measurement Scales When determining what type of descriptive statistics is appropriate for summarizing data contained within a particular variable, it is important to determine the measurement scale of the variable. Measurement scale (i.e., scale of measurement, level of measurement) refers to the type of information contained within a vector of data (e.g., variable), and the four measurement scales are: nominal, ordinal, interval, and ratio. 24.1.1.1 Nominal Variables with a nominal measurement scale have different category labels, which are sometimes referred to as levels. The category labels, however, do not have any inherent numeric properties. As an example, let’s operationalize gender identity as having a nominal measurement scale, such that gender identity includes the following category labels: agender, man, nonbinary, trans man, trans woman, and woman. These category labels do not have any inherent numeric values, and although we could assign numeric values to the gender identity category labels (e.g., agender = 1, man = 2, nonbinary = 3, etc.), doing so wouldn’t imply that one category label has a higher value than another. Variables with a nominal measurement scale are sometimes referred to as categorical variables. The Facility and Gender variables (i.e., columns) contain examples of nominal measurement scales, as each variable has category labels that lack any inherent numeric values and cannot be ordered in a meaningful way. 24.1.1.2 Ordinal Like variables with a nominal measurement scale, variables with an ordinal measurement scale are a specific type of categorical variable; however, unlike nominal variables, the category labels (i.e., levels) associated with ordinal variables can be ordered or ranked in a meaningful way. It should be noted that the gaps – or intervals – between categorical labels of an ordinal variable are unknown; in other words, we can’t quantify the exact difference between adjacent category labels (i.e., levels). For example, let’s operationalize employee education levels with the following ordered category labels: high school diploma, some college, and college degree. That is, completing some portion of a college degree is a higher level of education than earning a high school diploma, and completing a college degree is a higher level of education than completing some portion of a college degree. We don’t know, though, the size of the interval between earning a high school diploma and completing some college, and between some completing some college and earning a college degree; thus, as operationalized in this example, employee education level demonstrates an ordinal measurement scale (as opposed to an interval measurement scale, which is described in the following section). A controversial example of an ordinal measurement scale is any type of Likert (or Likert-type) scale or response format. Examples of Likert scales include agreement response formats (e.g., Strongly Disagree, Disagree, Neither Disagree Nor Agree, Agree, Strongly Agree) and frequency response formats (e.g., Never, Rarely, Sometimes, Always). Likert scales are commonly used in employee surveys; for example, survey respondents might be asked to indicate their level of agreement with the following survey item that is designed to assess job satisfaction: “In general, I am satisfied with my job.” Just like any variable with an ordinal measurement scale, we don’t know the exact quantitative intervals between adjacent category labels (i.e., response options) on a Likert scale. Nonetheless, in the social sciences, it is relatively common for analysts to apply numerical values to the ordered category labels on a Likert scale (e.g., 1 = Strongly Disagree, 2 = Disagree, 3 = Neither Disagree Nor Agree, 4 = Agree, 5 = Strongly Agree). After adding these numerical values, the analysts often treat Likert scales as though they were interval measurement scales for the purposes of data analysis, particularly when composite variables (i.e., overall scale score variables) are created by summing or averaging respondents’ scores across multiple survey items. The Education and Performance variables (i.e., columns) contain examples of ordinal measurement scales, as each variable has category labels can be ordered in a meaningful way but where the exact quantitative intervals between category labels are unknown or undefined. 24.1.1.3 Interval Variables with an interval measurement scale have a numeric scale (e.g., have inherent numeric values), and not only is there an order to the numeric values, equally sized intervals between values have the same meaning or interpretation – hence, the term interval measurement scale. With all that being said, interval variables lack a true or meaningful zero value; in other words, a value of zero is an arbitrary point on the scale – if it even appears in the possible range of values in the first place. Variables with an interval measurement scale are sometimes referred to as continuous variables. As an example, suppose we purchase a cognitive ability (i.e., intelligence) test that we plan to administer to job applicants. Let’s now imagine that this test operationalizes cognitive ability, such that scores can range from 0 to 200, where 100 indicates the average level of cognitive ability in the population. Further, the test is designed such that every 1-point interval holds the same interpretation and is of equal quantitative size when compared to other 1-point intervals on the scale. For instance, let’s imagine that the 1-point interval between 78 and 79 has the same meaning (and quantitative size) as the 1-point interval between 110 and 111. In other words, equally sized intervals between values have the same meaning or interpretation in terms of incremental differences in cognitive ability. Even though this cognitive ability test can produce a score of zero, the zero value is not meaningful, as it does not imply the absence of cognitive ability; rather, it just indicates the lowest point on the numeric scale used to assess cognitive ability happens to be zero, making the zero point on the scale somewhat arbitrary. The Cognitive Ability and BARS (Behaviorally Anchored Rating Scale) variables (i.e., columns) contain examples of interval measurement scales, as each variable has a numeric scale in which equally sized intervals between values have the same meaning or interpretation; however, both variables lack a meaningful or true zero. 24.1.1.4 Ratio Like variables with an interval measurement scale, variables with a ratio measurement scale are a specific type of continuous variable, as they have a numeric scale in which equally sized intervals between values have the same meaning or interpretation. Unlike interval variables, however, ratio variables have a true and meaningful zero value, such that zero indicates the absence of the construct being measured. Common examples of variables with a ratio measurement scale include those that measure (elapsed) time, where time is measured in standardized units like seconds, minutes, hours, days, months, years, decades, or centuries. Equally sized intervals between various time points have the same meaning, and a time of zero implies the absence of time having elapsed. In organizational settings, we often measure employee age and tenure as numeric elapsed time since a prior date. Because there is a true zero associated with ratio measurement scales, we can make statements like “this individual is twice as old as that individual” or “this individual has worked here one third as long as that individual.” Finally, I should note even if we do not observe a true-zero value in our acquired data, a variable can still have a ratio measurement scale. What matters is whether the scale used to measure the construct in question has a possible true-zero value. Using the example of employee age, we can safely assume that we won’t observe any employees who have an age of exactly zero years; however, because age is measured as a standardized unit of time (i.e., years), we know that when measuring time in this way a value of zero years does exist on this scale – and it it would indicate the absence of time having passed. That is, a value of zero could hypothetically indicate the lack of time having passed since the exact moment of a person’s birth. In sum, even if we don’t observe a zero score in our data, a variable can still be classified as having a ratio measurement scale, so long as the scale used to measure the underlying construct could theoretically include a true or meaningful zero value. The Age and Monthly Pay variables (i.e., columns) contain examples of ratio measurement scales, as each variable has a numeric scale in which equally sized intervals between values have the same meaning or interpretation; in addition, both variables have a meaningful or true zero, where zero implies the absence of whatever is being measured. 24.1.2 Constructs, Measures, &amp; Measurement Scales Importantly, we use measures to assess constructs (i.e., concepts), and often there are different ways in which we can measure or operationalize the same construct. Consequently, different measures might have a different measurement scale, even though they are each designed to assess the same construct. For example, if wish to assess the construct of job performance for sales professionals, we could have supervisors rate employee performance using a three-point scale, ranging from “Does Not Meet Expectations” to “Meets Expectations” to “Exceeds Expectations,” which could be described as an ordinal measurement scale. Alternatively, we might also assess the construct of job performance for sales professionals based on how much revenue they generate (in US dollars), which could be described as a ratio measurement scale. 24.1.3 Types of Descriptive Statistics Link to conceptual video: https://youtu.be/WCC4IXavits Once we have determined the measurement scale of a variable, we’re ready to choose an appropriate type of descriptive statistics to summarize the data associated with that variable. Descriptive statistics are used to describe the characteristics of a sample drawn from a population; often, when dealing with data about human beings in organizations, it’s not feasible to attain data for the entire population, so instead we settle for what is hopefully a representative sample of individuals from the focal population. Common types of descriptive statistics include counts (i.e., frequencies), measures of central tendency (e.g., mean, median, mode), and measures of dispersion (e.g., variance, standard deviation, interquartile range). Note that descriptive statistics are not tests of statistical significance; for tests of statistical significance, we need to look to inferential statistics (e.g., independent-samples t-test, multiple linear regression). When we analyze employee demographic data, for example, we often compute descriptive statistics like the number of employees who identify with each race/ethnicity category or the average employee age and standard deviation. It’s important to remember that descriptive statistics are, well, descriptive. That is, they help us summarize characteristics of a sample, which is why they are sometimes referred to as summary statistics. As discussed in the chapter on the Data Analysis phase of the HR Analytics Project Life Cycle, descriptive statistics are a specific type of descriptive analytics, as they summarize data that were collected in the past. Broadly speaking, when describing just a single variable (i.e., applying univariate descriptive statistics), we can distinguish between descriptive statistics that are appropriate for describing categorical versus continuous variables, where categorical variables have a nominal or ordinal measurement scale and continuous variable have an interval or ratio measurement scale. Often, counts (i.e., frequencies) are used to describe data associated with a categorical variable, and measures of central tendency and dispersion are used to describe data associated with a continuous variable. 24.1.3.1 Counts Counts are useful descriptive statistics when a variable has a nominal or ordinal measurement scale. Counts are also referred to as frequencies, so I’ll use those two terms interchangeably. As an added benefit, counts tend to be understood by a broad audience, as they simply refer to counting or tallying how many instances of each discrete instances of a category label (i.e., level) of a nominal or ordinal variable have occurred. In fact, sometimes it can be quite amazing what insights we can gleaned just by counting things. A common example of counts in the HR context is headcount by department, facility, or unit. Imagine if you will an organization with facilities in three locations: Beaverton, Hillsboro, and Portland. After tallying up how many employees work at each location, we might find that 15 work at the Beaverton facility, 5 at the Hillsboro facility, and 10 at the Portland facility. In this example, “Beaverton,” “Hillsboro,” and “Portland” are our category labels for this nominal variable, and the values 15, 5, and 10, respectively, are the counts associated with each of those category labels. 24.1.3.2 Measures of Central Tendency &amp; Dispersion Measures of central tendency (e.g., mean, median, mode) summarize the center or most common scores from a distribution of numeric scores, whereas measures of dispersion (e.g., variance, standard deviation, range, interquartile range) summarize variation in numeric scores. Typically, one would apply these specific types of descriptive statistics to describe or summarize variables that have an interval or ratio measurement scale. For example, we might compute the median pay (in US dollars) and the interquartile range in pay for a sample of workers, where pay in this example has a ratio measurement scale. In some instances, however, numeric values could be assigned to category labels of a variable that can be most accurately described as having an ordinal measurement scale – and upon doing so, the variable might be reclassified as having an interval measurement scale. Such a numeric conversion from ordinal to ratio allows for measures of central tendency and dispersion to be computed. For example, a variable with five Likert responses options ranging from “Strongly Disagree” to “Strongly Agree” would technically have an ordinal measurement scale because there are unknown intervals between each of the levels (i.e., category labels); in other words, the interval distance between “Strongly Disagree” and “Disagree” might not be equal to the interval distance between “Disagree” and “Neither Disagree Nor Agree”. Yet, in order to perform certain analyses, sometimes such variables are reconceptualized as having equal intervals and thus having an interval measurement scale. To do so, we would typically assign numeric values to each of the Likert response options, such as 1 = “Strongly Disagree” and 5 = “Strongly Agree” – which gives the illusion of equal intervals. Perhaps a more compelling case for treating a variable with Likert responses as a having an interval measurement scale is when we create a composite variable (i.e., overall scale score) based on the sum or average of scores from multiple Likert variables (e.g., multiple survey items from a measure). 24.1.4 Sample Write-Up Based on data stored in the organization’s HR information system, we sought out to describe the organization’s employee demographics. The employee gender and race/ethnicity variables have nominal measurement scales, and thus we computed counts to describe these variables. Specifically, 321 employees identified as women, 300 as men, 25 as nonbinary, 8 as trans women, and 7 as trans men. Further, 192 employees identified as Hispanic/Latino, 145 as White, 132 as Asian, 119 as Black, 40 as Native American, and 33 as Native Hawaiian. Given that employee age was measured in years since birth, we classified the variable as having a ratio measurement scale, meaning that measures of central tendency and dispersion would be appropriate for describing the variable. We found that employee ages were normally distributed, and that the average employee age was 42.13 years with a standard deviation of 7.71, indicating that roughly two-thirds of employees’ ages fall between 34.42 and 49.84 years. 24.2 Tutorial This chapter’s tutorial demonstrates how to compute various types of descriptive statistics and how to present the findings visually. 24.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch one of the following video tutorials below. Note that in the videos below, I show how to read in the data using the read.csv function from base R, whereas in the written tutorial portion of this chapter, I show how to read in the data using the read_csv function from the readr package. Link to video tutorial: https://youtu.be/Xg0wiBofjCU Link to video tutorial: https://youtu.be/10jYstRPDAU 24.2.2 Functions &amp; Packages Introduced Function Package table base R levels base R factor base R c base R barplot base R pie base R colors base R abline base R hist base R boxplot base R c base R mean base R median base R var base R sd base R min base R max base R range base R IQR base R quantile base R summary base R 24.2.3 Initial Steps If you haven’t already, save the file called “employee_demo.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “employee_demo.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demo &lt;- read_csv(&quot;employee_demo.csv&quot;) ## Rows: 30 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmpID, Facility, Education ## dbl (2): Performance, Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(demo) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;Education&quot; &quot;Performance&quot; &quot;Age&quot; # Print number of rows in data frame (tibble) object nrow(demo) ## [1] 30 # Print data frame (tibble) object print(demo) ## # A tibble: 30 × 5 ## EmpID Facility Education Performance Age ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE123 Beaverton College Degree 3.8 25 ## 2 EE124 Beaverton Some College 9 30 ## 3 EE125 Portland High School Diploma 8.3 32 ## 4 EE126 Beaverton Some College 9.8 28 ## 5 EE127 Beaverton Some College 5.7 30 ## 6 EE128 Beaverton College Degree 8.2 30 ## 7 EE129 Beaverton College Degree 7.3 28 ## 8 EE130 Beaverton College Degree 7.7 28 ## 9 EE131 Portland Some College 6.3 28 ## 10 EE132 Hillsboro Some College 8.4 27 ## # ℹ 20 more rows The demo data frame object contains five variables. EmpID, Facility, Education, Performance, and Age. The EmpID variable is the employee unique identifier, and in this data frame, each row corresponds to a unique employee. The Facility variable contains the name of the facility where each employee works. The Education variable includes the highest level of education each employee attained (i.e., High School Diploma, Some College, College Degree). The Performance variable includes the employees’ annual performance scores (as derived by a proprietary algorithm), where a score of 0.0 would indicate exceptionally low job performance and a score of 10 would indicate exceptionally high job performance. The Age variable includes employees’ age (in years). 24.2.4 Determine the Measurement Scale As described above, we have four employee-demographic variables at our disposal in the data frame object we named demo: Facility, Education, Performance, and Age. Now it’s time to determine which measurement scale best describes each variable – and spoiler alert: These four variables correspond to nominal, ordinal, interval, and ratio measurement scales respectively. Below, I describe why a particular measurement scale maps onto each variable. By viewing our the data frame object called demo using the print, head, or View functions (as show above in the Initial Steps), we can see that the Facility variable consists of the following categories (i.e., levels): Beaverton, Hillsboro, and Portland. These categories do not have inherent numeric properties, and they can’t be ordered meaningfully given that they just represent different facility locations for this fictitious organization. Given all that, the Facility variable can best be described as having a nominal measurement scale. The Education variable contains three levels (i.e., categories): High School Diploma, Some College, and College Degree. These three discrete categories do not have inherent numeric properties but can be ordered in terms of a conventional educational progression, where earning a high school diploma would be the lowest level and earning a college degree would be the highest level (of the three). Furthermore, although the three variable levels can be ordered, they do not necessarily have equal intervals between the levels; in other words, the distance (e.g., time) between a high school diploma and completing some college is not necessarily the same as the distance between completing some college and a college degree. Given all of those characteristics, the Education variable in these data can best be described as having an ordinal measurement scale. The Performance variable includes the annual performance score for each employee (as derived from a proprietary algorithm), where a score of 0.0 would indicate exceptionally low job performance and a score of 10 would indicate exceptionally high job performance. We can assume in this case that intervals between integers are equal, such that the distance between scores of 1 and 2 is the same as the distance between scores 2 and 3; however, because a value of zero (0.0) does not indicate the absence of performance for this variable (but rather exceptionally low job performance), we must conclude that it has an interval measurement scale as opposed to a ratio measurement scale. Finally, the Age variable includes the age of each employee measured in years. Because Age has ordered numeric values and because there are equal intervals between years as a standard measure of time, we can conclude that the variable does not have a nominal or ordinal measurement scale. What’s more, hypothetically, a value of zero when measuring something in years would imply the absence of years – which is to say Age as measured in years has a meaningful zero value. Given all that, the Age variable can be most accurately described as having a ratio measurement scale. 24.2.5 Describe Nominal &amp; Ordinal (Categorical) Variables We can describe variables with nominal or ordinal measurement scales by computing counts (i.e., frequencies) and by creating univariate bar charts (or pie charts), and we’ll work through each of these descriptive approaches in the following sections. 24.2.5.1 Compute Counts &amp; Frequencies Fortunately, it’s quite easy to run counts in R, and we’ll begin by running counts for the Facility variable. One of the simplest approaches is to use the table function from base R. As the sole parenthetical argument, just type the name of the data frame object (demo) followed by the $ operator and the name of the variable that belongs to that data frame object (Facility). # Compute counts for Facility variable (which has nominal measurement scale) table(demo$Facility) ## ## Beaverton Hillsboro Portland ## 15 5 10 As we can see, 15 employees work at the Beaverton facility, 5 at the Hillsboro facility, and 10 at the Portland facility. Simply put, the most employees work in Beaverton, followed by Portland and Hillsboro. Of course, we also would hope that these data are accurate and timely, and point-in-time headcount data in organizations can be surprisingly difficult to estimate accurately in some organizations, but that’s a story for another time. Because we have classified the Education variable as ordinal, we want to make sure that it has ordered levels. That is, High School Diploma should be the lowest level and College Degree should be the highest. To check to see if the variable is a factor with ordered levels, we can apply the levels function from base R and, as the sole parenthetical argument, type the name of the data frame object (demo) followed by the $ operator and the name of the variable that belongs to that data frame object (Education). # Determine whether the Education variable is a factor with ordered levels levels(demo$Education) ## NULL Running the levels function for the Education variable returns NULL, which indicates that this variable is not a factor variable with ordered levels. Never fear, we can fix that by using the factor function from base R. To convert the Education variable to an ordered factor variable, we will overwrite the existing Education variable from the demo data frame object. Thus, we will start by typing the name of the data frame object (demo) followed by the $ operator and the name of the variable (Education), and to the right, we will type the &lt;- operator so that we can perform the variable assignment. To the right of the &lt;- operator, we will type the name of the factor function. As the first argument, we will type the name of the data frame object (demo) followed by the $ operator and the name of the variable (Education). As the second argument, we will type ordered=TRUE to signify that this variable will have ordered levels. As the third argument, we’ll type levels= followed by a vector of the variable levels in ascending order. Note that we use the c (combine) function from base R to construct the vector, and we need to put each level within quotation marks (\" \"). # Convert Education variable to ordered factor demo$Education &lt;- factor(demo$Education, ordered=TRUE, levels=c(&quot;High School Diploma&quot;, &quot;Some College&quot;, &quot;College Degree&quot;)) Now that we’ve converted the Education variable to an ordered factor variable, let’s verify that we did so correctly by running the same levels function that we did above. # Determine whether the Education variable is a factor with ordered levels levels(demo$Education) ## [1] &quot;High School Diploma&quot; &quot;Some College&quot; &quot;College Degree&quot; Instead of NULL, now we see the levels of the variable in ascending order. Good for us! With the Education variable now an ordered factor, it now makes sense to run the table function to compute the counts. # Compute counts for Education variable table(demo$Education) ## ## High School Diploma Some College College Degree ## 4 15 11 Descriptively, we see that the most people completed some college (15), followed closely by 11 people who completed a full college degree. Relatively few employees in this sample had just a high school diploma (4). 24.2.5.2 Create Data Visualizations When interpreting descriptive statistics, it’s often useful to create some kind of data visualization to display the findings in a pictorial or graphical format. A bar chart is a simple data visualization that many potential audience members will be familiar with, making it a good choice. In addition, when the different categories (e.g., levels) are mutually exclusive and sum to a whole, we might also choose to create a pie chart. We’ll begin by creating a bar chart for the Facility variable and follow that up with creating a pie chart for the Education variable – though, we just as easily could make a bar chart for the Education variable and a pie chart for the Facility variable. Create Bar Charts: Using the barplot function from base R, we can create a very simple and straightforward bar chart without too many frills and embellishments. Let’s start with the Facility variable. As the sole parenthetical argument in the barplot function, simply, enter the table(demo$Facility) code that we wrote in the previous section. # Create a bar chart based on Facility counts barplot(table(demo$Facility)) As you can see, a very simple (and not super aesthetically pleasing) bar chart appears in our Plots window. When exploring data on our own, it is often fine to just complete a simple bar chart like this one, as opposed to fine-tuning the aesthetics (e.g., size, color, font) of the plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. If you’re feeling adventurous and would like to learn how to fine-tune the bar chart, feel free to continue on with this tutorial. Additional attention paid to aesthetics might be worthwhile if you plan to present the plot to others in a formal presentation or report. Using the barplot code we wrote above, we can add a second argument in which we apply ylim= followed by a vector (using the c function) of the lower and upper limits for the y-axis. In this example, I set the lower and upper y-axis limits to 0 and 20. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20)) Building on the previous code, we add additional arguments in which we provide more meaningful labels for the x- and y-axes. To do so, we use the xlab argument for the x-axis label and the ylab argument for the y-axis label. Just make sure to put quotation marks (\" \") around whatever text you come up with for your axis labels. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;) We can change the colors of the bars by adding the col (color) argument. There are many, many different colors that can be used in R, and one of my favorites is “dodgerblue”. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;, col=&quot;dodgerblue&quot;) If you’d like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and you’ll get a (huge) list of the color options. # List names of base R color choices colors() ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; &quot;antiquewhite2&quot; ## [6] &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; &quot;aquamarine1&quot; &quot;aquamarine2&quot; ## [11] &quot;aquamarine3&quot; &quot;aquamarine4&quot; &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; ## [16] &quot;azure3&quot; &quot;azure4&quot; &quot;beige&quot; &quot;bisque&quot; &quot;bisque1&quot; ## [21] &quot;bisque2&quot; &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; &quot;blanchedalmond&quot; ## [26] &quot;blue&quot; &quot;blue1&quot; &quot;blue2&quot; &quot;blue3&quot; &quot;blue4&quot; ## [31] &quot;blueviolet&quot; &quot;brown&quot; &quot;brown1&quot; &quot;brown2&quot; &quot;brown3&quot; ## [36] &quot;brown4&quot; &quot;burlywood&quot; &quot;burlywood1&quot; &quot;burlywood2&quot; &quot;burlywood3&quot; ## [41] &quot;burlywood4&quot; &quot;cadetblue&quot; &quot;cadetblue1&quot; &quot;cadetblue2&quot; &quot;cadetblue3&quot; ## [46] &quot;cadetblue4&quot; &quot;chartreuse&quot; &quot;chartreuse1&quot; &quot;chartreuse2&quot; &quot;chartreuse3&quot; ## [51] &quot;chartreuse4&quot; &quot;chocolate&quot; &quot;chocolate1&quot; &quot;chocolate2&quot; &quot;chocolate3&quot; ## [56] &quot;chocolate4&quot; &quot;coral&quot; &quot;coral1&quot; &quot;coral2&quot; &quot;coral3&quot; ## [61] &quot;coral4&quot; &quot;cornflowerblue&quot; &quot;cornsilk&quot; &quot;cornsilk1&quot; &quot;cornsilk2&quot; ## [66] &quot;cornsilk3&quot; &quot;cornsilk4&quot; &quot;cyan&quot; &quot;cyan1&quot; &quot;cyan2&quot; ## [71] &quot;cyan3&quot; &quot;cyan4&quot; &quot;darkblue&quot; &quot;darkcyan&quot; &quot;darkgoldenrod&quot; ## [76] &quot;darkgoldenrod1&quot; &quot;darkgoldenrod2&quot; &quot;darkgoldenrod3&quot; &quot;darkgoldenrod4&quot; &quot;darkgray&quot; ## [81] &quot;darkgreen&quot; &quot;darkgrey&quot; &quot;darkkhaki&quot; &quot;darkmagenta&quot; &quot;darkolivegreen&quot; ## [86] &quot;darkolivegreen1&quot; &quot;darkolivegreen2&quot; &quot;darkolivegreen3&quot; &quot;darkolivegreen4&quot; &quot;darkorange&quot; ## [91] &quot;darkorange1&quot; &quot;darkorange2&quot; &quot;darkorange3&quot; &quot;darkorange4&quot; &quot;darkorchid&quot; ## [96] &quot;darkorchid1&quot; &quot;darkorchid2&quot; &quot;darkorchid3&quot; &quot;darkorchid4&quot; &quot;darkred&quot; ## [101] &quot;darksalmon&quot; &quot;darkseagreen&quot; &quot;darkseagreen1&quot; &quot;darkseagreen2&quot; &quot;darkseagreen3&quot; ## [106] &quot;darkseagreen4&quot; &quot;darkslateblue&quot; &quot;darkslategray&quot; &quot;darkslategray1&quot; &quot;darkslategray2&quot; ## [111] &quot;darkslategray3&quot; &quot;darkslategray4&quot; &quot;darkslategrey&quot; &quot;darkturquoise&quot; &quot;darkviolet&quot; ## [116] &quot;deeppink&quot; &quot;deeppink1&quot; &quot;deeppink2&quot; &quot;deeppink3&quot; &quot;deeppink4&quot; ## [121] &quot;deepskyblue&quot; &quot;deepskyblue1&quot; &quot;deepskyblue2&quot; &quot;deepskyblue3&quot; &quot;deepskyblue4&quot; ## [126] &quot;dimgray&quot; &quot;dimgrey&quot; &quot;dodgerblue&quot; &quot;dodgerblue1&quot; &quot;dodgerblue2&quot; ## [131] &quot;dodgerblue3&quot; &quot;dodgerblue4&quot; &quot;firebrick&quot; &quot;firebrick1&quot; &quot;firebrick2&quot; ## [136] &quot;firebrick3&quot; &quot;firebrick4&quot; &quot;floralwhite&quot; &quot;forestgreen&quot; &quot;gainsboro&quot; ## [141] &quot;ghostwhite&quot; &quot;gold&quot; &quot;gold1&quot; &quot;gold2&quot; &quot;gold3&quot; ## [146] &quot;gold4&quot; &quot;goldenrod&quot; &quot;goldenrod1&quot; &quot;goldenrod2&quot; &quot;goldenrod3&quot; ## [151] &quot;goldenrod4&quot; &quot;gray&quot; &quot;gray0&quot; &quot;gray1&quot; &quot;gray2&quot; ## [156] &quot;gray3&quot; &quot;gray4&quot; &quot;gray5&quot; &quot;gray6&quot; &quot;gray7&quot; ## [161] &quot;gray8&quot; &quot;gray9&quot; &quot;gray10&quot; &quot;gray11&quot; &quot;gray12&quot; ## [166] &quot;gray13&quot; &quot;gray14&quot; &quot;gray15&quot; &quot;gray16&quot; &quot;gray17&quot; ## [171] &quot;gray18&quot; &quot;gray19&quot; &quot;gray20&quot; &quot;gray21&quot; &quot;gray22&quot; ## [176] &quot;gray23&quot; &quot;gray24&quot; &quot;gray25&quot; &quot;gray26&quot; &quot;gray27&quot; ## [181] &quot;gray28&quot; &quot;gray29&quot; &quot;gray30&quot; &quot;gray31&quot; &quot;gray32&quot; ## [186] &quot;gray33&quot; &quot;gray34&quot; &quot;gray35&quot; &quot;gray36&quot; &quot;gray37&quot; ## [191] &quot;gray38&quot; &quot;gray39&quot; &quot;gray40&quot; &quot;gray41&quot; &quot;gray42&quot; ## [196] &quot;gray43&quot; &quot;gray44&quot; &quot;gray45&quot; &quot;gray46&quot; &quot;gray47&quot; ## [201] &quot;gray48&quot; &quot;gray49&quot; &quot;gray50&quot; &quot;gray51&quot; &quot;gray52&quot; ## [206] &quot;gray53&quot; &quot;gray54&quot; &quot;gray55&quot; &quot;gray56&quot; &quot;gray57&quot; ## [211] &quot;gray58&quot; &quot;gray59&quot; &quot;gray60&quot; &quot;gray61&quot; &quot;gray62&quot; ## [216] &quot;gray63&quot; &quot;gray64&quot; &quot;gray65&quot; &quot;gray66&quot; &quot;gray67&quot; ## [221] &quot;gray68&quot; &quot;gray69&quot; &quot;gray70&quot; &quot;gray71&quot; &quot;gray72&quot; ## [226] &quot;gray73&quot; &quot;gray74&quot; &quot;gray75&quot; &quot;gray76&quot; &quot;gray77&quot; ## [231] &quot;gray78&quot; &quot;gray79&quot; &quot;gray80&quot; &quot;gray81&quot; &quot;gray82&quot; ## [236] &quot;gray83&quot; &quot;gray84&quot; &quot;gray85&quot; &quot;gray86&quot; &quot;gray87&quot; ## [241] &quot;gray88&quot; &quot;gray89&quot; &quot;gray90&quot; &quot;gray91&quot; &quot;gray92&quot; ## [246] &quot;gray93&quot; &quot;gray94&quot; &quot;gray95&quot; &quot;gray96&quot; &quot;gray97&quot; ## [251] &quot;gray98&quot; &quot;gray99&quot; &quot;gray100&quot; &quot;green&quot; &quot;green1&quot; ## [256] &quot;green2&quot; &quot;green3&quot; &quot;green4&quot; &quot;greenyellow&quot; &quot;grey&quot; ## [261] &quot;grey0&quot; &quot;grey1&quot; &quot;grey2&quot; &quot;grey3&quot; &quot;grey4&quot; ## [266] &quot;grey5&quot; &quot;grey6&quot; &quot;grey7&quot; &quot;grey8&quot; &quot;grey9&quot; ## [271] &quot;grey10&quot; &quot;grey11&quot; &quot;grey12&quot; &quot;grey13&quot; &quot;grey14&quot; ## [276] &quot;grey15&quot; &quot;grey16&quot; &quot;grey17&quot; &quot;grey18&quot; &quot;grey19&quot; ## [281] &quot;grey20&quot; &quot;grey21&quot; &quot;grey22&quot; &quot;grey23&quot; &quot;grey24&quot; ## [286] &quot;grey25&quot; &quot;grey26&quot; &quot;grey27&quot; &quot;grey28&quot; &quot;grey29&quot; ## [291] &quot;grey30&quot; &quot;grey31&quot; &quot;grey32&quot; &quot;grey33&quot; &quot;grey34&quot; ## [296] &quot;grey35&quot; &quot;grey36&quot; &quot;grey37&quot; &quot;grey38&quot; &quot;grey39&quot; ## [301] &quot;grey40&quot; &quot;grey41&quot; &quot;grey42&quot; &quot;grey43&quot; &quot;grey44&quot; ## [306] &quot;grey45&quot; &quot;grey46&quot; &quot;grey47&quot; &quot;grey48&quot; &quot;grey49&quot; ## [311] &quot;grey50&quot; &quot;grey51&quot; &quot;grey52&quot; &quot;grey53&quot; &quot;grey54&quot; ## [316] &quot;grey55&quot; &quot;grey56&quot; &quot;grey57&quot; &quot;grey58&quot; &quot;grey59&quot; ## [321] &quot;grey60&quot; &quot;grey61&quot; &quot;grey62&quot; &quot;grey63&quot; &quot;grey64&quot; ## [326] &quot;grey65&quot; &quot;grey66&quot; &quot;grey67&quot; &quot;grey68&quot; &quot;grey69&quot; ## [331] &quot;grey70&quot; &quot;grey71&quot; &quot;grey72&quot; &quot;grey73&quot; &quot;grey74&quot; ## [336] &quot;grey75&quot; &quot;grey76&quot; &quot;grey77&quot; &quot;grey78&quot; &quot;grey79&quot; ## [341] &quot;grey80&quot; &quot;grey81&quot; &quot;grey82&quot; &quot;grey83&quot; &quot;grey84&quot; ## [346] &quot;grey85&quot; &quot;grey86&quot; &quot;grey87&quot; &quot;grey88&quot; &quot;grey89&quot; ## [351] &quot;grey90&quot; &quot;grey91&quot; &quot;grey92&quot; &quot;grey93&quot; &quot;grey94&quot; ## [356] &quot;grey95&quot; &quot;grey96&quot; &quot;grey97&quot; &quot;grey98&quot; &quot;grey99&quot; ## [361] &quot;grey100&quot; &quot;honeydew&quot; &quot;honeydew1&quot; &quot;honeydew2&quot; &quot;honeydew3&quot; ## [366] &quot;honeydew4&quot; &quot;hotpink&quot; &quot;hotpink1&quot; &quot;hotpink2&quot; &quot;hotpink3&quot; ## [371] &quot;hotpink4&quot; &quot;indianred&quot; &quot;indianred1&quot; &quot;indianred2&quot; &quot;indianred3&quot; ## [376] &quot;indianred4&quot; &quot;ivory&quot; &quot;ivory1&quot; &quot;ivory2&quot; &quot;ivory3&quot; ## [381] &quot;ivory4&quot; &quot;khaki&quot; &quot;khaki1&quot; &quot;khaki2&quot; &quot;khaki3&quot; ## [386] &quot;khaki4&quot; &quot;lavender&quot; &quot;lavenderblush&quot; &quot;lavenderblush1&quot; &quot;lavenderblush2&quot; ## [391] &quot;lavenderblush3&quot; &quot;lavenderblush4&quot; &quot;lawngreen&quot; &quot;lemonchiffon&quot; &quot;lemonchiffon1&quot; ## [396] &quot;lemonchiffon2&quot; &quot;lemonchiffon3&quot; &quot;lemonchiffon4&quot; &quot;lightblue&quot; &quot;lightblue1&quot; ## [401] &quot;lightblue2&quot; &quot;lightblue3&quot; &quot;lightblue4&quot; &quot;lightcoral&quot; &quot;lightcyan&quot; ## [406] &quot;lightcyan1&quot; &quot;lightcyan2&quot; &quot;lightcyan3&quot; &quot;lightcyan4&quot; &quot;lightgoldenrod&quot; ## [411] &quot;lightgoldenrod1&quot; &quot;lightgoldenrod2&quot; &quot;lightgoldenrod3&quot; &quot;lightgoldenrod4&quot; &quot;lightgoldenrodyellow&quot; ## [416] &quot;lightgray&quot; &quot;lightgreen&quot; &quot;lightgrey&quot; &quot;lightpink&quot; &quot;lightpink1&quot; ## [421] &quot;lightpink2&quot; &quot;lightpink3&quot; &quot;lightpink4&quot; &quot;lightsalmon&quot; &quot;lightsalmon1&quot; ## [426] &quot;lightsalmon2&quot; &quot;lightsalmon3&quot; &quot;lightsalmon4&quot; &quot;lightseagreen&quot; &quot;lightskyblue&quot; ## [431] &quot;lightskyblue1&quot; &quot;lightskyblue2&quot; &quot;lightskyblue3&quot; &quot;lightskyblue4&quot; &quot;lightslateblue&quot; ## [436] &quot;lightslategray&quot; &quot;lightslategrey&quot; &quot;lightsteelblue&quot; &quot;lightsteelblue1&quot; &quot;lightsteelblue2&quot; ## [441] &quot;lightsteelblue3&quot; &quot;lightsteelblue4&quot; &quot;lightyellow&quot; &quot;lightyellow1&quot; &quot;lightyellow2&quot; ## [446] &quot;lightyellow3&quot; &quot;lightyellow4&quot; &quot;limegreen&quot; &quot;linen&quot; &quot;magenta&quot; ## [451] &quot;magenta1&quot; &quot;magenta2&quot; &quot;magenta3&quot; &quot;magenta4&quot; &quot;maroon&quot; ## [456] &quot;maroon1&quot; &quot;maroon2&quot; &quot;maroon3&quot; &quot;maroon4&quot; &quot;mediumaquamarine&quot; ## [461] &quot;mediumblue&quot; &quot;mediumorchid&quot; &quot;mediumorchid1&quot; &quot;mediumorchid2&quot; &quot;mediumorchid3&quot; ## [466] &quot;mediumorchid4&quot; &quot;mediumpurple&quot; &quot;mediumpurple1&quot; &quot;mediumpurple2&quot; &quot;mediumpurple3&quot; ## [471] &quot;mediumpurple4&quot; &quot;mediumseagreen&quot; &quot;mediumslateblue&quot; &quot;mediumspringgreen&quot; &quot;mediumturquoise&quot; ## [476] &quot;mediumvioletred&quot; &quot;midnightblue&quot; &quot;mintcream&quot; &quot;mistyrose&quot; &quot;mistyrose1&quot; ## [481] &quot;mistyrose2&quot; &quot;mistyrose3&quot; &quot;mistyrose4&quot; &quot;moccasin&quot; &quot;navajowhite&quot; ## [486] &quot;navajowhite1&quot; &quot;navajowhite2&quot; &quot;navajowhite3&quot; &quot;navajowhite4&quot; &quot;navy&quot; ## [491] &quot;navyblue&quot; &quot;oldlace&quot; &quot;olivedrab&quot; &quot;olivedrab1&quot; &quot;olivedrab2&quot; ## [496] &quot;olivedrab3&quot; &quot;olivedrab4&quot; &quot;orange&quot; &quot;orange1&quot; &quot;orange2&quot; ## [501] &quot;orange3&quot; &quot;orange4&quot; &quot;orangered&quot; &quot;orangered1&quot; &quot;orangered2&quot; ## [506] &quot;orangered3&quot; &quot;orangered4&quot; &quot;orchid&quot; &quot;orchid1&quot; &quot;orchid2&quot; ## [511] &quot;orchid3&quot; &quot;orchid4&quot; &quot;palegoldenrod&quot; &quot;palegreen&quot; &quot;palegreen1&quot; ## [516] &quot;palegreen2&quot; &quot;palegreen3&quot; &quot;palegreen4&quot; &quot;paleturquoise&quot; &quot;paleturquoise1&quot; ## [521] &quot;paleturquoise2&quot; &quot;paleturquoise3&quot; &quot;paleturquoise4&quot; &quot;palevioletred&quot; &quot;palevioletred1&quot; ## [526] &quot;palevioletred2&quot; &quot;palevioletred3&quot; &quot;palevioletred4&quot; &quot;papayawhip&quot; &quot;peachpuff&quot; ## [531] &quot;peachpuff1&quot; &quot;peachpuff2&quot; &quot;peachpuff3&quot; &quot;peachpuff4&quot; &quot;peru&quot; ## [536] &quot;pink&quot; &quot;pink1&quot; &quot;pink2&quot; &quot;pink3&quot; &quot;pink4&quot; ## [541] &quot;plum&quot; &quot;plum1&quot; &quot;plum2&quot; &quot;plum3&quot; &quot;plum4&quot; ## [546] &quot;powderblue&quot; &quot;purple&quot; &quot;purple1&quot; &quot;purple2&quot; &quot;purple3&quot; ## [551] &quot;purple4&quot; &quot;red&quot; &quot;red1&quot; &quot;red2&quot; &quot;red3&quot; ## [556] &quot;red4&quot; &quot;rosybrown&quot; &quot;rosybrown1&quot; &quot;rosybrown2&quot; &quot;rosybrown3&quot; ## [561] &quot;rosybrown4&quot; &quot;royalblue&quot; &quot;royalblue1&quot; &quot;royalblue2&quot; &quot;royalblue3&quot; ## [566] &quot;royalblue4&quot; &quot;saddlebrown&quot; &quot;salmon&quot; &quot;salmon1&quot; &quot;salmon2&quot; ## [571] &quot;salmon3&quot; &quot;salmon4&quot; &quot;sandybrown&quot; &quot;seagreen&quot; &quot;seagreen1&quot; ## [576] &quot;seagreen2&quot; &quot;seagreen3&quot; &quot;seagreen4&quot; &quot;seashell&quot; &quot;seashell1&quot; ## [581] &quot;seashell2&quot; &quot;seashell3&quot; &quot;seashell4&quot; &quot;sienna&quot; &quot;sienna1&quot; ## [586] &quot;sienna2&quot; &quot;sienna3&quot; &quot;sienna4&quot; &quot;skyblue&quot; &quot;skyblue1&quot; ## [591] &quot;skyblue2&quot; &quot;skyblue3&quot; &quot;skyblue4&quot; &quot;slateblue&quot; &quot;slateblue1&quot; ## [596] &quot;slateblue2&quot; &quot;slateblue3&quot; &quot;slateblue4&quot; &quot;slategray&quot; &quot;slategray1&quot; ## [601] &quot;slategray2&quot; &quot;slategray3&quot; &quot;slategray4&quot; &quot;slategrey&quot; &quot;snow&quot; ## [606] &quot;snow1&quot; &quot;snow2&quot; &quot;snow3&quot; &quot;snow4&quot; &quot;springgreen&quot; ## [611] &quot;springgreen1&quot; &quot;springgreen2&quot; &quot;springgreen3&quot; &quot;springgreen4&quot; &quot;steelblue&quot; ## [616] &quot;steelblue1&quot; &quot;steelblue2&quot; &quot;steelblue3&quot; &quot;steelblue4&quot; &quot;tan&quot; ## [621] &quot;tan1&quot; &quot;tan2&quot; &quot;tan3&quot; &quot;tan4&quot; &quot;thistle&quot; ## [626] &quot;thistle1&quot; &quot;thistle2&quot; &quot;thistle3&quot; &quot;thistle4&quot; &quot;tomato&quot; ## [631] &quot;tomato1&quot; &quot;tomato2&quot; &quot;tomato3&quot; &quot;tomato4&quot; &quot;turquoise&quot; ## [636] &quot;turquoise1&quot; &quot;turquoise2&quot; &quot;turquoise3&quot; &quot;turquoise4&quot; &quot;violet&quot; ## [641] &quot;violetred&quot; &quot;violetred1&quot; &quot;violetred2&quot; &quot;violetred3&quot; &quot;violetred4&quot; ## [646] &quot;wheat&quot; &quot;wheat1&quot; &quot;wheat2&quot; &quot;wheat3&quot; &quot;wheat4&quot; ## [651] &quot;whitesmoke&quot; &quot;yellow&quot; &quot;yellow1&quot; &quot;yellow2&quot; &quot;yellow3&quot; ## [656] &quot;yellow4&quot; &quot;yellowgreen&quot; Finally, the barplot function does not provide a horizontal line where the y-axis is equal to 0. If you’d like to add such a line, simply follow up your barplot function with the abline function, and as the sole argument, type h=0. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;, col=&quot;dodgerblue&quot;) abline(h=0) And finally, here’s a quick example of how you might visualize the Education variable using the barplot function. # Create a bar chart for Education variable barplot(table(demo$Education), ylim=c(0,20), xlab=&quot;Education Level&quot;, ylab=&quot;Counts&quot;, col=&quot;orange&quot;) abline(h=0) Create Pie Charts: Using the pie function from base R, we can create a very simple and straightforward bar chart without too many frills and embellishments. Let’s start with the Education variable. As the sole parenthetical argument in the barplot function, simply, enter the table(demo$Education) code that we wrote in the section called Compute Counts &amp; Frequencies. # Create a bar chart based on Education counts pie(table(demo$Education)) A very simple and generic pie chart appears in our Plots window. When exploring data on our own, it is often fine to just complete a simple pie chart like this one, as opposed to fine-tuning the aesthetics (e.g., size, color, font) of the plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. If you’re feeling adventurous and would like to learn how to adjust the colors pie chart, feel free to continue on with this tutorial. Using the pie code we wrote above, let’s add the col= argument followed by the c (combine) function containing a vector of colors – one color for each slice of the pie. Here, I chose the primar colors of red, yellow, and blue. # Create a bar chart based on Education counts pie(table(demo$Education), col=c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) If you’d like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and you’ll get a (huge) list of the color options. # List names of base R color choices colors() ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; &quot;antiquewhite2&quot; ## [6] &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; &quot;aquamarine1&quot; &quot;aquamarine2&quot; ## [11] &quot;aquamarine3&quot; &quot;aquamarine4&quot; &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; ## [16] &quot;azure3&quot; &quot;azure4&quot; &quot;beige&quot; &quot;bisque&quot; &quot;bisque1&quot; ## [21] &quot;bisque2&quot; &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; &quot;blanchedalmond&quot; ## [26] &quot;blue&quot; &quot;blue1&quot; &quot;blue2&quot; &quot;blue3&quot; &quot;blue4&quot; ## [31] &quot;blueviolet&quot; &quot;brown&quot; &quot;brown1&quot; &quot;brown2&quot; &quot;brown3&quot; ## [36] &quot;brown4&quot; &quot;burlywood&quot; &quot;burlywood1&quot; &quot;burlywood2&quot; &quot;burlywood3&quot; ## [41] &quot;burlywood4&quot; &quot;cadetblue&quot; &quot;cadetblue1&quot; &quot;cadetblue2&quot; &quot;cadetblue3&quot; ## [46] &quot;cadetblue4&quot; &quot;chartreuse&quot; &quot;chartreuse1&quot; &quot;chartreuse2&quot; &quot;chartreuse3&quot; ## [51] &quot;chartreuse4&quot; &quot;chocolate&quot; &quot;chocolate1&quot; &quot;chocolate2&quot; &quot;chocolate3&quot; ## [56] &quot;chocolate4&quot; &quot;coral&quot; &quot;coral1&quot; &quot;coral2&quot; &quot;coral3&quot; ## [61] &quot;coral4&quot; &quot;cornflowerblue&quot; &quot;cornsilk&quot; &quot;cornsilk1&quot; &quot;cornsilk2&quot; ## [66] &quot;cornsilk3&quot; &quot;cornsilk4&quot; &quot;cyan&quot; &quot;cyan1&quot; &quot;cyan2&quot; ## [71] &quot;cyan3&quot; &quot;cyan4&quot; &quot;darkblue&quot; &quot;darkcyan&quot; &quot;darkgoldenrod&quot; ## [76] &quot;darkgoldenrod1&quot; &quot;darkgoldenrod2&quot; &quot;darkgoldenrod3&quot; &quot;darkgoldenrod4&quot; &quot;darkgray&quot; ## [81] &quot;darkgreen&quot; &quot;darkgrey&quot; &quot;darkkhaki&quot; &quot;darkmagenta&quot; &quot;darkolivegreen&quot; ## [86] &quot;darkolivegreen1&quot; &quot;darkolivegreen2&quot; &quot;darkolivegreen3&quot; &quot;darkolivegreen4&quot; &quot;darkorange&quot; ## [91] &quot;darkorange1&quot; &quot;darkorange2&quot; &quot;darkorange3&quot; &quot;darkorange4&quot; &quot;darkorchid&quot; ## [96] &quot;darkorchid1&quot; &quot;darkorchid2&quot; &quot;darkorchid3&quot; &quot;darkorchid4&quot; &quot;darkred&quot; ## [101] &quot;darksalmon&quot; &quot;darkseagreen&quot; &quot;darkseagreen1&quot; &quot;darkseagreen2&quot; &quot;darkseagreen3&quot; ## [106] &quot;darkseagreen4&quot; &quot;darkslateblue&quot; &quot;darkslategray&quot; &quot;darkslategray1&quot; &quot;darkslategray2&quot; ## [111] &quot;darkslategray3&quot; &quot;darkslategray4&quot; &quot;darkslategrey&quot; &quot;darkturquoise&quot; &quot;darkviolet&quot; ## [116] &quot;deeppink&quot; &quot;deeppink1&quot; &quot;deeppink2&quot; &quot;deeppink3&quot; &quot;deeppink4&quot; ## [121] &quot;deepskyblue&quot; &quot;deepskyblue1&quot; &quot;deepskyblue2&quot; &quot;deepskyblue3&quot; &quot;deepskyblue4&quot; ## [126] &quot;dimgray&quot; &quot;dimgrey&quot; &quot;dodgerblue&quot; &quot;dodgerblue1&quot; &quot;dodgerblue2&quot; ## [131] &quot;dodgerblue3&quot; &quot;dodgerblue4&quot; &quot;firebrick&quot; &quot;firebrick1&quot; &quot;firebrick2&quot; ## [136] &quot;firebrick3&quot; &quot;firebrick4&quot; &quot;floralwhite&quot; &quot;forestgreen&quot; &quot;gainsboro&quot; ## [141] &quot;ghostwhite&quot; &quot;gold&quot; &quot;gold1&quot; &quot;gold2&quot; &quot;gold3&quot; ## [146] &quot;gold4&quot; &quot;goldenrod&quot; &quot;goldenrod1&quot; &quot;goldenrod2&quot; &quot;goldenrod3&quot; ## [151] &quot;goldenrod4&quot; &quot;gray&quot; &quot;gray0&quot; &quot;gray1&quot; &quot;gray2&quot; ## [156] &quot;gray3&quot; &quot;gray4&quot; &quot;gray5&quot; &quot;gray6&quot; &quot;gray7&quot; ## [161] &quot;gray8&quot; &quot;gray9&quot; &quot;gray10&quot; &quot;gray11&quot; &quot;gray12&quot; ## [166] &quot;gray13&quot; &quot;gray14&quot; &quot;gray15&quot; &quot;gray16&quot; &quot;gray17&quot; ## [171] &quot;gray18&quot; &quot;gray19&quot; &quot;gray20&quot; &quot;gray21&quot; &quot;gray22&quot; ## [176] &quot;gray23&quot; &quot;gray24&quot; &quot;gray25&quot; &quot;gray26&quot; &quot;gray27&quot; ## [181] &quot;gray28&quot; &quot;gray29&quot; &quot;gray30&quot; &quot;gray31&quot; &quot;gray32&quot; ## [186] &quot;gray33&quot; &quot;gray34&quot; &quot;gray35&quot; &quot;gray36&quot; &quot;gray37&quot; ## [191] &quot;gray38&quot; &quot;gray39&quot; &quot;gray40&quot; &quot;gray41&quot; &quot;gray42&quot; ## [196] &quot;gray43&quot; &quot;gray44&quot; &quot;gray45&quot; &quot;gray46&quot; &quot;gray47&quot; ## [201] &quot;gray48&quot; &quot;gray49&quot; &quot;gray50&quot; &quot;gray51&quot; &quot;gray52&quot; ## [206] &quot;gray53&quot; &quot;gray54&quot; &quot;gray55&quot; &quot;gray56&quot; &quot;gray57&quot; ## [211] &quot;gray58&quot; &quot;gray59&quot; &quot;gray60&quot; &quot;gray61&quot; &quot;gray62&quot; ## [216] &quot;gray63&quot; &quot;gray64&quot; &quot;gray65&quot; &quot;gray66&quot; &quot;gray67&quot; ## [221] &quot;gray68&quot; &quot;gray69&quot; &quot;gray70&quot; &quot;gray71&quot; &quot;gray72&quot; ## [226] &quot;gray73&quot; &quot;gray74&quot; &quot;gray75&quot; &quot;gray76&quot; &quot;gray77&quot; ## [231] &quot;gray78&quot; &quot;gray79&quot; &quot;gray80&quot; &quot;gray81&quot; &quot;gray82&quot; ## [236] &quot;gray83&quot; &quot;gray84&quot; &quot;gray85&quot; &quot;gray86&quot; &quot;gray87&quot; ## [241] &quot;gray88&quot; &quot;gray89&quot; &quot;gray90&quot; &quot;gray91&quot; &quot;gray92&quot; ## [246] &quot;gray93&quot; &quot;gray94&quot; &quot;gray95&quot; &quot;gray96&quot; &quot;gray97&quot; ## [251] &quot;gray98&quot; &quot;gray99&quot; &quot;gray100&quot; &quot;green&quot; &quot;green1&quot; ## [256] &quot;green2&quot; &quot;green3&quot; &quot;green4&quot; &quot;greenyellow&quot; &quot;grey&quot; ## [261] &quot;grey0&quot; &quot;grey1&quot; &quot;grey2&quot; &quot;grey3&quot; &quot;grey4&quot; ## [266] &quot;grey5&quot; &quot;grey6&quot; &quot;grey7&quot; &quot;grey8&quot; &quot;grey9&quot; ## [271] &quot;grey10&quot; &quot;grey11&quot; &quot;grey12&quot; &quot;grey13&quot; &quot;grey14&quot; ## [276] &quot;grey15&quot; &quot;grey16&quot; &quot;grey17&quot; &quot;grey18&quot; &quot;grey19&quot; ## [281] &quot;grey20&quot; &quot;grey21&quot; &quot;grey22&quot; &quot;grey23&quot; &quot;grey24&quot; ## [286] &quot;grey25&quot; &quot;grey26&quot; &quot;grey27&quot; &quot;grey28&quot; &quot;grey29&quot; ## [291] &quot;grey30&quot; &quot;grey31&quot; &quot;grey32&quot; &quot;grey33&quot; &quot;grey34&quot; ## [296] &quot;grey35&quot; &quot;grey36&quot; &quot;grey37&quot; &quot;grey38&quot; &quot;grey39&quot; ## [301] &quot;grey40&quot; &quot;grey41&quot; &quot;grey42&quot; &quot;grey43&quot; &quot;grey44&quot; ## [306] &quot;grey45&quot; &quot;grey46&quot; &quot;grey47&quot; &quot;grey48&quot; &quot;grey49&quot; ## [311] &quot;grey50&quot; &quot;grey51&quot; &quot;grey52&quot; &quot;grey53&quot; &quot;grey54&quot; ## [316] &quot;grey55&quot; &quot;grey56&quot; &quot;grey57&quot; &quot;grey58&quot; &quot;grey59&quot; ## [321] &quot;grey60&quot; &quot;grey61&quot; &quot;grey62&quot; &quot;grey63&quot; &quot;grey64&quot; ## [326] &quot;grey65&quot; &quot;grey66&quot; &quot;grey67&quot; &quot;grey68&quot; &quot;grey69&quot; ## [331] &quot;grey70&quot; &quot;grey71&quot; &quot;grey72&quot; &quot;grey73&quot; &quot;grey74&quot; ## [336] &quot;grey75&quot; &quot;grey76&quot; &quot;grey77&quot; &quot;grey78&quot; &quot;grey79&quot; ## [341] &quot;grey80&quot; &quot;grey81&quot; &quot;grey82&quot; &quot;grey83&quot; &quot;grey84&quot; ## [346] &quot;grey85&quot; &quot;grey86&quot; &quot;grey87&quot; &quot;grey88&quot; &quot;grey89&quot; ## [351] &quot;grey90&quot; &quot;grey91&quot; &quot;grey92&quot; &quot;grey93&quot; &quot;grey94&quot; ## [356] &quot;grey95&quot; &quot;grey96&quot; &quot;grey97&quot; &quot;grey98&quot; &quot;grey99&quot; ## [361] &quot;grey100&quot; &quot;honeydew&quot; &quot;honeydew1&quot; &quot;honeydew2&quot; &quot;honeydew3&quot; ## [366] &quot;honeydew4&quot; &quot;hotpink&quot; &quot;hotpink1&quot; &quot;hotpink2&quot; &quot;hotpink3&quot; ## [371] &quot;hotpink4&quot; &quot;indianred&quot; &quot;indianred1&quot; &quot;indianred2&quot; &quot;indianred3&quot; ## [376] &quot;indianred4&quot; &quot;ivory&quot; &quot;ivory1&quot; &quot;ivory2&quot; &quot;ivory3&quot; ## [381] &quot;ivory4&quot; &quot;khaki&quot; &quot;khaki1&quot; &quot;khaki2&quot; &quot;khaki3&quot; ## [386] &quot;khaki4&quot; &quot;lavender&quot; &quot;lavenderblush&quot; &quot;lavenderblush1&quot; &quot;lavenderblush2&quot; ## [391] &quot;lavenderblush3&quot; &quot;lavenderblush4&quot; &quot;lawngreen&quot; &quot;lemonchiffon&quot; &quot;lemonchiffon1&quot; ## [396] &quot;lemonchiffon2&quot; &quot;lemonchiffon3&quot; &quot;lemonchiffon4&quot; &quot;lightblue&quot; &quot;lightblue1&quot; ## [401] &quot;lightblue2&quot; &quot;lightblue3&quot; &quot;lightblue4&quot; &quot;lightcoral&quot; &quot;lightcyan&quot; ## [406] &quot;lightcyan1&quot; &quot;lightcyan2&quot; &quot;lightcyan3&quot; &quot;lightcyan4&quot; &quot;lightgoldenrod&quot; ## [411] &quot;lightgoldenrod1&quot; &quot;lightgoldenrod2&quot; &quot;lightgoldenrod3&quot; &quot;lightgoldenrod4&quot; &quot;lightgoldenrodyellow&quot; ## [416] &quot;lightgray&quot; &quot;lightgreen&quot; &quot;lightgrey&quot; &quot;lightpink&quot; &quot;lightpink1&quot; ## [421] &quot;lightpink2&quot; &quot;lightpink3&quot; &quot;lightpink4&quot; &quot;lightsalmon&quot; &quot;lightsalmon1&quot; ## [426] &quot;lightsalmon2&quot; &quot;lightsalmon3&quot; &quot;lightsalmon4&quot; &quot;lightseagreen&quot; &quot;lightskyblue&quot; ## [431] &quot;lightskyblue1&quot; &quot;lightskyblue2&quot; &quot;lightskyblue3&quot; &quot;lightskyblue4&quot; &quot;lightslateblue&quot; ## [436] &quot;lightslategray&quot; &quot;lightslategrey&quot; &quot;lightsteelblue&quot; &quot;lightsteelblue1&quot; &quot;lightsteelblue2&quot; ## [441] &quot;lightsteelblue3&quot; &quot;lightsteelblue4&quot; &quot;lightyellow&quot; &quot;lightyellow1&quot; &quot;lightyellow2&quot; ## [446] &quot;lightyellow3&quot; &quot;lightyellow4&quot; &quot;limegreen&quot; &quot;linen&quot; &quot;magenta&quot; ## [451] &quot;magenta1&quot; &quot;magenta2&quot; &quot;magenta3&quot; &quot;magenta4&quot; &quot;maroon&quot; ## [456] &quot;maroon1&quot; &quot;maroon2&quot; &quot;maroon3&quot; &quot;maroon4&quot; &quot;mediumaquamarine&quot; ## [461] &quot;mediumblue&quot; &quot;mediumorchid&quot; &quot;mediumorchid1&quot; &quot;mediumorchid2&quot; &quot;mediumorchid3&quot; ## [466] &quot;mediumorchid4&quot; &quot;mediumpurple&quot; &quot;mediumpurple1&quot; &quot;mediumpurple2&quot; &quot;mediumpurple3&quot; ## [471] &quot;mediumpurple4&quot; &quot;mediumseagreen&quot; &quot;mediumslateblue&quot; &quot;mediumspringgreen&quot; &quot;mediumturquoise&quot; ## [476] &quot;mediumvioletred&quot; &quot;midnightblue&quot; &quot;mintcream&quot; &quot;mistyrose&quot; &quot;mistyrose1&quot; ## [481] &quot;mistyrose2&quot; &quot;mistyrose3&quot; &quot;mistyrose4&quot; &quot;moccasin&quot; &quot;navajowhite&quot; ## [486] &quot;navajowhite1&quot; &quot;navajowhite2&quot; &quot;navajowhite3&quot; &quot;navajowhite4&quot; &quot;navy&quot; ## [491] &quot;navyblue&quot; &quot;oldlace&quot; &quot;olivedrab&quot; &quot;olivedrab1&quot; &quot;olivedrab2&quot; ## [496] &quot;olivedrab3&quot; &quot;olivedrab4&quot; &quot;orange&quot; &quot;orange1&quot; &quot;orange2&quot; ## [501] &quot;orange3&quot; &quot;orange4&quot; &quot;orangered&quot; &quot;orangered1&quot; &quot;orangered2&quot; ## [506] &quot;orangered3&quot; &quot;orangered4&quot; &quot;orchid&quot; &quot;orchid1&quot; &quot;orchid2&quot; ## [511] &quot;orchid3&quot; &quot;orchid4&quot; &quot;palegoldenrod&quot; &quot;palegreen&quot; &quot;palegreen1&quot; ## [516] &quot;palegreen2&quot; &quot;palegreen3&quot; &quot;palegreen4&quot; &quot;paleturquoise&quot; &quot;paleturquoise1&quot; ## [521] &quot;paleturquoise2&quot; &quot;paleturquoise3&quot; &quot;paleturquoise4&quot; &quot;palevioletred&quot; &quot;palevioletred1&quot; ## [526] &quot;palevioletred2&quot; &quot;palevioletred3&quot; &quot;palevioletred4&quot; &quot;papayawhip&quot; &quot;peachpuff&quot; ## [531] &quot;peachpuff1&quot; &quot;peachpuff2&quot; &quot;peachpuff3&quot; &quot;peachpuff4&quot; &quot;peru&quot; ## [536] &quot;pink&quot; &quot;pink1&quot; &quot;pink2&quot; &quot;pink3&quot; &quot;pink4&quot; ## [541] &quot;plum&quot; &quot;plum1&quot; &quot;plum2&quot; &quot;plum3&quot; &quot;plum4&quot; ## [546] &quot;powderblue&quot; &quot;purple&quot; &quot;purple1&quot; &quot;purple2&quot; &quot;purple3&quot; ## [551] &quot;purple4&quot; &quot;red&quot; &quot;red1&quot; &quot;red2&quot; &quot;red3&quot; ## [556] &quot;red4&quot; &quot;rosybrown&quot; &quot;rosybrown1&quot; &quot;rosybrown2&quot; &quot;rosybrown3&quot; ## [561] &quot;rosybrown4&quot; &quot;royalblue&quot; &quot;royalblue1&quot; &quot;royalblue2&quot; &quot;royalblue3&quot; ## [566] &quot;royalblue4&quot; &quot;saddlebrown&quot; &quot;salmon&quot; &quot;salmon1&quot; &quot;salmon2&quot; ## [571] &quot;salmon3&quot; &quot;salmon4&quot; &quot;sandybrown&quot; &quot;seagreen&quot; &quot;seagreen1&quot; ## [576] &quot;seagreen2&quot; &quot;seagreen3&quot; &quot;seagreen4&quot; &quot;seashell&quot; &quot;seashell1&quot; ## [581] &quot;seashell2&quot; &quot;seashell3&quot; &quot;seashell4&quot; &quot;sienna&quot; &quot;sienna1&quot; ## [586] &quot;sienna2&quot; &quot;sienna3&quot; &quot;sienna4&quot; &quot;skyblue&quot; &quot;skyblue1&quot; ## [591] &quot;skyblue2&quot; &quot;skyblue3&quot; &quot;skyblue4&quot; &quot;slateblue&quot; &quot;slateblue1&quot; ## [596] &quot;slateblue2&quot; &quot;slateblue3&quot; &quot;slateblue4&quot; &quot;slategray&quot; &quot;slategray1&quot; ## [601] &quot;slategray2&quot; &quot;slategray3&quot; &quot;slategray4&quot; &quot;slategrey&quot; &quot;snow&quot; ## [606] &quot;snow1&quot; &quot;snow2&quot; &quot;snow3&quot; &quot;snow4&quot; &quot;springgreen&quot; ## [611] &quot;springgreen1&quot; &quot;springgreen2&quot; &quot;springgreen3&quot; &quot;springgreen4&quot; &quot;steelblue&quot; ## [616] &quot;steelblue1&quot; &quot;steelblue2&quot; &quot;steelblue3&quot; &quot;steelblue4&quot; &quot;tan&quot; ## [621] &quot;tan1&quot; &quot;tan2&quot; &quot;tan3&quot; &quot;tan4&quot; &quot;thistle&quot; ## [626] &quot;thistle1&quot; &quot;thistle2&quot; &quot;thistle3&quot; &quot;thistle4&quot; &quot;tomato&quot; ## [631] &quot;tomato1&quot; &quot;tomato2&quot; &quot;tomato3&quot; &quot;tomato4&quot; &quot;turquoise&quot; ## [636] &quot;turquoise1&quot; &quot;turquoise2&quot; &quot;turquoise3&quot; &quot;turquoise4&quot; &quot;violet&quot; ## [641] &quot;violetred&quot; &quot;violetred1&quot; &quot;violetred2&quot; &quot;violetred3&quot; &quot;violetred4&quot; ## [646] &quot;wheat&quot; &quot;wheat1&quot; &quot;wheat2&quot; &quot;wheat3&quot; &quot;wheat4&quot; ## [651] &quot;whitesmoke&quot; &quot;yellow&quot; &quot;yellow1&quot; &quot;yellow2&quot; &quot;yellow3&quot; ## [656] &quot;yellow4&quot; &quot;yellowgreen&quot; 24.2.6 Describe Interval &amp; Ratio (Continuous) Variables We can describe variables with interval or ratio measurement scales (i.e., continuous variables) by computing measures of central tendency (e.g., mean, median) and dispersion (e.g., standard deviation, range); however, it’s often good practice to begin by creating data visualizations (e.g., histograms, box plots) that will enable us to understand the nature of each variable’s distribution. 24.2.6.1 Create Data Visualizations By visualizing the shape of a continuous variable’s distribution (e.g., normal distribution, positive skew, negative skew), we can make a more informed decision regarding how to select, interpret, and report measures of central tendency and dispersion. In this section, we’ll focus on creating histograms and box plots. Create Histograms: A histogram visually approximates the distribution of a set of numerical scores. The scores are grouped into ranges (which by default are often equally sized), and the boundaries of these ranges are referred to as breaks or break points. The bars in a histogram fill these ranges, and their heights represent the frequency (i.e., count) of sources within each range. Let’s begin with the Age variable. To create a histogram, we can use the hist function from base R. To get things started, let’s enter a single argument: the name of the data frame object (demo), followed by the $ operator and the name of the variable we wish to visualize (Age). # Create a histogram hist(demo$Age) This histogram will do just fine for our purposes. Note that the histogram indicates that the scores from the Age variable appear to be roughly normally distributed. With smaller sample sizes (e.g., fewer than 30 observations or cases), we’re less likely to see a clean, normal distribution of scores, and this relates to the central limit theorem; though, an explanation of this theorem is beyond the scope of this tutorial. Nevertheless, the take-home message is that histograms provide rough approximations of the shapes of distributions, and a normal distribution is less likely when their are fewer observations (i.e., a smaller sample) and thus fewer scores on a variable. For your own internal data-exploration purposes, it is often fine to create a simple histogram like the one we created above, meaning that you would not need to worry about the aesthetics (e.g., size, color) of the histogram. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. As optional next steps, you can play around with arguments to adjust the y-axis limits (ylim), x-axis label (xlab), y-axis label (ylab), main title (main), and the bar color (col). [If you’d like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and you’ll get a (huge) list of the color options.] A more in-depth description of these plot arguments is provided in the section above called Create Bar Charts. # Create a histogram and add style hist(demo$Age, ylim=c(0, 15), # y-axis limits xlab=&quot;Employee Age&quot;, # x-axis label ylab=&quot;Count&quot;, # y-axis label main=NULL, # main title col=&quot;dodgerblue&quot;) # bar color We can also specify a vector of the break points between the bars using the c function from base R. Just be sure that the lowest value in your vector is equal to or less than the minimum value for the variable and the the highest value is equal to or greater than the maximum value for the variable. To do so, we can add the breaks argument. # Create a histogram and add style hist(demo$Age, ylim=c(0, 25), # y-axis limits xlab=&quot;Employee Age&quot;, # x-axis label ylab=&quot;Count&quot;, # y-axis label main=NULL, # remove main title col=&quot;dodgerblue&quot;, # bar color breaks=c(20, 25, 30, 35)) # set break points between bars Create Box Plots: We could use a histogram to visualize the Performance variable, but let’s use this opportunity to create a box plot instead. Like a histogram, a box plot (sometimes called a “box and whiskers plot”) also reveals information about the shape of a distribution, including the median, 25th percentile (i.e., lower quartile), 75th percentile (i.e., upper quartile), and the variation outside the 25th and 75th percentiles. We’ll use the boxplot function from base R. To kick things off, let’s enter a single argument: the name of the data frame object (demo), followed by the $ operator and the name of the variable we wish to visualize (Performance). # Create a box plot boxplot(demo$Performance) The thick horizontal line in the middle of the box is the median score, the lower edge of the box represents the lower quartile (i.e., 25th percentile, median of lower half of the distribution), and the upper edge of the box represents the upper quartile (i.e., 75th percentile, median of the upper half of the distribution). The height of the box is the interquartile range. By default, the boxplot function sets the upper “whisker” (i.e., the horizontal line at the top of the upper dashed line) as the smaller of two values: the maximum value or 1.5 times the interquartile range. Further, the function sets the lower “whisker” (i.e., the horizontal line at the bottom of the lower dashed line) as the larger of two values: the minimum value or 1.5 times the interquartile range. In the box plot for Performance, we can see that the distribution of scores appears to be slightly negatively skewed, as the upper quartile is smaller than the lower quartile (i.e., the median is closer to the top of the box) and the upper whisker is shorter than the lower whisker. If there had been any outlier scores, these would appear beyond the upper and lower limits of the whiskers. If you plan to create a box plot for your own data-exploration purposes only, it is often fine to create a simple box plot like the one we created above, which means you would not need to proceed forward with subsequent steps in which I show how to refine the aesthetics of the box plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. As optional next steps, you can play around with arguments to adjust the y-axis label (ylab) and the box color (col). If you’d like to explore additional colors, check out this website. Or, you can run the colors() function (without any arguments), and you’ll get a (huge) list of the color options. # Create a box plot and add style boxplot(demo$Performance, ylab=&quot;Employee Job Performance&quot;, # y-axis label col=&quot;orange&quot;) # bar color 24.2.6.2 Compute Measures of Central Tendency &amp; Dispersion Now that we’ve visualized our interval and ratio measurement scale variables, we’re ready to compute some measures of central tendency and dispersion. In R the process is quite straightforward, as the function names are fairly intuitive: mean (mean), var (variance), sd (standard deviation), median (median), min (minimum), max (maximum), range (range), and IQR (interquartile range). Within each function’s parentheses, you will enter the same arguments. Specifically, you should include the name of the data frame (demo), followed by the $ operator and the name of the variable ofese measures of central tendency even if there are missing data for the varia interest (Age). Keep the na.rm=TRUE argument as is if you would like to calculate the variable of interest. Let’s start with some measures of central tendency for the Age variable, specifically the mean (mean) and median (median). # Mean of Age mean(demo$Age, na.rm=TRUE) ## [1] 28 # Median of Age median(demo$Age, na.rm=TRUE) ## [1] 28 As you can, see both the median and the mode happen to be 28, which indicates that center of the Age distribution is about 28 years. Should we have a skewed distribution (positive or negative), the median is often a better indicator of central tendency given that it is less susceptible to influential cases (e.g., outliers). A class example of a skewed distribution in organizations involves pay variables, especially when executive pay is included. In U.S. organizations, executive pay often is far greater than average worker’s pay, which often leads us to report the median pay as an indicator of central tendency. Let’s move on to some measures of dispersion, specifically the variance (var) and standard deviation (sd). # Variance of Age var(demo$Age, na.rm=TRUE) ## [1] 7.103448 # Standard deviation (SD) of Age sd(demo$Age, na.rm=TRUE) ## [1] 2.665229 The variance is a nonstandardized indicator of dispersion or variation, so we typically interpret the square root of the variance, which is called the standard deviation. Given that we found a mean age of 28 years for this sample of employees, the standard deviation of approximately 2.67 years indicates that approximately 68% of employees’ ages fall within 2.67 years (i.e., 1 SD) of 28 years (i.e., between 25.33 and 30.67 years), and 95% of employees’ ages fall within 5.34 years (i.e., 2 SD) of 28 years (i.e., between 22.66 and 33.34 years). As we saw in the histogram for Age, the variable has a roughly normal distribution. Let’s compute the minimum and maximum score for Age using the min and max functions, respectively. # Minimum of Age min(demo$Age, na.rm=TRUE) ## [1] 22 # Maximum of Age max(demo$Age, na.rm=TRUE) ## [1] 34 The minimum age is 22 years for this sample, and the maximum age is 34 years. Next let’s compute the range, which will give us the minimum and maximum scores using a single function. # Range of Age range(demo$Age, na.rm=TRUE) ## [1] 22 34 As you can see, the range functions provides both the minimum and maximum scores. Next, let’s compute the interquartile range (IQR), which is the distance between the lower and upper quartiles (i.e., between the 25th and 75th percentile). As noted above in the section called Create Box Plots, the lower and upper quartiles correspond to the outer edges of the box, whereas the median (50th percentile) corresponds to the line within the box. # Interquartile range (IQR) of Age IQR(demo$Age, na.rm=TRUE) ## [1] 3 The IQR is 3 years, which indicates that middle 50% of ages spans 3 years. As a follow-up, let’s compute the lower and upper quartiles (i.e., between the 25th and 75th percentiles) by using the quantile function from base R. As the first argument, type the name of the data frame (demo), followed by the $ operator and the name of the variable of interest (Age). As the second argument, type .25 if you would like to request the 25th percentile (lower quartile) and .75 if you would like to request the 75th percentile (upper quartile). Let’s do both. # Request specific quartiles/percentiles quantile(demo$Age, .25) # lower quartile / 25th percentile ## 25% ## 27 quantile(demo$Age, .75) # upper quartile / 75th percentile ## 75% ## 30 Corroborating what we found with the IQR, the difference between the upper and lower quartiles is 3 years (30 - 27 = 3). The IQR and lower and upper quartiles are typically reported along with the median (as evidenced by the box plot we created above), so let’s report them together. If you recall, the median age was 28 years for this sample, and the IQR spans 3 years from 27 years to 30 years. These measures indicate that the middle 50% of ages for this sample are between 27 and 30 years, and that the middle-most age (i.e., 50th percentile) is 28 years. Alternatively, if we wish to automatically compute the 0th, 25th, 50th, 75th, and 100th percentile all at once, we can simply type the name of the quantile function and then enter the name of the data frame object (df) followed by the $ operator and the name of the variable (Age). # Request 0, 25, 50, 75, and 100 percentiles quantile(demo$Age) ## 0% 25% 50% 75% 100% ## 22 27 28 30 34 Finally, one way to compute the minimum, lower quartile (1st quartile), median, mean, upper quartile (3rd quartile), and maximum all at once is to use the summary function from base R with the name of the data frame object (demo) followed by the $ operator and the name of the variable (Age) as the sole parenthetical argument. # Minimum, lower quartile (1st quartile), median, mean, upper quartile (3rd quartile), and maximum summary(demo$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 22 27 28 28 30 34 24.2.7 Summary In this chapter, we focused on descriptive statistics. First, we began by learning about four different measurements scales (i.e., nominal, ordinal, interval, ratio) and how identifying the measurement scale of a variable is an important first step in determining an appropriate descriptive statistic or data-visualization display type. Second, we learned how to compute counts (i.e., frequencies) for nominal and ordinal variables using the table function from base R. Further, you learned how to convert a variable to an ordered factor using the factor function from base R. Finally, you learned how to visualize counts data using the barplot function from base R. Finally, we learned how to visualize the distribution of a variable with an interval or ratio measurement scale using histograms (hist function from base R) and box plots (boxplot function from base R). In addition, we learned how to compute measures of central tendency and dispersion base R functions like mean (mean), var (variance), sd (standard deviation), median (median), min (minimum), max (maximum), range (range), and IQR (interquartile range). 24.3 Chapter Supplement In this chapter supplement, we will learn how to compute the coefficient of variation (CV). 24.3.1 Functions &amp; Packages Introduced Function Package mean base R sd base R 24.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demo &lt;- read_csv(&quot;employee_demo.csv&quot;) ## Rows: 30 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmpID, Facility, Education ## dbl (2): Performance, Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(demo) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;Education&quot; &quot;Performance&quot; &quot;Age&quot; 24.3.3 Compute Coefficient of Variation (CV) The coefficient of variation (CV) (also known as relative standard deviation) is a standardized indicator of dispersion that can be used to compare the relative variability of two or more variables with different scaling. Technically, it is really only appropriate and meaningful to compute the CV for variables that have a ratio measurement scale and thus a meaningful zero; however, sometimes people relax this assumption (albeit inappropriately) to allow for the CV to be computed for a variable with an interval measurement scale. I urge you to only compute CVs for variables with ratio measurement scales. As a hypothetical application of the CV, imagine you would like to compare the variability of these two measures – both having a ratio measurement scale: (a) monthly base pay measured in US dollars, and (b) monthly variable pay measured in US dollars. The formula to compute the CV for a variable is simple. In fact, it’s just the ratio of a variable’s standard deviation (SD) relative to its mean, which results in a proportion. If we multiply that proportion by 100, we can interpret the CV as a percentage. \\(CV = \\frac{SD}{mean} * 100\\) Let’s imagine that for a given sample of employees the mean monthly base pay is 3,553 dollars, and the SD is 593 dollars. Further, the mean monthly variable pay for these same employees is 422 dollars, and the SD is 98 dollars. Let’s compute the CV for each measure and then compare. \\(CV_{basepay} = \\frac{593}{3553} * 100 = 16.7\\) \\(CV_{variablepay} = \\frac{98}{422} * 100 = 23.2\\) Note that the CV for monthly base pay is 16.7%, and the CV for the monthly variable pay is 23.2%. We can interpret these descriptively as indicating that monthly variable pay shows higher variability around its mean relative to monthly base pay. In other words, monthly variable pay shows higher relative dispersion than monthly base pay. It’s important to note that comparing CVs in this way is entirely descriptive, which means that we cannot conclude that the two CVs differ significantly from one another in a statistical sense; to make such a conclusion, we would need to estimate an appropriate inferential statistical analysis (Feltz and Miller 1996; Lewontin 1966; Miller 1991). Alternatively, CVs can be computed to compare the relative variability of the same measure assessed with two independent samples. For example, in a clinical setting, the CV for a measure can be computed for each clinical trial sample in which it was administered to evaluate whether it’s appropriate to combine data from multiple samples. Now that we understand what a coefficient of variation is, let’s practice computing one by using the data frame we read in called called demo. Note that both the Performance and Age variables can be described as having interval and ratio measurement scales, respectively. Let’s begin by computing the coefficient of variation (CV) for the Performance variable. As noted in the introduction, the formula is simply a ratio, such that we divide the standard deviation (SD) for the measure by the mean for that measure. We can then convert the resulting proportion to a percentage by multiplying the proportion by 100. To compute the SD, we’ll use the sd function from base R, and to compute the mean, we’ll use the mean function from base R. Within each function, we enter the name of the data frame object (demo) followed by the $ operator and the name of the variable in question that belongs to the aforementioned data frame object. To divide we use the forward slash (/), and to multiply we use the asterisk (*), as shown below. # Compute coefficient of variation (CV) for Performance variable sd(demo$Performance) / mean(demo$Performance) * 100 ## [1] 21.15746 In the Console, we should see that the CV for the Performance variable is approximately 21.2%. Next, let’s compute the CV for the Age variable. We’ll use the same formula as above, except swap out the Performance variable for the Age variable. # Compute coefficient of variation (CV) for Age variable sd(demo$Age) / mean(demo$Age) * 100 ## [1] 9.518677 In the Console, we should see that the CV for the Age variable is approximately 9.5%. As noted in the introduction, we are being descriptive in our comparisons in this tutorial and are not applying an inferential statistical analysis. Given that, we cannot make statements indicating that one CV is significantly larger than the other. To make such a statement, we would need to apply an inferential statistical analysis (Lewontin 1966; Miller 1991), which is beyond the scope of this chapter on descriptive statistics. References "],["crosstabs.html", "Chapter 25 Summarizing Two or More Categorical Variables Using Cross-Tabulations 25.1 Conceptual Overview 25.2 Tutorial", " Chapter 25 Summarizing Two or More Categorical Variables Using Cross-Tabulations In this chapter, we will learn about how to summarize two or three categorical (nominal, ordinal) employee-demographic variables using cross-tabulations, and we’ll conclude the chapter with a tutorial. 25.1 Conceptual Overview In this section, we’ll review the purpose of cross-tabulations and how they can be useful for summarizing data from two or three categorical (nominal, ordinal) variables, followed by a sample-write up of cross-tabulation results. 25.1.1 Review of Cross-Tabulation A cross-tabulation is a specific type of table. A table in its simplest form is simply an object in which data are stored in rows and columns, and sometimes a table is referred to as a tabular display in the context of data visualization. Broadly speaking, in the R environment you can think of a data frame as a specific type of data table. When we create a table involving two or more categorical variables, we often refer to the the table as a cross-tabulation (cross-tabs). A cross-tabulation can be described more specifically as the process of creating a table from two or more categorical variables (i.e., dimensions), which frequencies (i.e., counts) of observations are displayed per combination of variable categories or levels. The table resulting from a cross tabulation is sometimes referred to as a contingency table. Cross-tabulation is a relatively simple way in which we can summarize data, but it serves as the foundation for the chi-square test of independence and allows for deriving insights via segmentation. Cross-tabulation is often most useful when the variables involved are nominal or ordinal. In this chapter, we focus on creating cross-tabulations to describe or summarize frequency (i.e., count) data from one or more categorical (i.e., nominal, ordinal) variables. Specifically, we will learn how to create different types of two-way and three-way cross-tabulations, where two-way implies that we are summarizing two variables and three-way implies that we are summarizing three variables. As a side note, the Aggregating &amp; Segmenting Data chapter provides additional approaches for creating descriptive or summary data tables (well, technically tibbles) using functions from the dplyr package; in that chapter, you can learn how to create tables when one variable is continuous (i.e., interval ratio) and one variable is categorical. Finally, please note that there is a package called data.table that has functions that allow one to convert data frames to a special kind of data table object that allows for faster and enhanced manipulations; if you’re interested, follow this link to learn more. 25.1.2 Sample Write-Up Based on data stored in the organization’s HR information system, we sought out to describe the organization’s employee demographics. The employee gender and race/ethnicity variables have nominal measurement scales, and thus we computed counts to describe these variables. Specifically, 321 employees identified as women and 300 as men. With respect to race/ethnicity, 192 employees identified as Hispanic/Latino and 429 as White. To describe how the gender and race/ethnicity variables relate to one another, we computed a two-way cross-tabulation. The results indicated that 77 (24%) women identified as Hispanic/Latino, and 244 (76%) women identified as White. Additionally, 115 (38%) men identified as Hispanic/Latino, and 185 (62%) identified as White. 25.2 Tutorial This chapter’s tutorial demonstrates how to compute cross-tabulations for combinations of two and three categorical (nominal, ordinal) variables. 25.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/Ja_CM253oDQ 25.2.2 Functions &amp; Packages Introduced Function Package table base R prop.table base R round base R ftable base R xtabs base R CrossTable gmodels 25.2.3 Initial Steps If you haven’t already, save the file called “EmployeeDemographics.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “EmployeeDemographics.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demodata &lt;- read_csv(&quot;EmployeeDemographics.csv&quot;) ## Rows: 163 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Sex, RaceEthnicity, Veteran ## dbl (4): EmployeeID, OrgTenureYrs_2019, JobLevel, AgeYrs_2019 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(demodata) ## [1] &quot;EmployeeID&quot; &quot;OrgTenureYrs_2019&quot; &quot;JobLevel&quot; &quot;Sex&quot; &quot;RaceEthnicity&quot; &quot;AgeYrs_2019&quot; ## [7] &quot;Veteran&quot; Note in the data frame that the EmployeeID variable (i.e., column, field) is a unique identifier variable, and each row contains an individual employee’s demographic data on the following variables: organizational tenure (OrgTenureYrs_2019), job level (JobLevel), sex (Sex), race/ethnicity (RaceEthnicity), age (AgeYrs_2019), and veteran status (Veteran). 25.2.4 Two-Way Cross-Tabulation A two-way cross-tabulation summarizes the association between two categorical (i.e., nominal, ordinal) variables. Using the data found in the data frame we named demodata, we will begin by creating two-way cross-tabulations using the categorical JobLevel and Sex variables. I’ll demonstrate how to create two-way cross-tabulations using three different functions (i.e., table, xtabs, CrossTable), and you can choose to follow along with all three or just one or two. 25.2.4.1 Option 1: Using the table Function Using the table function from base R, let’s create a two-way cross-tabulation table containing frequencies based on the JobLevel and Sex variables contained in the demodata data frame object. To begin, type the name of the table function. As the first argument in the function, specify the name of the data frame object (demodata) followed by the $ operator and the name of the first variable (JobLevel). As the second argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the second variable (Sex). # Create two-way cross-tabulation table from JobLevel and Sex variables table(demodata$JobLevel, demodata$Sex) ## ## Female Male ## 1 47 35 ## 2 22 22 ## 3 7 11 ## 4 3 7 ## 5 1 8 As you can see, the levels of the JobLevel variable (i.e., 1-5) appear as the row labels, and the categories of the Sex variable (i.e., Female, Male) appear as the column labels. If we wanted the categories of the Sex variable to appear as the row labels and the levels of the JobLevel variable to appear as the column labels, we would reverse the order of the two variables in our table function. Each “cell” in the cross-tabulation table contains the frequency (i.e., count) of employees who are in the intersecting categories. For example, the table shows that 47 female employees are in job level 1, whereas 35 male employees are in job level 2. Because there are five levels associated with the JobLevel variable (i.e., 1-5) and two levels associated with the Sex variable (i.e., Female, Male), the 5x2 cross-tabulation table has a total of 10 cells. Using the same code as above, let’s assign the cross-tabulation table we created to an object that we’ll (arbitrarily) call table_2. We’ll use the &lt;- operator to do this. # Assign two-way cross-tabulation table to object table_2 &lt;- table(demodata$JobLevel, demodata$Sex) Using the cross-tabulation table object we created above (table_2), we can apply the prop.table function from base R to estimate the proportions in each table cell. To calculate the cell proportions, simply type the name of the prop.table function, and as the only parenthetical argument, type the name of the cross-tabulation table object we created above (table_2). # Present the cell proportions for the cross-tabulation table prop.table(table_2) ## ## Female Male ## 1 0.288343558 0.214723926 ## 2 0.134969325 0.134969325 ## 3 0.042944785 0.067484663 ## 4 0.018404908 0.042944785 ## 5 0.006134969 0.049079755 When inspecting the cell proportion table displayed above, you might find it challenging to read given that number of decimal places after zero that are reported. To round values to 2 places after the decimal, let’s wrap the code from above in the round function from base R. As the first argument, let’s copy in our prop.table code from above, and as the second argument, let’s type the number of decimals after zero to which we wish to round (e.g., 2). # Round values to 2 places after decimal round(prop.table(table_2), 2) ## ## Female Male ## 1 0.29 0.21 ## 2 0.13 0.13 ## 3 0.04 0.07 ## 4 0.02 0.04 ## 5 0.01 0.05 If you were to sum all of the proportions in the table, you would get a total of 1 (100%). To demonstrate, let’s apply the sum function base R by wrapping our prop.table function code in the sum function. # Round values to 2 places after decimal sum(prop.table(table_2)) ## [1] 1 As expected, all of the proportions in the table sum to 1 (100%). What if we wish to compute the proportions by row or by column? Well, to compute those row-by-row or column-by-column proportions, we simply add an argument to the prop.table function. To compute the row proportions, enter 1 as the second argument. # Print the row proportions for the cross-tabulation table prop.table(table_2, 1) ## ## Female Male ## 1 0.5731707 0.4268293 ## 2 0.5000000 0.5000000 ## 3 0.3888889 0.6111111 ## 4 0.3000000 0.7000000 ## 5 0.1111111 0.8888889 In the output, you can see that the proportions in each row now sum to 1 (100%). Now, let’s round the row proportions to 2 places after the decimal. # Round cross-tabulation table values to 2 places after decimal round(prop.table(table_2, 1), 2) ## ## Female Male ## 1 0.57 0.43 ## 2 0.50 0.50 ## 3 0.39 0.61 ## 4 0.30 0.70 ## 5 0.11 0.89 Next, let’s convert those proportions to percentages by multiplying the previous code by 100. If you recall, the multiplication operator in R is the * symbol. Just be sure to remember that the values presented in the subsequent output represent percentages and not raw frequencies (i.e., counts). # Convert proportions to percentages by multiplying by 100 100 * round(prop.table(table_2, 1), 2) ## ## Female Male ## 1 57 43 ## 2 50 50 ## 3 39 61 ## 4 30 70 ## 5 11 89 We can retain two digits after the decimal by re-specifying the code from above in the following way. # Convert proportions to percentages by multiplying by 100 round(100 * prop.table(table_2, 1), 2) ## ## Female Male ## 1 57.32 42.68 ## 2 50.00 50.00 ## 3 38.89 61.11 ## 4 30.00 70.00 ## 5 11.11 88.89 Alternatively, we can compute the column proportions by typing 2 instead of 1 in the second argument of the prop.table function. As you can see below, each column now adds up to 1 (100%). # Print the row proportions for the cross-tabulation table prop.table(table_2, 2) ## ## Female Male ## 1 0.58750000 0.42168675 ## 2 0.27500000 0.26506024 ## 3 0.08750000 0.13253012 ## 4 0.03750000 0.08433735 ## 5 0.01250000 0.09638554 Now, let’s multiply by 100 to convert the proportions to percentages and round to 2 places after the decimal. # Round table values to 2 places after decimal and convert to percentages round(100 * prop.table(table_2, 2), 2) ## ## Female Male ## 1 58.75 42.17 ## 2 27.50 26.51 ## 3 8.75 13.25 ## 4 3.75 8.43 ## 5 1.25 9.64 25.2.4.2 Option 2: Using the xtabs Function The xtabs function from base R can also be used to create a two-way cross-tabulation table. To begin, type the name of the xtabs function. As the first argument in the function parentheses, insert the tilde (~) operator followed by the name of the first variable (JobLevel), the addition (+) operator, and the name of the second variable (Sex). As the second argument, type data= followed by the name of the data frame object two which the aforementioned variables belong (demodata). # Create cross-tabulation table from JobLevel and Sex variables xtabs(~ JobLevel + Sex, data=demodata) ## Sex ## JobLevel Female Male ## 1 47 35 ## 2 22 22 ## 3 7 11 ## 4 3 7 ## 5 1 8 Using the same code as above, let’s assign the cross-tabulation table we created to an object that we’ll (arbitrarily) call table_2. We’ll use the &lt;- operator to do this. # Assign two-way cross-tabulation table to object table_2 &lt;- xtabs(~ JobLevel + Sex, data=demodata) Using the cross-tabulation table object we created above (table_2), we can apply the prop.table function from base R to estimate the proportions in each table cell. To calculate the cell proportions, simply type the name of the prop.table function, and as the only parenthetical argument, type the name of the cross-tabulation table object we created above (table_2). # Print the cell proportions for the cross-tabulation table prop.table(table_2) ## Sex ## JobLevel Female Male ## 1 0.288343558 0.214723926 ## 2 0.134969325 0.134969325 ## 3 0.042944785 0.067484663 ## 4 0.018404908 0.042944785 ## 5 0.006134969 0.049079755 When inspecting the cell proportion table displayed above, you might find it challenging to read given that number of decimal places after zero that are reported. To round values to 2 places after the decimal, let’s wrap the code from above in the round function from base R. As the first argument, let’s copy in our prop.table code from above, and as the second argument, let’s type the number of decimals after zero to which we wish to round (e.g., 2). # Round values to 2 places after decimal round(prop.table(table_2), 2) ## Sex ## JobLevel Female Male ## 1 0.29 0.21 ## 2 0.13 0.13 ## 3 0.04 0.07 ## 4 0.02 0.04 ## 5 0.01 0.05 If you were to sum all of the proportions in the table, you would get a total of 1 (100%). To demonstrate, let’s apply the sum function base R by wrapping our prop.table function code in the sum function. # Round values to 2 places after decimal sum(prop.table(table_2)) ## [1] 1 As expected, all of the proportions in the table sum to 1 (100%). What if we wish to compute the proportions by row or by column? Well, to compute those row-by-row or column-by-column proportions, we simply add an argument to the prop.table function. To compute the row proportions, enter 1 as the second argument. # Print the row proportions for the cross-tabulation table prop.table(table_2, 1) ## Sex ## JobLevel Female Male ## 1 0.5731707 0.4268293 ## 2 0.5000000 0.5000000 ## 3 0.3888889 0.6111111 ## 4 0.3000000 0.7000000 ## 5 0.1111111 0.8888889 In the output, you can see that the proportions in each row now sum to 1 (100%). Now, let’s round the row proportions to 2 places after the decimal. # Round cross-tabulation table values to 2 places after decimal round(prop.table(table_2, 1), 2) ## Sex ## JobLevel Female Male ## 1 0.57 0.43 ## 2 0.50 0.50 ## 3 0.39 0.61 ## 4 0.30 0.70 ## 5 0.11 0.89 Next, let’s convert those proportions to percentages by multiplying the previous code by 100. If you recall, the multiplication operator in R is the * symbol. Just be sure to remember that the values presented in the subsequent output represent percentages and not raw frequencies (i.e., counts). # Convert proportions to percentages by multiplying by 100 100 * round(prop.table(table_2, 1), 2) ## Sex ## JobLevel Female Male ## 1 57 43 ## 2 50 50 ## 3 39 61 ## 4 30 70 ## 5 11 89 Alternatively, we can compute the column proportions by typing 2 instead of 1 in the second argument of the prop.table function. As you can see below, each column now adds up to 1 (100%). # Print the row proportions for the table prop.table(table_2, 2) ## Sex ## JobLevel Female Male ## 1 0.58750000 0.42168675 ## 2 0.27500000 0.26506024 ## 3 0.08750000 0.13253012 ## 4 0.03750000 0.08433735 ## 5 0.01250000 0.09638554 Now, let’s round to 2 places after the decimal and multiply by 100 to convert the proportions to percentages. # Round table values to 2 places after decimal and convert to percentages 100 * round(prop.table(table_2, 2), 2) ## Sex ## JobLevel Female Male ## 1 59 42 ## 2 28 27 ## 3 9 13 ## 4 4 8 ## 5 1 10 25.2.4.3 Option 3: Using the CrossTable Function The CrossTable function from the gmodels package is another option when it comes to creating a two-way cross-tabulation table. The function includes multiple arguments that can eliminate additional steps (e.g., rounding) that would be required when using the table or xtabs function from base R. Before using the CrossTable function, we must install and access the gmodels package (Warnes et al. 2018) using the install.packages and library functions, respectively. The downside of the CrossTable function is that it can only be used to create two-way cross-tabulation tables. # Install gmodels package if you haven&#39;t already install.packages(&quot;gmodels&quot;) # Access gmodels package library(gmodels) To create a two-way cross-tabulation table using the JobLevel and Sex variables, type the name of the CrossTable function. As the first argument in the function parentheses, type the name of first variable you wish to use to make the table, and use the $ symbol to indicate that the variable (JobLevel) belongs to the data frame in question (demodata), which should look like this: demodata$JobLevel. As the second argument, type the name of the second variable you wish to use to make the table, and use the $ symbol to indicate that the variable (Sex) belongs to the data frame in question (demodata), which should look like this: demodata$Sex. Make sure you use a comma (,) to separate the two arguments. # Create cross-tabulation table from JobLevel and Sex variables from demodata data frame CrossTable(demodata$JobLevel, demodata$Sex) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 163 ## ## ## | demodata$Sex ## demodata$JobLevel | Female | Male | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 47 | 35 | 82 | ## | 1.134 | 1.093 | | ## | 0.573 | 0.427 | 0.503 | ## | 0.588 | 0.422 | | ## | 0.288 | 0.215 | | ## ------------------|-----------|-----------|-----------| ## 2 | 22 | 22 | 44 | ## | 0.008 | 0.007 | | ## | 0.500 | 0.500 | 0.270 | ## | 0.275 | 0.265 | | ## | 0.135 | 0.135 | | ## ------------------|-----------|-----------|-----------| ## 3 | 7 | 11 | 18 | ## | 0.381 | 0.367 | | ## | 0.389 | 0.611 | 0.110 | ## | 0.087 | 0.133 | | ## | 0.043 | 0.067 | | ## ------------------|-----------|-----------|-----------| ## 4 | 3 | 7 | 10 | ## | 0.742 | 0.715 | | ## | 0.300 | 0.700 | 0.061 | ## | 0.037 | 0.084 | | ## | 0.018 | 0.043 | | ## ------------------|-----------|-----------|-----------| ## 5 | 1 | 8 | 9 | ## | 2.644 | 2.548 | | ## | 0.111 | 0.889 | 0.055 | ## | 0.012 | 0.096 | | ## | 0.006 | 0.049 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 80 | 83 | 163 | ## | 0.491 | 0.509 | | ## ------------------|-----------|-----------|-----------| ## ## The resulting cross-tabulation table is packed with information! Fortunately, there is a key titled “Cell Contents” that explains how to interpret the value displayed in each row within each cell. As you can see, by default, the table displays the raw frequencies (i.e., counts), the proportions by row, the proportions by column, and the overall cell proportions – and we only had to use a single function! In some cases, you may wish to reduce the amount of information displayed. To find additional documentation, use the help (?) feature for the CrossTable function. # Access background information on function ?CrossTable Once the Help window opens, you can explore the different arguments that can be used within the function to change the default settings. The default number of digits displayed after the decimal is 3, but let’s change to 2 by adding the argument digits=2. Next, let’s add the following arguments: (a) prop.r=FALSE to hide the row proportions, (b) prop.c=FALSE to hide the column proportions, (c) prop.t=TRUE to keep the total proportions visible, and (d) prop.chisq=FALSE to hide the chi-square contribution of each cell. # Create cross-tabulation table from JobLevel and Sex variables from demodata data frame CrossTable(demodata$JobLevel, demodata$Sex, digits=2, prop.r=FALSE, prop.c=FALSE, prop.t=TRUE, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 163 ## ## ## | demodata$Sex ## demodata$JobLevel | Female | Male | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 47 | 35 | 82 | ## | 0.29 | 0.21 | | ## ------------------|-----------|-----------|-----------| ## 2 | 22 | 22 | 44 | ## | 0.13 | 0.13 | | ## ------------------|-----------|-----------|-----------| ## 3 | 7 | 11 | 18 | ## | 0.04 | 0.07 | | ## ------------------|-----------|-----------|-----------| ## 4 | 3 | 7 | 10 | ## | 0.02 | 0.04 | | ## ------------------|-----------|-----------|-----------| ## 5 | 1 | 8 | 9 | ## | 0.01 | 0.05 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 80 | 83 | 163 | ## ------------------|-----------|-----------|-----------| ## ## 25.2.5 Three-Way Cross-Tabulation A three-way cross-tabulation table summarizes the association between three categorical (i.e., nominal, ordinal) variables. Using the data found in the data frame we named demodata, we will begin by creating two-way cross-tabulation tables using the categorical JobLevel, Sex, and RaceEthnicity variables. I’ll demonstrate how to create three-way cross-tabulation tables using two different functions (i.e., table, xtabs), and you can choose to follow along with all three or just one or two. 25.2.5.1 Option 1: Using the table Function Using the table function from base R, let’s create a three-way cross-tabulation table containing frequencies based on the JobLevel, Sex, and RaceEthnicity variables contained in the demodata data frame object. To begin, type the name of the table function. As the first argument in the function, specify the name of the data frame object (demodata) followed by the $ operator and the name of the first variable (JobLevel). As the second argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the second variable (Sex). As the third argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the third variable (RaceEthnicity). # Create three-way cross-tabulation table from JobLevel, Sex, and RaceEthnicity variables table(demodata$JobLevel, demodata$Sex, demodata$RaceEthnicity) ## , , = Asian ## ## ## Female Male ## 1 14 11 ## 2 6 10 ## 3 1 6 ## 4 0 5 ## 5 0 3 ## ## , , = Black ## ## ## Female Male ## 1 2 0 ## 2 0 1 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## ## , , = HispanicLatino ## ## ## Female Male ## 1 14 14 ## 2 3 5 ## 3 3 2 ## 4 3 2 ## 5 0 3 ## ## , , = White ## ## ## Female Male ## 1 17 10 ## 2 13 6 ## 3 3 3 ## 4 0 0 ## 5 1 2 As you can see, when used to create a three-way cross-tabulation table, the table function creates one two-way cross-tabulation table based on the first two variables for each category or level of the third variable. Let’s assign this table to an object that we’ll call table3. # Assign three-way cross-tabulation table to object table_3 &lt;- table(demodata$JobLevel, demodata$Sex, demodata$RaceEthnicity) If you would prefer to view the frequencies (i.e., counts) in a single table, then use the ftable function from base R. Simply type the name of the table object we created in the previous step (table_3) as the sole argument in the ftable function. # Print three-way cross-tabulation table in a different format ftable(table_3) ## Asian Black HispanicLatino White ## ## 1 Female 14 2 14 17 ## Male 11 0 14 10 ## 2 Female 6 0 3 13 ## Male 10 1 5 6 ## 3 Female 1 0 3 3 ## Male 6 0 2 3 ## 4 Female 0 0 3 0 ## Male 5 0 2 0 ## 5 Female 0 0 0 1 ## Male 3 0 3 2 Just as we did with the two-way tables above, you can also apply the prop.table function and round functions to this three-way cross-tabulation table object. 25.2.5.2 Option 2: Using the xtabs Function The xtabs function from base R can also be used to create a three-way cross-tabulation table. To begin, type the name of the xtabs function. As the first argument in the function parentheses, insert the tilde (~) operator followed by the name of the first variable (JobLevel), the addition (+) operator, and the name of the second variable (Sex), the addition (+) operator, and the name of the third variable (RaceEthnicity). As the second argument, type data= followed by the name of the data frame object two which the aforementioned variables belong (demodata). # Create cross-tabulation table from JobLevel and Sex variables xtabs(~ JobLevel + Sex + RaceEthnicity, data=demodata) ## , , RaceEthnicity = Asian ## ## Sex ## JobLevel Female Male ## 1 14 11 ## 2 6 10 ## 3 1 6 ## 4 0 5 ## 5 0 3 ## ## , , RaceEthnicity = Black ## ## Sex ## JobLevel Female Male ## 1 2 0 ## 2 0 1 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## ## , , RaceEthnicity = HispanicLatino ## ## Sex ## JobLevel Female Male ## 1 14 14 ## 2 3 5 ## 3 3 2 ## 4 3 2 ## 5 0 3 ## ## , , RaceEthnicity = White ## ## Sex ## JobLevel Female Male ## 1 17 10 ## 2 13 6 ## 3 3 3 ## 4 0 0 ## 5 1 2 As you can see, when used to create a three-way cross-tabulation table, the xtabs function creates one two-way cross-tabulation table based on the first two variables for each category or level of the third variable. Let’s assign this table to an object that we’ll call table3. # Assign three-way cross-tabulation table to object table_3 &lt;- xtabs(~ JobLevel + Sex + RaceEthnicity, data=demodata) If you would prefer to view the frequencies (i.e., counts) in a single table, then use the ftable function from base R. Simply type the name of the table object we created in the previous step (table_3) as the sole argument in the ftable function. # Print three-way cross-tabulation table in a different format ftable(table_3) ## RaceEthnicity Asian Black HispanicLatino White ## JobLevel Sex ## 1 Female 14 2 14 17 ## Male 11 0 14 10 ## 2 Female 6 0 3 13 ## Male 10 1 5 6 ## 3 Female 1 0 3 3 ## Male 6 0 2 3 ## 4 Female 0 0 3 0 ## Male 5 0 2 0 ## 5 Female 0 0 0 1 ## Male 3 0 3 2 Just as we did with the two-way tables above, you can also apply the prop.table function and round functions to this three-way table object. 25.2.6 Summary In this chapter, we learned how to create two-way cross-tabulation tables using the table and xtabs functions from base R and the CrossTable function from the gmodels package. In addition, we learned how to create three-way cross-tabulation tables using the table and xtabs. References "],["pivottables.html", "Chapter 26 Applying Pivot Tables to Explore Employee Demographic Data 26.1 Conceptual Overview 26.2 Tutorial", " Chapter 26 Applying Pivot Tables to Explore Employee Demographic Data If you are a Microsoft Excel user, you are probably familiar with the infamous pivot table (or PivotTable). If you’re not familiar with Excel, you are likely asking yourself the question: What is a pivot table? A pivot table is an interactive type of table that allows one to manipulate, arrange, or “pivot” data and descriptive and statistics interactively. The pivot table can be a useful tool for exploring and processing data, particularly employee demographic data. 26.1 Conceptual Overview If you’re looking for a conceptual overview of pivot tables, check out this resource created by Microsoft. 26.2 Tutorial This chapter’s tutorial demonstrates how to create pivot tables that you can share with others via interactive HTML files. 26.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/98UvbWW6fLo 26.2.2 Functions &amp; Packages Introduced Function Package N/A htmlwidgets N/A knitr rpivotTable rpivotTable c base R 26.2.3 Initial Steps If you haven’t already, save the file called “EmployeeDemographics.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “EmployeeDemographics.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demodata &lt;- read_csv(&quot;EmployeeDemographics.csv&quot;) ## Rows: 163 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Sex, RaceEthnicity, Veteran ## dbl (4): EmployeeID, OrgTenureYrs_2019, JobLevel, AgeYrs_2019 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(demodata) ## [1] &quot;EmployeeID&quot; &quot;OrgTenureYrs_2019&quot; &quot;JobLevel&quot; &quot;Sex&quot; &quot;RaceEthnicity&quot; &quot;AgeYrs_2019&quot; ## [7] &quot;Veteran&quot; Note in the data frame that the EmployeeID variable (i.e., column, field) is a unique identifier variable, and each row contains an individual employee’s demographic data on the following variables: organizational tenure (OrgTenureYrs_2019), job level (JobLevel), sex (Sex), race/ethnicity (RaceEthnicity), age (AgeYrs_2019), and veteran status (Veteran). 26.2.4 Create a Pivot Table Creating a pivot table is relatively simple. Before doing so, however, we need to make sure the following packages are installed: htmlwidgets (Vaidyanathan et al. 2021), knitr (Xie 2015b), and rpivotTable (Martoglio 2018). # Install htmlwidgets package if you haven&#39;t already install.packages(&quot;htmlwidgets&quot;) # Install knitr package if you haven&#39;t already install.packages(&quot;knitr&quot;) # Install rpivotTable package if you haven&#39;t already install.packages(&quot;rpivotTable&quot;) Now that you’ve installed those packages, you only need to access the rpivotTable package. # Access rpivotTable package library(rpivotTable) The simplest way to create a pivot table is to enter the name of the data frame of interest (demodata) as the sole argument in the function parentheses. The interactive pivot table will likely appear in the Plots window of the RStudio interface. You can expand the size of the window to see the entire pivot table Just like a pivot table in Excel, you can drag variables to the rows and columns areas and select the type of object (e.g., Table, Treemap, Bar Chart) and descriptive statistic (e.g., Count, Average) from the dropdown menus. # Create pivot table rpivotTable(demodata) The pivot table can also be exported to an HTML file, which allows one to open it as an interactive web browser, and even better, you can send the HTML file to colleagues, which allows them to use the pivot table without having access to or changing the source data. In the Viewer window of RStudio, select the Export drop-down menu, followed by Save as Web Page…. You may be prompted to downloaded additional required packages; please click “Yes” to download these packages if you would like to save the pivot table as an HTML file. Once you have done that, you can proceed to save the file in your working directory or elsewhere. When you open (e.g., double-click on) the HTML file in the folder in which you saved it, a web browser will open with your interactive pivot table. You can also preemptively specify the row variable(s), column variable(s), and descriptive statistic in the rpivotTable function itself. This allows you to keep a record of the different “pivots” you have applied and avoid having to drag-and-drop and point-and-click in the interactive interface. To add these specifications, we just add additional arguments (separated by commas) to the rpivotTable function. First, let’s type rows=\"JobLevel\" (make sure the variable is in quotation marks) to set the JobLevel variable as a row. Second, let’s type cols=\"Sex\" to set the Sex variable as a column. # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=&quot;Sex&quot;) Using the c (combine) function within the rpivotTable function, we can specify multiple row or column variables as a vector. For example, let’s build upon the previous syntax by adding RaceEthnicity as another column variable by typing cols=c(\"Sex\", \"RaceEthnicity\"). # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=c(&quot;Sex&quot;, &quot;RaceEthnicity&quot;)) Building upon the previous syntax even further, let’s add the argument rendererName=\"Heatmap\" to specify that we want the pivot table to be rendered as a heatmap display. # Access rpivotTable package rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=c(&quot;Sex&quot;, &quot;RaceEthnicity&quot;), rendererName=&quot;Heatmap&quot;) We can also render the pivot table as a treemap, where a treemap is type of data visualization in which hierarchical data are displayed as nested rectangles, such that the area of each rectangle correlates with its relative quantitative value. To create the treemap, let’s specify rows=c(\"Sex\", \"JobLevel\") to indicate that the row variables are Sex and JobLevel. Next, specify rendererName=\"Treemap\" to indicate that we want the pivot table to be rendered as a treemap. # Create pivot table rpivotTable(demodata, rows=c(&quot;Sex&quot;, &quot;JobLevel&quot;), rendererName=&quot;Treemap&quot;) Now it’s time to play around with applying a different type of descriptive (aggregate) statistic (as opposed to the default count (i.e., frequency) statistic). First, type rows=\"JobLevel\" to specify JobLevel as the row variable. Second, type cols=\"Sex\" to specify Sex as the column variable. Third, type rendererName=\"Bar Chart\" to request that the pivot table be rendered as a Bar Chart. Fourth, type aggregatorName=\"Average\" to specify that the average (mean) be computed for values (vals) on a specific variable (see next). Finally, type vals=\"AgeYrs_2019\" to specify the variable (AgeYrs_2019) to be used for the calculation of the average. # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=&quot;Sex&quot;, rendererName=&quot;Bar Chart&quot;, aggregatorName=&quot;Average&quot;, vals=&quot;AgeYrs_2019&quot;) 26.2.5 Summary In this chapter, we learned how to create interactive HTML pivot tables using the rpivotTable function from the rpivotTable package. References "],["employeesurveys.html", "Chapter 27 Introduction to Employee Surveys 27.1 Chapters Included", " Chapter 27 Introduction to Employee Surveys As introduced in the chapter on the Data Acquisition phase of the HR Analytics Project Life Cycle, the employee survey is a very common data-acquisition tool in HR analytics and can be used to gather information on employees’ perceptions of their thoughts, feelings, and behaviors – as well as on behaviors of others. They consist of items (e.g., questions) to which employees are asked to respond, and items can be open-ended (e.g., “Please describe our onboarding experience.”) or close-ended with fixed response options (e.g., “I am satisfied with my job.” [1 = Strongly Disagree, 5 = Strongly Agree]). Further, employee surveys can vary in length and purpose, ranging from shorter yet more frequent pulse surveys on a specific phenomenon to longer yet less frequent annual engagement surveys. Like any data-acquisition tool, we must be very diligent about evaluating the quality of the data obtained using an employee survey. In fact, one of the best approaches is to design a survey that will acquire exactly the data we wish to acquire and that many employees respond to in an honest and thoughtful manner. That being said, we should always carefully clean and evaluate the data acquired through an employee survey, which includes estimating the reliability of measures included in the survey. 27.1 Chapters Included In the following chapters, you will have an opportunity to learn how to work with data from employee surveys in various ways. Aggregating &amp; Segmenting Employee Survey Data Estimating Internal Consistency Reliability Using Cronbach’s alpha Creating a Composite Variable Based on a Multi-Item Measure "],["aggregatesegment.html", "Chapter 28 Aggregating &amp; Segmenting Employee Survey Data 28.1 Conceptual Overview 28.2 Tutorial 28.3 Chapter Supplement", " Chapter 28 Aggregating &amp; Segmenting Employee Survey Data In this chapter, we will learn how to aggregate and segment data from an employee survey. Respectively, these two processes allow us to examine data at a higher level of analysis and to examine data by group or cluster. 28.1 Conceptual Overview Aggregation refers to the process of reporting data at a higher level of analysis, where level of analyses might include teams, units, facilities, locations, or organization levels. In many instances, we report aggregate results (e.g., mean, standard deviation, counts) based on an entire sample drawn from a larger population; however, in some instances, we might wish to create a new variable within a data frame that represents descriptive (i.e., summary) statistics for a clusters of cases (e.g., teams of employees), as we might be interested in understanding subsamples. In other instances, we might aggregate data to a higher level of analysis because we are interested in analyzing associations or differences at that higher level. For example, we might aggregate employees’ engagement scores to the unit level in order to analyze whether significant differences in unit-level employee engagement exist. Sometimes the process of summarizing a variable by groups is referred to as segmentation. For example, after acquiring performance data for employees, we may wish to create a new variable that represents the average (i.e., mean) level of performance for each work team’s employees. It is important to note that aggregation does not always imply measures of central tendency like the mean, median, or mode; rather, we can summarize clusters of data in other ways, such as by the minimum or maximum value within a cluster of cases (e.g., performance of worst performing team member), the number of cases within a cluster (e.g., number of members of a team), or the dispersion (i.e., spread) of variable values for each cluster (e.g., standard deviation of performance values for members of a team). In terms of measurement scale, a grouping variable will typically be nominal or ordinal (i.e., categorical). 28.2 Tutorial This chapter’s tutorial demonstrates how to aggregate and segment employee survey data in R. 28.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/EdfeRpQtF20 28.2.2 Functions &amp; Packages Introduced Function Package group_by dplyr summarize dplyr n dplyr n_distinct dplyr mean base R sd base R median base R var base R min base R max base R mutate dplyr ungroup dplyr str base R as.data.frame base R Histogram lessR BarChart dplyr 28.2.3 Initial Steps If you haven’t already, save the file called “EmployeeSurveyData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “EmployeeSurveyData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object EmpSurvData &lt;- read_csv(&quot;EmployeeSurveyData.csv&quot;) ## Rows: 156 Columns: 19 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmployeeID, Unit, Supervisor ## dbl (16): JobSat1, JobSat2, JobSat3, TurnInt1, TurnInt2, TurnInt3, Engage1, Engage2, Engage3, Engage4, Engage5, ExpIncivil1, ExpInciv... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; ## [10] &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; # Print number of rows in data frame (tibble) object nrow(EmpSurvData) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(EmpSurvData) ## # A tibble: 6 × 19 ## EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 Engage5 ExpIncivil1 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EID294 Marketing EID373 3 3 3 3 3 3 2 1 2 2 3 2 ## 2 EID295 Marketing EID373 2 3 2 3 4 2 2 1 3 3 3 2 ## 3 EID296 Marketing EID373 2 2 3 3 3 3 1 1 2 1 2 2 ## 4 EID301 Marketing EID367 2 2 3 4 4 4 3 2 3 2 2 3 ## 5 EID306 Marketing EID367 2 2 2 4 4 4 3 2 3 4 3 2 ## 6 EID213 HumanReso… EID370 4 3 4 3 3 3 3 2 4 3 3 2 ## # ℹ 4 more variables: ExpIncivil2 &lt;dbl&gt;, ExpIncivil3 &lt;dbl&gt;, ExpIncivil4 &lt;dbl&gt;, ExpIncivil5 &lt;dbl&gt; The data for this exercise include employees’ unique identifiers (EmployeeID), the unit they work in (unit), their direct supervisor (Supervisor), and annual employee survey responses to three job satisfaction items (JobSat1, JobSat2_rev, JobSat3), three turnover intentions items (TurnInt1, TurnInt2, TurnInt3), five engagement items (Engage1, Engage2, Engage3, Engage4, Engage5), and five exposure to incivility items (ExpIncivil1, ExpIncivil2, ExpIncivil3, ExpIncivil4, ExpIncivil5_rev). All response scales are 5 points, ranging from strongly disagree (1) to strongly agree (5). 28.2.4 Counts By Group Let’s begin by learning how to summarize data in aggregate. When we don’t have any grouping variables of interest, we can simply compute descriptive statistics, such as the mean, standard deviation, or count for each variable of interest, and this will compute the descriptive statistics at the sample level. For our purposes, however, we will summarize data in accordance with a grouping (i.e., clustering) variable, which will yield aggregate estimates for each group. Specifically, we will summarize how many employees (who responded to the engagement survey) work in each unit. To do so, we will use the group_by, summarize, n, and n_distinct functions from the dplyr package (Wickham et al. 2023), so if you haven’t already, be sure to install and access that package before proceeding. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) I will demonstrate two approaches for applying the group_by, summarize, n, and n_distinct functions from dplyr. The first option uses “pipe(s),” which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2022), on which the dplyr is partially dependent. In short, a pipe allows one to code more efficiently and to improve the readability of an overall script under certain conditions. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. The second option is more traditional and lacks the efficiency and readability of pipes. You can use either approach, and if don’t you want to use pipes, skip to the section below called Without Pipes. For more information on the pipe operator, check out this link. 28.2.4.1 With Pipes We’ll begin by using an approach with the pipe (%&gt;%) operator. Type the name of our data frame object, which we previously named EmpSurvData, followed by the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. Either on the same line or on the next line, type the name of the group_by function, and within the parentheses, type Unit as the argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator in order to pipe the results of the group_by function to the subsequent function. Type the summarize function, and within the parentheses, type a new name (of your choosing) for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). The summarize function is most commonly used to describe/summarize data in some way (e.g., counts) that has been aggregated by the group_by function. The n function can be used within the summarize function to count the number of cases per group. Finally, type another pipe (%&gt;%) operator, followed by the function ungroup() (with no arguments in the parentheses); it’s good to get in the habit of ungrouping the data, particularly if you want to subsequently look at the data without grouping applied. # Counts by groups - n function (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(UnitCount = n()) %&gt;% ungroup() ## # A tibble: 5 × 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 Of note, we could arrive at the same output by using the n_distinct function instead of the n function. The n_distinct function quickly counts the number of unique values (e.g., EID294, EID295) in a variable (e.g., EmployeeID), and when preceded by the group_by function, it counts the number of unique values within each group. The n_distinct function is really handy when unique entities (e.g., employees) each have multiple rows of data, which (depending on how your data are structured) could be the case if you were to have a long-format data frame containing multiple survey administrations for employees over time. Using the function can reveal how many unique employees there are, even when some employees have multiple rows of data. In sum, the n function counts the number of cases (i.e., rows), and the n_distinct function counts the number of unique values. # Counts by groups - n_distinct function (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(UnitCount = n_distinct(EmployeeID)) %&gt;% ungroup() ## # A tibble: 5 × 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 If we wish to summarize by two or more grouping variables, then we just need to add those additional grouping variables to the group_by function. For example, using the same code from above, let’s add the Supervisor nominal variable as a second argument to the group_by function. # Counts by two groups - n_distinct function (with pipes) EmpSurvData %&gt;% group_by(Unit, Supervisor) %&gt;% summarize(UnitCount = n_distinct(EmployeeID)) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;Unit&#39;. You can override using the `.groups` argument. ## # A tibble: 16 × 3 ## Unit Supervisor UnitCount ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 HumanResources EID370 10 ## 2 HumanResources EID379 1 ## 3 Manufacturing EID368 17 ## 4 Manufacturing EID369 1 ## 5 Manufacturing EID371 7 ## 6 Manufacturing EID375 6 ## 7 Manufacturing EID380 19 ## 8 Manufacturing EID381 1 ## 9 Manufacturing EID382 21 ## 10 Marketing EID367 10 ## 11 Marketing EID373 9 ## 12 ResearchDevelopment EID372 18 ## 13 Sales EID372 19 ## 14 Sales EID374 3 ## 15 Sales EID376 11 ## 16 Sales EID377 3 28.2.4.2 Without Pipes We can achieve the same output as above without using the pipe (%&gt;%) operator. To begin, type the name of the summarize function. As the first argument within the summarize function, insert the group_by function with the name of our data frame (EmpSurvData) as the first argument and the name of the grouping variable as the second argument (Unit). Doing this informs the summarize function that the cases of interest is grouped by membership in the Unit variable. As the second argument within the summarize function, type a new name for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). # Counts by groups - n function (without pipes) summarize(group_by(EmpSurvData, Unit), UnitCount = n()) ## # A tibble: 5 × 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 In a similar fashion, we can apply the n_distinct function without pipes. # Counts by groups - n_distinct function (without pipes) summarize(group_by(EmpSurvData, Unit), UnitCount = n_distinct(EmployeeID)) ## # A tibble: 5 × 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 To summarize by two or more grouping variables, we can simply add additional grouping variables to the group_by function. As shown below, I add the Supervisor nominal variable as a third argument in the group_by function. # Counts by two groups - n_distinct function (without pipes) summarize(group_by(EmpSurvData, Unit, Supervisor), UnitCount = n_distinct(EmployeeID)) ## `summarise()` has grouped output by &#39;Unit&#39;. You can override using the `.groups` argument. ## # A tibble: 16 × 3 ## # Groups: Unit [5] ## Unit Supervisor UnitCount ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 HumanResources EID370 10 ## 2 HumanResources EID379 1 ## 3 Manufacturing EID368 17 ## 4 Manufacturing EID369 1 ## 5 Manufacturing EID371 7 ## 6 Manufacturing EID375 6 ## 7 Manufacturing EID380 19 ## 8 Manufacturing EID381 1 ## 9 Manufacturing EID382 21 ## 10 Marketing EID367 10 ## 11 Marketing EID373 9 ## 12 ResearchDevelopment EID372 18 ## 13 Sales EID372 19 ## 14 Sales EID374 3 ## 15 Sales EID376 11 ## 16 Sales EID377 3 28.2.5 Measures of Central Tendency and Dispersion By Group We can also aggregate data by computing the average of a variable for each group. Because measures of central tendency (e.g., mean, median) and dispersion (e.g., standard deviation, interquartile range) are estimated for continuous variables (i.e., variables with interval or ratio measurement scale), we will choose continuous variables that we wish to summarize in aggregate. For the sake of illustration, we will treat the JobSat1 and JobSat2 variables as having an interval measurement scale even though they would best be described as having an ordinal measurement scale; for a review of measurement scales, please refer to this section from a previous chapter. Further, we will use organizational unit (Unit) as our grouping variable. We’ll learn two approaches for aggregating data by computing the group average of a variable: with the pipe (%&gt;%) operator and without the pipe (%&gt;%) operator. 28.2.5.1 With Pipes Let’s begin by calculating the mean of JobSat1 for each value of the Unit variable using the pipe (%&gt;%) operator. Type the name of the data frame, which we previously named EmpSurvData, followed by the pipe (%&gt;%) operator. Type the name of the group_by function, and within the function parentheses, type Unit as the sole argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator. Type the name of the summarize function, and within the parentheses, type a new name (of your choosing) for a variable that will contain the the mean JobSat1 score for each organizational unit (Mean_JobSat1), and follow that with the = operator. After the = operator, type the name of the mean function from base R, and within it, type the name of our variable of interest (JobSat1) as the first argument; as the second argument, type na.rm=TRUE, which instructs R to exclude missing values for JobSat1 when computing the mean. # Means by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(Mean_JobSat1 = mean(JobSat1, na.rm=TRUE)) ## # A tibble: 5 × 2 ## Unit Mean_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 ## 2 Manufacturing 3.08 ## 3 Marketing 2.79 ## 4 ResearchDevelopment 3.28 ## 5 Sales 3.22 We can extend our code by estimating the unit-level means and standard deviations for JobSat1 and JobSat2 variables by using the mean and sd base R functions. # Means &amp; SDs by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mean_JobSat2 = mean(JobSat2, na.rm=TRUE), SD_JobSat2 = sd(JobSat2, na.rm=TRUE) ) ## # A tibble: 5 × 5 ## Unit Mean_JobSat1 SD_JobSat1 Mean_JobSat2 SD_JobSat2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.447 ## 2 Manufacturing 3.08 0.818 3.32 0.819 ## 3 Marketing 2.79 0.787 2.89 0.737 ## 4 ResearchDevelopment 3.28 1.02 3.17 0.924 ## 5 Sales 3.22 0.722 3.36 0.723 In addition to the mean and sd functions from base R, we can also apply the base R functions for median (median), variance (var), minimum (min), maximum (max), and interquartile range (IQR). # Multiple descriptive statistics by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 × 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopment 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 If we wish to save our work, we can assign the data frame (tibble) we generated to an object. Let’s call this new object agg_EmpSurvData and assign the results of our operations above to that object using the &lt;- assignment operator. # Multiple descriptive statistics by group (with pipes) # Assign to object agg_EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) Let’s print our new aggregated data frame object called agg_EmpSurvData using the print function. # Print data frame (tibble) object print(agg_EmpSurvData) ## # A tibble: 5 × 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopment 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 28.2.5.2 Without Pipes In my opinion, these operations can get a bit harder to read when the pipe (%&gt;%) operator is not used. That being said, some people prefer not to use pipes, and thus, I’ll demonstrate how to perform the same operations as above without that operator. Let’s begin by computing the JobSat1 means for each Unit value. Type the name of the summarize function. As the first argument in the summarize function, type the name of the group_by function, where the first argument of the group_by function should be the name of the data frame object (EmpSurvData), and the second argument should be the name of the grouping variable (Unit). As the second argument in the summarize function, type a new name (of your choosing) for a variable that will contain the the mean JobSat1 score for each organizational unit (Mean_JobSat1), and follow that with the = operator. After the = operator, type the name of the mean function from base R, and within it, type the name of our variable of interest (JobSat1) as the first argument; as the second argument, type na.rm=TRUE, which instructs R to exclude missing values for JobSat1 when computing the mean. # Means by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 × 2 ## Unit Mean_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 ## 2 Manufacturing 3.08 ## 3 Marketing 2.79 ## 4 ResearchDevelopment 3.28 ## 5 Sales 3.22 We can extend our code by estimating the unit-level means and standard deviations for JobSat1 and JobSat2 variables by using the mean and sd base R functions. # Means &amp; SDs by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mean_JobSat2 = mean(JobSat2, na.rm=TRUE), SD_JobSat2 = sd(JobSat2, na.rm=TRUE) ) ## # A tibble: 5 × 5 ## Unit Mean_JobSat1 SD_JobSat1 Mean_JobSat2 SD_JobSat2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.447 ## 2 Manufacturing 3.08 0.818 3.32 0.819 ## 3 Marketing 2.79 0.787 2.89 0.737 ## 4 ResearchDevelopment 3.28 1.02 3.17 0.924 ## 5 Sales 3.22 0.722 3.36 0.723 In addition to the mean and sd functions from base R, we could also apply the base R functions for median (median), variance (var), minimum (min), maximum (max), and interquartile range (IQR). # Multiple descriptive statistics by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 × 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopment 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 If we wish to save our work, we can assign the data frame (tibble) we generated to an object. Let’s call this new object agg_EmpSurvData and assign the results of our operations above to that object using the &lt;- assignment operator. # Multiple descriptive statistics by group (with pipes) # Assign to object agg_EmpSurvData &lt;- summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) Let’s print our new aggregated data frame object called agg_EmpSurvData using the print function. # Print data frame (tibble) object print(agg_EmpSurvData) ## # A tibble: 5 × 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopment 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 28.2.6 Add Variable to Data Frame Containing Aggregated Values In the previous section, we learned how to create a data frame (tibble) in which the data were aggregated to a higher level of analysis. In this section, we will learn how to add a variable to the existing data frame object that contains aggregated values. This second approach comes in handy when preparing the data to estimate certain types of multilevel models (e.g., multilevel regression, hierarchical linear models, random coefficients models). To add a new variable containing aggregated values, we can swap out the summarize function from the dplyr package (that we used in the previous section) with the mutate function, which is also from the dplyr package. The arguments remain the same, and as before, we can carry out this work with or without the use of the pipe (%&gt;%) operator. We do, however, need to create a new data frame or overwrite the existing data frame to incorporate the new variable. In this example, we will overwrite the existing EmpSurvData data frame by entering the name of that data frame, followed by &lt;- assignment operator and the appropriate code. We will use functions from the dplyr package as we did above, so if you haven’t already, make sure that you have installed and accessed that package before proceeding. 28.2.6.1 With Pipes Using the pipe (%&gt;%) operator, for the first example, let’s add a new variable that we will call UnitCount. The new variable will include the total number of employees within each respective organizational unit (Unit). Type the name of our data frame object, which we previously named EmpSurvData, followed by the &lt;- assignment operator, the name of the data frame object (EmpSurvData), and the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. Either on the same line or on the next line, type the group_by function, and within the function parentheses, type Unit as the argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator in order to pipe the results of the group_by function to the subsequent function. Type the name of the mutate function, and within the function parentheses, type a new name (of your choosing) for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). The mutate function is used to add a new variable to a data frame based on some type of operation or analysis. The n function can be used within the mutate function to count the number of cases per value of a grouping variable. Finally, type another pipe (%&gt;%) operator, followed by the function ungroup() (with no arguments in the parentheses); it’s good to get in the habit of ungrouping the data, particularly if you want to subsequently look at the data without grouping applied. # Add new variable based on counts by group (with pipes) EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% mutate(UnitCount = n()) %&gt;% ungroup() To verify that we added the new UnitCount variable to our EmpSurvData data frame object, let’s print the variable names using the names function from base R and then note the addition of the new UnitCount variable. # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; ## [10] &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; &quot;UnitCount&quot; Let’s work through another example in which we will add a new variable that contains values for the mean level of JobSat1 for each level of the Unit variable. Again, we will use the mutate function; however, we will apply the mean function from base R. Let’s call the new aggregated variable Mean_JobSat1. # Add new variable based on means by group (with pipes) EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% mutate(Mean_JobSat1 = mean(JobSat1, na.rm=TRUE)) %&gt;% ungroup() # Print variable names from data frame names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; ## [9] &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; ## [17] &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; 28.2.6.2 Without Pipes Let’s repeat the same processes as above, except this time without the the pipe (%&gt;%) operator. Type the name of our data frame, which we previously named EmpSurvData, followed by the &lt;- assignment operator. To the right of the &lt;- operator type the name of the mutate function. As the first argument of the mutate function, type the name of the group_by function. As the first argument of the group_by function, type the name of the data frame object (EmpSurvData). As the second argument of the group_by function, type the name of the grouping variable (Unit). Doing this informs the mutate function that the cases of interest are grouped by membership in the Unit variable. As the second argument in the mutate function, type a new name for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). On a new line, apply the ungroup function to the EmpSurvData data frame object; this final step, will ungroup the data frame object. # Add new variable based on counts by group (without pipes) EmpSurvData &lt;- mutate( group_by(EmpSurvData, Unit), UnitCount = n() ) # Ungroup the data frame object EmpSurvData &lt;- ungroup(EmpSurvData) To verify that we added the new UnitCount variable to our EmpSurvData data frame object, let’s print the variable names using the names function from base R. # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; ## [9] &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; ## [17] &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; You should see that a new variable called UnitCount is now a part of the EmpSurvData data frame. You can also view your data using the following function and argument: View(EmpSurvData). Let’s work through another example. This time we will add a new variable that contains values for the mean level of JobSat1 for each level of the Unit variable. Again, we will use the mutate function; however, we will apply the mean function from base R. Let’s call the new aggregated variable Mean_JobSat1. # Add new variable based on means by group (without pipes) EmpSurvData &lt;- mutate( group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE) ) # Ungroup the data frame object EmpSurvData &lt;- ungroup(EmpSurvData) # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; ## [9] &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; ## [17] &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; You should see that a new variable called Mean_JobSat1 is now a part of the EmpSurvData data frame. You can view your data using the following function and argument: View(EmpSurvData). 28.2.7 Visualize Data By Group We can visualize a variable by group. For instance, we can use a histogram to visualize the distribution of a variable for each group. Let’s visualize the distribution of the JobSat1 variable for each level of the Unit variable. As we did before, we’re going to treat the JobSat1 variable as a continuous variable (for the sake of demonstration), even though it would be most accurately described as having an ordinal measurement scale. To do so, we will use the Histogram function from the lessR package (Gerbing, Business, and University 2021), which will generate one histogram for each unique value of the grouping variable. Let’s begin by installing and accessing the lessR package (if you haven’t already). # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) When working with readr and dplyr functions (which are part of the tidyverse), as we previously did, we end up working with a tibble, which is a lot like a data frame but with some enhanced features. Sometimes using a tibble (instead of a standard data frame) can result in some issues with certain functions from other packages. To figure out whether the object we’ve been working with called EmpSurvData is a data frame object or a tibble, We will use the str function from base R. # Print the structure of object to determine whether a tibble or data frame str(EmpSurvData) ## tibble [156 × 21] (S3: tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:156] &quot;EID294&quot; &quot;EID295&quot; &quot;EID296&quot; &quot;EID301&quot; ... ## $ Unit : chr [1:156] &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; ... ## $ Supervisor : chr [1:156] &quot;EID373&quot; &quot;EID373&quot; &quot;EID373&quot; &quot;EID367&quot; ... ## $ JobSat1 : num [1:156] 3 2 2 2 2 4 3 4 3 3 ... ## $ JobSat2 : num [1:156] 3 3 2 2 2 3 3 4 4 2 ... ## $ JobSat3 : num [1:156] 3 2 3 3 2 4 2 4 4 3 ... ## $ TurnInt1 : num [1:156] 3 3 3 4 4 3 3 2 2 2 ... ## $ TurnInt2 : num [1:156] 3 4 3 4 4 3 2 2 2 2 ... ## $ TurnInt3 : num [1:156] 3 2 3 4 4 3 2 2 2 2 ... ## $ Engage1 : num [1:156] 2 2 1 3 3 3 3 3 3 2 ... ## $ Engage2 : num [1:156] 1 1 1 2 2 2 2 2 2 2 ... ## $ Engage3 : num [1:156] 2 3 2 3 3 4 3 3 2 3 ... ## $ Engage4 : num [1:156] 2 3 1 2 4 3 3 3 1 4 ... ## $ Engage5 : num [1:156] 3 3 2 2 3 3 4 2 2 2 ... ## $ ExpIncivil1 : num [1:156] 2 2 2 3 2 2 2 1 2 2 ... ## $ ExpIncivil2 : num [1:156] 2 2 1 3 3 3 2 2 2 2 ... ## $ ExpIncivil3 : num [1:156] 3 2 2 3 3 3 3 2 1 2 ... ## $ ExpIncivil4 : num [1:156] 2 2 2 4 3 3 3 2 2 2 ... ## $ ExpIncivil5 : num [1:156] 2 1 2 3 3 3 2 2 2 2 ... ## $ UnitCount : int [1:156] 19 19 19 19 19 11 11 72 72 18 ... ## $ Mean_JobSat1: num [1:156] 2.79 2.79 2.79 2.79 2.79 ... We see that the object called EmpSurvData is in fact a tibble. To convert our data frame called EmpSurvData to a regular data frame, we will use the as.data.frame function. First, type EmpSurvData &lt;- to overwrite the existing data frame, and then type the name of the as.data.frame function with the current data frame name (EmpSurvData) as the sole parenthetical argument. # Convert tibble object to standard data frame object EmpSurvData &lt;- as.data.frame(EmpSurvData) Now, let’s apply the str function once more to see if the conversion was successful. # Print the structure of object to determine whether conversion successful str(EmpSurvData) ## &#39;data.frame&#39;: 156 obs. of 21 variables: ## $ EmployeeID : chr &quot;EID294&quot; &quot;EID295&quot; &quot;EID296&quot; &quot;EID301&quot; ... ## $ Unit : chr &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; ... ## $ Supervisor : chr &quot;EID373&quot; &quot;EID373&quot; &quot;EID373&quot; &quot;EID367&quot; ... ## $ JobSat1 : num 3 2 2 2 2 4 3 4 3 3 ... ## $ JobSat2 : num 3 3 2 2 2 3 3 4 4 2 ... ## $ JobSat3 : num 3 2 3 3 2 4 2 4 4 3 ... ## $ TurnInt1 : num 3 3 3 4 4 3 3 2 2 2 ... ## $ TurnInt2 : num 3 4 3 4 4 3 2 2 2 2 ... ## $ TurnInt3 : num 3 2 3 4 4 3 2 2 2 2 ... ## $ Engage1 : num 2 2 1 3 3 3 3 3 3 2 ... ## $ Engage2 : num 1 1 1 2 2 2 2 2 2 2 ... ## $ Engage3 : num 2 3 2 3 3 4 3 3 2 3 ... ## $ Engage4 : num 2 3 1 2 4 3 3 3 1 4 ... ## $ Engage5 : num 3 3 2 2 3 3 4 2 2 2 ... ## $ ExpIncivil1 : num 2 2 2 3 2 2 2 1 2 2 ... ## $ ExpIncivil2 : num 2 2 1 3 3 3 2 2 2 2 ... ## $ ExpIncivil3 : num 3 2 2 3 3 3 3 2 1 2 ... ## $ ExpIncivil4 : num 2 2 2 4 3 3 3 2 2 2 ... ## $ ExpIncivil5 : num 2 1 2 3 3 3 2 2 2 2 ... ## $ UnitCount : int 19 19 19 19 19 11 11 72 72 18 ... ## $ Mean_JobSat1: num 2.79 2.79 2.79 2.79 2.79 ... As you can see, we successfully converted EmpSurvData to a conventional data frame object, which often makes it more amenable to non-tidyverse functions. Now we are ready to apply the Histogram from lessR. When specified correctly, this function will display multiple histograms (one per each value of the grouping variable) in a trellis structure. First, type the name of the function Histogram. Second, as the first argument, enter the name of the variable of interest (JobSat1). Third, using the by1= argument, note that the grouping variable is Unit in this instance Fourth, using the data= argument, type the name of the data frame object, which in this instance is EmpSurvData. # Create trellis of histograms corresponding to different values of grouping variable Histogram(JobSat1, by1=Unit, data=EmpSurvData) ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## JobSat1 ## - by levels of - ## Unit ## ## n miss mean sd min mdn max ## HumanResources 11 0 3.18 0.75 2.00 3.00 4.00 ## Manufacturing 72 0 3.08 0.82 1.00 3.00 5.00 ## Marketing 19 0 2.79 0.79 1.00 3.00 4.00 ## ResearchDevelopment 18 0 3.28 1.02 1.00 3.00 5.00 ## Sales 36 0 3.22 0.72 2.00 3.00 5.00 If we want to look at the counts (i.e., frequencies) of cases for each unique value of a grouping variable, we can apply the BarChart from the lessR package. Simply type BarChart as the function. As the first argument, enter the name of the grouping variable (Unit). As the second argument, type data= followed by the name of the data frame object (EmpSurvData). # Create bar chart that shows counts for each level of grouping variable BarChart(Unit, data=EmpSurvData) ## &gt;&gt;&gt; Suggestions ## BarChart(Unit, horiz=TRUE) # horizontal bar chart ## BarChart(Unit, fill=&quot;reds&quot;) # red bars of varying lightness ## PieChart(Unit) # doughnut (ring) chart ## Plot(Unit) # bubble plot ## Plot(Unit, stat=&quot;count&quot;) # lollipop plot ## ## --- Unit --- ## ## Missing Values: 0 ## ## Unit Count Prop ## --------------------------------- ## HumanResources 11 0.071 ## Manufacturing 72 0.462 ## Marketing 19 0.122 ## ResearchDevelopment 18 0.115 ## Sales 36 0.231 ## --------------------------------- ## Total 156 1.000 ## ## Chi-squared test of null hypothesis of equal probabilities ## Chisq = 77.526, df = 4, p-value = 0.000 Finally, if we wish to visually examine the means for each unique value of a grouping variable, using the the BarChart function, we will begin by specifying the x-axis as the grouping variable using the x= argument and the y-axis as the continuous (interval or ratio) variable using the y= argument. Next, we will apply the stat=\"mean\" argument to request that the mean be computed for the y-axis variable by value of the grouping variable. # Create bar chart that shows counts for each level of grouping variable BarChart(x=Unit, y=JobSat1, stat=&quot;mean&quot;, data=EmpSurvData) ## JobSat1 ## - by levels of - ## Unit ## ## n miss mean sd min mdn max ## HumanResources 11 0 3.18 0.75 2.00 3.00 4.00 ## Manufacturing 72 0 3.08 0.82 1.00 3.00 5.00 ## Marketing 19 0 2.79 0.79 1.00 3.00 4.00 ## ResearchDevelopment 18 0 3.28 1.02 1.00 3.00 5.00 ## Sales 36 0 3.22 0.72 2.00 3.00 5.00 ## &gt;&gt;&gt; Suggestions ## Plot(JobSat1, Unit) # lollipop plot ## ## Plotted Values ## -------------- ## HumanResources Manufacturing Marketing ResearchDevelopment Sales ## 3.182 3.083 2.789 3.278 3.222 28.2.8 Summary When it comes to describing and summarizing data with multiple levels of analysis, aggregation and segmentation are important processes to consider and implement. In this tutorial, we learned how to summarize data at an aggregate level of analysis, which can allow us to segment the data to look at, perhaps, unique patterns within specific subsamples as opposed to across the entire sample. We also learned how to add new variables that contain aggregate data to an existing data frame object. Finally, we learned how to visualize data across different values of a grouping variable. 28.3 Chapter Supplement In the main portion of this chapter, we learned how to aggregate and segment data using functions from the dplyr package. In this chapter supplement, we will learn additional methods the can be used to aggregate and segment data. 28.3.1 Functions &amp; Packages Introduced Function Package describeBy psych aggregate base R list base R 28.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object EmpSurvData &lt;- read_csv(&quot;EmployeeSurveyData.csv&quot;) ## Rows: 156 Columns: 19 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmployeeID, Unit, Supervisor ## dbl (16): JobSat1, JobSat2, JobSat3, TurnInt1, TurnInt2, TurnInt3, Engage1, Engage2, Engage3, Engage4, Engage5, ExpIncivil1, ExpInciv... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; ## [10] &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; 28.3.3 describeBy Function from psych Package The psych package (Revelle 2023) has a useful function called describeBy, which allows us to compute descriptive (i.e., summary) statistics by unique values from a grouping variable. The describeBy is an extension of the describe function, where the latter generates descriptive statistics for the entire sample. Before using the describeBy function, we must install and access the psych package (if we haven’t already). # Install psych package if not already installed install.packages(&quot;psych&quot;) # Access psych package library(psych) Type the name of the describeBy function, and as the first argument, enter the name of the data frame, which in this example is EmpSurvData. Next, enter the argument group= followed by the name of the name of the data frame (EmpSurvData), followed by $, and the name of the grouping variable (Unit). Remember, the $ operator indicates that a variable belongs to a particular data frame; some functions require the $ operator to be explicitly written as part of the argument, whereas others don’t. # Describe/summarize data by grouping variable describeBy(EmpSurvData, group=EmpSurvData$Unit) ## ## Descriptive statistics by group ## group: HumanResources ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 11 18.91 44.57 6 6.00 4.45 1 153 152 2.45 4.46 13.44 ## Unit* 2 11 1.00 0.00 1 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 11 4.73 2.41 4 4.00 0.00 4 12 8 2.47 4.52 0.73 ## JobSat1 4 11 3.18 0.75 3 3.22 1.48 2 4 2 -0.25 -1.37 0.23 ## JobSat2 5 11 3.00 0.45 3 3.00 0.00 2 4 2 0.00 1.55 0.13 ## JobSat3 6 11 3.09 0.83 3 3.11 1.48 2 4 2 -0.14 -1.67 0.25 ## TurnInt1 7 11 2.91 0.54 3 2.89 0.00 2 4 2 -0.11 -0.01 0.16 ## TurnInt2 8 11 2.73 0.79 3 2.67 1.48 2 4 2 0.43 -1.41 0.24 ## TurnInt3 9 11 2.82 0.60 3 2.78 0.00 2 4 2 0.02 -0.73 0.18 ## Engage1 10 11 3.55 0.52 4 3.56 0.00 3 4 1 -0.16 -2.15 0.16 ## Engage2 11 11 3.18 0.75 3 3.22 1.48 2 4 2 -0.25 -1.37 0.23 ## Engage3 12 11 3.64 0.92 4 3.67 1.48 2 5 3 -0.02 -1.16 0.28 ## Engage4 13 11 3.73 0.65 4 3.67 0.00 3 5 2 0.22 -1.04 0.19 ## Engage5 14 11 3.64 0.67 4 3.56 1.48 3 5 2 0.44 -1.08 0.20 ## ExpIncivil1 15 11 2.09 0.30 2 2.00 0.00 2 3 1 2.47 4.52 0.09 ## ExpIncivil2 16 11 2.36 0.67 2 2.22 0.00 2 4 2 1.34 0.36 0.20 ## ExpIncivil3 17 11 2.45 0.52 2 2.44 0.00 2 3 1 0.16 -2.15 0.16 ## ExpIncivil4 18 11 2.55 0.52 3 2.56 0.00 2 3 1 -0.16 -2.15 0.16 ## ExpIncivil5 19 11 2.55 0.52 3 2.56 0.00 2 3 1 -0.16 -2.15 0.16 ## ------------------------------------------------------------------------------------------------------ ## group: Manufacturing ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 72 48.54 27.04 46.5 46.50 26.69 11 156 145 1.51 4.27 3.19 ## Unit* 2 72 2.00 0.00 2.0 2.00 0.00 2 2 0 NaN NaN 0.00 ## Supervisor* 3 72 9.75 5.34 13.0 10.05 2.97 2 15 13 -0.47 -1.54 0.63 ## JobSat1 4 72 3.08 0.82 3.0 3.09 1.48 1 5 4 0.00 -0.46 0.10 ## JobSat2 5 72 3.32 0.82 3.0 3.36 1.48 1 5 4 -0.33 -0.23 0.10 ## JobSat3 6 72 3.36 0.83 3.0 3.36 1.48 2 5 3 0.00 -0.66 0.10 ## TurnInt1 7 72 3.08 0.88 3.0 3.10 1.48 1 5 4 -0.16 -0.63 0.10 ## TurnInt2 8 72 2.92 0.82 3.0 2.95 0.00 1 5 4 -0.31 0.16 0.10 ## TurnInt3 9 72 2.92 0.75 3.0 2.93 0.00 1 4 3 -0.27 -0.32 0.09 ## Engage1 10 72 3.58 0.75 4.0 3.59 1.48 2 5 3 -0.18 -0.32 0.09 ## Engage2 11 72 3.39 0.74 3.0 3.38 0.00 2 5 3 0.28 -0.24 0.09 ## Engage3 12 72 3.28 0.74 3.0 3.31 1.48 2 5 3 -0.06 -0.56 0.09 ## Engage4 13 72 3.51 0.90 4.0 3.53 1.48 1 5 4 -0.27 -0.31 0.11 ## Engage5 14 72 3.40 0.85 3.0 3.40 1.48 1 5 4 -0.04 -0.08 0.10 ## ExpIncivil1 15 72 2.14 0.59 2.0 2.17 0.00 1 3 2 -0.03 -0.30 0.07 ## ExpIncivil2 16 72 2.36 0.51 2.0 2.34 0.00 1 3 2 0.25 -1.25 0.06 ## ExpIncivil3 17 72 2.50 0.75 2.0 2.48 1.48 1 4 3 0.10 -0.40 0.09 ## ExpIncivil4 18 72 2.56 0.65 3.0 2.57 0.00 1 4 3 -0.20 -0.25 0.08 ## ExpIncivil5 19 72 2.57 0.67 3.0 2.57 0.74 1 4 3 -0.11 -0.26 0.08 ## ------------------------------------------------------------------------------------------------------ ## group: Marketing ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 19 90.00 5.63 90 90.00 7.41 81 99 18 0.00 -1.39 1.29 ## Unit* 2 19 3.00 0.00 3 3.00 0.00 3 3 0 NaN NaN 0.00 ## Supervisor* 3 19 3.84 3.08 1 3.82 0.00 1 7 6 0.10 -2.09 0.71 ## JobSat1 4 19 2.79 0.79 3 2.82 0.00 1 4 3 -0.30 -0.44 0.18 ## JobSat2 5 19 2.89 0.74 3 2.94 0.00 1 4 3 -0.64 0.43 0.17 ## JobSat3 6 19 3.42 1.12 3 3.41 1.48 2 5 3 0.30 -1.41 0.26 ## TurnInt1 7 19 3.05 0.85 3 3.12 1.48 1 4 3 -0.61 -0.33 0.19 ## TurnInt2 8 19 2.84 0.76 3 2.82 1.48 2 4 2 0.24 -1.35 0.18 ## TurnInt3 9 19 2.89 0.81 3 2.88 1.48 2 4 2 0.17 -1.53 0.19 ## Engage1 10 19 3.53 1.17 3 3.59 1.48 1 5 4 -0.26 -0.88 0.27 ## Engage2 11 19 3.26 1.28 4 3.29 1.48 1 5 4 -0.62 -0.97 0.29 ## Engage3 12 19 3.37 0.96 3 3.35 1.48 2 5 3 0.36 -0.94 0.22 ## Engage4 13 19 3.47 1.02 4 3.53 1.48 1 5 4 -0.68 -0.15 0.23 ## Engage5 14 19 3.21 0.71 3 3.18 0.00 2 5 3 0.59 0.28 0.16 ## ExpIncivil1 15 19 2.11 0.46 2 2.12 0.00 1 3 2 0.43 1.06 0.11 ## ExpIncivil2 16 19 2.32 0.58 2 2.35 0.00 1 3 2 -0.10 -0.88 0.13 ## ExpIncivil3 17 19 2.58 0.69 2 2.53 0.00 2 4 2 0.68 -0.83 0.16 ## ExpIncivil4 18 19 2.53 0.70 2 2.47 0.00 2 4 2 0.85 -0.64 0.16 ## ExpIncivil5 19 19 2.58 0.69 3 2.59 0.00 1 4 3 -0.27 -0.39 0.16 ## ------------------------------------------------------------------------------------------------------ ## group: ResearchDevelopment ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 18 108.50 5.34 108.5 108.50 6.67 100 117 17 0.00 -1.40 1.26 ## Unit* 2 18 4.00 0.00 4.0 4.00 0.00 4 4 0 NaN NaN 0.00 ## Supervisor* 3 18 6.00 0.00 6.0 6.00 0.00 6 6 0 NaN NaN 0.00 ## JobSat1 4 18 3.28 1.02 3.0 3.31 1.48 1 5 4 -0.21 -0.35 0.24 ## JobSat2 5 18 3.17 0.92 3.0 3.12 1.48 2 5 3 0.12 -1.21 0.22 ## JobSat3 6 18 3.50 0.92 3.0 3.50 1.48 2 5 3 0.21 -1.00 0.22 ## TurnInt1 7 18 2.83 0.51 3.0 2.81 0.00 2 4 2 -0.27 0.01 0.12 ## TurnInt2 8 18 2.67 0.59 3.0 2.62 0.00 2 4 2 0.18 -0.92 0.14 ## TurnInt3 9 18 2.61 0.70 3.0 2.62 0.00 1 4 3 -0.37 -0.30 0.16 ## Engage1 10 18 3.67 0.59 4.0 3.75 0.00 2 4 2 -1.41 0.87 0.14 ## Engage2 11 18 3.44 0.70 4.0 3.50 0.00 2 4 2 -0.77 -0.77 0.17 ## Engage3 12 18 3.61 0.61 4.0 3.56 0.74 3 5 2 0.34 -0.95 0.14 ## Engage4 13 18 3.83 0.51 4.0 3.81 0.00 3 5 2 -0.27 0.01 0.12 ## Engage5 14 18 3.50 0.79 4.0 3.50 0.74 2 5 3 -0.34 -0.65 0.19 ## ExpIncivil1 15 18 2.00 0.77 2.0 2.00 1.48 1 3 2 0.00 -1.39 0.18 ## ExpIncivil2 16 18 2.11 0.58 2.0 2.12 0.00 1 3 2 0.01 -0.33 0.14 ## ExpIncivil3 17 18 2.39 0.70 2.0 2.38 0.00 1 4 3 0.37 -0.30 0.16 ## ExpIncivil4 18 18 2.44 0.62 2.5 2.50 0.74 1 3 2 -0.52 -0.86 0.15 ## ExpIncivil5 19 18 2.22 0.65 2.0 2.25 0.00 1 3 2 -0.19 -0.88 0.15 ## ------------------------------------------------------------------------------------------------------ ## group: Sales ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 36 135.56 10.64 135.5 135.50 13.34 118 155 37 0.03 -1.25 1.77 ## Unit* 2 36 5.00 0.00 5.0 5.00 0.00 5 5 0 NaN NaN 0.00 ## Supervisor* 3 36 7.81 2.04 6.0 7.67 0.00 6 11 5 0.33 -1.77 0.34 ## JobSat1 4 36 3.22 0.72 3.0 3.23 0.00 2 5 3 0.11 -0.42 0.12 ## JobSat2 5 36 3.36 0.72 3.0 3.37 0.00 2 5 3 0.26 -0.25 0.12 ## JobSat3 6 36 3.58 0.77 4.0 3.60 0.74 2 5 3 -0.27 -0.40 0.13 ## TurnInt1 7 36 2.83 0.65 3.0 2.80 0.00 2 4 2 0.17 -0.79 0.11 ## TurnInt2 8 36 2.72 0.57 3.0 2.70 0.00 2 4 2 0.02 -0.64 0.09 ## TurnInt3 9 36 2.67 0.68 3.0 2.60 1.48 2 4 2 0.48 -0.87 0.11 ## Engage1 10 36 3.75 0.73 4.0 3.73 0.74 2 5 3 -0.03 -0.54 0.12 ## Engage2 11 36 3.44 0.81 3.0 3.43 1.48 2 5 3 0.02 -0.60 0.13 ## Engage3 12 36 3.64 0.64 4.0 3.63 0.00 2 5 3 -0.19 -0.24 0.11 ## Engage4 13 36 3.86 0.83 4.0 3.90 1.48 2 5 3 -0.33 -0.56 0.14 ## Engage5 14 36 3.50 0.91 4.0 3.53 1.48 1 5 4 -0.44 0.09 0.15 ## ExpIncivil1 15 36 2.11 0.52 2.0 2.13 0.00 1 3 2 0.15 0.33 0.09 ## ExpIncivil2 16 36 2.28 0.57 2.0 2.30 0.00 1 3 2 -0.02 -0.64 0.09 ## ExpIncivil3 17 36 2.58 0.65 3.0 2.57 0.74 1 4 3 0.00 -0.40 0.11 ## ExpIncivil4 18 36 2.69 0.71 3.0 2.67 0.74 1 4 3 0.02 -0.48 0.12 ## ExpIncivil5 19 36 2.39 0.64 2.0 2.47 1.48 1 3 2 -0.53 -0.77 0.11 As you can see, the describeBy function generates a table of common descriptive statistics for each unique value of the grouping variable. 28.3.4 aggregate Function from Base R As an alternative to the summarize function from the dplyr package, we can use the aggregate function from base R to aggregate variables to a higher level of analysis. This function is sometimes a nice alternative because (a) it doesn’t require installation of an outside package, and (b) it automatically and efficiently aggregates all numeric/integer variables in the data frame object (if that’s desired). Type the name of the aggregate function. As the first argument in the aggregate function, specify the name of the data frame object (EmpSurvData). As the second argument in the aggregate function, type by= followed by the list function from base R. As the sole parenthetical argument in the list function, specify what you would like for the group variable to be called in the new aggregated data frame (e.g., Unit), the = operator, and the name of the data frame object (EmpSurvData) followed by the $ operator and the name of the grouping variable (Unit). As the third argument in the aggregate function, type FUN=mean to request that the mean be computed for each value of the grouping variable and for each quantitative (numeric, integer) variable in the data frame object. # Summarize all numeric/integer variables by grouping variable aggregate(EmpSurvData, by=list(Unit=EmpSurvData$Unit), FUN=mean) ## Unit EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 ## 1 HumanResources NA NA NA 3.181818 3.000000 3.090909 2.909091 2.727273 2.818182 3.545455 3.181818 3.636364 ## 2 Manufacturing NA NA NA 3.083333 3.319444 3.361111 3.083333 2.916667 2.916667 3.583333 3.388889 3.277778 ## 3 Marketing NA NA NA 2.789474 2.894737 3.421053 3.052632 2.842105 2.894737 3.526316 3.263158 3.368421 ## 4 ResearchDevelopment NA NA NA 3.277778 3.166667 3.500000 2.833333 2.666667 2.611111 3.666667 3.444444 3.611111 ## 5 Sales NA NA NA 3.222222 3.361111 3.583333 2.833333 2.722222 2.666667 3.750000 3.444444 3.638889 ## Engage4 Engage5 ExpIncivil1 ExpIncivil2 ExpIncivil3 ExpIncivil4 ExpIncivil5 ## 1 3.727273 3.636364 2.090909 2.363636 2.454545 2.545455 2.545455 ## 2 3.513889 3.402778 2.138889 2.361111 2.500000 2.555556 2.569444 ## 3 3.473684 3.210526 2.105263 2.315789 2.578947 2.526316 2.578947 ## 4 3.833333 3.500000 2.000000 2.111111 2.388889 2.444444 2.222222 ## 5 3.861111 3.500000 2.111111 2.277778 2.583333 2.694444 2.388889 Using the aggregate function, we can also summarize data using two or more grouping variables. To do so, we just need to add a second grouping variable within the list function, as shown below. # Summarize all numeric/integer variables by two grouping variables aggregate(EmpSurvData, by=list(Unit=EmpSurvData$Unit, Supervisor=EmpSurvData$Supervisor), FUN=mean) ## Unit Supervisor EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 ## 1 Marketing EID367 NA NA NA 2.800000 2.900000 3.700000 3.200000 2.900000 3.000000 3.800000 3.500000 ## 2 Manufacturing EID368 NA NA NA 3.294118 3.529412 3.352941 3.058824 2.705882 2.705882 3.529412 3.588235 ## 3 Manufacturing EID369 NA NA NA 3.000000 4.000000 4.000000 3.000000 3.000000 3.000000 5.000000 5.000000 ## 4 HumanResources EID370 NA NA NA 3.300000 3.100000 3.200000 2.900000 2.700000 2.800000 3.500000 3.100000 ## 5 Manufacturing EID371 NA NA NA 3.571429 4.000000 3.428571 2.142857 2.142857 2.285714 3.714286 3.142857 ## 6 ResearchDevelopment EID372 NA NA NA 3.277778 3.166667 3.500000 2.833333 2.666667 2.611111 3.666667 3.444444 ## 7 Sales EID372 NA NA NA 3.052632 3.263158 3.368421 2.947368 2.789474 2.684211 3.526316 3.368421 ## 8 Marketing EID373 NA NA NA 2.777778 2.888889 3.111111 2.888889 2.777778 2.777778 3.222222 3.000000 ## 9 Sales EID374 NA NA NA 3.666667 3.333333 4.000000 2.333333 2.333333 2.333333 4.000000 3.333333 ## 10 Manufacturing EID375 NA NA NA 3.166667 3.500000 3.666667 3.000000 3.000000 3.166667 3.666667 3.666667 ## 11 Sales EID376 NA NA NA 3.454545 3.636364 3.818182 2.818182 2.727273 2.818182 4.000000 3.727273 ## 12 Sales EID377 NA NA NA 3.000000 3.000000 3.666667 2.666667 2.666667 2.333333 4.000000 3.000000 ## 13 HumanResources EID379 NA NA NA 2.000000 2.000000 2.000000 3.000000 3.000000 3.000000 4.000000 4.000000 ## 14 Manufacturing EID380 NA NA NA 2.842105 3.210526 3.263158 3.157895 2.947368 2.894737 3.473684 3.263158 ## 15 Manufacturing EID381 NA NA NA 4.000000 4.000000 3.000000 3.000000 4.000000 3.000000 4.000000 3.000000 ## 16 Manufacturing EID382 NA NA NA 2.904762 2.904762 3.333333 3.380952 3.238095 3.238095 3.571429 3.285714 ## Engage3 Engage4 Engage5 ExpIncivil1 ExpIncivil2 ExpIncivil3 ExpIncivil4 ExpIncivil5 ## 1 3.400000 3.700000 3.300000 2.300000 2.500000 2.700000 2.600000 2.800000 ## 2 3.352941 3.705882 3.764706 2.411765 2.529412 2.529412 2.705882 2.411765 ## 3 4.000000 5.000000 4.000000 2.000000 3.000000 3.000000 3.000000 3.000000 ## 4 3.500000 3.700000 3.500000 2.100000 2.300000 2.500000 2.500000 2.600000 ## 5 3.285714 3.285714 3.142857 2.142857 2.285714 2.285714 2.428571 2.142857 ## 6 3.611111 3.833333 3.500000 2.000000 2.111111 2.388889 2.444444 2.222222 ## 7 3.631579 3.631579 3.368421 2.157895 2.315789 2.578947 2.736842 2.421053 ## 8 3.333333 3.222222 3.111111 1.888889 2.111111 2.444444 2.444444 2.333333 ## 9 4.000000 4.000000 3.666667 2.000000 2.000000 2.666667 2.666667 2.333333 ## 10 3.333333 3.666667 3.666667 1.666667 2.333333 2.166667 2.666667 3.000000 ## 11 3.636364 4.181818 3.727273 2.000000 2.272727 2.636364 2.636364 2.363636 ## 12 3.333333 4.000000 3.333333 2.333333 2.333333 2.333333 2.666667 2.333333 ## 13 5.000000 4.000000 5.000000 2.000000 3.000000 2.000000 3.000000 2.000000 ## 14 3.210526 3.421053 3.210526 2.263158 2.315789 2.736842 2.473684 2.526316 ## 15 4.000000 5.000000 4.000000 2.000000 2.000000 1.000000 3.000000 3.000000 ## 16 3.190476 3.333333 3.238095 1.952381 2.285714 2.476190 2.476190 2.714286 References "],["cronbachsalpha.html", "Chapter 29 Estimating Internal Consistency Reliability Using Cronbach’s alpha 29.1 Conceptual Overview 29.2 Tutorial", " Chapter 29 Estimating Internal Consistency Reliability Using Cronbach’s alpha In this chapter, we will learn how to estimate the internal consistency reliability of a multi-item measure (i.e, scale, inventory, test) by using Cronbach’s alpha (\\(\\alpha\\)). 29.1 Conceptual Overview Link to conceptual video: https://youtu.be/IAjR0lnCu-s We can think of reliability as how consistently or dependably we have measured something in a given sample. Common types of reliability that we encounter in human resource management include inter-rater reliability, test-retest reliability, and internal consistency reliability. Conventionally, a measurement tool demonstrates an acceptable level of reliability in a sample when the reliability estimate is .70 or higher, where .00 indicates very low reliability and 1.00 indicates very high reliability. That being said, we should always strive for reliability estimates that are much closer to 1.00. When we working with multi-item measures multi-item measures, we often estimate internal consistency reliability can be defined as “a reliability estimate based on intercorrelation (i.e., homogeneity) among items on a test, with [Cronbach’s] alpha being a prime example” (Schultz and Whitney 2005). In other words, internal consistency reliability tells us how consistent scores on different items (e.g., questions) are to one another. Homogeneity among items provides some evidence that the items are reliably measuring the same construct (i.e., concept). Of course, just because we are consistently measuring something doesn’t necessary mean we are measuring the correct something, which echoes the notion that high reliability is a necessary but not sufficient condition for high validity. Nonetheless, internal consistency reliability is a useful form of reliability when it comes to evaluating multi-item scales and determining whether it is appropriate to create a composite variable (i.e., overall scale score variable) based on the sum or mean of item scores for each case (e.g., observation, person, employee, individual). Cronbach’s alpha (\\(\\alpha\\)) is commonly used as an indicator of internal consistency reliability. If our goal is to understand the extent to which the items relate to underlying factor(s), exploratory and/or confirmatory factor analysis would be appropriate. Cronbach’s alpha can be used to assess internal consistency reliability when the variables (e.g., survey items, measure items) analyzed are continuous (interval or ratio measurement scale); however, we often relax this assumption to allow the analysis of Likert-type response scale formats (e.g., 1 = Strongly Disagree, 5 = Strongly Disagree) for variables that are technically ordinal in nature. For dichotomous (binary) items, we might use the Kuder-Richardson (K-R) coefficient of equivalence to assess internal consistency reliability. There are different thresholds we might apply to evaluate the internal consistency reliability based on Cronbach’s alpha, and the table below shows the thresholds for qualitative descriptors that we’ll apply throughout this book. Cronbach’s alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable For additional information on internal consistency reliability, Cronbach’s alpha, and reliability in general, please check out this open-source resource (Price et al. 2017). 29.2 Tutorial This chapter’s tutorial demonstrates how to estimate internal consistency reliability using Cronbach’s alpha in R. 29.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/7k35isYrE4Q 29.2.2 Functions &amp; Packages Introduced Function Package alpha psych c base R 29.2.3 Initial Steps If you haven’t already, save the file called “survey.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “survey.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;survey.csv&quot;) ## Rows: 156 Columns: 11 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (11): SurveyID, JobSat1, JobSat2, JobSat3, TurnInt1, TurnInt2, TurnInt3, Engage1, Engage2, Engage3, Engage4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 11 ## SurveyID JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 3 3 3 2 1 2 2 ## 2 2 4 4 4 3 3 2 4 4 4 4 ## 3 3 4 4 5 2 1 2 4 4 4 4 ## 4 4 2 3 3 4 4 4 4 4 4 4 ## 5 5 3 3 3 4 3 3 3 3 3 3 ## 6 6 3 3 3 3 2 2 4 4 5 3 The data frame includes annual employee survey responses from 156 employees to three Job Satisfaction items (JobSat1, JobSat2, JobSat3), three Turnover Intentions items (TurnInt1, TurnInt2, TurnInt3), and four Engagement items (Engage1, Engage2, Engage3, Engage4). Employees responded to each item using a 5-point response format, ranging from Strongly Disagree (1) to Strongly Agree (5). Assume that higher scores on an item indicate higher levels of that variable; for example, a higher score on TurnInt1 would indicate that the respondent has higher intentions of quitting the organization. 29.2.4 Compute Cronbach’s alpha Prior to creating a composite score (i.e., overall scale score) for each case (e.g., observation, respondent, individual, employee) within our data frame based on their responses to each of the multi-item survey measures, it is common to compute Cronbach’s alpha as an estimate of internal consistency reliability. Cronbach’s alpha provides us with information we can use to judge whether variables (e.g., items) are internally consistent with each another. If they’re not internally consistent, then it won’t make much sense to compute a composite variable (i.e., overall scale score variable) based on the mean or sum of the variables (e.g., items). For the purposes of this book, we will consider a scale with an alpha greater than or equal to .70 to demonstrate acceptable internal consistency for the particular sample, whereas an alpha that falls within the range of .60-.69 would be considered questionable, and an alpha below .60 would be deemed unacceptable. Here is a table of more nuanced qualitative descriptors for Cronbach’s alpha: Cronbach’s alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable To compute internal consistency reliability, we will use the alpha function from the psych package (Revelle 2023). To get started, install and access the psych package using the install.packages and library functions, respectively (if you haven’t already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Presumably, each set of similarly named items are intended to “tap into” the same underlying concept (e.g., Turnover Intentions: TurnInt1, TurnInt2, TurnInt3), which we are attempting to assess using the items. Let’s practice estimating Cronbach’s alpha for some of the measures in our data frame, beginning with the three Turnover Intention items (i.e., variables) (TurnInt1, TurnInt2, TurnInt3). Note that you must list the name of the data frame (df) containing these items prior to the first bracket ([. After that and within the c (combine) function, provide the item (variable) names of the scale for which you would like to estimate the internal consistency reliability; because we have included a comma (,) before the c function, we are using matrix/bracket notation to reference columns, which correspond to variables in this data frame; if we had placed the c function before the comma, then we would (,) be referencing rows using matrix/bracket notation (which will almost never be the case when we’re using a data frame with the alpha function. Finally, the item (variable) names listed as arguments within the c function should be in quotation marks. # Estimate Cronbach&#39;s alpha for Turnover Intentions items alpha(df[,c(&quot;TurnInt1&quot;,&quot;TurnInt2&quot;,&quot;TurnInt3&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;TurnInt1&quot;, &quot;TurnInt2&quot;, &quot;TurnInt3&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.83 0.83 0.78 0.63 5 0.023 2.9 0.64 0.59 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.78 0.83 0.87 ## Duhachek 0.79 0.83 0.88 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## TurnInt1 0.75 0.75 0.59 0.59 2.9 0.041 NA 0.59 ## TurnInt2 0.73 0.74 0.58 0.58 2.8 0.042 NA 0.58 ## TurnInt3 0.83 0.83 0.70 0.70 4.8 0.028 NA 0.70 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## TurnInt1 154 0.88 0.88 0.80 0.72 3.0 0.77 ## TurnInt2 154 0.88 0.88 0.81 0.73 2.8 0.73 ## TurnInt3 154 0.83 0.84 0.69 0.64 2.8 0.72 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## TurnInt1 0.02 0.23 0.51 0.23 0.01 0.01 ## TurnInt2 0.03 0.29 0.54 0.14 0.01 0.01 ## TurnInt3 0.02 0.31 0.51 0.16 0.00 0.01 Note: If you see the following message at the top or bottom of your output, you can often safely ignore it – that is, unless you know that one or more items should have been reverse-coded. If an item needs to be reverse coded, then you would need to take care of that prior to running the alpha function. Some items ( [ITEM NAME] ) were negatively correlated with the total scale and probably should be reversed. Based on the output from the alpha function, we can conclude that the raw alpha (raw_alpha) of .83 for all three items exceeds our cutoff of .70 for acceptable internal consistency, and enters into the realm of what we would consider to be good internal consistency. Next, take a look at the output table called Reliability if an item is dropped; this table indicates what would happen to Cronbach’s alpha if you were to drop the item listed in the row in which the item appears and then re-estimate Cronbach’s alpha. For example, if you dropped TurnInt1 and retained all other items, Cronbach’s alpha would drop to approximately .75. Similarly, if you dropped TurnInt2 and retained all other items, Cronbach’s alpha would drop to .75. Finally, if you dropped TurnInt3 and retained all other items, Cronbach’s alpha would remain the same (.83). Thus, given that Cronbach’s alpha for all three items exceeds .70 and that dropping any one of the items would not increase Cronbach’s alpha for the items from the Turnover Intentions survey measure, from an empirical perspective, we can be reasonably confident that the three Turnover Intentions items are internally consistent with one another, which means it would be acceptable to create an overall scale score based on the sum or mean of these three items. Importantly, before making a final decision on whether to retain all of the items, however, we should review the qualitative content of each item to determine whether it meets our conceptual definition for turnover intentions. Let’s imagine our conceptual definition of turnover intentions is a person’s thoughts and intentions to leave an organization, and the three turnover intentions items follow. TurnInt1 - “I regularly think about leaving this organization.” TurnInt2 - “I plan on quitting this job within the next year.” TurnInt3 - “I regularly search for new jobs outside of this organization.” In this example, all three of these items appear to “tap into” our conceptual definition of turnover intentions . Thus, when combined with the acceptable internal consistency reliability for these three items, we can reasonably justify creating a composite variable (i.e., overall scale score variable) based on the mean or sum of these three items; you will learn how to do this in the following chapter. Now, let’s apply the alpha function to the three Job Satisfaction items (JobSat1, JobSat2, JobSat3). Given that we’re working the same data frame object (df), all we need to do is swap out the three turnover intentions item names (i.e., variable names) with the three job satisfaction items names. Please note that if we had fewer or more than three variables, we would simply list fewer variable-name arguments in the c function nested within the alpha function. # Estimate Cronbach&#39;s alpha for Job Satisfaction items alpha(df[,c(&quot;JobSat1&quot;,&quot;JobSat2&quot;,&quot;JobSat3&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;JobSat1&quot;, &quot;JobSat2&quot;, &quot;JobSat3&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.78 0.78 0.72 0.54 3.5 0.032 3.3 0.68 0.47 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.71 0.78 0.83 ## Duhachek 0.71 0.78 0.84 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## JobSat1 0.62 0.62 0.45 0.45 1.6 0.061 NA 0.45 ## JobSat2 0.63 0.64 0.47 0.47 1.7 0.058 NA 0.47 ## JobSat3 0.82 0.82 0.70 0.70 4.7 0.028 NA 0.70 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## JobSat1 156 0.86 0.87 0.79 0.68 3.1 0.82 ## JobSat2 152 0.85 0.86 0.78 0.67 3.2 0.80 ## JobSat3 156 0.78 0.77 0.55 0.50 3.4 0.86 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## JobSat1 0.02 0.19 0.48 0.28 0.03 0.00 ## JobSat2 0.01 0.14 0.47 0.33 0.04 0.03 ## JobSat3 0.00 0.15 0.39 0.36 0.10 0.00 Based on the output from the alpha function shown above, we can conclude that the raw alpha (raw_alpha) of .78 for all three items exceeds our cutoff of .70 for acceptable internal consistency. Take a look at the output table called Reliability if an item is dropped; this table indicates what would happen to Cronbach’s alpha if we were to drop the item listed in the row in which the item appears. For example, if we dropped JobSat1 and retained all other items, Cronbach’s alpha would drop to .62. Similarly, if we dropped JobSat2 and retained all other items, Cronbach’s alpha would drop to .63. Finally, if we dropped JobSat3 and retained all other items, Cronbach’s alpha would increase to .82. Now we are faced with a dilemma: Should we drop JobSat3 to improve Cronbach’s alpha by .04? Or should we retain JobSat3 because this increase might be described by some as only marginal? Well, this is a situation where it is especially important to look at the actual qualitative item content – just like we did with the turnover intentions items. Let’s imagine our conceptual definition for job satisfaction is a person’s evaluation of their work and job, and the three job satisfaction items are as follows. JobSat1 - “I enjoy completing my daily work for my job.” JobSat2 - “I am satisfied with my job.” JobSat3 - “I am satisfied with my work and with my direct supervisor.” In this example, the qualitative item content corroborates the increase in internal consistency reliability should we drop JobSat3. Specifically, the item content for JobSat3 indicates that it is double-barreled (i.e., references two objects), and this likely explains why this item seems to be less consistent with the other two items. Given all that, we would make the decision to drop the JobSat3 and create the composite variable for overall job satisfaction using all items except for JobSat3. I encourage you to practice computing Cronbach’s alpha on your own using the alpha function for the four Engagement items (Engage1, Engage2, Engage3, Engage4). Let’s say that the conceptual definition for engagement is: The extent to which a person feels enthusiastic, energized, and driven to perform their work. And let’s pretend that the actual items are: Engage1 - “When I’m working, I’m full of energy.” Engage2 - “I complete my work with enthusiasm.” Engage3 - “I find inspiration in my work.” Engage4 - “I have no problem working for long periods of time.” In the following chapter, we will begin by estimating Cronbach’s alpha for these four engagement items and then determine which of the items should be included in a composite variable for engagement. 29.2.5 Summary In this chapter, we learned how to estimate the internal consistency reliability of a multi-item measures by computing Cronbach’s alpha (\\(\\alpha\\)). Cronbach’s alpha represents one way to estimate the internal consistency reliability of a set of variables (e.g., items). Cronbach’s alpha can help us understand whether a set of variables (e.g., items) are homogeneous. The alpha function from the psych package offers an efficient approach to estimating Cronbach’s alpha. References "],["compositevariable.html", "Chapter 30 Creating a Composite Variable Based on a Multi-Item Measure 30.1 Conceptual Overview 30.2 Tutorial", " Chapter 30 Creating a Composite Variable Based on a Multi-Item Measure In this chapter, we will learn how to create a composite variable based on scores from a multi-item measure. To justify which items should be included or excluded when creating the composite variable, we will compute Cronbach’s alpha (\\(\\alpha\\)) as an indicator of internal consistency reliability. 30.1 Conceptual Overview Sometimes it is useful to create a composite variable out of the sum or mean of multiple variables’ scores. This process results in each case (e.g., observation, employee, individual) receiving a composite score based on the sum or mean of their scores on the variables used to create the composite variable. As an example, imagine that a direct supervisor rates each of their team members on five dimensions from a performance evaluation measure, which results in five separate variables corresponding to the five dimensions. A composite variable could be created for each team member by taking the average of the five ratings each individual received, such that the resulting variable represents each team member’s overall level of job performance. So why do we create composite variables? Well, psychological constructs (e.g., attitudes, behaviors, feelings) are often multi-faceted, which means that single item will likely not “capture” the full underlying concept (i.e., construct) space or domain. Continuing with the performance evaluation example, job performance is often multi-faceted, as for a given job, the conceptual performance domain often involves a variety of different behavior types (e.g., customer service, administrative). By creating a composite variable out of variables that are intended to measure the different sub-facets of the conceptual performance domain, we can create an indicator of overall job performance. An overall scale score variable is a specific type of composite variable in which scores on items from a multi-item scale (i.e., measure) are combined (by computing the sum or the mean) into a new variable. This process results in each case (e.g., observation, employee, individual) receiving an overall scale score based on the sum or mean of their scores on the items from a given measure. For example, imagine that employees respond to an annual survey containing a three-item job satisfaction measure. An overall scale score variable could be created by computing each respondent’s average response scores to the three job satisfaction items and applying these to a new composite variable that represents each respondent’s overall level of job satisfaction. To justify whether it is appropriate to create a composite variable or which variables should be used to create the composite variable, we can estimate internal consistency reliability (via Cronbach’s alpha) for the set of variables (or subsets of those items). Based on data from a given sample, internal consistency reliability provides an indication of how homogeneous a set of variables are in terms of their scores. If you need a refresher on internal consistency reliability, refer to the previous chapter on estimating internal consistency reliability using Cronbach’s alpha. Finally, as a reminder, the following table includes the qualitative descriptors that can be used to interpret Cronbach’s alpha. Cronbach’s alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable 30.2 Tutorial This chapter’s tutorial demonstrates how to create a composite variable based on scores from a multi-item measure. To justify which items to include in the creation of the composite variable, we will use Cronbach’s alpha (\\(\\alpha\\)) as an indicator of internal consistency reliability. 30.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/vdmYv0YnWEE 30.2.2 Functions &amp; Packages Introduced Function Package rowMeans base R rowSums base R c base R names base R 30.2.3 Initial Steps If you haven’t already, save the file called “survey.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “survey.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;survey.csv&quot;) ## Rows: 156 Columns: 11 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (11): SurveyID, JobSat1, JobSat2, JobSat3, TurnInt1, TurnInt2, TurnInt3, Engage1, Engage2, Engage3, Engage4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 11 ## SurveyID JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 3 3 3 2 1 2 2 ## 2 2 4 4 4 3 3 2 4 4 4 4 ## 3 3 4 4 5 2 1 2 4 4 4 4 ## 4 4 2 3 3 4 4 4 4 4 4 4 ## 5 5 3 3 3 4 3 3 3 3 3 3 ## 6 6 3 3 3 3 2 2 4 4 5 3 The data frame includes annual employee survey responses from 156 employees to three Job Satisfaction items (JobSat1, JobSat2, JobSat3), three Turnover Intentions items (TurnInt1, TurnInt2, TurnInt3), and four Engagement items (Engage1, Engage2, Engage3, Engage4). Employees responded to each item using a 5-point response format, ranging from Strongly Disagree (1) to Strongly Agree (5). Assume that higher scores on an item indicate higher levels of that variable; for example, a higher score on TurnInt1 would indicate that the respondent has higher intentions of quitting the organization. 30.2.4 Compute Cronbach’s alpha To justify the creation of a composite variable (i.e., overall scale score variable) for one of the multi-item survey measures, we’ll first estimate internal consistency reliability using Cronbach’s alpha. To do so, we will use the alpha function from the psych package. To get started, install and access the psych package using the install.packages and library functions, respectively (if you haven’t already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Now let’s compute Cronbach’s alpha for the four-item engagement measure. # Estimate Cronbach&#39;s alpha for the four-item Engagement measure alpha(df[,c(&quot;Engage1&quot;,&quot;Engage2&quot;,&quot;Engage3&quot;,&quot;Engage4&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;Engage1&quot;, &quot;Engage2&quot;, &quot;Engage3&quot;, &quot;Engage4&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.84 0.84 0.8 0.56 5.1 0.021 3.5 0.66 0.56 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.79 0.84 0.87 ## Duhachek 0.79 0.84 0.88 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Engage1 0.78 0.78 0.71 0.55 3.6 0.030 0.00048 0.54 ## Engage2 0.78 0.78 0.71 0.54 3.5 0.030 0.00355 0.53 ## Engage3 0.82 0.82 0.75 0.60 4.5 0.025 0.00072 0.60 ## Engage4 0.79 0.79 0.72 0.55 3.7 0.030 0.00524 0.54 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Engage1 156 0.83 0.83 0.75 0.69 3.6 0.77 ## Engage2 156 0.84 0.84 0.76 0.70 3.4 0.83 ## Engage3 156 0.77 0.78 0.66 0.61 3.4 0.76 ## Engage4 156 0.84 0.83 0.74 0.68 3.6 0.86 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Engage1 0.01 0.06 0.35 0.49 0.10 0 ## Engage2 0.02 0.10 0.42 0.39 0.06 0 ## Engage3 0.00 0.10 0.44 0.40 0.06 0 ## Engage4 0.01 0.08 0.30 0.47 0.13 0 Note: If you see the following message at the top or bottom of your output, you can often safely ignore it – that is, unless you know that one or more items should have been reverse-coded. If an item needs to be reverse coded, then you would need to take care of that prior to running the alpha function. Some items ( [ITEM NAME] ) were negatively correlated with the total scale and probably should be reversed. The raw alpha (raw_alpha) based on all four engagement items exceeds the acceptable threshold of .70, and the Reliability if an item is dropped output table indicates that removing an item would result in a lower Cronbach’s alpha (i.e., lower internal consistency reliability estimate). Further, let’s imagine that the conceptual definition for engagement is the extent to which a person feels enthusiastic, energized, and driven to perform their work., and the items’ content are as follows: Engage1 - “When I’m working, I’m full of energy.” Engage2 - “I complete my work with enthusiasm.” Engage3 - “I find inspiration in my work.” Engage4 - “I have no problem working for long periods of time.” We will retain all four items when computing the composite variable for engagement because: Cronbach’s alpha for all four items is above .70 (and thus acceptable); Removing an item would decrease Cronbach’s alpha; and The content of all four items seems to fit within the conceptual definition of engagement. As a reminder, for the purposes of this book, we will consider a scale with an alpha greater than or equal to .70 to demonstrate acceptable internal consistency for the particular sample, whereas an alpha that falls within the range of .60-.69 would be considered questionable, and an alpha below .60 would be deemed unacceptable. Here is a table of more nuanced qualitative descriptors for Cronbach’s alpha: Cronbach’s alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable For a more in-depth review of internal consistency reliability and justifying which items (if any) to remove, be sure to check out the previous chapter. 30.2.5 Create a Composite Variable To create a composite variable, we will use the rowMeans function from base R, as it offers a straightforward approach. The function also allows us to decide what to do with cases that have missing data on one or more of the variables (e.g., items). Given that we feel justified to created an composite variable (i.e., overall scale score variable) based on the four engagement items (Engage1, Engage2, Engage3, Engage4), we will include all four of those items in our rowMeans function. First, let’s come up with a name for the composite variable we’re about to create; here, I decided to call the new variable Engage_Overall, as the variable will represent overall engagement. Second, we’ll append the new variable called Engage_Overall to the df data frame using the $ operator to indicate that the new variable will be added to that data frame object. Third, we’ll use the &lt;- operator to indicate that we are assigning the results of the rowMeans function to the new variable. Fourth, we will type the name of the rowMeans function. Fifth, as the first argument, type the name of the data frame object to which the items belong (df). Sixth, following the data frame name, type in brackets ([ ]), and within the brackets, type a comma (,) followed by the c (combine) function; by placing a comma in front of the c function, we are indicating that we will be referencing the names of columns (i.e., variables); within the c function, list the name of each item in quotation marks (\" \"), separated by commas (,). Finally, as the second argument in the rowMeans function, type the na.rm=TRUE argument, which will tell the function to compute the mean for each case that has at least one score for the specified items; in other words, this function allows for the row means to be computed even if there are missing data. [Note: If you wish to create a composite variable based on the sum of item scores, you can use the rowSums function from base R.] # Create composite (overall scale score) variable based on Engagement items df$Engage_Overall &lt;- rowMeans(df[,c(&quot;Engage1&quot;,&quot;Engage2&quot;,&quot;Engage3&quot;,&quot;Engage4&quot;)], na.rm=TRUE) Let’s take a look at the variables in our df data frame object by using the names function from base R. # Print variable names to Console names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; ## [8] &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage_Overall&quot; Note that we now have a variable called Engage_Overall, which is our composite variable meant to represent overall engagement for each survey respondent. You can take a closer look at the composite scores for the new Engage_Overall variable by using the View function from base R. # View variable names View(df) 30.2.6 Summary In this chapter, we learned how to create a composite variable (e.g., overall scale score variable) based on scores from a multi-item measure. To justify which items to include in the composite variable, we computed Cronbach’s alpha (\\(\\alpha\\)) as an estimate of internal consistency reliability. The rowMeans function (and rowSums function) from base R is quite useful when it comes to creating composite variables. References "],["employeetraining.html", "Chapter 31 Introduction to Employee Training 31.1 Needs Assessment 31.2 Learning Environment &amp; Enhancement 31.3 Training Methods 31.4 Training Evaluation 31.5 Chapters Included", " Chapter 31 Introduction to Employee Training Employee training is intended to enhance employees’ learning and develop their knowledge, skills, abilities, and other characteristics (KSAOs). In most circumstances, a goal of training is to help employees transfer what they learned during training to their job. A comprehensive review of employee training, in general, is beyond the scope of this chapter; however, if you’re looking for a very general and basic over of employee training, please check out the following conceptual video. In the sections that follow, you will also have an opportunity to drill down to specific steps in the training process. Link to conceptual video: https://youtu.be/nv6N-WqgkKA 31.1 Needs Assessment The needs assessment is a critical early step when developing a new training program, as it helps us target who needs training, what they need to be trained on, and what resources the organization has to support the training initiative. The following video provides a conceptual overview of a needs assessment. Link to conceptual video: https://youtu.be/PiW0gFxGgbs 31.2 Learning Environment &amp; Enhancement When developing a training program, the learning environment and learning enhancement should be emphasized. The following conceptual videos review the importance of these two priorties in the context of the training process. Link to conceptual video: https://youtu.be/uKSOyTm3XFY Link to conceptual video: https://youtu.be/uKSOyTm3XFY 31.3 Training Methods When we think of training, the training methods used to deploy the training program usually come to mind. In the following conceptual video, I review some examples of common training methods. Link to conceptual video: https://youtu.be/b_h3YHBzuzo 31.4 Training Evaluation The primary aim of this chapter is to describe a key step in the employee training process: training evaluation. Thus, we will spend more time reviewing this step of the training process. We can evaluate the effectiveness of a training program by using different types of training evaluation designs (i.e., research designs). Namely, we can apply various types of pre-experimental, quasi-experimental, or true experimental designs in order to evaluate a training program on selected outcomes (i.e., measures). Depending upon the type of training evaluation design we choose to apply, we will have varying degrees of confidence that and differences or changes we observe were caused by the training itself as opposed to other (confounding) factors. 31.4.1 Causal Inferences As described by Cascio and Boudreau (2011), for two hypothetical variables (e.g., X, Y) to be causally related, the following must be true. Y must not occur until after X, which is referred to as temporal precedence. X and Y are associated with one another, which is referred to as covariation. Other explanations of the association between X and Y can be ruled out or eliminated, which is referred to as control for confounding variables. 31.4.2 Training Evaluation Designs &amp; Statistical Analysis Examples of common training evaluation designs include (but are not limited to): Post-test-only without control group; Post-test-only with control group; Post-test-only with two comparison groups; Pre-test/post-test without control group; Pre-test/post-test with control group. Each type of training evaluation design generates data, and thus another important consideration during training evaluation is to determine which type of analysis is most appropriate for analyzing the data from a given training evaluation design. For example, an independent-samples t-test is often appropriate when analyzing data from a post-test-only with control group design, whereas a paired-samples t-test is often appropriate when analyzing data from a pre-test/post-test without control group training evaluation design. More advanced types of training evaluation designs (e.g., mixed-factorial designs) often require other types of statistical analyses, such as the analysis of (co)variance or regression. For a review of training evaluation designs as well as common outcome measures, please watch the following conceptual video. Link to conceptual video: https://youtu.be/XFJCKyAfW7Q 31.5 Chapters Included In the following chapters, you will have an opportunity to learn how to work with training evaluation data originating from different designs. Evaluating a Post-Test/Post-Test without Control Group Design Using Paired-Samples t-test Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA References "],["pretestposttest.html", "Chapter 32 Evaluating a Pre-Test/Post-Test without Control Group Design Using Paired-Samples t-test 32.1 Conceptual Overview 32.2 Tutorial 32.3 Chapter Supplement", " Chapter 32 Evaluating a Pre-Test/Post-Test without Control Group Design Using Paired-Samples t-test In this chapter, we will learn about the post-test/pre-test without control group training evaluation design and how a paired-samples t-test can be used to analyze the data acquired from this design. We’ll begin with conceptual overviews of this particular training evaluation design and of the paired-samples t-test, and then we’ll conclude with a tutorial. 32.1 Conceptual Overview In this section, we’ll begin by describing the post-test/pre-test without control group training evaluation design, and we’ll conclude by reviewing the paired-samples t-test, with discussions of statistical assumptions, statistical significance, and practical significance; the section wraps up with a sample-write up of a paired-samples t-test used to evaluate data from a post-test/pre-test without control group training evaluation design. 32.1.1 Review of Pre-Test/Post-Test without Control Group Design In a pre-test/post-test without control group training evaluation design (i.e., research design), all employees participate in the same training program, and there is no random assignment and no control group. This design can best be described as pre-experimental. A paired-samples t-test can be used to analyze the data from a pre-test/post-test without control group design, provided the statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a pre-test/post-test without control group design. As a strength, this design includes a pre-test, which assesses initial performance on the focal outcome measure. The pre-test serves as a baseline and gives us information about where the employees started with respect to outcome measure prior to completing the training. Further, the addition of a pre-test allows us to assess whether employees’ scores on the outcome measure have changed from before to after training. As a major weakness, this design lacks a control group and, moreover, random assignment to treatment and control groups; for these reasons, this design is not considered (quasi-)experimental. The lack of a control group (i.e., comparison group), means that we are unable to compare if the direction and amount of any observed change from pre-test to post-test differs from a group that did not receive the program training program. Consequently, this design doesn’t give us much confidence that any change we observe was due to the training itself – as opposed to natural maturation and developmental processes or other confounding factors. In other words, this design doesn’t given us much confidence that the training “caused” any change we observe from pre-test to post-test. 32.1.2 Review of Paired-Samples t-test Link to conceptual video: https://youtu.be/o_F3Y0A2q3Q The paired-samples t-test is an inferential statistical analysis that can be used to compare the to compare the mean of the differences between two sets of dependent scores to some population mean; in the context of training evaluation, the population mean is typically set to zero. That is, this analysis helps us understand whether one sample of cases shows evidence of change or difference between two dependent sets of scores. Often dependence refers to the fact that the two sets of scores (through which a difference score is calculated) come from the same group of individuals who were measured (assessed) twice over time (e.g., time 1 assessment and time 2 assessment for same employees), twice using the same scale associated with different foci (e.g., ratings of supervisor satisfaction and coworker satisfaction from same employees), or twice using the same scale evaluating a common target but associated with two different raters/sources (e.g., employee self-ratings of performance and supervisor-ratings of employee performance). The paired-samples t-test is sometimes called a dependent-samples t-test, a repeated-measures t-test, or a Student’s t-test. The formula for a paired-samples t-test can be written as follows: \\(t = \\frac{\\overline{X}_D - \\mu_0}{\\frac{s_D}{\\sqrt{n}}}\\) where \\(\\overline{X}_D\\) is the mean of the differences between the two sets of dependent scores, \\(\\mu_0\\) is the zero or non-zero population mean to which \\(\\overline{X}_D\\) is compared, \\(s_D\\) is the standard deviation of the differences between the dependent scores, and \\(n\\) refers to the number of cases (i.e., sample size). 32.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a paired-samples t-test include: The difference scores based on the two outcome measures (e.g., pre-test, post-test) are independent of each other, suggesting that cases (employees) are randomly sampled from the underlying population; The difference scores have a univariate normal distribution in the underlying population. 32.1.2.2 Statistical Significance If we wish to know whether the mean of the differences differs from the population mean to a statistically significant extent, we can compare our t-value value to a table of critical values of a t-distribution. If our calculated value is larger than the critical value given the number of degrees of freedom (df = n - 2) and the desired alpha level (i.e., significance level, p-value threshold), we would conclude that there is evidence that the mean of the differences differs to a statistically significant extent from the population mean (e.g., zero). Alternatively, we can calculate the exact p-value if we know the t-value and the degrees of freedom. Fortunately, modern statistical software calculates the t-value, degrees of freedom, and p-value for us. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the mean of the differences between the two sets of dependent scores is equal to the population mean; as noted above, in the training evaluation context, the population mean is often set to zero. In other words, if the p-value is less than .05, we conclude that the mean of the differences differs from zero to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the mean of the differences is equal to zero. Put differently, if the p-value is equal to or greater than .05, we conclude that the mean of the differences does not differ from zero to a statistically significant extent, leading us to conclude that there is no change/difference between the two sets of dependent scores in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our paired-samples t-test is estimated using data from a sample drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, the observed mean of the differences is a point estimate of the population parameter and is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether the difference between two means is is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying populations and construct CIs for each of those samples, then the true parameter (i.e., true value of the mean of the differences in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 32.1.2.3 Practical Significance A significant paired-samples t-test and associated p-value only tells us that the mean of the differences is statistically different from zero. It does not, however, tell us about the magnitude of the mean of the differences – or in other words, the practical significance. The standardized mean difference score (Cohen’s d) is an effect size, which means that it is a standardized metric that can be used to compare d-values compare samples. In essence, the Cohen’s d indicates the magnitude of the mean of the differences in standard deviation units. A d-value of .00 would indicate that the mean of the differences is equal to zero, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohen’s d Description .20 Small .50 Medium .80 Large Here is the formula for computing d: \\(d = \\frac{\\overline{X}_D} {s_D}\\) where \\(\\overline{X}_D\\) is the mean of the differences between the two sets of dependent scores, and \\(s_D\\) is the standard deviation of the differences between the dependent scores. 32.1.2.4 Sample Write-Up Example 1: The same groups of employees was assessed on the exact same knowledge test before and after completing the same training program; in other words, the employees took part in a pre-test/post-test without control group training evaluation design. Because each employee has two scores on the same test (corresponding to the pre-test and post-test), a paired-samples t-test is an appropriate inferential statistical analysis for determining whether the mean of the differences between the sets of pre-test and post-test scores differs from zero. More specifically, a paired-samples t-test was used to determine whether there were significant changes within participants in terms of their average test scores from before participating in the training program to after participating. The mean of the differences (i.e., before-training scores subtracted from after-training scores) for 25 participants was 19.64 (SD = 11.60), and possible test scores could range from 1 to 100 points, and the paired-samples t-test indicated that this mean of the differences is significantly greater than zero (t = 8.47, p &lt; .01, 95% CI[14.85, 24.43]) and is large in magnitude (d = 1.69). In sum, knowledge test scores were found to increase to a significantly significant and large extent from before to after completion of the training. Example 2: A single sample of 30 employees was asked to rate their pay satisfaction and their supervisor satisfaction. Here, the same employees rated their satisfaction with two different targets: pay and supervisor. Because the same employees rated two different ratings targets, we can classify it as a repeated-measures design, which is amenable to analysis using a paired-samples t-test, assuming the requisite statistical assumptions are met. A paired-samples t-test is used to determine whether there the mean of the differences between employees pay satisfaction and supervisor satisfaction differs from zero, and we found that the mean of the differences (M = 2.70, SD = 13.20) did not differ significantly from zero (t = 1.12, p = .27, 95% CI[-2.23, 7.62]) based on this sample of 30 employees. Because we did not find evidence of statistical significance, we assume that, statistically, the mean of the differences does not differ from zero, and thus we will not interpret the level of practical significance (i.e., effect size). 32.2 Tutorial This chapter’s tutorial demonstrates how to estimate a paired-samples t-test, test the associated statistical assumptions, and present the findings in writing and visually. 32.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/ZMc9IBFdGsw 32.2.2 Functions &amp; Packages Introduced Function Package ttest lessR c base R mean base R data.frame base R remove base R BarChart lessR pivot_longer tidyr factor base R 32.2.3 Initial Steps If you haven’t already, save the file called “TrainingEvaluation_PrePostOnly.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “TrainingEvaluation_PrePostOnly.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PrePostOnly.csv&quot;) ## Rows: 25 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (3): EmpID, PreTest, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;PreTest&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [25 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:25] 26 27 28 29 30 31 32 33 34 35 ... ## $ PreTest : num [1:25] 43 58 52 47 43 50 66 48 49 49 ... ## $ PostTest: num [1:25] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. PreTest = col_double(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID PreTest PostTest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 43 66 ## 2 27 58 74 ## 3 28 52 62 ## 4 29 47 84 ## 5 30 43 78 ## 6 31 50 73 There are 25 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), PreTest (pre-training scores on training assessment, ranging from 1-100, where higher scores indicate higher performance), and PostTest (post-training scores on training assessment, ranging from 1-100, where higher scores indicate higher performance). 32.2.4 Estimate Paired-Samples t-test In this chapter, we will review how to estimate a paired-samples t-test using the ttest function from the lessR package (Gerbing, Business, and University 2021). Using this function, we will evaluate whether the mean of the differences between the pre-test (PreTest) and post-test (PostTest) outcome variables differs significantly from zero. In other words, let’s find out if assessment scores increased, stayed the same, or decreased from before to after the employees participated in training. Prior to running the paired-samples t-test, we’ll have to assume (because we don’t have access to information that would indicate otherwise) that the data meet the statistical assumption that the difference scores based on the two outcome measures (e.g., pre-test, post-test) are independent of each other (i.e., randomly sampled from the underlying population). To access and use the ttest function, we need to install and/or access the lessR package. If you haven’t already, be sure to install the lessR package; if you’ve recently installed the package, then you likely don’t need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming it’s been installed), so don’t forget that important step. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package and its functions library(lessR) Please note that when you use the library function to access the lessR package in a new R or RStudio session, you will likely receive a message in red font in your Console. Typically, red font in your Console is not a good sign; however, accessing the lessR package is one of the unique situations in which a red-font message is not a warning or error message. You may also receive a warning message that indicates that certain “objects are masked.” For our purposes, we can ignore that message. The ttest function from the lessR package makes running a paired-samples t-test relatively straightforward, as wrapped up in the function are tests of the second statistical assumption (see Statistical Assumptions section). Now we’re ready to run a paired-samples t-test. To begin, type the name of the ttest function. As the first argument in the parentheses, specify the name of the first outcome variable (PreTest). As the second argument, specify the name of the second outcome variable (PostTest). Difference scores will be calculated as part of the function by subtracting the variable in the first argument from the variable in the second argument. For the third argument, use data= to specify the name of the data frame (td) where the outcome and predictor variables are located. For the fourth argument, enter paired=TRUE to inform R that the data are paired and, thus, you are requesting a paired-samples t-test. # Paired-samples t-test using ttest function from lessR ttest(PreTest, PostTest, data=td, paired=TRUE) ## ## ## ------ Describe ------ ## ## Difference: n.miss = 0, n = 25, mean = 19.640, sd = 11.597 ## ## ## ------ Normality Assumption ------ ## ## Null hypothesis is a normal distribution of Difference. ## Shapiro-Wilk normality test: W = 0.9687, p-value = 0.613 ## ## ## ------ Infer ------ ## ## t-cutoff for 95% range of variation: tcut = 2.064 ## Standard Error of Mean: SE = 2.319 ## ## Hypothesized Value H0: mu = 0 ## Hypothesis Test of Mean: t-value = 8.468, df = 24, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.787 ## 95% Confidence Interval for Mean: 14.853 to 24.427 ## ## ## ------ Effect Size ------ ## ## Distance of sample mean from hypothesized: 19.640 ## Standardized Distance, Cohen&#39;s d: 1.694 ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for 6.941 As you can see in the output, the ttest function provides descriptive statistics, assumption tests regarding the distribution of the difference scores, a statistical significance test of the mean comparison, and an indicator of practical significance in the form of the standardized mean difference (Cohen’s d). In addition, the default lollipop data visualization depicts the distance from the PreTest score to the PostTest score for each case, and the default density distribution depicts the shape of the difference-score distribution, and the extent to which the mean of this distribution appears (visually) to differ from zero. Description: The Description section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees (n = 25), and descriptively, the mean of the differences (PostTest minus PreTest) is 19.64 (SD = 11.60), which indicates that, on average, scores increased from before to after training (although we will not know if this difference is statistically significant until we get to the Inference section of the output). Normality Assumption: If the sample size is less than or equal to 30, then the Shapiro-Wilk normality test is used to test the null hypothesis that the distribution of the difference scores demonstrates univariate normality; as such, if the p-values associated with the test statistic (W) is less than the conventional alpha level of .05, then we would reject the null hypothesis and assume that the distribution is not normal. If, however, we fail to reject the null hypothesis, then we don’t have statistical evidence that the distribution is anything other than normal; in other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is likely normally distributed. In the output, we can see that the difference is normally distributed (W = .969, p = .613). Inference: In the Inference section of the output, you will find the statistical test of the null hypothesis (e.g., mean of the differences is equal to zero). First, take a look at the line prefaced with Hypothesis Test of Mean, as this line contains the results of the paired-samples t-test (t = 8.468, p &lt; .001). Because the p-value is less than our conventional two-tailed alpha cutoff of .05, we reject the null hypothesis and conclude that the mean of the differences differs from zero to a statistically significant extent. But in what direction? To answer this question, we need to look back to the Description section; in that section, the mean of the differences (PostTest minus PreTest) is 19.64; as such, we can conclude the following: On average, employees’ performed’ performance on the assessment increased after completing training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001). Regarding the 95% confidence interval, we can conclude that the true mean of the differences in the population is likely between 14.85 and 24.43 (on a 1-100 point assessment). Effect Size: In the Effect Size section, the standardized mean difference (Cohen’s d) is provided as an indicator of practical significance. In the output, d is equal to 1.69, which is considered to be a (very) large effect, according to conventional rules-of-thumb (see table below). Please note that typically we only interpret practical significance when the mean of the differences has been found to be statistically significant. Cohen’s d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program, which was evaluated using a pre-test/post-test without control group training evaluation design. Employees completed an assessment of their knowledge before training and after training, where assessment scores could range from 1-100. On average, employees’ performance on the assessment increased to a statistically significant extent from before to after completing the training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001, 95% CI[14.85, 24.43]). This improvement can be considered very large (d = 1.69). 32.2.5 Visualize Results Using Bar Chart When we find a statistically significant difference between the mean of the differences, we may decide to create a bar chart in which the mean of the first outcome variable (PreTest) and the mean of the second outcome variable (PostTest) are displayed as separate bars. We will use the BarChart function from lessR to do so. Before doing so, however, we must prep (i.e., restructure, manipulate) the data to meet the needs of the BarChart function. I will show you two approaches for prepping the data. The first approach requires the installation of the tidyr package (from the tidyverse universe of packages), and the second uses only functions from base R. In my opinion, the logic needed to apply the first approach is more straightforward; however, if you don’t find that to be the case, then by all means try out the second approach. 32.2.5.1 First Approach For the first approach to prepping the data and generating a bar chart, we will use the pivot_longer function from the tidyr package, which means that we need to install and/or access the tidyr package. If you’d like additional information on the restructuring data frames using the pivot_longer function, please check out the chapter on manipulating and restructuring data. If you haven’t already, be sure to install the lessR package; if you’ve recently installed the package, then you likely don’t need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming it’s been installed), so don’t forget that important step. [Note: If you run into any installation errors, try installing and access the tidyverse “grand” package, which subsumes the tidyr package, along with many others.] # Install tidyr package install.packages(&quot;tidyr&quot;) # Access tidyr package and its functions library(tidyr) Next, let’s use the pivot_longer function from the tidyr package to “pivot” (i.e., manipulate, restructure) our data into “long format”, where “long format” is sometimes referred to as “stacked format”. As the first step, create a unique name for an object that we can subsequently assign the manipulated data frame object to. Here, I call the new data frame object td_long to indicate that the new data frame object is in long format. Insert the &lt;-operator to the right of the new data frame object name. This operator will assign the manipulated data frame object to the new object we’ve named. Type the name of the pivot_longer function. As the first argument, type data= followed by the exact name of the original data frame object we were working with (td). This will tell the function which data frame object we wish to manipulate. As the second argument, type the cols= argument to specify a vector of two or more variables we wish to pivot from wide format (i.e., separate columns) to long format (i.e., a single column). Our goal here is to “stack” the PreTest and PostTest variable names as categories (i.e., levels) within a new variable that we will subsequently create. Thus, we will type the name of the c (combine) function from base R, and as the two arguments, we will list the names of the PreTest and PostTest variables. As the third argument, type the names_to= argument followed by the name of the new “stacked” variable with categorical labels that we’d like to create; make sure the name of the new variable is within quotation marks (\" \"). This is the variable that will contain the categories (i.e., levels) that were the PreTest and PostTest variable names from the original td data frame object. Here, I name this new variable \"Test\". As the fourth argument, type the values_to= argument followed by the name of a new variable containing the test scores that were originally nested within the PreTest and PostTest variable names from the original td data frame object; make sure the name of the new variable is within quotation marks (\" \"). Here, I name this new variable \"Score\". # Manipulate data from wide to long format (i.e., stack) td_long &lt;- pivot_longer(data=td, cols=c(PreTest,PostTest), names_to=&quot;Test&quot;, values_to=&quot;Score&quot;) Let’s take a peek at our new td_long data frame object by using the Print function. # Print new data frame object to Console print(td_long) ## # A tibble: 50 × 3 ## EmpID Test Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 PreTest 43 ## 2 26 PostTest 66 ## 3 27 PreTest 58 ## 4 27 PostTest 74 ## 5 28 PreTest 52 ## 6 28 PostTest 62 ## 7 29 PreTest 47 ## 8 29 PostTest 84 ## 9 30 PreTest 43 ## 10 30 PostTest 78 ## # ℹ 40 more rows As you can see, each observation (i.e., employee) now has two rows of data (as opposed to one), such that they have a row for PreTest scores and PostTest scores. These data are now in long (i.e., stacked) format. By default, the PreTest and PostTest levels of the Test variable we created will be in alphabetical order, such that PostTest will come before PreTest if we created any data visualizations or go to analyze the data from this variable. Intuitively, this is not the order we want, as temporally PreTest should precede (i.e., come before) PostTest. Fortunately, we can use the factor function from base R to convert the Test variable to an ordered factor. Let’s overwrite the existing Test variable from the td_long data frame object by typing the name of the data frame object (td_long) followed by the $ operator and the name of the variable (Test). To the right of td_long$Test, type the &lt;- operator so that we can assign the new ordered factored variable to the existing variable called Test from the td_long data frame object. To the right of the &lt;- operator, type the name of the factor function. As the first argument, type the name of the data frame object (td_long) followed by the $ operator and the name of the variable (Test). As the second argument, type ordered=TRUE to signify that this variable will have ordered levels. As the third argument, type levels= followed by a vector of the variable levels (i.e., categories) in ascending order. Note that we use the c (combine) function from base R to construct the vector, and we need to put each level within quotation marks (\" \"). Here, we wish to order the levels such that PreTest comes before PostTest, so we insert the following: c(\"PreTest\",\"PostTest\"). # Re-order the Test variable so that PreTest comes first td_long$Test &lt;- factor(td_long$Test, ordered=TRUE, levels=c(&quot;PreTest&quot;,&quot;PostTest&quot;)) We are now ready to create our bar chart of the average scores on the pre-test and the post-test from our pre-test/post-test without control group evaluation design. Begin by typing the name of the BarChart function from the lessR package; if you haven’t already, be sure that you have recently installed the lessR package and that you have accessed the package in your current R session. See above for more details on how to install and access the lessR package. As the first argument, type x= followed by the name of the categorical (i.e., nominal, ordinal) variable that contains the names of the different test times. In this example, the name of the variable is Test, and it contains the levels (i.e., categories) PreTest and PostTest. As the second argument, type y= followed by the name of the variable that contains the scores for the different test administration times. In this example, the name of the variable is Score, and it contains the numeric scores on the PreTest and PostTest for each employee (i.e., observation). As the third argument, type data= followed by the exact name of the data frame object to which the variables specified in the two previous arguments belong. In this example, the name of the data frame object is td_long. As the fourth argument, type stat=\"mean\" to indicate that we wish for the BarChart function to compute the average (i.e., mean) score from the Score variable for each level of the Test variable. # Create bar chart BarChart(x=Test, y=Score, data=td_long, stat=&quot;mean&quot;) ## Score ## - by levels of - ## Test ## ## n miss mean sd min mdn max ## PreTest 25 0 52.72 7.05 43.00 51.00 66.00 ## PostTest 25 0 72.36 6.98 60.00 73.00 84.00 ## &gt;&gt;&gt; Suggestions ## Plot(Score, Test) # lollipop plot ## ## Plotted Values ## -------------- ## PreTest PostTest ## 52.720 72.360 If you wish, you can then add additional arguments like xlab= and ylab= to re-label the x- and y-axes, respectively. # Create bar chart BarChart(x=Test, y=Score, data=td_long, stat=&quot;mean&quot;, xlab=&quot;Time&quot;, ylab=&quot;Average Score&quot;) ## Score ## - by levels of - ## Test ## ## n miss mean sd min mdn max ## PreTest 25 0 52.72 7.05 43.00 51.00 66.00 ## PostTest 25 0 72.36 6.98 60.00 73.00 84.00 ## &gt;&gt;&gt; Suggestions ## Plot(Score, Test) # lollipop plot ## ## Plotted Values ## -------------- ## PreTest PostTest ## 52.720 72.360 32.2.5.2 Second Approach For the second approach, we will create a data frame of the means and their names so that we can use the data frame as input into the BarChart function. Come up with a unique name for a vector of means that we will create; here, I name the vector prepost_means. Type the &lt;- operator to the right of the vector name (prepost_means) so that we can assign the vector we create to the right of the &lt;- operator to the new object. Second, type the name of the c function from base R. As the first argument within the c function, type the name of the mean function from base R, and within that function’s parentheses, type the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). As the second argument within the mean function, type na.rm=TRUE in case there are missing data. As the second argument within the c function, type the name of the mean function from base R, and within that function’s parentheses, type the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest); as the second argument within the mean function, type na.rm=TRUE in case there are missing data. # Create vector of means for first and second outcome variables prepost_means &lt;- c(mean(td$PreTest, na.rm=TRUE), mean(td$PostTest, na.rm=TRUE)) Now it’s time to create a vector of the names (i.e., variable names) that correspond with the means contained within the vector we just created. First, come up with a name for the vector object that will contain the names; here, I name the vector meannames using the &lt;- operator. Second, type the name of the c function from base R. As the first argument within the function, type what you wish to name the first mean (i.e., variable name for the first value), and put it in quotation marks (\"Pre-Test\"). As the second argument within the function, type what you wish to name the second mean (i.e., variable name for the second value), and put it in quotation marks (\"Post-Test\"). # Create vector of names corresponding to the first and second means above meannames &lt;- c(&quot;Pre-Test&quot;,&quot;Post-Test&quot;) To combine these two vectors into a data frame, we will use the data.frame function from base R. First, come up with a name for the data frame object; here, I name the data frame meansdf using the &lt;- naming symbol. Second, type the name of the data.frame function. As the first argument, type the name of meannames vector we created. As the second argument, type the name of the prepost_means vector we created. # Create data frame from the two vectors created above meansdf &lt;- data.frame(meannames, prepost_means) Remove the vectors called meannames and prepost_means from your R Global environment to avoid any confusion when specifying the BarChart function. Use the remove function from base R to accomplish this. # Remove the two vectors created above from R Global Environment remove(prepost_means) remove(meannames) Now have data in a format that can be used as input in the BarChart function from the lessR package. Begin by typing the name of the BarChart function. As the first argument, type the name of the variable that contains the names of the means (meannames). As the second argument, type the name of the variable that contains the actual means (prepost_means). As the third argument, type data= followed by the name of the data frame to which the aforementioned variables belong (meansdf). As the fourth argument, use xlab= to provide the x-axis label (\"Time\"). As the fifth argument, use ylab= to provide the y-axis label (\"Average Score\"). # Create bar chart BarChart(meannames, prepost_means, data=meansdf, xlab=&quot;Time&quot;, ylab=&quot;Average Score&quot;) ## &gt;&gt;&gt; Suggestions ## Plot(prepost_means, meannames) # lollipop plot ## ## Plotted Values ## -------------- ## Pre-Test Post-Test ## 52.720 72.360 32.2.6 Summary In this chapter, we learned how to estimate a paired-samples t-test to determine whether the mean of the differences between scores on two dependent variables is significantly different from zero. This analysis is useful when evaluating a pre-test/post-test without control group training design. The ttest function from lessRcan be used to run paired-samples t-tests. We also learned how to visualize the results using the BarChart function from lessR. 32.3 Chapter Supplement In addition to the ttest function from the lessR package covered above, we can use the t.test function from base R to estimate a paired-samples t-test. Because this function comes from base R, we do not need to install and access an additional package. 32.3.1 Functions &amp; Packages Introduced Function Package shapiro.test base R t.test base R cohen.d effsize c base R mean base R 32.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PrePostOnly.csv&quot;) ## Rows: 25 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (3): EmpID, PreTest, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;PreTest&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [25 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:25] 26 27 28 29 30 31 32 33 34 35 ... ## $ PreTest : num [1:25] 43 58 52 47 43 50 66 48 49 49 ... ## $ PostTest: num [1:25] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. PreTest = col_double(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID PreTest PostTest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 43 66 ## 2 27 58 74 ## 3 28 52 62 ## 4 29 47 84 ## 5 30 43 78 ## 6 31 50 73 32.3.3 t.test Function from Base R By itself, the t.test function from base R does not generate statistical assumption tests and and estimates of effect size (practical significance) like the ttest function from lessR does. Thus, we will need to apply additional functions to achieve all of the necessary output. Before running our paired-samples t-test using the t.test function from base R, we should test the second assumption (see Statistical Assumptions section). To do so, we will estimate the Shapiro-Wilk normality test. This test was covered in detail above when we applied the ttest function from lessR, so we will breeze through the interpretation in this section. Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then we can assume that the assumption of univariate normality has been met, which means we won’t formally test the assumption for the two independent samples. To compute the Shapiro-Wilk normality test to test the assumption that the difference scores are normally distributed, we will use the shapiro.test function from base R. First, type the name of the shapiro.test function. As the sole argument, type the name of the data frame (td), followed by the $ symbol and the difference between the PostTest scores and PreTest scores: td$PostTest - td$PreTest. # Compute Shapiro-Wilk normality test for normal distribution shapiro.test(td$PostTest - td$PreTest) ## ## Shapiro-Wilk normality test ## ## data: td$PostTest - td$PreTest ## W = 0.96874, p-value = 0.6134 The output of the script/code above indicates that the p-value associated with the test is equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the difference scores are normally distributed. In other words, we have no evidence to suggest that the difference score variable scores are anything other than normally distributed. With confidence that we met the statistical assumptions for a paired-samples t-test, we are ready to apply the t.test function from base R. To begin, type the name of the t.test function. As the first argument, specify the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest). As the second argument, specify the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). Note that the ordering of the variables is opposite of the ttest function from lessR. Difference scores will be calculated as part of the function by subtracting the variable in the second argument from the variable in the first variable argument. For the third argument, enter paired=TRUE to inform R that the data are paired, which means you are requesting a paired-samples t-test. # Paired-samples t-test using t.test function from base R t.test(td$PostTest, td$PreTest, paired=TRUE) ## ## Paired t-test ## ## data: td$PostTest and td$PreTest ## t = 8.4677, df = 24, p-value = 0.00000001138 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 14.853 24.427 ## sample estimates: ## mean difference ## 19.64 Note that the t.test output provides you with the results of the paired-samples t-test in terms of the formal statistical test (t = 8.4677, p &lt; .001), the 95% confidence interval (95% CI[14.853, 24.427]), and the mean of the differences (M = 19.64). The output, however, does not include an estimate of practical significance. To compute Cohen’s d as an estimate of practical significance we will use the cohen.d function from the effsize package (Torchiano 2020). If you haven’t already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function, specify the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest). As the second argument, specify the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). As the third argument, type paired=TRUE to indicate that the data are indeed paired (i.e., dependent). # Compute Cohen&#39;s d cohen.d(td$PostTest, td$PreTest, paired=TRUE) ## ## Cohen&#39;s d ## ## d estimate: 2.800502 (large) ## 95 percent confidence interval: ## lower upper ## 1.325315 4.275689 The output indicates that Cohen’s d is 1.694, which would be considered large by conventional cutoff standards. Cohen’s d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program, which was evaluated using a pre-test/post-test without control group training evaluation design. Employees completed an assessment of their knowledge before training and after training, where assessment scores could range from 1-100. On average, employees’ performance on the assessment increased to a statistically significant extent from before to after completing the training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001, 95% CI[14.85, 24.43]). This improvement can be considered very large (d = 1.69). References "],["posttestonly.html", "Chapter 33 Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test 33.1 Conceptual Overview 33.2 Tutorial 33.3 Chapter Supplement", " Chapter 33 Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test In this chapter, we learn about the post-test-only with control group training evaluation design and how a independent-samples t-test can be used to analyze the data acquired from this design. We’ll begin with conceptual overviews of this training evaluation design and of the independent-samples t-test, and then we’ll conclude with a tutorial. 33.1 Conceptual Overview In this section, we’ll begin by describing the post-test-only with control group training evaluation design, and we’ll conclude by reviewing the independent-samples t-test, with discussions of statistical assumptions, statistical significance, and practical significance; the section wraps up with a sample-write up of a independent-samples t-test used to evaluate data from a post-test-only with control group training evaluation design. 33.1.1 Review of Post-Test-Only with Control Group Design In a post-test-only with control group training evaluation design (i.e., research design), employees are assigned (randomly or non-randomly) to either a treatment group (e.g., new training program) or a control group (e.g., comparison group, old training program), and every participating employee is assessed on selected training outcomes (i.e., measures) after the training has concluded. If random assignment to groups is used, then a post-test-only with control group design is considered experimental. Conversely, if non-random assignment to groups is used, then the design is considered quasi-experimental. Regardless of whether random or non-random assignment is used, an independent-samples t-test can be used to analyze the data from a post-test-only with control group design, provided key statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a post-test-only with control group design. As a major strength, this design includes a control group, and if coupled with random assignment to groups, then the design qualifies as a true experimental design. With that being said, if we use non-random assignment to the treatment and control groups, then we are less likely to have equivalent groups of individuals who enter each group, which may bias how they engage in the demands of their respective group and how they complete the outcome measures. Further, because this design lacks a pre-test (i.e., assessment of initial performance on the outcome measures), we cannot be confident that employees in the treatment and control groups “started in the same place” with respect to the outcome(s) we might measure at post-test. Consequently, any differences we observe between the two groups on a post-test outcome measure may reflect pre-existing differences – meaning, the training may not have “caused” the differences that are apparent at post-test. 33.1.2 Review of Independent-Samples t-test Link to conceptual video: https://youtu.be/9uLd4nzGyGQ The independent-samples t-test is an inferential statistical analysis that can be used to compare the to compare the means of two independent groups, such as a treatment and control group. That is, this analysis compares differences in means when we have two separate groups of cases drawn from two populations; critically, each case must appear in only one of the two samples, hence the name independent-samples t-test. In the context of a post-test-only training evaluation design, we can conceptualize group membership (e.g., treatment vs. control) as a categorical (nominal, ordinal) predictor variable with just two categories (i.e., levels), and the post-test outcome measure as a continuous (interval, ratio) outcome variable. Importantly, the outcome variable must be continuous for an independent-samples t-test to be an appropriate analysis. The independent-samples t-test is sometimes called a between-subjects t-test or a two-groups t-test. The formula for an independent-samples t-test can be written as follows: \\(t = \\frac{\\overline{X}_1 - \\overline{X}_2}{s_{{X}_{1}{X}_2} \\sqrt{\\frac{2}{n}}}\\) where \\(X_{1}\\) is the mean of the first group and \\(X_{2}\\) is the mean of the second group, \\(s_{{X}_{1}{X}_2}\\) is the pooled standard deviation of \\(X_{1}\\) and \\(X_{2}\\), and \\(n\\) refers to the number of cases (i.e., sample size). 33.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from an independent-samples t-test include: The outcome (dependent, response) variable has a univariate normal distribution in each of the two underlying populations (e.g., samples, groups, conditions), which correspond to the two categories (levels) of the predictor (independent, explanatory) variable; The variances of the outcome (dependent, response) variable are equal across the two populations (e.g., samples, groups, conditions), which is often called the equality of variances or homogeneity of variances assumption. 33.1.2.2 Statistical Significance If we wish to know whether the two means we are comparing differ to a statistically significant extent, we can compare our t-value value to a table of critical values of a t-distribution. If our calculated value is larger than the critical value given the number of degrees of freedom (df = n - 2) and the desired alpha level (i.e., significance level, p-value threshold), we would conclude that there is evidence of a significant difference in means between the two independent samples. Alternatively, we can calculate the exact p-value if we know the t-value and the degrees of freedom. Fortunately, modern statistical software calculates the t-value, degrees of freedom, and p-value for us. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the difference between the two means is equal to zero. In other words, if the p-value is less than .05, we conclude that the two means differ from each other to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the difference between the two means is equal to zero. Put differently, if the p-value is equal to or greater than .05, we conclude that the two means do not differ from zero to a statistically significant extent, leading us to conclude that there is no difference between the two means in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our independent-samples t-test is estimated using data from a sample drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, the observed difference between the two means is a point estimate of the population parameter and is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether the difference between two means is is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying populations and construct CIs for each of those samples, then the true parameter (i.e., true value of the difference in means in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 33.1.2.3 Practical Significance A significant independent-samples t-test and associated p-value only tells us that the two means are statistically different from one another. It does not, however, tell us about the magnitude of the difference between means – or in other words, the practical significance. The standardized mean difference score (Cohen’s d) is an effect size, which means that it is a standardized metric that can be used to compare d-values compare samples. In essence, the Cohen’s d indicates the magnitude of the difference between means in standard deviation units. A d-value of .00 indicates that there is no difference between the two means, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohen’s d Description .20 Small .50 Medium .80 Large Here is the formula for computing d: \\(d = t \\sqrt{\\frac{n_1 + n_2} {n_1n_2}}\\) where \\(t\\) refers to the calculated \\(t\\)-value, \\(n_1\\) refers to the sample size of the first independent sample, and \\(n_2\\) refers to the sample size of the second independent sample. 33.1.2.4 Sample Write-Up One group of 25 participants completes a new safety training program (treatment group), and a separate group of 25 participants completes the old safety training program (control group). Both groups of participants complete a safety knowledge test one week after completing their respective training programs. In other words, these participants were part of a post-test-only with control group training evaluation design. An independent-samples t-test was used to determine whether the average safety knowledge test score for the group that completed the new training program was significantly different than the average safety knowledge test score for the group that completed the old training program. We found a statistically significant difference between the means on the safety knowledge test for the treatment group that participated in the new training program (M = 72.36, SD = 6.98, n = 25) and the control group that participated in the old training program (M = 61.32, SD = 9.15, n = 25), such that those who participated in the new training program performed better on the safety knowledge test (t = 4.80, p &lt; .01, 95% CI[6.41, 15.67]). Further, because we found a statistically significant difference in means, we then interpreted the practical significance of this effect, which was large (d = 1.36). 33.2 Tutorial This chapter’s tutorial demonstrates how to estimate an independent-samples t-test, test the associated statistical assumptions, and present the findings in writing and visually. 33.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/oATcHuMZtuo 33.2.2 Functions &amp; Packages Introduced Function Package ttest lessR BarChart lessR 33.2.3 Initial Steps If you haven’t already, save the file called “TrainingEvaluation_PostControl.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “TrainingEvaluation_PostControl.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PostControl.csv&quot;) ## Rows: 50 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (2): EmpID, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [50 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:50] 26 27 28 29 30 31 32 33 34 35 ... ## $ Condition: chr [1:50] &quot;New&quot; &quot;New&quot; &quot;New&quot; &quot;New&quot; ... ## $ PostTest : num [1:50] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 New 66 ## 2 27 New 74 ## 3 28 New 62 ## 4 29 New 84 ## 5 30 New 78 ## 6 31 New 73 There are 50 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), Condition (training condition: New = new training program, Old = old training program), and PostTest (post-training scores on training assessment, ranging from 1-100, where 100 indicates better performance). Regarding participation in the training conditions, 25 employees participated in the old training program, and 25 employees participated in the new training program. 33.2.4 Estimate Independent-Samples t-test Different functions are available that will allow us to estimate an independent-samples t-test in R. In this chapter, we will review how to run an independent-samples t-test using the ttest function from the lessR package (Gerbing, Business, and University 2021), as it produces a generous amount of output automatically. Using this function, we will evaluate whether the means on the post-test variable (PostTest) differ based on the two levels of the training Condition variable (New, Old) differ from one another to a statistically significant extent; in other words, let’s find out if we should treat the means as being different from one another. To access and use the ttest function, we need to install and/or access the lessR package. If you haven’t already, be sure to install the lessR package; if you’ve recently installed the package, then you likely don’t need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming it’s been installed), so don’t forget that important step. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package and its functions library(lessR) Please note that when you use the library function to access the lessR package in a new R or RStudio session, you will likely receive a message in red font in your Console. Typically, red font in your Console is not a good sign; however, accessing the lessR package is one of the unique situations in which a red-font message is not a warning or error message. You may also receive a warning message that indicates that certain “objects are masked.” For our purposes, we can ignore that message. Now we’re ready to run an independent-samples t-test. To begin, type the name of the ttest function. As the first argument in the parentheses, specify the statistical model that we wish to estimate. To do so, type the name of the continuous outcome (dependent) variable (PostTest) to the left of the ~ operator and the name of the categorical predictor (independent) variable (Condition) to the right of the ~ operator. For the second argument, use data= to specify the name of the data frame where the outcome and predictor variables are located (td). For the third argument, enter paired=FALSE to inform R that the data are not paired (i.e., you are not requesting a paired-samples t-test). # Estimate independent-samples t-test ttest(PostTest ~ Condition, data=td, paired=FALSE) ## ## Compare PostTest across Condition with levels New and Old ## Response Variable: PostTest, PostTest ## Grouping Variable: Condition, ## ## ## ------ Describe ------ ## ## PostTest for Condition New: n.miss = 0, n = 25, mean = 72.360, sd = 6.975 ## PostTest for Condition Old: n.miss = 0, n = 25, mean = 61.320, sd = 9.150 ## ## Mean Difference of PostTest: 11.040 ## ## Weighted Average Standard Deviation: 8.136 ## ## ## ------ Assumptions ------ ## ## Note: These hypothesis tests can perform poorly, and the ## t-test is typically robust to violations of assumptions. ## Use as heuristic guides instead of interpreting literally. ## ## Null hypothesis, for each group, is a normal distribution of PostTest. ## Group New Shapiro-Wilk normality test: W = 0.950, p-value = 0.253 ## Group Old Shapiro-Wilk normality test: W = 0.969, p-value = 0.621 ## ## Null hypothesis is equal variances of PostTest, homogeneous. ## Variance Ratio test: F = 83.727/48.657 = 1.721, df = 24;24, p-value = 0.191 ## Levene&#39;s test, Brown-Forsythe: t = -0.955, df = 48, p-value = 0.344 ## ## ## ------ Infer ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## t-cutoff for 95% range of variation: tcut = 2.011 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t-value = 4.798, df = 48, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.627 ## 95% Confidence Interval for Mean Difference: 6.413 to 15.667 ## ## ## --- Do not assume equal population variances of PostTest for each Condition ## ## t-cutoff: tcut = 2.014 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t = 4.798, df = 44.852, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.635 ## 95% Confidence Interval for Mean Difference: 6.405 to 15.675 ## ## ## ------ Effect Size ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## Standardized Mean Difference of PostTest, Cohen&#39;s d: 1.357 ## ## ## ------ Practical Importance ------ ## ## Minimum Mean Difference of practical importance: mmd ## Minimum Standardized Mean Difference of practical importance: msmd ## Neither value specified, so no analysis ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for Condition New: 4.175 ## Density bandwidth for Condition Old: 5.464 As you can see in the output, the ttest function provides descriptive statistics, assumption tests regarding the distributions and the variances, a statistical significance test of the mean comparison, and an indicator of practical significance in the form of the standardized mean difference (Cohen’s d). In addition, the default data visualization depicting the two density distributions is helpful for understanding the difference between the two means. Let’s now review each section of the output. Description: The Description section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees in each condition (n = 25), and descriptively, the mean PostTest score for the New condition is 72.36 (SD = 6.98), and the mean PostTest score for the Old condition is 61.32 (SD = 9.15). The difference between the two means is 11.04. Assumptions: As noted in the output: “These hypothesis tests can perform poorly, and the t-test is typically robust to violations of assumptions. Use as heuristic guides instead of interpreting literally.” Given that, we shouldn’t put too much of an emphasis on these statistical assumption tests, but nevertheless than can provide some guidance when it comes to detecting potential statistical-assumption violations that might preclude us from choosing to interpret the results of the independent-samples t-test itself. Let’s dive into interpreting these assumption tests. The Shapiro-Wilk normality test is used to test the null hypothesis that a distribution is normal; as such, if the p-value associated with the test statistic (W) is less than the conventional alpha level of .05, then we reject the null hypothesis and assume that the distribution is not normal. If, however, p-value associated with the test statistic (W) is greater than .05, then we fail to reject the null hypothesis, which means that we do not have statistical evidence that the distribution is anything other than normal; in other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is normally distributed, and can feel satisfied that we have met the statistical assumption of normality for that particular distribution. In the output, we can see that the PostTest variable for those in the New training condition is normally distributed (W = .950, p = .253), and for those in the Old training condition, the PostTest variable is also normally distributed (W = .969, p = .621). Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then the ttest function won’t report the Shapiro-Wilk test, as univariate normality will be assumed. As for the equal variances assumption, Levene’s test (i.e., homogeneity of variances test) is commonly used. The null hypothesis of this test is that the variances are equal. Thus, if the p-value is less than the conventional alpha level of .05, then we reject the null hypothesis and assume the variances are different. If, however, the p-value is equal to or greater than .05, then we fail to reject the null hypothesis and assume that the variances are equal (i.e., variances are homogeneous). In the output, we see that the test is nonsignificant (t = -.955, p = .344), which suggests that, based on this test, we have no reason to believe that the two variances are anything but equal. All in all, we found evidence to support that we met the two statistical assumptions necessary to proceed forward with making inferences. Inference: The Inference section of the output is where you will find the independent-samples t-test itself. This section is called Inference because this is where we’re making statistical inferences about the underlying population of employees. Specifically, the t-test and its associated p-value represent the statistical test of the null hypothesis (i.e., the two means are equal). If we have evidence that the variances are equal (which we do based on Levene’s test), then we should interpret the sub-section titled Assume equal population variances of PostTest for each Condition. If we had instead found evidence that the variances were not equal, then we would want to interpret the sub-section titled Do not assume equal population variances of PostTest for each Condition. First, take a look at the line prefaced with Hypothesis Test of 0 Mean Diff; this line contains the results of the independent-samples t-test (t = 4.798, p &lt; .001). Because the p-value is less than our conventional two-tailed alpha cutoff of .05, we reject the null hypothesis and conclude that the two means are different from one another. How do we know which mean is greater (or less than) the other? Well, we need to look back to the Description section; in that section, we see that the mean PostTest score for the New training condition is greater than the mean for the Old training condition; as such, we can conclude the following: The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001). Regarding the 95% confidence interval, we can conclude that the true mean difference in the population is likely between 6.41 and 15.67 (on a 1-100 point assessment). Effect Size: In the Effect Size section, the standardized mean difference (Cohen’s d) is provided as an indicator of practical significance. In the output, d is equal to 1.36, which is considered to be a (very) large effect, according to conventional rules-of-thumb (see table below). Please note that typically we only interpret practical significance when a difference has been found to be statistically significant (see Inference section). Cohen’s d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program and 25 employees participated in the old training program. After completely their respective training programs, employees completed an assessment of their knowledge, where scores could range from 1-100. The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001, 95% CI[6.41, 15.67]). This difference can be considered to be very large (d = 1.36). 33.2.5 Visualize Results Using Bar Chart When we find a statistically significant difference between two means based on an independent-samples t-test, you may decide to present the two means in a bar chart. We will use the BarChart function from lessR to do so. Type the name of the BarChart function. As the first argument, type x= followed by the name of the categorical predictor variable (Condition). As the second argument, type y= followed by the name of the continuous outcome variable (PostTest). As the third argument, specify stat=\"mean\" to request the application of the mean function to the PostTest variable based on the levels of the Condition variable. As the fourth argument, type data= followed by the name of the data frame object to which our predictor and outcome variables belong (td). As the fifth argument, use xlab= to provide the x-axis label (\"Training Condition\"). As the sixth argument, use ylab= to provide the y-axis label (\"Post-Test Score\"). # Create bar chart BarChart(x=Condition, y=PostTest, stat=&quot;mean&quot;, data=td, xlab=&quot;Training Condition&quot;, ylab=&quot;Post-Test Score&quot;) ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## Old 25 0 61.32 9.15 42.00 61.00 79.00 ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, Condition) # lollipop plot ## ## Plotted Values ## -------------- ## New Old ## 72.360 61.320 33.2.6 Summary In this chapter, we learned to use the independent-samples t-test to compare two means from independent groups of cases, which is the case when we evaluating a training program using a post-test-only with control group design. The ttest function from lessR can be used to run an independent-samples t-test, and the BarChart function from lessR can be used to present the results of a significant difference visually. 33.3 Chapter Supplement In addition to the ttest function from the lessR package covered above, we can use the t.test function from base R to estimate an independent-samples t-test. Because this function comes from base R, we do not need to install and access an additional package. 33.3.1 Functions &amp; Packages Introduced Function Package tapply base R shapiro.test base R leveneTest car t.test base R cohen.d effsize mean base R 33.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PostControl.csv&quot;) ## Rows: 50 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (2): EmpID, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [50 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:50] 26 27 28 29 30 31 32 33 34 35 ... ## $ Condition: chr [1:50] &quot;New&quot; &quot;New&quot; &quot;New&quot; &quot;New&quot; ... ## $ PostTest : num [1:50] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 New 66 ## 2 27 New 74 ## 3 28 New 62 ## 4 29 New 84 ## 5 30 New 78 ## 6 31 New 73 33.3.3 t.test Function from Base R By itself, the t.test function from base R does not generate statistical assumption tests and and estimates of effect size (practical significance) like the ttest function from lessR does. Thus, we will need to apply additional functions to achieve all of the necessary output. Before running our independent-samples t-test using the t.test function from base R, we should test two assumptions (see Statistical Assumptions section). We will begin by estimating the Shapiro-Wilk normality test and Levene’s test of equal variances (i.e., homogeneity of variances), which are two statistical tests of the two statistical assumptions. Both of these tests were covered in detail above when we applied the ttest function from lessR, so we will breeze through the interpretation in this section. To compute the Shapiro-Wilk normality test to test the assumption of normal distributions, we will use the shapiro.test function from base R. Because we need to test the assumption of normality of the outcome variable (PostTest) for both levels of the predictor variable (Condition), we also need to use the tapply function from base R. The tapply function can be quite useful, as it allows us to apply a function to a variable for each level of another categorical (nominal, ordinal) variable. To begin, type the name of the tapply function. As the first argument, type the name of the data frame (td), followed by the $ symbol and the name of the outcome variable (PostTest). As the second argument, type the name of the data frame (td), followed by the $ symbol and the name of the categorical predictor variable (Condition). Finally, as the third argument, type the name of the shapiro.test function. Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then we can assume that the assumption of univariate normality has been met, which means we won’t formally test the assumption for the two independent samples. # Compute Shapiro-Wilk normality test for normal distributions tapply(td$PostTest, td$Condition, shapiro.test) ## $New ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.95019, p-value = 0.2533 ## ## ## $Old ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.96904, p-value = 0.6208 The output indicates that the p-values associated with both tests are equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed. In other words, we have evidence that the outcome variable is normally distributed for both conditions, which suggests that we have met the first statistical assumption. To test the equality (homogeneity) of variances assumption, we will use the leveneTest function from the car package. More than likely the car package is already installed, as many other packages are dependent on it. That being said, you may still need to install the package prior to accessing it using the library function. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the outcome variable (PostTest) to the left of the ~ symbol and the name of the predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). # Compute Levene&#39;s test for equal variances leveneTest(PostTest ~ Condition, data=td) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.9121 0.3443 ## 48 The output indicates that the p-value (i.e., Pr(&gt;F) = .3443) associated with Levene’s test is equal to or greater than an alpha of .05; thus, we fail to reject the null hypothesis that the variances are equal and thus conclude that the variances are equal. We have satisfied the second statistical assumption. Now that we seem to have satisfied the two statistical assumptions, we are ready to apply the t.test function from base R. To begin, type the name of the t.test function. As the first argument, type the name of the outcome (dependent) variable (PostTest) to the left of the ~ symbol and the name of the predictor (independent) variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). As the third argument, type paired=FALSE to indicate that the data are not paired, which means to that are not requesting a paired-samples t-test. As the final argument, type var.equal=TRUE to indicate that we found evidence that the variances were equal; if you had found evidence that the variances were not equal, then you would use the argument var.equal=FALSE. # Independent-samples t-test using t.test function from base R t.test(PostTest ~ Condition, data=td, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: PostTest by Condition ## t = 4.7976, df = 48, p-value = 0.000016 ## alternative hypothesis: true difference in means between group New and group Old is not equal to 0 ## 95 percent confidence interval: ## 6.413209 15.666791 ## sample estimates: ## mean in group New mean in group Old ## 72.36 61.32 Note that the output provides you with the results of the independent-samples t-test in terms of a formal statistical test (t = 4.798, p &lt; .001), the 95% confidence interval (95% CI[6.413, 15.667]), and the mean of the outcome variable for each level of the categorical predictor variable (New condition: M = 72.36; Old condition: M = 61.32). Thus, we found evidence that the mean PostTest score for the New training condition was statistically significantly higher than the mean of the Old training condition. The output, however, does not include an estimate of practical significance (i.e., effect size). To compute Cohen’s d as an indicator of practical significance, we will use the cohen.d function from the effsize package (Torchiano 2020). If you haven’t already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the outcome variable (PostTest) to the left of the ~ symbol and the name of the predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). As the third argument, type paired=FALSE to indicate that the data are not paired (i.e., dependent). # Compute Cohen&#39;s d cohen.d(PostTest ~ Condition, data=td, paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.356961 (large) ## 95 percent confidence interval: ## lower upper ## 0.7262066 1.9877157 The output indicates that Cohen’s d is 1.357, which would be considered large by conventional cutoff standards. Cohen’s d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program and 25 employees participated in the old training program. After completely their respective training programs, employees completed an assessment of their knowledge, where scores could range from 1-100. The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001, 95% CI[6.41, 15.67]). This difference can be considered to be very large (d = 1.36). References "],["posttestonly_threegroups.html", "Chapter 34 Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA 34.1 Conceptual Overview 34.2 Tutorial 34.3 Chapter Supplement", " Chapter 34 Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA In this chapter, we learn about the post-test-only with two comparison groups training evaluation design and how a one-way analysis of variance (ANOVA) can be used to analyze the data acquired from this design. We’ll begin with conceptual overviews of this training evaluation design and of the one-way ANOVA, and then we’ll conclude with a tutorial. 34.1 Conceptual Overview In this section, we will begin with a description of the post-test-only with two comparison groups training evaluation design. The section concludes with a review of the one-way analysis of variance (ANOVA), including discussions of statistical assumptions, omnibus F-test, post-hoc pairwise mean comparisons, statistical significance, and practical significance; the section wraps up with a sample-write up of a one-way ANOVA used to evaluate data from a post-test-only with two comparison groups training evaluation design. 34.1.1 Review of Post-Test-Only with Two Comparison Groups Design In a post-test-only with two comparison groups training evaluation design (i.e., research design), employees are assigned (randomly or non-randomly) to either a treatment group (e.g., new training program), or one of two comparison groups (e.g., old training program and control group), and every participating employee is assessed on selected training outcomes (i.e., measures) after the training has concluded. If random assignment to groups is used, then a post-test-only with two comparison groups design is considered experimental. Conversely, if non-random assignment to groups is used, then the design is considered quasi-experimental. Regardless of whether random or non-random assignment is used, a one-way analysis of variance (ANOVA) can be used to analyze the data from a post-test-only with two comparison groups design, provided key statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a post-test-only two comparison groups design. As a strength, this design includes two comparison groups, and if coupled with random assignment to groups, then the design qualifies as a true experimental design. With that being said, if we use non-random assignment to the different groups, then we are less likely to have equivalent groups of individuals who enter each group, which may bias how they engage in the demands of their respective group and how they complete the outcome measures. Further, because this design lacks a pre-test (i.e., assessment of initial performance on the outcome measures), we cannot be confident that employees in the three groups “started in the same place” with respect to the outcome(s) we might measure at post-test. Consequently, any differences we observe between the three groups on a post-test outcome measure may reflect pre-existing differences – meaning, the training may not have “caused” the differences that are apparent at post-test. 34.1.2 Review of One-Way ANOVA Link to conceptual video: https://youtu.be/kwwU9N8WCfQ Analysis of variance (ANOVA) is part of a family of analyses aimed at comparing means, which includes one-way ANOVA, repeated-measures ANOVA, factorial ANOVA, and mixed-factorial ANOVA. In general, an ANOVA is used to compare three or more means; however, it can be used to compare two means on a single factor – but you might as well just use an independent-samples t-test if this is the case. When comparing means, an omnibus F-test is employed to determine whether there are any differences in means across the levels of the categorical (nominal, ordinal) predictor variable. In other words, with an ANOVA, we are attempting to reject the null hypothesis that that there are no mean differences across levels of the categorical predictor variable. It is important to remember, however, that the F-test is an omnibus test, which means that its p-value only indicates whether mean differences exist across two or more means and not where those specific differences in means exist. Typically, post-hoc pairwise comparison tests can be used to uncover which specific pairs of levels (categories) show differences in means. The one-way ANOVA refers to one of the most basic forms of ANOVA – specifically, an ANOVA in which there is only a single categorical predictor variable and a continuous outcome variable. The term “one-way” indicates that there is just a single factor (i.e., predictor variable, independent variable). If we were to have two categorical predictor variables, then we could call the corresponding analysis a factorial ANOVA or more specifically a two-way ANOVA. A one-way ANOVA is employed to test the equality of two or more means on a continuous (interval, ratio) outcome variable (i.e., dependent variable) all at once by using information about the variances. For a one-way ANOVA the null hypothesis is typically that all means are equal, or rather, there are no differences between the means. More concretely, an F-test is used as an omnibus test for a one-way ANOVA. In essence, the F-test reflects the between-level (between-group, between-category) variance divided by the within-group variance. To calculate the degrees of freedom (df) for the numerator (between-group variance), we subtract 1 from the number of groups (df = k - 1). To calculate the df for the denominator (within-group variance), we subtract the number of groups from the number of people in the overall sample (df = n - k). Note: In this chapter, we will focus on exclusively on applying a one-way ANOVA with balanced groups, where balanced groups means that each group (i.e., category) has the same number of independent cases. The default approach to calculating sum of squares in most R ANOVA functions is to use what is often referred to as Type I sum of squares. Type II and Type III sum of squares refer to the other approaches to calculating sum of squares for an ANOVA. Results are typically similar between Type I, Type II, and Type III approaches when the data are balanced across groups designated by the factors (i.e., predictor variables). To use Type II and Type III sum of squares, I recommend that you use the Anova function from the car package, which we will not cover in this tutorial. Because we are only considering considering a single between-subjects factor (i.e., one-way ANOVA) and have a balanced design, we won’t concern ourselves with this distinction. If, however, you wish to extend the one-way ANOVA to a two-way ANOVA, I recommend checking out this link to the R-Bloggers site. To illustrate the computational underpinnings of a one-way ANOVA, we can dive into some formulas for the grand mean, total variation, between-group variation, within-group variation, and the F-test. For the sake of simplicity, I am presenting formulas in which we will assume equal sample sizes for each level of the categorical predictor variable (i.e., equal sample sizes for each independent sample or group); this is often referred to as a balanced design, as noted above. Grand Mean: The formula for the grand mean is as follows: \\(\\overline{Y}_{..} = \\frac{\\sum Y_{ij}}{n}\\) where \\(\\overline{Y}_{..}\\) is the grand mean, \\(Y_{ij}\\) represents a score on the outcome variable, and \\(n\\) is the total sample size. Sum of Squares Between Groups (Between-Group Variation): The formula for the sum of squares between groups is as follows: \\(SS_{between} = \\sum n_{j}(\\overline{Y}_{.j} - \\overline{Y}_{..})^{2}\\) where \\(SS_{between}\\) refers to the sum of squares between groups, \\(n_{j}\\) is the sample size for each group (i.e., level of the categorical predictor variable) assuming equal sample sizes, \\(\\overline{Y}_{.j}\\) represents each group’s mean on the continuous outcome variable, and \\(\\overline{Y}_{..}\\) is the grand mean for the outcome variable (i.e., sample mean). In essence, \\(SS_{between}\\) represents variation between the group means. We can compute the variance between groups dividing \\(SS_{between}\\) by the between-groups degrees of freedom (df = k - 1), as shown below. Variance Between Groups: The formula for the variance between groups is as follows: \\(s_{between}^{2} = \\frac{SS_{between}}{k-1}\\) where \\(s_{between}^{2}\\) refers to the variance between groups, \\(SS_{between}\\) refers to the sum of squares between groups, \\(k\\) is the number of levels (categories, groups) for the categorical predictor variable, and \\(k-1\\) is the between-groups df. Sum of Squares Within Groups (Within-Group Variation): The formula for the sum of squares within groups is as follows: \\(SS_{within} = \\sum \\sum (Y_{ij} - Y_{.j})^{2}\\) where \\(SS_{within}\\) refers to the sum of squares within groups (or error variation), \\(Y_{ij}\\) represents a score on the continuous outcome variable, and \\(Y_{.j}\\) represents each group’s mean. In essence, \\(SS_{within}\\) represents variation within the groups. We can compute the variance within groups by dividing \\(SS_{within}\\) by the within-groups degrees of freedom (df = n - k), as shown below. Variance Within Groups: The formula for the variance within groups is as follows: \\(s_{within}^{2} = \\frac{SS_{within}}{n-k}\\) where \\(s_{within}^{2}\\) refers to the variance within groups, \\(SS_{within}\\) refers to the sum of squares within groups, \\(n\\) is the total sample size, \\(k\\) is the the number of groups (i.e., levels of the categorical predictor variable), and \\(n-k\\) is the within-groups df. F-value: We can compute the F-value associated with the omnibus tests of means using the following formula: \\(F = \\frac{s_{between}^{2}}{s_{within}^{2}}\\) where \\(F\\) is the F-test value, \\(s_{between}^{2}\\) is the variance between groups, and \\(s_{within}^{2}\\) is the variance within groups. 34.1.2.1 Post-Hoc Pairwise Mean Comparison Tests As a reminder, the F-test indicates whether differences between the group means exist, but it doesn’t indicate which specific pairs of groups differ with respect to their means. Thus, we use post-hoc pairwise mean comparison tests to evaluate which groups have (or do not have) significantly different means and the direction of those differences. Post-hoc tests like Tukey’s test and Fisher’s test help us account for what is called family-wise error, where family-wise error refers to the increased likelihood of making Type I errors (i.e., finding something that doesn’t really exist in the population; false positive) because we are running multiple pairwise comparisons and may capitalize on chance. Other tests like Dunnett’s C are useful when the assumption of equal variances cannot be met. Essentially, the post-hoc pairwise mean comparison tests are independent-samples t-tests that account for the fact that we are making multiple comparisons, resulting in adjustments to the associated p-values. 34.1.2.2 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple linear regression model include: The outcome (dependent, response) variable has a univariate normal distribution in each of the two or more underlying populations (e.g., samples, groups, conditions), which correspond to the two or more categories (levels, groups) of the predictor (independent, explanatory) variable; The variances of the outcome (dependent, response) variable are equal across the two or more populations (e.g., levels, groups, categories), which is often called the equality of variances or homogeneity of variances assumption. 34.1.2.3 Statistical Significance As noted above, for a one-way ANOVA, we use an omnibus F-test and associated p-value to test whether there are statistical significant differences across groups means. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the differences between the two or means are equal to zero. In other words, if the p-value is less than .05, we conclude that there are statistically significant differences across the group means. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that there are differences across the two or more means. If our omnibus F-test is found to be statistical significant, then we will typically move ahead by performing post-hoc pairwise comparison tests (e.g., Tukey’s test, Fisher’s test, Dunnett’s C) and examine the p-value associated with each pairwise comparison test. We interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the difference between the two means is equal to zero. In other words, if the p-value is less than .05, we conclude that the two means differ from each other to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the difference between the two means is equal to zero. As noted above, typically, the pairwise comparison tests adjust the p-values for family-wise error to reduce the likelihood of making Type I errors (i.e., false positives). When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. 34.1.2.4 Practical Significance A significant omnibus F-test and associated p-value only tells us that the means in question differ to a statistically significant across groups (e.g., levels, categories). It does not, however, tell us about the magnitude of the difference across means – or in other words, the practical significance. Fortunately, there are multiple model-level effect size indicators, like R2, \\(\\eta^{2}\\), \\(\\omega^{2}\\), and Cohen’s \\(f\\). All of these provide an indication of the amount of variance explained by the predictor variable in the outcome variable. In the table below, I provide some qualitative descriptors that we can apply when interpreting the magnitude of one of the effect size indicators. Please note that typically we only interpret practical significance when the F-test indicates statistical significance. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohen’s f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large After finding a statistically significant omnibus F-test, it is customary to then compute post-hoc pairwise comparisons between specific means to determine which pairs of means differ to a statistically significant extent. If a pair of means is found to show a statistically significant difference, then we will proceed forward with interpreting the magnitude of that difference, typically using an effect size indicator like Cohen’s d, which is the standardized mean difference. In essence, the Cohen’s d indicates the magnitude of the difference between means in standard deviation units. A d-value of .00 would indicate that there is no difference between the two means, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohen’s d Description .20 Small .50 Medium .80 Large 34.1.2.5 Sample Write-Up As part of a post-test-only with two comparison groups design, 75 employees were randomly assigned to one (and only one) of three following groups associated with our categorical (nominal, ordinal) predictor variable: no noise, some noise, and loud noise. A total of 25 participants were assigned to each group, resulting in a balanced design. For all employees, verbal fluency was assessed while they were exposed to one of the noise conditions, where verbal fluency serves as the continuous (interval, ratio) outcome variable. We applied a one-way ANOVA to determine whether verbal fluency differed across the levels of noise each group of employees experienced as part of our study. We found a significant omnibus F-test, which indicated that there were differences across the means in verbal fluency for the three noise conditions (F = 7.55, p = .03). The R2 associated with the model F-value was .12, which indicates that 12% of the variance in verbal fluency can be explained by the level of noise employees were exposed to. Given the significant omnibus F-test, we computed Tukey’s tests to examine the post-hoc pairwise comparisons between each pair of verbal-fluency means. The mean verbal fluency score for the no-noise condition was 77.00 (SD = 3.20), 52.00 (SD = 3.10) for the some-noise condition, and 50.00 (SD = 3.10) for the loud-noise condition. Further, the pairwise comparisons indicated, as expected, that mean verbal fluency was significantly higher for the no-noise condition compared to the some-noise condition (\\(M_{diff}\\) = 15.00, adjusted p = .03) and loud noise condition (\\(M_{diff}\\) = 17.00, adjusted p = .02). We found that the standardized mean difference (Cohen’s d) for the two significant differences in means was .96 and .91, respectively, which both can considered large. Finally, a significant difference in means was not found when comparing verbal-fluency scores for the some-noise and no-noise conditions (\\(M_{diff}\\) = 2.00, adjusted p = .34). 34.2 Tutorial This chapter’s tutorial demonstrates how to estimate a one-way ANOVA and post-hoc pairwise mean comparisons, test the associated statistical assumptions, and present the findings in writing and visually. 34.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/e6oVV1ynfWo 34.2.2 Functions &amp; Packages Introduced Function Package Plot lessR tapply base R shapiro.test base R leveneTest car ANOVA lessR cohen.d effsize 34.2.3 Initial Steps If you haven’t already, save the file called “TrainingEvaluation_ThreeGroupPost.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “TrainingEvaluation_ThreeGroupPost.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_ThreeGroupPost.csv&quot;) ## Rows: 75 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (2): EmpID, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [75 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:75] 1 2 3 4 5 6 7 8 9 10 ... ## $ Condition: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ PostTest : num [1:75] 74 65 62 68 70 61 79 67 79 59 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 No 74 ## 2 2 No 65 ## 3 3 No 62 ## 4 4 No 68 ## 5 5 No 70 ## 6 6 No 61 There are 75 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), Condition (training condition: New = new training program, Old = old training program, No = no training program), and PostTest (post-training scores on training assessment, ranging from 1-100). Regarding participation in the training conditions, 25 employees participated in each condition, with no employee participating in more than one condition; this means that we have a balanced design. Per the output of the str (structure) function above, all of the variables except for Condition are of type integer (continuous: interval/ratio), and Condition is of type character (nominal/categorical). 34.2.4 Test Statistical Assumptions Prior to estimating and interpreting the one-way ANOVA, let’s generate a VBS (violin-box-scatter) plot to visualize the statistical assumptions regarding the outcome variable having a univariate normal distribution in each of the three training conditions and the variances of the outcome variable being approximately equal across the three conditions. To do so, we’ll use the Plot function from the lessR package (Gerbing, Business, and University 2021). If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the Plot function. As the first argument within the function, type the name of the outcome variable (PostTest). As the second argument, type data= followed by the name of the data frame (td). As the third argument, type by1= followed by the name of the grouping variable (Condition), as this will create the trellis (lattice) structure wherein three VBS plots will be created (one for each independent group). # VBS plots of the PostTest distributions by Condition Plot(PostTest, data=td, by1=Condition) ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(PostTest, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## ANOVA(PostTest ~ Condition) # Add the data parameter if not the d data frame ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## No 25 0 62.36 8.09 47.00 61.00 79.00 ## Old 25 0 69.60 9.11 51.00 70.00 89.00 ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## New 4 73 ## No 3 60 ## Old 3 70 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.55 size of plotted points ## out_size: 0.80 size of plotted outlier points ## jitter_y: 1.00 random vertical movement of points ## jitter_x: 0.28 random horizontal movement of points ## bw: 3.43 set bandwidth higher for smoother edges Based on the output from the Plot function, note that (at least visually) the three distributions seem to be roughly normally distributed, and the the variances appear to be approximately equal. These are by no means stringent tests of the statistical assumptions, but they provide us with a cursory understanding of the shape of the distributions and the variances. If we were to see evidence of non-normality across the conditions, then we might (a) transform the outcome variable to achieve normality (if possible), or (b) apply a nonparametric analysis like the Kruskal-Wallis rank sum test. We can also go a step further by testing the normal distribution and equal variances statistical assumptions using statistical tests. Assumption of Normally Distributed Outcome Variable Scores for Each Level of Predictor Variable: As a reminder, the first statistical assumption is that the outcome variable has a univariate normal distribution in each of the underlying populations (e.g., groups, conditions), which correspond to the levels of the categorical predictor variable. The Shapiro-Wilk normality test can be used to test the null hypothesis that a distribution is normal; if the p-value associated with the test statistic (W) is less than the conventional alpha level of .05, then we would reject the null hypothesis and assume that the distribution is not normal. If, however, we fail to reject the null hypothesis, then we do not have statistical evidence that the distribution is anything other than normal. In other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is normally distributed. To compute the Shapiro-Wilk normality test, we will use the shapiro.test function from base R. Because we need to test the assumption of normality of the outcome variable (PostTest) for all three levels of the predictor variable (Condition), we also need to use the tapply function from base R. The tapply function can be quite useful, as it allows us to apply a function to a variable for each level of another categorical variable. To begin, type the name of the tapply function. As the first argument, type the name of the data frame (td), followed by the $ symbol and the name of the outcome variable (PostTest). As the second argument, type the name of the data frame (td), followed by the $ symbol and the name of the categorical predictor variable (Condition). Finally, as the third argument, type the name of the shapiro.test function. # Compute Shapiro-Wilk normality test for normal distributions tapply(td$PostTest, td$Condition, shapiro.test) ## $New ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.95019, p-value = 0.2533 ## ## ## $No ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.97029, p-value = 0.6525 ## ## ## $Old ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.98644, p-value = 0.977 In the output, we can see that the PostTest variable is normally distributed for those in the New training condition (W = .95019, p = .2533), the Old training condition (W = .98644, p = .977), and the No training condition (W = .97029, p = .6525). That is, because the p-values were each equal to or greater than .05, we failed to reject the null hypothesis that the distributions of outcome variable scores were normal. Thus, we have statistical support for having met the first assumption. Assumption of Equal Variances (Homogeneity of Variances): As for the equal variances assumption, Levene’s test (i.e., homogeneity of variances test) is commonly used. The null hypothesis of this test is that the variances of the outcome variable are equal across levels of the categorical predictor variable. Thus, if the p-value is less than the conventional alpha level of .05, then we reject the null hypothesis and assume the variances are different. If, however, the p-value is equal to or less than .05, then we fail to reject the null hypothesis and assume that the variances are equal (i.e., variances are homogeneous). To test the equality (homogeneity) of variances assumption, we will use the leveneTest function from the car package. More than likely the car package is already installed on your computer, as many other packages are dependent on it. That being said, you may still need to install the package prior to accessing it using the library function. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the outcome (dependent) variable (PostTest) to the left of the ~ symbol and the name of the predictor (independent) variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). # Compute Levene&#39;s test for equal variances leveneTest(PostTest ~ Condition, data=td) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.6499 0.5251 ## 72 In the output, we see that the test is nonsignificant (F = .6499, p = .5251), which suggests that, based on this test, we have no reason to believe that the three variances are anything but equal. In other words, because the p-value for this test is equal to or greater than .05, we fail to reject the null hypothesis that the variances are equal. All in all, we found evidence to support that we met the two statistical assumptions necessary to proceed forward with estimating our one-way ANOVA. 34.2.5 Estimate One-Way ANOVA There are different functions that can be used to run a one-way ANOVA in R. In this chapter, we will review how to run a one-way ANOVA using the ANOVA function from the lessR package, and if you’re interested, in the chapter supplement I demonstrate how to carry out the same processes using the aov function from base R. Using ANOVA function from the lessR package, we will evaluate whether the means on the post-test (PostTest) continuous outcome variable differ between levels of the Condition categorical predictor variable (New, Old, No); in other words, let’s find out if we should treat the means as being different from one another. A big advantage of using the ANOVA function from the lessR package to estimate a one-way ANOVA is that the function automatically generates descriptive statistics, the omnibus F-test and associated indicators of effect size (i.e., practical significance), and post-hoc pairwise comparisons. If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Now we’re ready to estimate a one-way ANOVA. To begin, type the name of the ANOVA function. As the first argument in the parentheses, specify the statistical model. To do so, type the name of the continuous outcome variable (PostTest) to the left of the ~ symbol and the name of the categorical predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame where the outcome and predictor variables are located (td). # One-way ANOVA using ANOVA function from lessR ANOVA(PostTest ~ Condition, data=td) ## ## BACKGROUND ## ## Response Variable: PostTest ## ## Factor Variable: Condition ## Levels: New No Old ## ## Number of cases (rows) of data: 75 ## Number of cases retained for analysis: 75 ## ## ## DESCRIPTIVE STATISTICS ## ## n mean sd min max ## New 25 72.36 6.98 60.00 84.00 ## No 25 62.36 8.09 47.00 79.00 ## Old 25 69.60 9.11 51.00 89.00 ## ## Grand Mean: 68.107 ## ## ## ANOVA ## ## df Sum Sq Mean Sq F-value p-value ## Condition 2 1333.63 666.81 10.16 0.0001 ## Residuals 72 4727.52 65.66 ## ## R Squared: 0.220 ## R Sq Adjusted: 0.198 ## Omega Squared: 0.196 ## ## ## Cohen&#39;s f: 0.494 ## ## ## TUKEY MULTIPLE COMPARISONS OF MEANS ## ## Family-wise Confidence Level: 0.95 ## ----------------------------------- ## diff lwr upr p adj ## No-New -10.00 -15.48 -4.52 0.00 ## Old-New -2.76 -8.24 2.72 0.45 ## Old-No 7.24 1.76 12.72 0.01 ## ## ## RESIDUALS ## ## Fitted Values, Residuals, Standardized Residuals ## [sorted by Standardized Residuals, ignoring + or - sign] ## [res_rows = 20, out of 75 cases (rows) of data, or res_rows=&quot;all&quot;] ## ----------------------------------------------- ## Condition PostTest fitted residual z-resid ## 75 Old 89.00 69.60 19.40 2.44 ## 57 Old 51.00 69.60 -18.60 -2.34 ## 7 No 79.00 62.36 16.64 2.10 ## 9 No 79.00 62.36 16.64 2.10 ## 64 Old 54.00 69.60 -15.60 -1.96 ## 25 No 47.00 62.36 -15.36 -1.93 ## 73 Old 56.00 69.60 -13.60 -1.71 ## 60 Old 83.00 69.60 13.40 1.69 ## 32 New 60.00 72.36 -12.36 -1.56 ## 29 New 84.00 72.36 11.64 1.47 ## 1 No 74.00 62.36 11.64 1.47 ## 56 Old 81.00 69.60 11.40 1.44 ## 33 New 61.00 72.36 -11.36 -1.43 ## 42 New 61.00 72.36 -11.36 -1.43 ## 20 No 51.00 62.36 -11.36 -1.43 ## 35 New 83.00 72.36 10.64 1.34 ## 43 New 83.00 72.36 10.64 1.34 ## 28 New 62.00 72.36 -10.36 -1.30 ## 41 New 82.00 72.36 9.64 1.21 ## 51 Old 79.00 69.60 9.40 1.18 ## ## ---------------------------------------- ## Plot 1: 95% family-wise confidence level ## Plot 2: Scatterplot with Cell Means ## ---------------------------------------- As you can see in the output, the ANOVA function provides background information about your variables, descriptive statistics, an omnibus statistical significance test of the mean comparison, post-hoc pairwise mean comparisons, and indicators of practical significance. In addition, the default data visualizations include a scatterplot with the cell (group) means and a chart with mean differences between conditions presented. Background: The Background section provides information about the name of the data frame, the name of the response (i.e., outcome) variable, the factor (i.e., categorical predictor) variable and its levels (i.e., conditions, groups), and the number of cases. Descriptive Statistics: The Descriptive Statistics section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees in each condition (n = 25), and descriptively, the mean PostTest score for the New training condition is 72.36 (SD = 6.98), the mean PostTest score for the Old training condition is 69.60 (SD = 9.11), and the mean PostTest score for the No training condition is 62.36 (SD = 8.09). The grand mean (i.e., overall mean for the entire sample) is 68.107. Thus, descriptively we can see that the condition means are not the same, but the question remains whether these means are different from one another to a statistically significant extent. Basic Analysis: In the Basic Analysis section of the output, you will find the statistical test of the null hypothesis (i.e., the means are equal). This is called an omnibus test because we are testing whether or not their is evidence that we should treat all means as equal. First, in the Summary Table, take a look at the line prefaced with Condition; in this line, you will find the degrees of freedom (df), the sum of squares (Sum Sq), and the mean square (Mean Sq) between groups/conditions; in addition, you will find the omnibus F-value and its associated p-value. The F-value and its associated p-value reflect the null hypothesis significance test of the means being equal, and because the p-value is less than the conventional two-tailed alpha of .05, we reject the null hypothesis that means are equal (F = 10.16, p &lt; .001); meaning, we have evidence that at least two of the means differ from one another to a statistically significant extent. But which ones? To answer this question, we will need to look at the pairwise mean comparison tests later in the output. Next, look at the Association and Effect Size table. The (unadjusted) R-squared (R2) value indicates the extent to which the predictor variable explains variance in the outcome variable in this sample; if you multiply the value by 100, you get a percentage. In this case, we find that 22% of the variance in PostTest scores is explained by the different levels of the Condition variable (i.e., New, Old, No) for this example. The adjusted R2 value, however, is an indicator of the magnitude of the association in the underlying population (as opposed to specifically for this sample), and here we see that the adjusted R2 value is .20 (or 20%). We also find information about other effect-size indicators, including omega-squared (\\(\\omega\\)2) and Cohen’s f. \\(\\omega\\)2 is a population-level indicator of effect size like the adjusted R2 value, and like the adjusted R2 value will tend to be smaller. Some functions compute an effect size indicator called eta-squared (\\(\\eta\\)2), which is equivalent to an unadjusted R2 value in this context. Finally, Cohen’s f focuses not on the variance explained (i.e., the association) but on the magnitude of the differences in means between groups/conditions. By most standards, these effect sizes would be considered to be a medium-to-large or large in magnitude; remember, these effect-size indicators correspond to the omnibus F-test. In the table below, I provide conventional rules of thumb for qualitatively interpreting the magnitude of R2 (adjusted or unadjusted) and Cohen’s f; I suggest picking one and using it consistently. Finally, please note that typically we only interpret practical significance when a difference has been found to be statistically significant. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohen’s f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large Tukey Multiple Comparisons of Means: Recall that based on the omnibus F-test above, we found evidence that the group means were not equal; in other words, the p-value associated with your F-value indicated the means differed significantly across the groups. Because the omnibus F-test indicated statistical significance at the model level, we should proceed forward with post-hoc pairwise mean comparison tests, such as Tukey’s test. If the omnibus test had not been statistically significant, then we would not proceed forward with interpreting the post-hoc pairwise mean comparison tests. In the Tukey Multiple Comparisons of Means section, we find the pairwise mean comparisons corrected for family-wise error based on Tukey’s approach. Family-wise error refers to instances in which we run multiple statistical tests, which means that we may be more likely to capitalize on chance when searching for statistically significant finds. When tests are adjusted for family-wise error, the p-values (or confidence intervals) are corrected (i.e., penalized) for the fact that multiple statistical tests were run, thereby increasing the threshold for finding a statistically significant result. Each row in the pairwise-comparison table in the output shows the raw difference (i.e., diff) in means between the two specified groups, such that the first group mean is subtracted from the second group mean listed. Next, the lower (i.e., lwr) and upper (i.e., upr) 95% confidence interval limits are presented. Finally, the adjusted p-value is presented (i.e., p adj). In our output, we find that employees who participated in the No training condition scored, on average, 10.00 points lower (-10.00) on their post-test (PostTest) assessment than employees who participated in the New training condition, which is a statistically significant difference (p-adjusted &lt; .01, 95% CI[-15.48, -4.52]). Similarly, employees who participated in the Old training condition scored, on average, 7.24 points higher on their post-test (PostTest) assessment than employees who participated in the No training condition (p-adjusted = .01, 95% CI[1.76, 12.72]). Note that, as evidenced by the 95% confidence intervals, the uncertainty around the mean difference between the Old and No training conditions appears to be notably greater than the uncertainty around the mean difference between the New and No training conditions. Finally, we find that the mean difference of -2.76 between the New and the Old training conditions is not statistically significant (p-adjusted = .45, 95% CI[-8.24, 2.72]). Note that the ANOVA function from lessR does not provide effect size estimates for the post-hoc pairwise mean comparisons, so if you would like those, you can do the following. Effect Sizes of Significant Post-Hoc Pairwise Mean Comparisons: There are various ways that we could go about computing an effect size such as Cohen’s d for those statistically significant post-hoc pairwise mean comparisons. In the post-hoc pairwise mean comparisons section of the output, we identified that the New and Old training conditions resulted in significantly higher post-training assessment (PostTest) scores compared to the No training condition. The question then becomes: How much better than the No training condition are the New and Old training conditions? To compute Cohen’s d as an estimate of practical significance we will use the cohen.d function from the effsize package. If you haven’t already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the continuous outcome variable (PostTest) to the left of the ~ symbol and the name of the categorical predictor variable (Condition) to the right of the ~ symbol. For the second argument, we are going to apply the subset function from base R after data= to indicate that we only want to run a subset of our data frame. The subset function is a simpler version of the filter function from dplyr. Why are we doing this? The cohen.d function will only allow predictor variables with two levels, and our Condition variable has three levels: New, Old, and No. After data=, type subset, and within the subset function parentheses, enter the name of the data frame (td) as the first argument and a conditional statement that removes one of the three predictor variable levels (Condition!=\"Old\"); in this first example, we remove the Old level so that we can compare just the New and Old conditions. Back to the cohen.d function, as the third argument, type paired=FALSE to indicate that the data are not paired (i.e., the data are not dependent). # Compute Cohen&#39;s d for New and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;Old&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.324165 (large) ## 95 percent confidence interval: ## lower upper ## 0.6962342 1.9520949 The output indicates that Cohen’s d is 1.324, which would be considered large by conventional cutoff standards (see table below). Cohen’s d Description .20 Small .50 Medium .80 Large Let’s repeat the same process as above, except this time we will focus on the Old and No levels of the Condition predictor variable by removing the level called New. # Compute Cohen&#39;s d for Old and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;New&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: -0.8407151 (large) ## 95 percent confidence interval: ## lower upper ## -1.4339989 -0.2474312 The output indicates that Cohen’s d is .841, which is large but not as large as the Cohen’s d we saw when comparing the PostTest means for the New and No training conditions. Note: Cohen’s d was actually negative (-.841), but typically we just report the absolute value in this context, as the negative or positive sign of a Cohen’s d simply indicates which mean was subtracted from the other mean; and reversing this order would result in the opposite sign. Sample Write-Up: To evaluate the effectiveness of a new training program, we applied a post-test-only with two comparison groups training evaluation design. In total, 25 employees participated in the new training program, 25 employees participated in the old training program, and 25 employees did not participate in a training program. After completing their respective training conditions, employees were assessed on the knowledge they acquired during training, where scores could range from 1-100. We found that post-training assessments differed across training conditions to a statistically significant extent (F = 10.16, p &lt; .001); together, participation in the different training conditions explained 20% of the variability in post-training assessment scores (R2 = .22; R2adjusted = .20). Results of follow-up tests indicated that employees who participated in the new training program performed, on average, 10.00 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[4.52, 15.48]), which was a large difference (d = 1.324). Further, employees who participated in the old training program performed, on average, 7.24 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[1.76, 12.72]), which was a large difference (d = .841). Average post-training assessment scores were not found to differ to a statistically significant extent for those who participated in the new versus old training programs (Mdifference = 2.76, padjusted = .45, 95% CI[-8.24, 2.72]). Note: When interpreting the results, I flipped the sign (+ vs. -) of some of the findings to make the interpretation more consistent. Feel free to do the same. 34.2.6 Visualize Results Using Bar Chart When we find a statistically significant difference between two or more pairs of means based on an one-way ANOVA, we may want to present the means in a bar chart to facilitate storytelling. To do so, we will use the BarChart function from lessR. If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the BarChart function. As the first argument, type x= followed by the name of the categorical predictor variable (Condition). As the second argument, type y= followed by the name of the continuous outcome variable (PostTest). As the third argument, specify stat=\"mean\" to request the application of the mean function to the PostTest variable based on the levels of the Condition variable. As the fourth argument, type data= followed by the name of the data frame object to which our predictor and outcome variables belong (td). As the fifth argument, use xlab= to provide the x-axis label (\"Training Condition\"). As the sixth argument, use ylab= to provide the y-axis label (\"Post-Test Score\"). # Create bar chart BarChart(x=Condition, y=PostTest, stat=&quot;mean&quot;, data=td, xlab=&quot;Training Condition&quot;, ylab=&quot;Post-Test Score&quot;) ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## No 25 0 62.36 8.09 47.00 61.00 79.00 ## Old 25 0 69.60 9.11 51.00 70.00 89.00 ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, Condition) # lollipop plot ## ## Plotted Values ## -------------- ## New No Old ## 72.360 62.360 69.600 34.2.7 Summary In this chapter, we learned how to estimate a one-way ANOVA using the ANOVA function from the lessR package. We also learned how to test statistical assumptions, compute post-hoc pairwise mean comparisons, estimate an effect size for the omnibus test, and estimate effect sizes for the pairwise mean comparisons. 34.3 Chapter Supplement In addition to the ANOVA function from the lessR package covered above, we can use the aov function from base R to estimate an one-way ANOVA. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of the one-way ANOVA results. 34.3.1 Functions &amp; Packages Introduced Function Package aov base R summary base R anova_stats sjstats TukeyHSD base R plot base R mean base R cohen.d effsize apa.aov.table apaTables apa.1way.table apaTables apa.d.table apaTables 34.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_ThreeGroupPost.csv&quot;) ## Rows: 75 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (2): EmpID, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [75 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:75] 1 2 3 4 5 6 7 8 9 10 ... ## $ Condition: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ PostTest : num [1:75] 74 65 62 68 70 61 79 67 79 59 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 No 74 ## 2 2 No 65 ## 3 3 No 62 ## 4 4 No 68 ## 5 5 No 70 ## 6 6 No 61 34.3.3 aov Function from Base R The aov function from base R offers another route for running a one-way ANOVA. Prior to using the aov function, it is advisable to perform statistical tests to give us a better understanding if we have satisfied the two statistical assumptions necessary to estimate and interpret a one-way ANOVA; rather than repeat those same diagnostics tests here, please refer to the Test Statistical Assumptions section to learn how to perform those tests. Assuming we have satisfied the statistical assumptions, we are now ready to estimate the one-way ANOVA. To begin, come up with a name for the one-way ANOVA model that you are specifying; you can call this model object whatever you’d like, and here I refer to it as model1. To the right of the model name, type the &lt;- operator to indicate that you are assigning the one-way ANOVA model to the object. Next, to the right of the &lt;- operator, type the name of the aov function from base R. As the first argument, type the name of the continuous outcome variable (PostTest) to the left of the ~ operator and the name of the categorical predictor variable (Condition) to the right of the ~ operator. For the second argument, use data= to specify the name of the data frame (td). On the next line, type the name of the summary function from base R, and as the sole argument, enter the name of the model object you created and named on the previous line. # One-way ANOVA using aov function from base R model1 &lt;- aov(PostTest ~ Condition, data=td) summary(model1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Condition 2 1334 666.8 10.16 0.00013 *** ## Residuals 72 4728 65.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the aov output provides you with the results of the one-way ANOVA. In the output table, take a look at the line prefaced with Condition; in this line, you will find the degrees of freedom (df), the sum of squares (Sum Sq), and the mean square (Mean Sq) between groups/conditions; in addition, you will find the omnibus F-value and its associated p-value. The F-value and its associated p-value reflect the null hypothesis significance test of the means being equal, and because the p-value is less than the conventional two-tailed alpha of .05, we reject the null hypothesis that means are equal (F = 10.16, p = .00013); meaning, we have evidence that at least two of the means differ from one another to a statistically significant extent. But how big is this effect? To assess the practical significance of a statistically significant effect, we will apply the anova_stats function from the sjstats package. If you haven’t already, install and access the sjstats package. # Install package install.packages(&quot;sjstats&quot;) # Note: You may also have to install the pwr package independently # install.packages(&quot;pwr&quot;) # Access package library(sjstats) Within the anova_stats function parentheses, enter the name of the object for the one-way ANOVA model you created (model1). # Compute effect sizes for omnibus test anova_stats(model1) ## term | df | sumsq | meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power ## -------------------------------------------------------------------------------------------------------------------------------------------- ## Condition | 2 | 1333.627 | 666.813 | 10.156 | &lt; .001 | 0.220 | 0.220 | 0.196 | 0.196 | 0.198 | 0.531 | 0.986 ## Residuals | 72 | 4727.520 | 65.660 | | | | | | | | | In the output, we see the same model information regarding degrees of freed sum of squares, mean of squares, F-value, and p-value, but we also see estimates for effect-size indicators like eta-squared (\\(\\eta^2\\)), omega-squared (\\(\\omega^2\\)), and Cohen’s f. As a reminder, \\(\\eta^2\\) will be equivalent to an unadjusted R2 value in this context, and \\(\\omega^2\\) will be equivalent to an adjusted R2 value. Finally, unlike the aforementioned effect-size indicators, Cohen’s f focuses not on the variance explained (i.e., the association) but rather on the magnitude of the differences in means between groups. By most standards, these effect sizes would be considered to be a medium-large or large in magnitude; remember, these effect-size indicators correspond to the omnibus F-test. In the table below, I provide conventional rules of thumb for qualitatively interpreting the magnitude of these effect sizes. Please note that typically we only interpret practical significance when a difference has been found to be statistically significant. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohen’s f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large Based on the omnibus F-test we know that these means are not equivalent to one another, and we know that the effect is medium-large or large in magnitude. What we don’t yet know is which pairs of means are significantly from one another and by how much. To answer this question, we will need to run some post-hoc pairwise comparison tests. Tukey Multiple Comparisons of Means: Recall that based on the omnibus F-test above, we found evidence that the group means were not equal; in other words, the p-value associated with your F-value indicated a statistically significant finding. Because the omnibus test was statistically significant, we should proceed forward with post-hoc pairwise mean comparison tests, such as Tukey’s test. If the omnibus test had not been statistically significant, then we would not proceed forward to interpret the post-hoc pairwise mean comparison tests. To compute Tukey’s test, we will use the TukeyHSD function from base R. First, come up with a name for the object that will contain the results of our Tukey’s test. In this example, I call this model TukeyTest. Second, use the &lt;- operator to indicate that you’re creating a new object based on the results of the TukeyHSD function that you will write next. Third, type the name of the TukeyHSD function, and within the parentheses, enter the name of the one-way ANOVA model object that you specified above (model1). Finally, on the next line, type the name of the print function from base R, and enter the name of the TukeyTest object you created as the sole argument. # Compute Tukey&#39;s test for pairwise mean comparisons TukeyTest &lt;- TukeyHSD(model1) print(TukeyTest) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = PostTest ~ Condition, data = td) ## ## $Condition ## diff lwr upr p adj ## No-New -10.00 -15.484798 -4.515202 0.0001234 ## Old-New -2.76 -8.244798 2.724798 0.4545618 ## Old-No 7.24 1.755202 12.724798 0.0064718 In the output, we find the pairwise mean comparisons corrected for family-wise error based on Tukey’s approach. Family-wise error refers to instances in which we run multiple statistical tests, which means that we may be more likely to capitalize on chance when searching for statistically significant finds. When tests are adjusted for family-wise error, the p-values (or confidence intervals) are corrected (i.e., penalized) for the fact that multiple statistical tests were run, thereby increasing the threshold for finding a statistically significant result. Each row in the pairwise-comparison table in the output shows the raw difference (i.e., diff) in means between the two specified groups, such that the first group mean is subtracted from the second group mean listed. Next, the lower (i.e., lwr) and upper (i.e., upr) 95% confidence interval limits are presented. Finally, the adjusted p-value is presented (i.e., p adj). In our output, we find that employees who participated in the No training condition scored, on average, 10.00 points lower (-10.00) on their post-test (PostTest) assessment than employees who participated in the New training condition, which is a statistically significant difference (p-adjusted &lt; .01, 95% CI[-15.48, -4.52]). Similarly, employees who participated in the Old training condition scored, on average, 7.24 points higher on their post-test (PostTest) assessment than employees who participated in the No training condition (p-adjusted = .01, 95% CI[1.76, 12.72]). Note that, as evidenced by the 95% confidence intervals, the uncertainty around the mean difference between the Old and No training conditions appears to be notably greater than the uncertainty around the mean difference between the New and No training conditions. Finally, we find that the mean difference of -2.76 between the New and the Old training conditions is not statistically significant (p-adjusted = .45, 95% CI[-8.24, 2.72]). We can also plot the pairwise mean comparisons by entering the name of the TukeyTest object we created as the sole argument in the plot function from base R. # Plot pairwise mean comparisons from Tukey&#39;s test plot(TukeyTest) The TukeyHSD function from base R does not provide effect size estimates for the post-hoc pairwise mean comparisons, so if you would like those, you can do the following. Effect Sizes of Significant Post-Hoc Pairwise Comparisons: There are various ways that we could go about computing an effect size such as Cohen’s d for those post-hoc pairwise mean comparisons that were statistically significant. In the post-hoc pairwise mean comparisons, we identified that the New and Old training conditions resulted in significantly higher post-training assessment (PostTest) scores compared to the No training condition. The question then becomes: How much better than the No training condition are the New and Old training conditions? To compute Cohen’s d as an estimate of practical significance we will use the cohen.d function from the effsize package (Torchiano 2020). If you haven’t already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the continuous outcome variable (PostTest) to the left of the ~ operator and the name of the categorical predictor variable (Condition) to the right of the ~ operator. For the second argument, we are going to apply the subset function from base R after data= to indicate that we only want to run a subset of our data frame. The subset function is a simpler version of the filter function from dplyr. Why are we doing this? The cohen.d function will only allow predictor variables with two levels/categories, and our Condition variable has three levels: New, Old, and No. After data=, type subset, and within the subset function parentheses, enter the name of the data frame (td) as the first argument and a conditional statement that removes one of the three predictor variable levels (Condition!=\"Old\"); in this first example, we remove the Old level so that we can compare just the New and Old conditions. Back to the cohen.d function, as the third argument, type paired=FALSE to indicate that the data are not paired (i.e., the data are not dependent). # Compute Cohen&#39;s d for New and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;Old&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.324165 (large) ## 95 percent confidence interval: ## lower upper ## 0.6962342 1.9520949 The output indicates that Cohen’s d is 1.324, which would be considered large by conventional cutoff standards (see table below). Cohen’s d Description .20 Small .50 Medium .80 Large Let’s repeat the same process as above, except this time we will focus on the Old and No levels of the Condition predictor variable by removing the level called New. # Compute Cohen&#39;s d for Old and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;New&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: -0.8407151 (large) ## 95 percent confidence interval: ## lower upper ## -1.4339989 -0.2474312 The output indicates that Cohen’s d is .841, which is large but not as large as the Cohen’s d we saw when comparing the PostTest means for the New and No training conditions. Note: Cohen’s d was actually negative (-.841), but typically we just report the absolute value in this context, as the negative or positive sign of a Cohen’s d simply indicates which mean was subtracted from the other mean; and reversing this order would result in the opposite sign. Sample Write-Up: To evaluate the effectiveness of a new training program, we applied a post-test-only with two comparison groups training evaluation design. In total, 25 employees participated in the new training program, 25 employees participated in the old training program, and 25 employees did not participate in a training program. After completing their respective training conditions, employees were assessed on the knowledge they acquired during training, where scores could range from 1-100. We found that post-training assessments differed across training conditions to a statistically significant extent (F = 10.16, p &lt; .001); together, participation in the different training conditions explained 20% of the variability in post-training assessment scores (R2 = .22; R2adjusted = .20). Results of follow-up tests indicated that employees who participated in the new training program performed, on average, 10.00 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[4.52, 15.48]), which was a large difference (d = 1.324). Further, employees who participated in the old training program performed, on average, 7.24 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[1.76, 12.72]), which was a large difference (d = .841). Average post-training assessment scores were not found to differ to a statistically significant extent for those who participated in the new versus old training programs (Mdifference = 2.76, padjusted = .45, 95% CI[-8.24, 2.72]). Note: When interpreting the results, I flipped the sign (+ vs. -) of some of the findings to make the interpretation more consistent. Feel free to do the same. 34.3.4 APA-Style Table of Results If you want to present the results of your one-way ANOVA to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. Using the aov function from base R, as we did above, let’s begin by specifying a one-way ANOVA model and naming the model object (model1). # One-way ANOVA using aov function from base R model1 &lt;- aov(PostTest ~ Condition, data=td) If you haven’t already, install and access the apaTables package (Stanley 2021) using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) As a precaution, consider installing the MBESS package, as the function we are about to use is dependent on that package. If you don’t have the MBESS package installed, you’ll get an error message when you run the apa.aov.table from apaTables. You may need to re-access the apaTables package using the library function after installing the MBESS package (Kelley 2021). # Install package install.packages(&quot;MBESS&quot;) To create an APA-style table that contains model summary information like the sum of squares, degrees of freedom, F-value, and p-value, we will use the apa.aov.table function. As the sole argument in the function, type the name of the one-way ANOVA model object you specified above (model1). # Create APA-style one-way ANOVA model summary table apa.aov.table(model1) ## ## ## ANOVA results using PostTest as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 CI_90_partial_eta2 ## (Intercept) 130899.24 1 130899.24 1993.59 .000 ## Condition 1333.63 2 666.82 10.16 .000 .22 [.08, .34] ## Error 4727.52 72 65.66 ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared If you would like to write (export) the table to a Word document (.doc), as a second argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style one-way ANOVA model summary table (write to working directory) apa.aov.table(model1, filename=&quot;One-Way ANOVA Summary Table.doc&quot;) ## ## ## ANOVA results using PostTest as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 CI_90_partial_eta2 ## (Intercept) 130899.24 1 130899.24 1993.59 .000 ## Condition 1333.63 2 666.82 10.16 .000 .22 [.08, .34] ## Error 4727.52 72 65.66 ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared The apa.reg.table function from the apaTables package can table the model-level results of a one-way ANOVA in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. To create a summary table that contains the mean and standard deviation (SD) of the outcome variable for each level of the categorical predictor variable, use the apa.1way.table function. As the first argument, type iv= followed by the name of the categorical predictor (independent) variable. As the second argument, type dv= followed by the name of the categorical outcome (dependent) variable. As the third argument, data= followed by the name of the data frame object to which the predictor and outcome variables belong. # Create APA-style means/SDs table apa.1way.table(iv=Condition, dv=PostTest, data=td) ## ## ## Descriptive statistics for PostTest as a function of Condition. ## ## Condition M SD ## New 72.36 6.98 ## No 62.36 8.09 ## Old 69.60 9.11 ## ## Note. M and SD represent mean and standard deviation, respectively. ## If you would like to write (export) the table to a Word document (.doc), as a fourth argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style means/SDs table (write to working directory) apa.1way.table(iv=Condition, dv=PostTest, data=td, filename=&quot;Means-SDs Table.doc&quot;) ## ## ## Descriptive statistics for PostTest as a function of Condition. ## ## Condition M SD ## New 72.36 6.98 ## No 62.36 8.09 ## Old 69.60 9.11 ## ## Note. M and SD represent mean and standard deviation, respectively. ## The apa.reg.table function from the apaTables package can table the group means and standard deviations in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. To create a summary table that contains the mean and standard deviation (SD) of the outcome variable for each level of the categorical predictor variable and Cohen’s d values for pair-wise comparisons, use the apa.d.table function. As the first argument, type iv= followed by the name of the categorical predictor (independent) variable. As the second argument, type dv= followed by the name of the categorical outcome (dependent) variable. As the third argument, data= followed by the name of the data frame object to which the predictor and outcome variables belong. # Create APA-style means/SDs &amp; Cohen&#39;s ds table apa.d.table(iv=Condition, dv=PostTest, data=td) ## ## ## Means, standard deviations, and d-values with confidence intervals ## ## ## Variable M SD 1 2 ## 1. New 72.36 6.98 ## ## 2. No 62.36 8.09 1.32 ## [0.70, 1.93] ## ## 3. Old 69.60 9.11 0.34 0.84 ## [-0.22, 0.90] [0.26, 1.42] ## ## ## Note. M indicates mean. SD indicates standard deviation. d-values are estimates calculated using formulas 4.18 and 4.19 ## from Borenstein, Hedges, Higgins, &amp; Rothstein (2009). d-values not calculated if unequal variances prevented pooling. ## Values in square brackets indicate the 95% confidence interval for each d-value. ## The confidence interval is a plausible range of population d-values ## that could have caused the sample d-value (Cumming, 2014). ## If you would like to write (export) the table to a Word document (.doc), as a fourth argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style means/SDs &amp; Cohen&#39;s ds table (write to working directory) apa.d.table(iv=Condition, dv=PostTest, data=td, filename=&quot;Means-SDs &amp; Cohen&#39;s ds Table.doc&quot;) ## ## ## Means, standard deviations, and d-values with confidence intervals ## ## ## Variable M SD 1 2 ## 1. New 72.36 6.98 ## ## 2. No 62.36 8.09 1.32 ## [0.70, 1.93] ## ## 3. Old 69.60 9.11 0.34 0.84 ## [-0.22, 0.90] [0.26, 1.42] ## ## ## Note. M indicates mean. SD indicates standard deviation. d-values are estimates calculated using formulas 4.18 and 4.19 ## from Borenstein, Hedges, Higgins, &amp; Rothstein (2009). d-values not calculated if unequal variances prevented pooling. ## Values in square brackets indicate the 95% confidence interval for each d-value. ## The confidence interval is a plausible range of population d-values ## that could have caused the sample d-value (Cumming, 2014). ## The apa.reg.table function from the apaTables package can table the group means, standard deviations, and Cohen’s d values in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. References "],["selection.html", "Chapter 35 Introduction to Employee Selection 35.1 Evaluating Selection Tools 35.2 Chapters Included", " Chapter 35 Introduction to Employee Selection Employee selection includes processes and systems that are used to determine who – from the applicant pool – should be extended an offer to work in the organization. While employee selection can include selecting individuals from outside of the organization, it can also be used to select individuals within the organization for promotions or lateral moves. As part of this process, we use selection tools, which might include interviews, personality tests, cognitive ability tests, work samples or simulations, application blanks, or recommendations. Selection tools can also be referred to as assessments, tests, or procedures. For a brief introduction to employee selection, please consider watching the following conceptual video. Link to conceptual video: https://youtu.be/eKtUYHHXVjg 35.1 Evaluating Selection Tools Our primary focus will be to learn methods for evaluating selection tools, which includes estimating their reliability and validity and whether they demonstrate bias. In the following conceptual video, I provide a general review of how organizations can go about evaluating selection tools. Link to conceptual video: https://youtu.be/SpDP-z6XhE0 35.2 Chapters Included In the following chapters, you will have opportunities to learn how to evaluate employee selection tools. Investigating Disparate Impact Estimating Criterion-Related Validity of a Selection Tool Using Correlation Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method Testing for Differential Prediction Using Moderated Multiple Linear Regression Statistically &amp; Empirically Cross-Validating a Selection Tool "],["disparateimpact.html", "Chapter 36 Investigating Disparate Impact 36.1 Conceptual Overview 36.2 Tutorial", " Chapter 36 Investigating Disparate Impact In this chapter, we learn about how to detect evidence of disparate impact by using tests like the 4/5ths Rule (80% Rule), \\(Z_{D}\\) test, \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence (i.e., chi-square test of independence), and Fisher exact test. 36.1 Conceptual Overview Disparate impact, which is also referred to as adverse impact, refers to situations in which organizations “use seemingly neutral criteria that have a discriminatory effect on a protected group” (Bauer et al. 2025). There are various tests – both statistical and nonstatistical – that may be used to evaluate whether there is evidence of disparate impact, such as the 4/5ths Rule (80% Rule), \\(Z_{Difference}\\) test (i.e., \\(Z_{D}\\) test), \\(Z_{Impact Ratio}\\) test (i.e., \\(Z_{IR}\\) test), \\(\\chi^2\\) test of independence, and Fisher exact test. In the United States, it is legally advisable to begin with testing the 4/5ths Rule followed by a statistical test like the \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence, or Fisher exact test. If you would like to learn more about how to evaluate disparate impact along with empirical evidence regarding under which conditions various tests perform best, I recommend checking out the following resources: Collins, M. W., &amp; Morris, S. B. (2008). Testing for adverse impact when sample size is small. Journal of Applied Psychology, 93(2), 463-471. Dunleavy, E., Morris, S., &amp; Howard, E. (2015). Measuring adverse impact in employee selection decisions. In C. Hanvey &amp; K. Sady (Eds.), Practitioner’s guide to legal issues in organizations (pp. 1-26). Switzerland: Springer, Cham. Finch, D. M., Edwards, B. D., &amp; Wallace, J. C. (2009). Multistage selection strategies: Simulating the effects on adverse impact and expected performance for various predictor combinations. Journal of Applied Psychology, 94(2), 318-340. Morris, S. B. (2001). Sample size required for adverse impact analysis. Applied HRM Research, 6(1-2), 13-32. Morris, S. B., &amp; Lobsenz, R. E. (2000). Significance tests and confidence intervals for the adverse impact ratio. Personnel Psychology, 53(1), 89-111. Office of Federal Contract Compliance Programs. (1993). Federal contract compliance manual (SUDOC L 36.8: C 76/993). Washington, DC: U. S. Department of Labor, Employment Standards Administration. Roth, P. L., Bobko, P., &amp; Switzer, F. S. III. (2006). Modeling the behavior of the 4/5ths rule for determining adverse impact: Reasons for caution. Journal of Applied Psychology, 91(3), 507-522. 36.2 Tutorial This chapter’s tutorial demonstrates how to compute an (adverse) impact ratio to test the 4/5ths Rule (80% Rule) and how to estimate the \\(Z_{D}\\) test, \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence, and Fisher exact test. 36.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorials below. Link to video tutorial: https://youtu.be/1e-ZW-AC_6o Link to video tutorial: https://youtu.be/oddQdDH73oQ 36.2.2 Functions &amp; Packages Introduced Function Package xtabs base R print base R addmargins base R proportions base R chisq.test base R phi psych fisher.test base R sum base R prop.table base R sqrt base R abs base R pnorm base R log base R exp base R 36.2.3 Initial Steps If you haven’t already, save the file called “DisparateImpact.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “DisparateImpact.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DisparateImpact.csv&quot;) ## Rows: 274 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Cognitive_Test, Gender ## dbl (1): ID ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;ID&quot; &quot;Cognitive_Test&quot; &quot;Gender&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [274 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:274] 1 2 3 4 5 6 7 8 9 10 ... ## $ Cognitive_Test: chr [1:274] &quot;Pass&quot; &quot;Pass&quot; &quot;Pass&quot; &quot;Pass&quot; ... ## $ Gender : chr [1:274] &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_double(), ## .. Cognitive_Test = col_character(), ## .. Gender = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 3 ## ID Cognitive_Test Gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Pass Man ## 2 2 Pass Man ## 3 3 Pass Man ## 4 4 Pass Man ## 5 5 Pass Man ## 6 6 Pass Man There are 3 variables and 274 cases (i.e., applicants) in the df data frame: ID, Cognitive_Test, and Gender. ID is the unique applicant identifier variable. Imagine that these data were collected as part of selection process, and this data frame contains information about the applicants. The Cognitive_Test variable is a categorical (nominal) variable that indicates whether an applicant passed (Pass) or failed (Fail) a cognitive ability selection test. Gender is also a categorical (nominal) variable that provides each applicant’s self-reported gender identify, which in this sample happens to include Man and Woman. 36.2.4 4/5ths Rule Evaluating the 4/5ths Rule involves simple arithmetic. As an initial step, we will create a cross-tabulation (i.e., cross-tabs, contingency) table using the xtabs function from base R. As the first argument, in the xtabs function, type the tilde (~) operator followed by the projected class variable (Gender), followed by the addition (+) operator, followed by the selection tool variable containing the pass/fail information (Cognitive_Test). As the second argument, type data= followed by the name of the data frame object (df) to which the two variables mentioned in the first argument “live.” Next, to the left of the xtabs function, type any name you’d like for the new cross-tabs table object we’re creating using the xtabs function, followed by the left arrow (&lt;-) operator; I call this cross-tabs table object observed because it will contain our observed frequency/count data for the number of people from each gender who passed and failed this cognitive ability test. # Create cross-tabs table of observed data observed &lt;- xtabs(~ Gender + Cognitive_Test, data=df) Let’s take a look at the table object we created called observed by entering its name as the sole parenthetical argument in the print function from base R. # Print cross-tabs table of observed data print(observed) ## Cognitive_Test ## Gender Fail Pass ## Man 90 70 ## Woman 72 42 As you can see, the observed object contains the raw counts/frequencies of those men and women who passed and failed the cognitive ability test. Now we’re ready to compute the selection ratios (i.e., selection rates, pass rates) for the two groups we wish to compare. In this example, we will focus on the protected class variable Gender and the two Gender categories available in these data: Man and Woman. The approach we’ll use is not the most efficient or elegant way to compute selection ratios, but it is perhaps more transparent than other approaches – and thus good for learning. Let’s begin by calculating the total number of men who participated in this cognitive ability test. To do so, we will use matrix/bracket notation to “pull” specific values from our observed table object that correspond to the number of men who passed and the number of men who failed; if we add those two values together, we will get the total sample size for men who participated in this cognitive ability test. To illustrate how matrix/bracket notation works, let’s practice by just pulling the number of men who passed. Simply, type the name of the table object (observed), followed by brackets [ ]. Within the brackets, a number that comes before the comma represents the row number in the table or matrix object, and the number that comes after the comma represents the column number. To pull the number of men who passed the test, we would reference the cell from our table that corresponds to row number 1 and column number 2 (as shown below). # Practice matrix notation observed[1,2] ## [1] 70 Now we’re ready to pull the pass and fail counts for men and assign them to object names of our choosing. For clarity, I’m labeling these objects as pass_men and fail_men, and we use the left arrow (&lt;-) operator to assign the values to these new objects. # Create pass and fail objects for men containing the raw frequencies/counts pass_men &lt;- observed[1,2] fail_men &lt;- observed[1,1] If you’d like you can print the pass_men and fail_men objects to verify that the correct table values were assigned to each object. By adding our pass_men and fail_men values together, we can compute the total number of men who participated in this cognitive ability test. Let’s call this new object N_men. # Create an object that contains the total number of men who participated in the test N_men &lt;- pass_men + fail_men Let’s now proceed with doing the same for the number of women who passed and failed, as well as the total number of women. # Create object containing number of women who passed, number of women who failed, # and the total number of women who participated in the test pass_women &lt;- observed[2,2] fail_women &lt;- observed[2,1] N_women &lt;- pass_women + fail_women We now have the ingredients necessary to compute the selection ratios for men and women. To calculate the selection ratio for men, simply divide the number of men who passed (pass_men) by the total number of men (N_men). Using the left arrow (&lt;-) operator, we will assign this quotient to an object that I’m arbitrarily calling SR_men (for “selection ratio for men”). # Create object containing selection ratio for men SR_men &lt;- pass_men / N_men Let’s do create an object called SR_women that contains the selection ratio for women. # Create object containing selection ratio for women SR_women &lt;- pass_women / N_women As the final computational step, we’ll create an object called IR containing the impact ratio value. Simply divide our SR_women object by the SR_men object, and assign that quotient to an object called IR using the left arrow (&lt;-) operator. In most cases, it is customary to set the group with the lower selection ratio as the numerator and the group with the higher selection ratio as the denominator. There can be exceptions, though, such as in instances in which a law is directional, such as the Age Discrimination in Employment Act of 1967, which at the federal level stipulates that workers older than 40 years of age are projected; in this particular case, we would typically always set the selection ratio for workers older 40 as the numerator and the selection ratio for workers younger than 40 as the denominator. # Create impact ratio object IR &lt;- SR_women / SR_men To view the IR object, use the print function. # Print impact ratio object print(IR) ## [1] 0.8421053 Because the impact ratio (IR) value is .84 and thus greater than .80 (80%, 4/5ths), then we would conclude based on this test that there is not evidence of disparate impact on this cognitive ability test when comparing the selection ratios of men and women. If the impact ratio had been less than .80, then we would have concluded that based on the 4/5ths Rule, there was evidence of disparate impact. It is sometimes recommended that we apply the “flip-flop rule”, which is essentially a sensitivity test for the 4/5ths Rule test. This is often called the impact ratio adjusted (IR_adj). To compute the impact ratio adjusted, we re-apply the impact ratio formula; however, this time we add in a hypothetical women who passed and subtract a hypothetical man who passed. # Apply &quot;flip-flop rule&quot; (impact ratio adjusted) IR_adj &lt;- ((pass_women + 1) / N_women) / ((pass_men - 1) / N_men) print(IR_adj) ## [1] 0.8746504 If the impact ratio adjusted (IR_adj) value is less than 1.0, then the original interpretation of the impact ratio stands; in this example, the impact ratio adjusted value is less than 1.0, and thus we continue on with our original interpretation that there is no violation of the 4/5ths Rule and thus no evidence of disparate impact. Finally, it’s important to note that the impact ratio associated with the 4/5ths Rule is an effect size and, thus, can be compared across samples and tests. With that being said, on its own, the 4/5ths Rule doesn’t produce a test of statistical significance. For a test of adverse impact that yield a statistical significance estimate, we should should tern to the \\(\\chi^2\\) test of independence, Fisher Exact test, \\(Z\\)-test, or \\(Z_{ImpactRatio}\\). 36.2.4.1 Optional: Alternative Approach to Computing 4/5ths Rule If you’d like to practice your R skills and learn some additional functions, you’re welcome to apply this mathematically and operationally equivalent approach to the 4/5ths Rule. As a side note, it’s worth noting that the addmargins function from base R can be used to automatically compute the row and column marginals for a table object. This can be a handy function for simplifying some operations, and matrix/bracket notation can still be used on a new table object that is created using the addmargins function. # View row and column marginals for table addmargins(observed) ## Cognitive_Test ## Gender Fail Pass Sum ## Man 90 70 160 ## Woman 72 42 114 ## Sum 162 112 274 Now, check out the following steps to see an alternative approach to computing an impact ratio. Note that the proportions function from base R is introduced and that we’re referencing the same table object (observed) as as above. # Convert table object values to proportions by Gender (columns) prop_observed &lt;- proportions(observed, &quot;Gender&quot;) # Compute selection ratios for men SR_men &lt;- prop_observed[1,2] # Compute selection ratios for women SR_women &lt;- prop_observed[2,2] # Compute impact ratio (IR) IR &lt;- SR_women / SR_men # Print impact ratio (IR) print(IR) ## [1] 0.8421053 36.2.5 Chi-Square (\\(\\chi^2\\)) Test of Independence The chi-square (\\(\\chi^2\\)) test of independence is a statistical test than can be applied to evaluating whether this evidence of disparate impact. It’s relatively simple to compute. For background information on the \\(\\chi^2\\) test of independence, please refer to this chapter Using the chisq.test function from base R, as the first argument, type the name of the table object containing the raw counts/frequencies (observed). As the second argument, enter the argument correct=FALSE, which stipulates that we wish for our \\(\\chi^2\\) test to be computed the traditional way without a continuity correction. # Compute chi-square test of independence chisq.test(observed, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: observed ## X-squared = 1.3144, df = 1, p-value = 0.2516 In the output, we should focus our attention on the p-value and whether it is equal to or greater than the conventional two-tailed alpha level of .05. In this case, p-value is greater than .05, so we would conclude that there appears to be no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, gender does not appear have an effect on whether someone passes or fails this particular test, and thus there is no evidence of disparate impact based on this statistical test. This corroborates what we found when test the 4/5ths Rule above. If the p-value had been less than .05, then we would have concluded there is statistical evidence of disparate impact. For fun, we can append $observed and $expected to our function to see the observed and expected counts/frequencies that were used to compute the \\(\\chi^2\\) test of independence behind the scenes. # View observed values chisq.test(observed, correct=FALSE)$observed ## Cognitive_Test ## Gender Fail Pass ## Man 90 70 ## Woman 72 42 # View expected values chisq.test(observed, correct=FALSE)$expected ## Cognitive_Test ## Gender Fail Pass ## Man 94.59854 65.40146 ## Woman 67.40146 46.59854 As an aside, to apply the Yates continuity correction, we would simply flip correct=FALSE to correct=TRUE. This continuity correction is available when we are evaluating data from a 2x2 table, which is the case in this example. There is a bit of a debate regarding whether to apply this continuity correction. Briefly, this continuity correction was introduced to account for the fact that in the specific case of 2x2 contingency tables (like ours), \\(\\chi^2\\) values tend to be upwardly biased. Others, however, counter that this test is too strict. For a more conservative test, apply the continuity correction. # Compute chi-square test of independence (with Yates continuity correction) chisq.test(observed, correct=TRUE) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: observed ## X-squared = 1.0441, df = 1, p-value = 0.3069 36.2.5.1 Optional: Compute Phi (\\(\\phi\\)) Coefficient (Effect Size) for Chi-Square Test of Independence The p-value we computed for the 2x2 chi-square (\\(\\chi^2\\)) test of independence is a test of statistical significance, which informs whether we should treat the association between two categorical variables is statistically significant. The p-value, however, does not give us any indication of practical significance or effect size, where an effect size is a standardized metric that can be compared across tests and samples. As an indicator of effect size, the phi (\\(\\phi\\)) coefficient can be computed for a 2x2 \\(\\chi^2\\) test of independence, where both variables have two levels (i.e., categories). Important note: Typically, we would only compute the phi (\\(\\phi\\)) coefficient in the event we observed a statistically significant \\(\\chi^2\\) test of independence. As shown in the previous section, we did not find evidence that the \\(\\chi^2\\) test of independence was statistically significant, so would not usually go ahead and compute an effect size. That being said, we will compute phi (\\(\\phi\\)) coefficient just for demonstration purposes. To compute the phi (\\(\\phi\\)) coefficient, we will use the phi function from the psych package (Revelle 2023), which means we first need to install (if not already installed) the psych package using the install.packages function from base R and then access the package using the library function from base R. # Install psych package (if not already installed) install.packages(&quot;psych&quot;) # Access the psych package library(psych) Now we’re ready to deploy the phi function. # Compute phi coefficient phi(observed) ## [1] 0.06926145 The phi (\\(\\phi\\)) coefficient is -.07. Because this is as an effect size, we can describe it qualitatively (e.g., small, medium, large). For a 2x2 contingency table, the phi (\\(\\phi\\)) coefficient is equivalent to a Pearson product-moment correlation (r). Like a correlation (r) coefficient, the (\\(\\phi\\)) coefficient can range from -1.00 (perfect negative association) to 1.00 (perfect positive association), with .00 indicating no association. Further, we can use the conventional correlation thresholds for describing the magnitude of the effect, which I display in the table below. \\(\\phi\\) Description .10 Small .30 Medium .50 Large Because the absolute value of the phi (\\(\\phi\\)) coefficient is .07 and thus falls below the .10 threshold for “small,” we might describe the effect as “negligible” or “very small.” Better yet, because the p-value associated with the \\(\\chi^2\\) test of independence indicated that the association was nonsignificant, we should just conclude that there is no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, we treat the effect size as though it were zero. Finally, one limitation of the phi (\\(\\phi\\)) coefficient is that the upper limit of the observed coefficient will be attenuated to the extent that the two categorical variables don’t have a 50/50 distribution (Dunleavy, Morris, and Howard 2015), which is often the case in the specific context of selection ratios and adverse impact. For example, if the proportion of applicants who passed the selection test is anything but .50, then the estimated \\(\\phi\\) value will not have the potential to reach the upper limits of -1.00 or 1.00. Similarly, if the proportion of applicants from one group relative to another group is anything but .50, then the estimated \\(\\phi\\) value will not have the potential to reach the upper limits of -1.00 or 1.00. 36.2.6 Fisher Exact Test In instances in which we have a small sample size (e.g., N &lt; 30) and/or one of the 2x2 cells has an expected frequency that is less than 5, the Fisher exact test is a more appropriate test than other tests like the chi-square (\\(\\chi^2\\)) test of independence or the \\(Z\\)-test (Federal Contract Compliance Programs 1993). The Fisher exact test is very simple to compute when using the fisher.test function from base R. Simply, enter the name of the table object (observed) as the sole parenthetical argument in the function. # Compute fisher test fisher.test(observed) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: observed ## p-value = 0.2643 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4441286 1.2622356 ## sample estimates: ## odds ratio ## 0.7507933 In the output, we should focus our attention on the p-value and whether it is equal to or greater than the conventional two-tailed alpha level of .05. In this case, p-value is greater than .05, so we would conclude that there appears to be no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, gender does not appear have an effect on whether someone passes or fails this particular test, and thus there is no evidence of disparate impact based on this statistical test. This corroborates what we found when test the 4/5ths Rule above. If the p-value had been less than .05, then we would have concluded there is statistical evidence of disparate impact based on this statistical test. If we’d like to compute an effect size as an indicator of practical significance for a significant Fisher exact test, then we can compute the phi (\\(\\phi\\)) coefficient (see previous section) – or another effect size like an odds ratio. 36.2.7 \\(Z_{D}\\) Test The \\(Z_{D}\\) test – or just \\(Z\\) test – offers another method for evaluating whether evidence of disparate impact exists, and more specifically, this test tests whether there is a significant difference between two selection ratios. The \\(Z_{D}\\) test is also referred to as the Two Standard Deviation Test. Of note, the \\(Z_{D}\\) test is statistically equivalent to a 2x2 chi-square (\\(\\chi^2\\)) test of independence, which means that they will yield the same p-value. To compute the \\(Z_{D}\\) test, we’ll use the contingency table (cross-tabulation) object called observed that we created previously. To begin, we’ll create objects called N_men and N_woman that represent the total number of men and women in our sample, respectively. First, we’ll use matrix notation to reference row 1 of our contingency table (observed[1,]), which contains the number of men who passed and failed the cognitive ability test. Second, we’ll “wrap” row 1 from the matrix in the sum function from base R. # Create object for the total number of men N_men &lt;- sum(observed[1,]) We’ll do the same thing to compute the total number of women in our sample, and their counts reside row 2 of our contingency table. # Create object for the total number of women N_women &lt;- sum(observed[2,]) Now it’s time to compute the total selection ratio for the sample, irrespective of gender identity, and we’ll assign the total selection ratio to an object that we’ll name SR_total by using the &lt;- operator. To the right of the &lt;-, we will specify the ratio. First, we’ll specify the numerator (i.e., total number of individuals who passed the cognitive ability test) by computing the sum of column 2 in the observed contingency table (sum(observed[,2])). Second, we’ll divide the numerator (using the / operator) by the denominator (i.e., total number of individuals in the sample) by computing the sum of the all counts in our contingency table (sum(observed)). We’ll set aside the resulting SR_total object until we’re ready to plug it into the \\(Z_{D}\\) test formula. # Calculate marginal (overall, total) selection ratio/rate SR_total &lt;- sum(observed[,2]) / sum(observed) Our next objective is to convert the counts in the observed contingency table to proportions by row. Using the prop.table function from base R, we will type the name of the contingency object as the first argument, and type 1 as the second argument, where the latter argument requests proportions by row. Using the &lt;- operator, we’ll assign the table with row proportions to an object that we’ll name prop_observed. # Convert contingency table to proportions by row prop_observed &lt;- prop.table(observed, 1) Now we’re ready to assign the selection ratios for men and women, respectively, to objects that we’ll call SR_men and SR_women, and we’ll accomplish this by using the &lt;- operator. Our selection ratios for each gender identity have already been computed in the row proportions contingency table object that we created above (prop_observed). The selection ratio for men appears in the cell corresponding to row 1 and column 2 (i.e., the proportion of men who passed the test), and the selection ratio for women appears in the cell corresponding to row 2 and column 2 (i.e., the proportion of women who passed the test). We’ll grab these values from the prop_observed object using matrix notation and then assign them to their respective objects. # Compute selection ratios/rates (SR) SR_men &lt;- prop_observed[1,2] SR_women &lt;- prop_observed[2,2] The time has come to apply the objects we created above to the \\(Z_{D}\\) test formula, which is shown below. \\(Z_{D} = \\frac{SR_{women} - SR_{men}} {\\sqrt{SR_{total}(1-SR_{total})(\\frac{1}{N_{women}} + \\frac{1}{N_{men}})}}\\) Let’s apply the objects we created using the mathematical operations detailed in the formula, and let’s assign the resulting \\(Z\\)-value to an object we’ll call Z. # Compute Z-value Z &lt;- (SR_women - SR_men) / sqrt( (SR_total*(1 - SR_total) * ((1/N_women) + (1/N_men))) ) Using the abs function from base R, let’s compute the absolute value of the Z object so that we can compare it to the critical \\(Z\\)-values for statistical significance. # Compute absolute value of Z-value abs(Z) ## [1] 1.146481 The absolute value of the \\(Z\\)-value is 1.15, which falls below the critical value for significance of 1.96 for two-tailed test with an alpha of .05, where alpha is our p-value cutoff for statistical significance. Based on this critical value, we can conclude that there is not a statistical difference between the selection ratios for men and women for the cognitive ability test based on this sample. With that being said, some argue that a more liberal one-tailed test with alpha equal to .05 might be more appropriate, so as to avoid false negatives (Type II Error). The critical value for a one-tailed test with an alpha of .05 is 1.645. We can see that our observed \\(Z\\)-value of 1.15 is less than that critical value as well. Thus, at least with respect to the \\(Z_{D}\\) test, we can conclude that the two selection ratios are not statistically different from one another. More specifically, failure to exceed the critical values indicates that the difference between the two selection ratios for men and women is less than two standard deviations, which is where the other name for this test comes from (i.e., Two Standard Deviations Test) In many cases, we might also want want to report the exact p-value for the \\(Z_{D}\\) test. To calculate the exact p-value for a one-tailed test, we will plug the absolute value of our \\(Z\\)-value into the first argument of the pnorm function from base R; and as the second argument, we will specify lower.tail=FALSE, to specify that only want the upper tail of the test, thereby requesting a one-tailed test. # Calculate exact p-value (one-tailed) pnorm(abs(Z), lower.tail=FALSE) ## [1] 0.1257981 To compute a two-tailed p-value, we will take the pnorm function from above, and multiple the resulting p-value by 2 using the * (multiplication) operator. # Calculate exact p-value (two-tailed) 2*pnorm(abs(Z), lower.tail=FALSE) ## [1] 0.2515962 Both the one- and two-tailed p-values fall below the conventional alpha of .05, and thus both indicate that we should fail to reject the null hypothesis that there is zero difference between the population selection ratios for these two gender identities. 36.2.8 \\(Z_{IR}\\) Test The \\(Z_{IR}\\) test is more a direct test of the 4/5ths Rule and associated impact ratio than, say, the chi-square (\\(\\chi^2\\)) test of independence or the \\(Z_{D}\\) test, which if you recall is the selection ratio of one group divided by the selection ratio of another group. The \\(Z_{IR}\\) also tends to be a more appropriate test when sample sizes are small, when the selection ratios for specific groups are low, and when the proportion of the focal group (e.g., women) relative to the overall sample is small (Finch, Edwards, and Wallace 2009; Morris 2001; Morris and Lobsenz 2000). The formula for the \\(Z_{IR}\\) test is as follows. \\(Z_{IR} = \\frac{ln(\\frac{SR_{women}}{SR_{men}})} {\\sqrt{(\\frac{ 1-SR_{total}}{SR_{total}})(\\frac{1}{N_{women}} + \\frac{1}{N_{men}})}}\\) Fortunately, when we prepared to compute the \\(Z_{D}\\) test above, we created all of the necessary components (i.e., objects) needed to compute the \\(Z_{IR}\\) test. So let’s plug them into our formula and assign the resulting \\(Z_{IR}\\) to an object called Z_IR. Note that the log function from base R computes the natural logarithm. # Compute Z-value Z_IR &lt;- log(SR_women/SR_men) / sqrt(((1 - SR_total)/SR_total) * (1/N_women + 1/N_men)) Using the abs function from base R, let’s compute the absolute value of the Z_IR object so that we can compare it to the critical \\(Z\\)-values for statistical significance. # Compute absolute value of Z-value abs(Z_IR) ## [1] 1.16584 The absolute value of the \\(Z\\)-value is 1.17, which falls below the critical value for significance of 1.96 for two-tailed test with an alpha of .05, where alpha is our p-value cutoff for statistical significance. Based on this critical value, we can conclude that the ratio of the selection ratios for men and women (i.e., the impact ratio) does not differ significantly from an impact ratio of 1.0, where the latter would indicate that the selection ratios are equal. Like we did with the \\(Z_{D}\\) test, we can apply the more liberal one-tailed test with alpha equal to .05, where he critical value for a one-tailed test with an alpha of .05 is 1.645. We can see that our observed \\(Z\\)-value of 1.17 is also less than that critical value. Thus, with respect to the \\(Z_{IR}\\) test, we can conclude that the there is no relative difference between the selection ratios for men and women. We might also want want to report the exact p-value for the \\(Z_{IR}\\) test. To calculate the exact p-value for a one-tailed test, we will plug the absolute value of our \\(Z\\)-value into the first argument of the pnorm function from base R; and as the second argument, we will specify lower.tail=FALSE, to specify that only want the upper tail of the test, thereby requesting a one-tailed test. # Calculate exact p-value (one-tailed) pnorm(abs(Z_IR), lower.tail=FALSE) ## [1] 0.1218396 To compute a two-tailed p-value, we will take the pnorm function from above, and multiple the resulting p-value by 2 using the * (multiplication) operator. # Calculate exact p-value (two-tailed) 2*pnorm(abs(Z_IR), lower.tail=FALSE) ## [1] 0.2436793 Both the one- and two-tailed p-values are above the conventional alpha of .05, and thus both indicate that we should fail to reject the null hypothesis that there is zero relative difference between the population selection ratios for men and women with respect to the cognitive ability test. 36.2.8.1 Optional: Compute Confidence Intervals for \\(Z_{IR}\\) Test Given that tests of disparate impact often involve small sample sizes and thus are susceptible to low statistical power, some have recommended that effect sizes and confidence intervals be used instead of interpreting statistical significance using a p-value (see Morris and Lobsenz 2000). Confidence intervals around an impact ratio estimate reflect sampling error and provide a range of possible values in which the population parameter (i.e., population impact ratio) may fall – or put differently, how the impact ratio will likely vary across different samples drawn from the same population. To compute the confidence intervals for the \\(Z_{IR}\\) test, we’ll first need to compute the natural log of the impact ratio using the log function from base R. We’ll assign the natural log of the impact ratio to an object that we’ll call IR_log. # Compute natural log of impact ratio (IR) IR_log &lt;- log(SR_women/SR_men) Next, we will compute the standard error of the impact ratio using the formula shown in the code chunk below, and we will assign it to an object that we’ll call SE_IR. # Compute standard error of IR (SE_IR) SE_IR &lt;- sqrt( ((1 - SR_women) / (N_women * SR_women) + (1 - SR_men)/(N_men * SR_men)) ) Using the standard error of the impact ratio object (SE_IR) and the natural log of the impact ratio object (IR_log), we can compute the lower and upper limits of the a 95% confidence interval by adding and subtracting, respectively, the product of the standard error of the impact ratio and 1.96 to/from the natural log of the impact ratio. # Compute bounds of 95% confidence interval for natural log # of impact ratio LCL_log &lt;- IR_log - 1.96 * SE_IR # lower UCL_log &lt;- IR_log + 1.96 * SE_IR # upper Finally, we can convert the lower and upper limits of the 95% confidence interval to the original scale of the impact ratio metric by exponentiating the lower and upper limits of the natural log of the impact ratio. # Convert to scale of original IR metric LCL &lt;- exp(LCL_log) # lower UCL &lt;- exp(UCL_log) # upper # Print the 95% confidence intervals print(LCL) ## [1] 0.6252696 print(UCL) ## [1] 1.134137 Thus, we can be 95% confident that the true population impact ratio falls somewhere between .63 and 1.13 (95% CI[.63, 1.13]), and we already know that the effect size (i.e., impact ratio) for this cognitive ability test with respect to gender is .84 based on our test of the 4/5ths Rule (see above). Note that if the population parameter were to fall at the lower limit of the confidence interval (.63), then we would conclude that there was disparate impact based on the 4/5ths Rule; however, if the the population parameter were to fall at the upper limit of the confidence interval (1.13), we would conclude that there is no evidence of disparate impact based on the 4/5ths Rule. Thus, the confidence interval indicates that if we were to sample from the same population many more times, we would sometimes find a violation of the 4/5ths Rule – but sometimes not. If we want, we can also compute the 90% confidence intervals for the impact ratio by swapping the 1.96 critical \\(Z\\)-value for a two-tailed test (alpha = .05) with the 1.645 critical \\(Z\\)-value for a one-tailed test (alpha = .05). Everything else will remain the same as when we computed the 95% confidence interval. # Compute bounds of 90% confidence interval for natural log # of impact ratio LCL_log &lt;- IR_log - 1.645 * SE_IR # lower UCL_log &lt;- IR_log + 1.645 * SE_IR # upper # Convert to scale of original IR metric LCL &lt;- exp(LCL_log) # lower UCL &lt;- exp(UCL_log) # upper # Print the 90% confidence intervals print(LCL) ## [1] 0.655915 print(UCL) ## [1] 1.081148 Based on these calculations, we can be 90% confident that the true population impact ratio falls somewhere between .66 and 1.08 (90% CI[.66, 1.08]). 36.2.9 Summary In this chapter, we learned how to evaluate whether there is evidence of disparate impact using the 4/5ths Rule, chi-square (\\(\\chi^2\\)) test of independence, Fisher exact test, \\(Z_{D}\\) test, and \\(Z_{IR}\\) test. References "],["criterionrelatedvalidity.html", "Chapter 37 Estimating Criterion-Related Validity of a Selection Tool Using Correlation 37.1 Conceptual Overview 37.2 Tutorial 37.3 Chapter Supplement", " Chapter 37 Estimating Criterion-Related Validity of a Selection Tool Using Correlation In this chapter, we will learn how to estimate the criterion-related validity of an employee selection tool by using a correlation. We’ll begin with a conceptual overview of criterion-related validity and correlation, and we’ll conclude with a tutorial. 37.1 Conceptual Overview In this section, we’ll begin by reviewing the concept of criterion-related validity and then conclude by reviewing the Pearson product-moment correlation. With respect to the latter, we’ll discuss the statistical assumptions that should be satisfied prior to estimating and interpreting a correlation as well as statistical significance and and practical significance in the context of correlation. The section will wrap up with a sample-write up of a correlation when used to estimate the criterion-related validity of a selection tool. 37.1.1 Review of Criterion-Related Validity Criterion-related validity (criterion validity) has to do with the association between a variable and some outcome variable. The term criterion can be thought of as some outcome or correlate of practical or theoretical interest. In the employee selection context, the criterion of interest is often some indicator of job performance (e.g., performance evaluations). Where possible, the U.S. court system generally favors evidence of criterion-related validity when demonstrating that a selection tool is job-relevant and job-related. The data necessary to estimate criterion-related validity can be acquired from concurrent validation studies/designs or predictive validation studies/designs, both of which can be classified beneath the umbrella term of criterion-related validation studies/designs. For additional information on criterion-related validity and other types of validity (e.g., content, construct, convergent, discriminant), I recommend reading the following free resources: (a) pp. 10-15 from the Principles for the Validiation and Use of Personnel Selection Procedures (Society for Industrial &amp; Organizational Psychology 2018); and (b) Chapter 4.2 from Research Methods in Psychology (Price et al. 2017). 37.1.2 Review of Correlation Link to conceptual video: https://youtu.be/xqdfA01krGs A correlation represents the sign (i.e., direction) and magnitude (i.e., strength) of an association between two variables. There are different types of correlations we can estimate, and their appropriateness will depend on the measurement scales of the two variables. For instance, the Pearson product-moment correlation is used when both variables are continuous (i.e., have interval or ratio measurement scale), whereas the biserial correlation is used when one variable is continuous and the other is dichotomous (e.g., has a nominal measurement scale with just two levels or categories). In this chapter, we will focus on estimating Pearson product-moment correlations, which I will henceforth refer to as just “correlation.” Correlation coefficients can range from -1.00 to +1.00, where zero (.00) represents no association, -1.00 represents a perfect negative (inverse) association, and +1.00 represents a perfect positive association. A Pearson product-moment correlation coefficient (r) for a sample can be computed using the following formula: \\(r = \\frac{\\sum XY - \\frac{(\\sum X)(\\sum Y)}{n}} {\\sqrt{(\\sum X^{2} - \\frac{\\sum X^{2}}{n}) (\\sum Y^{2} - \\frac{\\sum Y^{2}}{n})}}\\) where \\(X\\) refers to scores from one variable and \\(Y\\) refers to scores from the other variable, \\(n\\) refers to the sample size (i.e., the number of pairs of data corresponding to the number of cases – with complete data). When estimated using data acquired from a criterion-related validation study (e.g, concurrent or predictive validation study/design), a correlation coefficient can be referred to as a validity coefficient. 37.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Each variable shows a univariate normal distribution; Each variable is free of univariate outliers, and together the variables are free of bivariate outliers; Variables demonstrate a bivariate normal distribution - meaning, each variable is normally distributed at each level/value of the other variable. Often, this roughly takes the form of an ellipse shape, if you were to superimpose an oval that would fit around most cases in a bivariate scatter plot; The association between the two variables is linear. 37.1.2.2 Statistical Significance The significance level of a correlation coefficient is determined by the sample size and the magnitude of the correlation coefficient. Specifically, a t-statistic with N-2 degrees of freedom (df) is calculated and compared to a Student’s t-distribution with N-2 df and a given alpha level (usually two-tailed, alpha = .05). If the calculated t-statistic is greater in magnitude than the chosen t-distribution, we conclude that the population correlation coefficient is significantly greater than zero. We use the following formula to calculate the t-statistic: \\(t = \\frac{r \\sqrt{N-2}}{1-r^{2}}\\) where \\(r\\) refers to the estimated correlation coefficient and \\(N\\) refers to the sample size. Alternatively, the exact p-value can be computed using a statistical software program like R if we know the df and t-value. In practice, however, we don’t always report the associated t-value; instead, we almost always report the exact p-value associated with the t-value when reporting information about statistical significance. When using null hypothesis significance testing, we interpret a p-value that is less than our chosen alpha level (which is conventionally .05, two-tailed) to meet the standard for statistical significance. This means that we reject the null hypothesis that the correlation is equal to zero. By rejecting this null hypothesis, we conclude that the correlation is significantly different from zero. If the p-value is equal to or greater than our chosen alpha level (e.g., .05, two-tailed), then we fail to reject the null hypothesis that the correlation is equal to zero; meaning, we conclude that there is no evidence of linear association between the two variables. 37.1.2.3 Practical Significance The size of a correlation coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large Some people like to also report the coefficient of determination as an indicator of effect size. The coefficient of determination is calculated by squaring the correlation coefficient to create r2. When multiplied by 100, the coefficient of determination (r2) can be interpreted as the percentage of variance/variability shared between the two variables, which is sometimes stated as follows: Variable \\(X\\) explains \\(X\\)% of the variance in Variable \\(Y\\) (or vice versa). Please note that we use the lower-case r in r2 to indicate that we are reporting the variance overlap between only two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. A bivariate scatter plot comes in handy when visually depicting the direction and strength of an association between two continuous (interval or ratio measurement scale) variables. Further, the scatter plot can be applied to understand whether statistical assumptions related to linearity and a bivariate normal distribution have been met. To create examples of scatter plots based on simulated data, check out this free tool. The tool also does a nice job of depicting the concept of shared variance in the context of correlation (i.e., coefficient of determination) using a Venn diagram. 37.1.2.4 Sample Write-Up Our organization implemented a concurrent validation study to collect the data necessary to estimate the criterion-related validity of a new structured interview selection tool for customer service representatives. The concurrent validation study included a sample of 67 job incumbents (N = 67) and their scores on the structured interview and on the criterion of job performance. Using a Pearson product-moment correlation, we found a statistically significant, medium-sized correlation between structured interview scores and job performance scores (r = .29, p = .02), such that those who scored higher on the structured interview showed moderately higher levels of job performance. The 95% confidence interval indicated that the true correlation value for the entire population of employees likely falls somewhere between .05 and .50 (95% CI[.05, .50]). In this context, the correlation coefficient can be conceptualized as a validity coefficient and, thus, an indicator of criterion-related validity. Given that the correlation is statistically significant, we can conclude that the structured interview shows evidence of criterion-related validity for the customer service representative job, thereby demonstrating that this selection tool is job-related. Note: If the p-value were equal to or greater than our alpha level (e.g., .05, two-tailed), then we would typically state that the association between the two variables is not statistically significant, and we would not proceed forward with interpreting the effect size (i.e., level of practical significance) because the test of statistical significance indicates that it is very unlikely based on our sample that a true association between these two variables exists in the population. 37.2 Tutorial This chapter’s tutorial demonstrates how to estimate criterion-related validity using a correlation, and how to present the results in writing. 37.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/yPAfqmISQ3U 37.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR Correlation lessR 37.2.3 Initial Steps If you haven’t already, save the file called “SelectionData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “SelectionData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## Rows: 163 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): EmployeeID, Conscientiousness, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [163 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 The data frame contains 4 variables and 163 cases (i.e., employees): EmployeeID, Conscientiousness, Interview, and Performance. Let’s assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., Conscientiousness, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The Conscientiousness variable contains the scores on a personality test designed to “tap into” the psychological concept of conscientiousness; potential scores on this variable could range from 1 (low conscientiousness) to 5 (high conscientiousness). The Interview variable contains the scores for a structured interview designed to assess interviewees’ level customer-service knowledge and skills; potential scores on this variable could range from 1 (poor customer service) to 5 (strong customer service). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 5 (exceeds performance standards). 37.2.4 Visualize Association Using a Scatter Plot Two of the key statistical assumptions that should be satisfied prior to estimating a Pearson product-moment correlation are that (a) the association between the two variables is approximately linear and that (b) a bivariate normal distribution exists between the two variables. A data visualization called a scatter plot can be used to test both of these assumptions. The ScatterPlot function from the lessR package (Gerbing, Business, and University 2021) does a nice job generate generating scatter plots – and it even provides an estimate of the correlation by default. If you haven’t already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To begin, type the name of the ScatterPlot function. As the first two arguments of the function, type the names of the two variables you wish to visualize; let’s start by visualizing the association between the Conscientiousness selection tool and the criterion (Performance). The variable name that we type after the x= argument will set the x-axis, and the variable name that we type after the y= argument will set the y-axis. Conventionally, we place the criterion variable on the y-axis, as it is the outcome. As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (df). # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Conscientiousness, y=Performance, data=df) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 ## In our plot window, we can see a fairly clear positive linear trend between Conscientiousness and Performance, which provides us with some evidence that the assumption of a linear association has been satisfied. Furthermore, the distribution is ellipse-shaped, which gives us some evidence that the underlying distribution between the two variables is likely bivariate normal – thereby satisfying the second assumption mentioned above. Note that the ScatterPlot function automatically provides an estimate of a (Pearson product-moment) correlation in the output (r = .509), along with the associated p-value (p &lt; .001). 37.2.4.1 Optional: Stylizing the ScatterPlot Function from lessR If you would like to optionally stylize your scatter plot, we can use the xlab= and ylab= arguments to change the default names of the x-axis and y-axis, respectively. # Optional: Styling the scatter plot ScatterPlot(x=Conscientiousness, y=Performance, data=df, xlab=&quot;Conscientiousness Test Score&quot;, ylab=&quot;Job Performance Score&quot;) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 ## We can also superimpose an ellipse by adding the argument ellipse=TRUE, which can visually aid our judgument on whether the distribution is bivariate normal. # Scatterplot using ScatterPlot function from lessR ScatterPlot(x=Conscientiousness, y=Performance, data=df, xlab=&quot;Conscientiousness Test Score&quot;, ylab=&quot;Job Performance Score&quot;, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 ## 37.2.5 Estimate Correlation Important note: For space considerations, we will assume that the variables we wish to correlate have met the necessary statistical assumptions (see Statistical Assumptions section). For instance, we will assume that employees in this sample were randomly drawn from the underlying population of employees, that there is evidence of univariate and bivariate normal distributions for the variables in question, that the data are free of outliers, and that the association between variables is linear. To learn how to create a histogram or VBS plot (violin-box-scatter plot) to estimate the shape of the univariate distribution for each variable and to flag any potential univariate outliers, which can be used to assess the assumptions of univariate normal distribution and absence of univariate outliers, check out the chapter called Descriptive Statistics. There are different functions we could use to estimate a Pearson product-moment correlation between two variables (or a biserial correlation). In this chapter, I will demonstrate how to use the Correlation function from the lessR package. If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively (see above). To begin, type the name of the Correlation function from the lessR package. As the first two arguments of the function, type the names of the two variables you wish to visualize; let’s start by visualizing the association between the Conscientiousness selection tool and the criterion (Performance). Conventionally, the variable name that appears after the x= argument is our predictor (e.g., selection tool), and the variable name that appears after the y= argument is our criterion (e.g., job performance). As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (df). # Estimate correlation using Correlation function from lessR Correlation(x=Conscientiousness, y=Performance, data=df) ## Correlation Analysis for Variables Conscientiousness and Performance ## ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 163 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = 0.324 ## ## Sample Correlation: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 As you can see, we find the following: r = .469, p &lt; .001, 95% CI[.339, .581]. We can interpret the finding as follows: Scores on the conscientiousness test are positively associated with job performance scores to a statistically significant extent (r = .469, p &lt; .001, 95% CI[.339, .581]), such that individuals with higher conscientiousness tend to have higher job performance. How do we know the correlation coefficient is statistically significant? Well, the p-value is less than the conventional alpha cutoff of .05. Given the conventional rules of thumb for interpreting a correlation coefficient as an effect size (see Practical Significance section and table below), we can describe this correlation coefficient of .469 as approximately medium or medium-to-large in magnitude. The point estimate of the correlation coefficient (r = .469) does not directly reflect the sampling error that inevitably affects the estimated correlation coefficient derived from our sample; this is where the confidence interval can augment our interpretation by expressing a range, within which we can be reasonably confident that the true (population) correlation coefficient likely falls. With regard to the 95% confidence interval, it is likely that the range from .339 to .581 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. r Description .10 Small .30 Medium .50 Large In the context of selection tool validation, the correlation coefficient between a selection tool and a criterion can be used as an indicator of criterion-related validity. Because the correlation above is statistically significant, this provides initial evidence that the conscientiousness test has sufficiently high criterion-related validity, as it appears to be significantly associated with the criterion of job performance. In this context, we can refer to the correlation coefficient as a validity coefficient. Moreover, the practical significance (as indicated by the effect size) is approximately medium or medium-to-large, given the thresholds mentioned above. Thus, as a selection tool, the conscientiousness test appears to have relatively good criterion-related validity for this population. 37.2.6 Summary In this chapter, we learned how to create a scatter plot using the ScatterPlot function from the lessR package, and how to estimate a correlation using the Correlation function from the lessR package. 37.3 Chapter Supplement In addition to the Correlation function from the lessR package covered above, we can use the cor and cor.test functions from base R to estimate a correlation. Because this function comes from base R, we do not need to install and access an additional package. 37.3.1 Functions &amp; Packages Introduced Function Package cor base R cor.test base R 37.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## Rows: 163 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): EmployeeID, Conscientiousness, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [163 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 37.3.3 cor Function from Base R The cor function from base R is a quick-and-easy approach to estimating a correlation coefficient if you’re only interested in the sign and magnitude (and not the significance level). To begin, type the name of the cor function. As the first argument, type the name of your data frame (df) followed by the $ symbol and the name of one of your continuous (interval/ratio) variables (Conscientiousness). As the second argument, type the name of your data frame (df) followed by the $ symbol and the name of one of your continuous (interval/ratio) variables (Performance). Finally, as the third argument, specify method=\"pearson\" to estimate a Pearson product-moment correlation. If you were estimating the association between two rank-order variables, you could replace “pearson” with “spearman” to estimate a Spearman correlation. # Estimate correlation using cor function cor(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4686663 37.3.4 cor.test Function from Base R To estimate the correlation coefficient and associated p-value (along with the confidence interval), we can use the cor.test function from base R. By default, the alpha level is set to .05 (two-tailed), and thus a 95% confidence interval is used. As another default, the alternative/research hypothesis is that the true (population) correlation is not equal to zero. If wish to use these defaults, which would be most consistent with common practice. We can use the exact same three arguments as we used for the cor function above. # Estimate correlation using cor.test function cor.test(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: df$Conscientiousness and df$Performance ## t = 6.7318, df = 161, p-value = 0.0000000002801 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3393973 0.5805611 ## sample estimates: ## cor ## 0.4686663 If we know that we have missing values (i.e., missing data) on at least one of the variables used to estimate the correlation, we can add the na.action=na.omit argument, which excludes cases with missing values on one or both of the variables. We don’t have any missing values in this data frame, so the estimate remains the same. # Estimate correlation using cor.test function cor.test(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;, na.action=na.omit) ## ## Pearson&#39;s product-moment correlation ## ## data: df$Conscientiousness and df$Performance ## t = 6.7318, df = 161, p-value = 0.0000000002801 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3393973 0.5805611 ## sample estimates: ## cor ## 0.4686663 References "],["predictingcriterionscores.html", "Chapter 38 Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression 38.1 Conceptual Overview 38.2 Tutorial 38.3 Chapter Supplement", " Chapter 38 Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression In this chapter, we will learn how to estimate a simple linear regression model and apply the model’s equation to predict future scores on the criterion variable. We’ll begin with conceptual overviews of simple linear regression and predicting criterion scores, and we’ll conclude with a tutorial. 38.1 Conceptual Overview Like correlation, regression can provide us with information about the strength and sign of the relationship between a predictor variable and an outcome variable. Regression expands upon basic correlation by providing additional information about the nature of a linear relation, such that we can predict changes in a criterion (i.e., outcome) variable based on changes in a predictor variable. More specifically, regression provides us with the basic equation for predicting scores on the outcome variable based on one or more predictor variable scores. Regression equations include information about the Y-intercept and the estimated regression coefficients (i.e., weights, slopes) associated with each predictor in the model. There are many different types of regression models, and in this chapter, we’re going to review simple linear regression and how we can apply the associated equation to predict scores on the criterion. 38.1.1 Review of Simple Linear Regression Link to conceptual video: https://youtu.be/0ChqmiOK5Q4 A simple linear regression model has one predictor (i.e., independent) variable and one outcome (i.e., dependent) variable. In this tutorial, we will learn how to estimate an ordinary least squares (OLS) simple linear regression model, where OLS refers to the process of estimating the unknown components (i.e., parameters) of the regression model by attempting to minimize the sum of squared residuals. The sum of the squared residuals are the result of a process in which the differences between the observed outcome variable values and the predicted outcome variable values are calculated, squared, and then summed in order to identify a model with the least amount of error (residuals). This is the where the “line of best fit” comes into play, as the line of best fit represents the regression model (equation) in which the error (residuals) between the predicted and observed outcome variable values are minimized. In other words, the goal is to find the linear model that best fits the data at hand. The equation for a simple linear regression with unstandardized regression coefficients (\\(b\\)) is as follows: \\(\\hat{Y} = b_{0} + b_{1}X + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variable \\(X\\) is equal to zero, \\(b_{1}\\) represents the unstandardized coefficient (i.e., weight, slope) of the association between the predictor variable \\(X\\) and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. Importantly, the unstandardized regression coefficient \\(b_{1}\\) represents the “raw” slope (i.e., weight, coefficient) – or rather, how many unstandardized units of \\(\\hat{Y}\\) increase or decrease as a result of a single unit increase in \\(X\\). That is, unstandardized regression coefficients reflect the nature of the association between two variables when the variables retain their original scaling. Often this is why we choose to use the unstandardized regression coefficients when making predictions about \\(\\hat{Y}\\), as the predicted scores will be have the same scaling as the outcome variable in its original form. As shown below, we can conceptualize the our simple linear regression model as depicting a “line of best fit,” wherein the estimate linear equation attempts to minimize the errors in prediction in \\(\\hat{Y}\\) based on observed scores of \\(X\\). A simple linear regression equation can be thought of as a “line of best fit.” We can map our regression equation estimates and concepts onto the “line of best fit” as shown below. The intercept and regression coefficient values signal how the “line of best fit” is constructed. We can also obtain standardized regression coefficients. To do so, the predictor variable (\\(X\\)) and outcome variable (\\(Y\\)) scores must be standardized. To standardize variables, we convert the predictor and outcome variables to z-scores, such that their respective means are standardized to 0 and their variances and standard deviations are standardized to 1. When standardized, our simple linear regression model equation will have a \\(\\hat{Y}\\)-intercept value equal to zero (and thus is not typically reported) and the standardized regression coefficient is commonly signified using the Greek letter \\(\\beta\\): \\(\\hat{Y} = \\beta_{1}X + e\\) where \\(\\hat{Y}\\) represents the predicted standardized score on the outcome variable (\\(Y\\)), \\(\\beta_{1}\\) represents the standardized coefficient (i.e., weight, slope) of the association between the predictor variable \\(X\\) and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. When we have obtain standardized regression coefficients in a simple linear regression model, the \\(\\beta_{1}\\) value will be equal to the correlation (r) between \\(X\\) and \\(\\hat{Y}\\). A standardized regression coefficient (\\(\\beta\\)) also allows us to also compare the relative magnitude of one \\(\\beta\\) to another \\(\\beta\\); with that being said, in the case of a multiple linear regression model (in which there are 2+ more predictor variables), comparing \\(\\beta\\) coefficients is only appropriate when the predictor variables in the model share little to no intercorrelation (i.e., have low collinearity). Given that, I recommend that you proceed with caution should you choose to make such comparisons. In terms of interpretation, in a simple linear regression model, the standardized regression coefficient (\\(\\beta_{1}\\)) indicates the standardized slope or, rather, how many standard units of \\(\\hat{Y}\\) increase or decrease as a result of a single standard unit increase in \\(X\\). 38.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple linear regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of bivariate outliers; The association between predictor and outcome variables is linear; Variables demonstrate a bivariate normal distribution; Average residual error value is zero for each level of the predictor variable; Variances of residual errors are equal for all levels of the predictor variable, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for each level of the predictor variable. Note: Regarding the first statistical assumption (i.e., cases randomly sampled from population), we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 38.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficient’s p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficient’s p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 38.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a simple linear regression model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. As noted above, a standardized regression coefficient (\\(\\beta\\)) will be equal to a correlation coefficient (r) in the specific case of a simple linear regression model in which there is just one predictor variable and a single outcome variable. Thus, a standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) using the same interpretation thresholds as a correlation coefficient. Accordingly, in the case of a simple linear regression model, we could just estimate a correlation coefficient to get an idea of the level of practical significance. The size of a correlation coefficient (r) – or a standardized regression coefficient (\\(\\beta\\)) in the case of a simple linear regression model – can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large In a simple linear regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variable (i.e., R2). In fact, in the case of a simple linear regression, the R2 estimate will be equal to the coefficient of determination (r2), as described in the chapter on estimating criterion-related validity using correlations. Conceptually, we can think of the overlap between the variability in the predictor and outcome variables as the variance explained (R2). I’ve found that the R2 is often more readily interpretable by non-analytics audiences. For example, an R2 of .10 in a simple linear regression model can be interpreted as: 10% of the variability in scores on the outcome variable can be explained by scores on the predictor variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large As an effect size, R2 indicates the proportion of variance explained by the predictor variable in the outcome variable – or in other words, the shared variance between the two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 38.1.1.4 Sample Write-Up A team of researchers is interested in whether a basketball player’s height relates to the number of points scored during a 10-game season, and they use a sample of 100 basketball players to estimate a simple linear regression model. In this case, the predictor variable is basketball players’ heights (in inches), and the outcome variable is the number of points scored by the basketball players. Our hypothesis for such a situation might be: Basketball players’ heights will be positively associated with the number of points players score in a 10-game season, such that taller players will tend to score more points in a season. If we find that the unstandardized regression coefficient associated with player height in relation to points scored is statistically significant and positive (b = 2.44, p = .01) and the R2 value is .05 (p = .01), we could summarize the findings as follows: Based on a sample of 100 basketball players, basketball player height was found to predict points scored in a 10-game season, such that taller players tended to score more points (b = 2.44, p = .01). Specifically, for every 1 inch increase in height, players tended to score 2.44 additional points during the season. Further, this reflects a small-to-medium effect, as approximately 5% of the variability in points scored was explained by players’ heights (R2 = .05, p = .01). 38.1.2 Predicting Future Criterion Scores Using Simple Linear Regression Link to conceptual video: https://youtu.be/EbTdE0DFOJ8 As noted above, the intercept and coefficients estimated from a simple linear regression model can be used to construct a linear equation. This equation can be estimated based on one sample of data and then applied to a second sample of data. In doing so, new data for the predictor variable from the second sample can be “plugged into” the model that we estimated from the first sample, thereby allowing us to predict future criterion (i.e., outcome variable) scores. This two-sample process moves us a step towards true predictive analytics. [Note: In the employee selection context, we often refer to the outcome variable as the “criterion” and multiple outcome variables as “criteria.”] In the context of selection tool validation, the process of predicting criterion scores can be quite useful. It allows us to estimate a model for a given selection tool (e.g., assessment, procedure, test) based on data from a criterion-related validity study (e.g., concurrent validation design/study, predictive validation design/study), and then apply that model to data from actual job applicants who have taken the same selection tool. In doing so, we can make predictions about applicants future criterion scores (e.g., job performance scores). Often, we can improve the accuracy of predicted criterion scores by adding additional predictor variables to our model, which transitions the model from a simple linear regression model to a multiple linear regression model, where the latter is covered in the next chapter. 38.2 Tutorial This chapter’s tutorial demonstrates how to estimate a simple linear regression model and then apply the model’s equation to future applicants’ selection tool scores to predict their future criterion scores. We also learn how to present the results in writing. 38.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/TxUaAAR55bs Additionally, in the following video tutorial, I go into greater depth on how to test the statistical assumptions of a simple linear regression model – just as I do in the written tutorial below. Link to video tutorial: https://youtu.be/Qe6LLJAmJ6c 38.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR Regression lessR order base R 38.2.3 Initial Steps If you haven’t already, save the file called “SelectionData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “SelectionData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## Rows: 163 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): EmployeeID, Conscientiousness, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [163 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 The data frame contains 4 variables and 163 cases (i.e., employees): EmployeeID, Conscientiousness, Interview, and Performance. Let’s assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., Conscientiousness, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The Conscientiousness variable contains the scores on a personality test designed to “tap into” the psychological concept of conscientiousness; potential scores on this variable could range from 1 (low conscientiousness) to 5 (high conscientiousness). The Interview variable contains the scores for a structured interview designed to assess interviewees’ level customer-service knowledge and skills; potential scores on this variable could range from 1 (poor customer service) to 5 (strong customer service). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 5 (exceeds performance standards). 38.2.4 Estimate Simple Linear Regression Model When we estimate the criterion-related validity of a selection tool, we typically use a correlation coefficient, and in this context, the correlation coefficient can be referred to as a validity coefficient. That being said, if we would like to make predictions based on selection tool scores, simple linear regression (or multiple linear regression) is the way to go. Thus, when it comes to selection tool validation, my recommendation is to run a correlation when estimating criterion-related validity and to estimate a simple linear regression model when attempting to predict criterion scores based on selection tool scores. As the name implies, a simple linear regression model assumes a linear association between the predictor and outcome scores. Check out the chapter on estimating criterion-related validity using a correlation if you need a refresher on how to interpret a correlation coefficient as an indicator of criterion-related validity. In this chapter, we’ll specify a simple linear regression model in which the selection tool called Conscientiousness is the predictor variable and the job performance variable called Performance is our criterion. 38.2.4.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a simple linear regression model, we need to first test the statistical assumptions. To begin, let’s generate a scatter plot to visualize the association between the two variables and get a rough idea of whether a few of the statistical assumptions have been met. To do so, we’ll use the ScatterPlot function from the lessR package (Gerbing, Business, and University 2021). If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the ScatterPlot function. As the first argument within the function, type x= followed by the name of the predictor variable (Conscientiousness). As the second argument, type y= followed by the name of the outcome variable (Performance). As the third argument, type data= followed by the name of the data frame (df). As the fourth argument, type ellipse=TRUE to generate an ellipse around the data points. # Scatterplot of Conscientiousness &amp; Performance variables ScatterPlot(x=Conscientiousness, y=Performance, data=df, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 ## Based on the output from the ScatterPlot function, first, note that the association between the two variables appears to be linear, which means that a simple linear regression model might be appropriate. Second, note that the association between the two variables seems to have a bivariate normal distribution, as we can see a general ellipse (or American football) shape of the data points. Third, it does look like we might have one or more bivariate outliers, which are those points that seem to notably deviate from the rest. As you can see, we can “eyeball” the data to give us an initial (yet rough) idea of whether three of the statistical assumptions may have been met: (a) data are free of bivariate outliers, (b) association between variables is linear, and (c) variables display a bivariate normal distribution. Fortunately, we have some other diagnostic tools that we will use to further test some of these assumptions (as well as the others) when we estimate our simple linear regression model. To generate these additional statistical assumption tests, we need to estimate our simple linear regression model. There are different functions we could use to estimate a simple linear regression model, but we’ll begin by focusing on the Regression function from the lessR package. In the chapter supplement, you can learn how to carry how the same tests using the lm function from base R. To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome (i.e., criterion) variable (Performance) to the left of the tilde (~) operator and the name of the predictor (e.g., selection tool) variable (Conscientiousness) to the right of the ~ operator. We are telling the function to “regress Performance on Conscientiousness.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate simple linear regression model Regression(Performance ~ Conscientiousness, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ Conscientiousness, data=df, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable: Conscientiousness ## ## Number of cases (rows) of data: 163 ## Number of cases retained for analysis: 163 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 0.780 0.333 2.343 0.020 0.123 1.437 ## Conscientiousness 0.631 0.094 6.732 0.000 0.446 0.817 ## ## Standard deviation of Performance: 0.966 ## ## Standard deviation of residuals: 0.856 for 161 degrees of freedom ## 95% range of residual variation: 3.379 = 2 * (1.975 * 0.856) ## ## R-squared: 0.220 Adjusted R-squared: 0.215 PRESS R-squared: 0.199 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 45.317 df: 1 and 161 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 33.176 33.176 45.317 0.000 ## Residuals 161 117.866 0.732 ## Performance 162 151.043 0.932 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance Conscientiousness ## Performance 1.00 0.47 ## Conscientiousness 0.47 1.00 ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 163 rows of data, or do n_res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## Conscientiousness Performance fitted resid rstdnt dffits cooks ## 134 1.667 3.350 1.832 1.518 1.829 0.399 0.078 ## 161 4.667 1.820 3.726 -1.906 -2.283 -0.352 0.060 ## 130 4.833 2.200 3.831 -1.631 -1.951 -0.333 0.054 ## 116 4.333 1.880 3.516 -1.636 -1.943 -0.240 0.028 ## 64 5.000 5.000 3.936 1.064 1.267 0.238 0.028 ## 110 3.833 1.000 3.200 -2.200 -2.628 -0.231 0.026 ## 39 2.000 1.000 2.042 -1.042 -1.241 -0.227 0.026 ## 106 2.167 1.000 2.148 -1.148 -1.363 -0.226 0.025 ## 43 2.333 1.000 2.253 -1.253 -1.486 -0.222 0.024 ## 99 2.500 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 127 2.500 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 34 3.000 4.560 2.674 1.886 2.242 0.212 0.022 ## 143 4.167 5.000 3.410 1.590 1.884 0.207 0.021 ## 126 2.333 3.390 2.253 1.137 1.347 0.201 0.020 ## 55 3.000 4.450 2.674 1.776 2.107 0.199 0.019 ## 132 2.833 1.000 2.569 -1.569 -1.858 -0.197 0.019 ## 92 4.000 5.000 3.305 1.695 2.009 0.196 0.019 ## 95 4.333 4.800 3.516 1.284 1.519 0.187 0.017 ## 28 3.000 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## 142 3.000 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## Conscientiousness Performance pred s_pred pi.lwr pi.upr width ## 134 1.667 3.350 1.832 0.875 0.104 3.560 3.455 ## 144 1.667 1.880 1.832 0.875 0.104 3.560 3.455 ## ... ## 150 3.333 2.600 2.884 0.858 1.189 4.579 3.390 ## 27 3.500 2.500 2.989 0.858 1.295 4.684 3.390 ## 32 3.500 2.220 2.989 0.858 1.295 4.684 3.390 ## ... ## 36 5.000 3.420 3.936 0.870 2.218 5.655 3.436 ## 64 5.000 5.000 3.936 0.870 2.218 5.655 3.436 ## 107 5.000 3.220 3.936 0.870 2.218 5.655 3.436 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- By default, the output for the Regression function produces three plots that are useful for assessing statistical assumptions and for interpreting the results, as well as text output. Let’s begin by reviewing the first two plots depicting the residuals and the section of the text called Residuals and Influence. We previously saw using the ScatterPlot function from lessR that the association between the two variables appeared to be linear and showed evidence of a bivariate normal distribution. We did note, however, that there might be some bivariate outliers that could influence the estimated regression model. Let’s take a look at the second plot in your Plot window; you may need to hit the back arrow button to review the three plots. Fitted Values &amp; Residuals Plot: The second plot is scatterplot that shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimates – or in other words, how much our predicted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential bivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be only one case that is flagged as a potential bivariate outlier (case associated with row number 134). This case was flagged based on an outlier/influence statistic called Cook’s distance (D). Residuals &amp; Influence Output: Moving to the text output section called Residuals and Influence, we see a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: (a) studentized residual (rstdnt), (b) number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and (c) Cook’s distance (cooks). Corroborating what we saw in the plot, the case associated with row number 134 has the highest Cook’s distance value (.078) and stands somewhat apart from the next highest values (i.e., .060, .054). There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cook’s distance values exceed 1, where values in excess of 1 would indicate a problematic case. Regardless, as a sensitivity analysis, we would perhaps want to estimate our model once more after removing the case associated with row number 134 from our data frame. Overall, we may have found what seems to be one potentially influential bivariate outlier case, but we can feel reasonably good about satisfying the assumptions of average residuals being approximately zero and homoscedasticity of residuals. Next, let’s consider the only statistical assumption we haven’t yet tested: Residual errors are normally distributed for each level of the predictor variable. Distribution of Residuals Plot: Moving on to the first plot, which displays the distribution of the residuals, we can use the display to determine whether or not we have satisfied that final statistical assumption. We see a histogram and density distribution of our residuals with the shape of a normal distribution superimposed. As you can see, our residuals show a mostly normal distribution, which is great and in line with the assumption. Because we have satisfied this and other assumptions to a reasonable extent, we should feel confident that we can interpret our statistical tests, confidence intervals, and prediction intervals in a meaningful way, beginning with the Background section of the output. 38.2.4.2 Interpret Simple Linear Regression Model Results Background: The Background section of the text output section shows which data frame object was used to estimate the model, the name of the response (outcome, criterion) variable, and the name of the predictor variable. In addition, it shows the number of cases in the data frame as well as how many were used in the estimation of the model; by default, the Regression function uses listwise deletion when one or more of the variables in the model has a missing value, which means that a case with any missing value on one of the focal variables is removed as part of the analysis. Here we can see that all 163 cases in the data frame were retained for the analysis, which means that neither Performance or Conscientiousness had missing values. Basic Analysis: The Basic Analysis section of the output first displays a table containing the estimated regression model (Estimated Model for [INSERT OUTCOME VARIABLE NAME]), including the regression coefficients (slopes, weights) and their standard errors, t-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (more on that later). The regression coefficient associated with the predictor variable (Conscientiousness) in relation to the outcome variable (Performance) is often of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .682, and its associated p-value is less than .001 (b = .631, p &lt; .001). [NOTE: Because the regression coefficient is unstandardized, its practical significance cannot be directly interpreted, and it is not a standardized effect size like a correlation coefficient.] Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. Further, the 95% confidence interval ranges from .446 to .817 (i.e., 95% CI[.446, .817]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (Conscientiousness), the outcome variable (Performance) increases by .631 (unstandardized) units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = .780 + (.631 * Conscientiousness_{observed})\\) Note that the equation above is simply the equation for a line: \\(Y_{predicted} = .780 + (.631 * X_{observed})\\). If we plug in, for instance, the value 3 as an observed value of Conscientiousness, then we get a predicted criterion score of 2.673, as shown below: \\(2.673 = .780 + (.631 * 3)\\) Thus, based on our estimate model (i.e., equation), we are able to predict values of Performance. We’ll bring this process to life in the following section. The Model Fit section of the output appears below the table containing the regression coefficient estimates. In this section, you will find the (unadjusted) R-squared (R2) estimate, which is an indicator of the model’s fit to the data as well as the extent to which the predictor variable explains variance (i.e., variability) in the outcome variable. The R-squared (R2) value of .220 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 22.0% of the variance in Performance is explained by Conscientiousness. You can also think of the R2 values as effect sizes (i.e., indicators of practical significance) at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the R2 effect size, which is another way of saying “determining the level of practical significance”: R2 Description .01 Small .09 Medium .25 Large The raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .215 (or 21.5%). If space permits, it’s a good idea to report both values, but given how close the unadjusted and adjusted R2 estimates tend to be, reporting and interpreting just the unadjusted R2 is usually fine. The Model Fit section also contains the sum of squares, F-value, and p-value includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS).. In this table, we are mostly interested in the F-value and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variable(s). In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.317 and that its associated p-value is less than .001 – the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). Again, you can think of the R2 value as an indicator of effect size at the model level, and in the table above, you will find the conventional thresholds for qualitatively describing a small, medium, or large R2 value. Relations Among the Variables: The section called Relations Among the Variables displays the zero-order (Pearson product-moment) correlation between the predictor variable and outcome variable. This correlation can be used to gauge the effect size of the predictor variable in relation to the outcome variable, as a correlation coefficient is a standardized metric that can be compared across samples - unlike an unstandardized regression coefficient. In the specific case of a simple linear regression model, the square-root of the unadjusted R2 value for the model and the correlation coefficient (r) between the predictor variable and the outcome variable will be equivalent. For example: \\(R_{unadjusted}^{2} = .220\\) \\(r = \\sqrt{R_{unadjusted}^{2}}\\) \\(r = \\sqrt{.220}\\) \\(r = .469\\) As you can see in the Correlation Matrix section, the correlation coefficient between Conscientiousness and Performance is indeed approximately .47 (after rounding to two digits after the decimal). By conventional standards, in terms of practical significance, this association can be described qualitatively as medium or medium-large. Below, I provide a table containing conventional rules of thumb for interpreting a correlation coefficient as an effect size (r); you can find a more detailed overview of interpreting correlation coefficients in the chapter on estimating criterion-related validity using correlation. r Description .10 Small .30 Medium .50 Large Prediction Error: In the output section called Prediction Error, information about the forecasting error and prediction intervals. This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. What lessR is doing in the Prediction Error section is taking the model you estimated using the focal dataset, which we could call our training dataset, and then it takes the values for our predictor and outcome variables from our sample and plugs them into the model and accounts for forecasting error for each set of values. Specifically, the standard error of forecast (sf) for each set of values is base on a combination of the standard deviation of the residuals for the entire model (modeling error) and the sampling error for the value on the regression line. Consequently, each set of values is assigned a lower and upper bound of a prediction interval for the outcome variable. The width of the prediction interval is specific to the values used to test the model, so the widths vary across the values. In fact, the further one gets from the mean of the outcome variable (in either direction), the wider the prediction intervals become. The 95% prediction intervals, along with the 95% confidence intervals and regression line of best fit, are plotted on the third and final plot of the function output. As you can, see the prediction intervals are the outermost lines as they include both sampling error and the modeling error, whereas the confidence intervals are the inner lines, as they reflect just the sampling error. Sample Technical Write-Up of Results: A concurrent validation study was conducted to assess the criterion-related validity of a conscientiousness personality test in relation to the criterion of job performance. A simple linear regression model was estimated based on a sample of 163 job incumbents’ scores on the conscientiousness test and job performance. A statistically significant association was found between scores on the conscientiousness test and job performance, such that for every one point increase in conscientiousness, job performance tended to increase by .631 points (b = .631, p &lt; .001, 95% CI[.446, .817]. The unadjusted R2 value of .220 indicated that the model fit the data reasonably well, as evidenced by what can be described as a medium or medium-large effect by conventional standards. Specifically, the adjusted R2 value of .220 indicates that scores on the conscientiousness test explained 22.0% of the variability in job performance scores. In corroboration, the zero-order correlation coefficient for conscientiousness test in relation to the criterion of job performance was medium or medium-large in magnitude (r = .47), where the correlation coefficient can be referred to as a validity coefficient in this context. 38.2.4.3 Optional: Obtaining Standardized Coefficients As an optional detour, if you would like to estimate the same simple linear regression model but view the standardized regression coefficients, simply add the argument new_scale=\"z\" to your previous Regression function; that argument rescales your outcome and predictor variables to z-scores prior to estimating the model, which in effect produces standardized coefficients. # Estimate multiple linear regression model with standardized coefficients Regression(Performance ~ Conscientiousness, data=df, new_scale=&quot;z&quot;) ## ## Rescaled Data, First Six Rows ## Performance Conscientiousness ## 134 3.35 -2.523 ## 144 1.88 -2.523 ## 11 2.80 -2.058 ## 39 1.00 -2.058 ## 49 2.18 -1.826 ## 106 1.00 -1.826 ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ Conscientiousness, data=df, new_scale=&quot;z&quot;, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable: Conscientiousness ## ## Number of cases (rows) of data: 163 ## Number of cases retained for analysis: 163 ## ## Data are Standardized ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 2.974 0.067 44.377 0.000 2.842 3.106 ## Conscientiousness 0.453 0.067 6.732 0.000 0.320 0.585 ## ## Standard deviation of Performance: 0.966 ## ## Standard deviation of residuals: 0.856 for 161 degrees of freedom ## 95% range of residual variation: 3.379 = 2 * (1.975 * 0.856) ## ## R-squared: 0.220 Adjusted R-squared: 0.215 PRESS R-squared: 0.199 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 45.325 df: 1 and 161 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 33.181 33.181 45.325 0.000 ## Residuals 161 117.862 0.732 ## Performance 162 151.043 0.932 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance Conscientiousness ## Performance 1.00 0.47 ## Conscientiousness 0.47 1.00 ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 163 rows of data, or do n_res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## Conscientiousness Performance fitted resid rstdnt dffits cooks ## 134 -2.523 3.350 1.832 1.518 1.829 0.399 0.078 ## 161 1.662 1.820 3.726 -1.906 -2.283 -0.352 0.060 ## 130 1.894 2.200 3.831 -1.631 -1.951 -0.333 0.054 ## 116 1.197 1.880 3.516 -1.636 -1.943 -0.240 0.028 ## 64 2.127 5.000 3.937 1.063 1.267 0.238 0.028 ## 110 0.499 1.000 3.200 -2.200 -2.628 -0.231 0.026 ## 39 -2.058 1.000 2.043 -1.043 -1.241 -0.227 0.026 ## 106 -1.826 1.000 2.148 -1.148 -1.363 -0.226 0.025 ## 43 -1.593 1.000 2.253 -1.253 -1.486 -0.222 0.024 ## 99 -1.361 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 127 -1.361 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 34 -0.663 4.560 2.674 1.886 2.242 0.212 0.022 ## 143 0.964 5.000 3.410 1.590 1.884 0.206 0.021 ## 126 -1.593 3.390 2.253 1.137 1.347 0.201 0.020 ## 55 -0.663 4.450 2.674 1.776 2.107 0.199 0.019 ## 132 -0.896 1.000 2.568 -1.568 -1.857 -0.197 0.019 ## 92 0.732 5.000 3.305 1.695 2.009 0.196 0.019 ## 95 1.197 4.800 3.516 1.284 1.519 0.187 0.017 ## 28 -0.663 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## 142 -0.663 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## Conscientiousness Performance pred s_pred pi.lwr pi.upr width ## 134 -2.523 3.350 1.832 0.875 0.104 3.560 3.455 ## 144 -2.523 1.880 1.832 0.875 0.104 3.560 3.455 ## ... ## 150 -0.198 2.600 2.884 0.858 1.189 4.579 3.390 ## 27 0.034 2.500 2.989 0.858 1.295 4.684 3.390 ## 32 0.034 2.220 2.989 0.858 1.295 4.684 3.390 ## ... ## 36 2.127 3.420 3.937 0.870 2.218 5.655 3.436 ## 64 2.127 5.000 3.937 0.870 2.218 5.655 3.436 ## 107 2.127 3.220 3.937 0.870 2.218 5.655 3.436 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- In the Estimated Model section of the output, note that the intercept value is zeroed out due to the standardization, and the regression coefficient associated with Conscientiousness is now in standardized units (\\(\\beta\\) = .469, p &lt; .001). Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: For every one standardized unit increase in Conscientiousness, Performance increases by .469 standardized units. In the special case of a simple linear regression model, the standardized coefficient associated with the single predictor variable will be equal to the correlation coefficient between the predictor and outcome variables. 38.2.5 Predict Criterion Scores Using the simple linear regression model we estimated above, we can write code that generates predicted criterion scores based on new data we feed into the model. In fact, we can even add these predictions to the new data frame to which the new data belong. As we did above, let’s practice using the conscientiousness selection tool (Conscientiousness) and the job performance criterion (Performance). We’ll begin by specifying the same simple linear regression model as above – except this time we will assign the estimated model to an object that we can subsequently reference. To do so, we’ll use the &lt;- operator. In this example, I’m naming the model reg_mod. # Assign simple linear regression model to object reg_mod &lt;- Regression(Performance ~ Conscientiousness, data=df) Now that we have assigned the simple linear regression model to the object reg_mod, we can reference specific elements from that model, such as the regression coefficients. We’re going to do so to build our regression model equation as an R formula. Let’s begin by referencing the model intercept value and assigning it to an object called b0. I’m calling this object b0 because conventionally the “b” or “B” notation is used to signify regression coefficients and because the “0” (zero) is often used as a subscript to signify our intercept value in a regression model equation. To “pull” or reference the intercept value from our regression model, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients. If we were to run this by itself, it would print all of the regression coefficient values estimated for the model; we, however, want just the intercept value. Given that, immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact text: \"(Intercept)\". This will call up the intercept coefficient for any model estimated using the Regression function from lessR. # Reference estimated model intercept and assign to object b0 &lt;- reg_mod$coefficients[&quot;(Intercept)&quot;] Next, let’s “pull” the regression coefficient associated with the slope, which in a simple linear regression is the coefficient associated with the sole predictor variable (Conscientiousness). This time, let’s assign this regression coefficient to an object called b1, which adheres to conventional notation. To reference this coefficient, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients – just as we did above with the model intercept. Immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact name of the predictor variable associated with the coefficient you wish to reference: \"Conscientiousness\". # Reference estimated model slope coefficient and assign to object b1 &lt;- reg_mod$coefficients[&quot;Conscientiousness&quot;] Now that we’ve pulled the coefficients and assigned them to objects (b0, b1) that we can reference in our regression model equation, let’s read in new data so that we can reference new applicants’ scores on the same conscientiousness selection tool. As we did above in the Initial Steps section, let’s read in the data file called “ApplicantData.csv” and assign it to a new object. Here I call this new object new_df. # Read in data new_df &lt;- read.csv(&quot;ApplicantData.csv&quot;) Let’s take a peek at first six rows of the new_df data frame object. # Print first six rows head(new_df) ## ApplicantID Conscientiousness Interview ## 1 AA1 4.3 4.1 ## 2 AA2 3.1 4.9 ## 3 AA3 1.1 2.5 ## 4 AA4 1.0 3.3 ## 5 AA5 2.4 4.4 ## 6 AA6 2.9 3.3 Note that the data frame object includes an ApplicantID unique identifier variable as well as variables associated with two selection tools: Conscientiousness and Interview. Note that we don’t have any criterion (Performance) scores here because these folks are still applicants. Our goal then is to predict their future criterion scores using our regression model from above. To make criterion-score predictions based on the new conscientiousness test data for applicants, we’ll need to create a regression equation based on estimates from the simple linear regression model. Fortunately, we’ve already created objects corresponding to our model intercept (b0) and slope coefficient (b1). Using these values, we’ll specify the equation for a line (e.g., Y = b0 + b1 * X). In our model equation, we’ll start by specifying a new variable containing what will be our vector of criterion-score predictions based on the Conscientiousness selection tool variable. I’m going to call this new variable Perf_Predict_Consc, as hopefully that name signals that the variable contains performance predictions based on the conscientiousness test scores. Using the $ operator, we can attach this new variable to the new data frame object we read in called new_df. To the right of the &lt;- operator, we’ll specify the rest of the equation; in this context, you can think of the &lt;- operator as being the equal sign in our equation; in fact, you could replace the &lt;- operator with = if you wanted to. First, type in the object associated with model intercept (b0) followed by the + operator. Second, type in the object associated with the slope coefficient associated with the predictor variable (b1), and follow that with the multiplication operator (*) so that we can multiple the slope coefficient by values of the predictor variable (Conscientiousness). Finally, after the * operator, type in the name of the predictor variable from the new data frame object: new_df$Conscientiousness. # Assemble regression equation and assign to new variable in second data frame new_df$Perf_Predict_Consc &lt;- b0 + b1 * new_df$Conscientiousness Let’s take a look at the new_df data frame object to verify that the new Perf_Predict_Consc variable (containing the predicted criterion scores) was added successfully. # View data frame object View(new_df) In the viewer tab, we can use the up and down arrows to sort by variable scores. Alternatively and optionally, we can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 4 AA4 1.0 3.3 1.411143 ## 3 AA3 1.1 2.5 1.474276 ## 5 AA5 2.4 4.4 2.295012 ## 6 AA6 2.9 3.3 2.610680 ## 2 AA2 3.1 4.9 2.736947 ## 9 AA9 3.9 4.2 3.242015 ## 1 AA1 4.3 4.1 3.494549 ## 7 AA7 4.4 5.0 3.557682 ## 10 AA10 4.7 4.6 3.747083 ## 8 AA8 4.9 4.5 3.873350 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. # Sort data frame by new variable in descending order new_df[order(-new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 8 AA8 4.9 4.5 3.873350 ## 10 AA10 4.7 4.6 3.747083 ## 7 AA7 4.4 5.0 3.557682 ## 1 AA1 4.3 4.1 3.494549 ## 9 AA9 3.9 4.2 3.242015 ## 2 AA2 3.1 4.9 2.736947 ## 6 AA6 2.9 3.3 2.610680 ## 5 AA5 2.4 4.4 2.295012 ## 3 AA3 1.1 2.5 1.474276 ## 4 AA4 1.0 3.3 1.411143 As you can see, applicant AA8 has the highest predicted criterion score, where the criterion in this context is future job performance. Importantly, this process of applying a model to make predictions using new data can be expanded to multiple linear regression models. We would simply expand our regression equation to include the coefficients associated with the other predictor variables in such a model. 38.2.6 Summary In this chapter, we learned how to estimate a simple linear regression model using the Regression function from the lessR package, and how to apply the model to subsequent data to predict criterion scores based on employee selection tool scores. 38.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a simple linear regression model and the predict function from base R to predict criterion scores. Because these functions come from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 38.3.1 Functions &amp; Packages Introduced Function Package lm base R print base R plot base R cooks.distance base R sort base R head base R summary base R confint base R cor base R scale base R predict base R apa.reg.table apaTables 38.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## Rows: 163 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): EmployeeID, Conscientiousness, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [163 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 38.3.3 lm Function from Base R As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variable (Conscientiousness) to the right of the ~ operator. We are telling the function to “regress Performance on Conscientiousness.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify simple linear regression model and assign to object reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) Note: You won’t see any output in your console by specifying the regression model above. If you print the model (reg.mod1) to your console using the print function from base R, you only get the regression coefficients (but no statistical tests or model fit information). Later on, we’ll apply a different function to obtain the full model results. # Print very brief model information (NOT NECESSARY) print(reg.mod1) ## ## Call: ## lm(formula = Performance ~ Conscientiousness, data = df) ## ## Coefficients: ## (Intercept) Conscientiousness ## 0.7798 0.6313 Statistical Assumptions: Recall, towards the beginning of this tutorial, we used the ScatterPlot function from lessR to generate a bivariate scatterplot between the Conscientiousness and Performance variables to assess the assumption of linearity, look for bivariate outliers, and assess whether the assumption of bivariate normality has been satisfied. Now we will generate additional plots and other output to inform our conclusions whether we have satisfied certain statistical assumptions. We will begin by generating a scatterplot displaying the association between the fitted (predicted) values and residuals. To do so, we will use the plot function from base R. As the first argument, enter the name of the regression model object you created above (reg.mod1). As the second argument, type the numeral 1, which will request the first of four possible diagnostic plots, of which we will review three. # Diagnostics plot: fitted values &amp; residuals plot(reg.mod1, 1) We previously saw using the ScatterPlot function from lessR that the association between the two variables appeared to be linear and evidenced a bivariate normal distribution. We did, however, note that there might be some bivariate outliers that could influence the estimated regression model. The current plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable, but the language “fitted” is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential bivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be three cases that are flagged as a potential bivariate outlier (i.e., row numbers 34, 110, and 161). As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the plot script from above, but this time, enter the numeral 2 (instead of 1) to request the second diagnostic plot. # Diagnostics plot: normal Q-Q plot plot(reg.mod1, 2) Normally distributed residuals will fall along the dotted diagonal line. As you can see, the residuals fall, for the most part, on or near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers 34, 110, and 161. As the last diagnostic plot, let’s look at Cook’s distance (D) across cases. Once again, adapt the plot script from above, but this time, enter the numeral 4 to request the fourth diagnostic plot. We’re skipping the third plot. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.mod1, 4) Here we see flagged cases associated with row numbers 130, 134, and 161 based on Cook’s distance. We can also grab the cases with the highest Cook’s distances. Let’s create an object called cooksD that we will assign a vector of Cook’s distance values to using the cooks.distance function from base R. Just enter the name of the regression model object (reg.mod1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.mod1) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 134 161 130 116 64 110 39 106 43 99 127 34 ## 0.07848460 0.06028472 0.05444552 0.02820208 0.02819805 0.02575822 0.02559242 0.02537055 0.02443426 0.02292889 0.02292889 0.02189203 ## 143 126 55 132 92 95 28 142 ## 0.02098965 0.02012473 0.01941305 0.01905427 0.01887586 0.01739145 0.01723973 0.01723973 Here we get the numeric values associated with Cook’s distances, which corroborates what we saw in the plot of Cook’s distances above. Again, the case associated with row 134 is by far the largest, and the next largest values are clustered closer together. There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cook’s distance values exceed 1. Across the three diagnostic plots and the vector of Cook’s distances, 134 appears to be consistently the most extreme outlier, but it still may not be extreme enough to consider removing. One could perform a sensitivity analysis by estimating the model with and without case 134 to see if the pattern of results remains the same. Obtaining the Model Results: Type the name of the summary function from base R and include whatever you named your regression model (reg.mod1) as the sole parenthetical argument; we specified the regression model object called reg.mod1) earlier in the tutorial. The summary function simply returns a summary of your estimated regression model results. # Get summary of simple linear regression model results summary(reg.mod1) ## ## Call: ## lm(formula = Performance ~ Conscientiousness, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.19993 -0.49687 -0.00948 0.66074 1.88619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.77981 0.33276 2.343 0.0203 * ## Conscientiousness 0.63134 0.09378 6.732 0.00000000028 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8556 on 161 degrees of freedom ## Multiple R-squared: 0.2196, Adjusted R-squared: 0.2148 ## F-statistic: 45.32 on 1 and 161 DF, p-value: 0.0000000002801 The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called Coefficients contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, t-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficient for the predictor variable (Conscientiousness) in relation to the outcome variable (Performance) is often of substantive interest. Here, we see that the unstandardized regression coefficient for Conscientiousness is .631, and its associated p-value is less than .001 (b = .631, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (Conscientiousness), the outcome variable (Performance) increases by .631 (unstandardized) units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = .780 + (.631 * Conscientiousness_{observed})\\) Note that the equation above is simply the equation for a line: \\(Y_{predicted} = .780 + (.631 * X_{observed})\\). If we plug in, for instance, the value 3 as an observed value of Conscientiousness, then we get a predicted criterion score of 2.673, as shown below: \\(2.673 = .780 + (.631 * 3)\\) Thus, we are able to predict future values of Performance based on our estimated regression model. Below the table containing the regression coefficient estimates, the (unadjusted, multiple) R-squared (R2) and adjusted R2 values appear, which are indicators of the model’s fit to the data as well as the extent to which the predictor variable explains variability in the outcome variable. First, the R-squared (R2) value of .220 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 22.0% of the variance in Performance is explained by Conscientiousness. This raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .215 (or 21.5%). Below the R-squared (R2) and adjusted R2 values, the F-value and p-value is a statitical test of overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. The F-value and its associated p-value indicate whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variable(s). In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.32 and its associated p-value is very small (less than .001), the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). You can think of the R2 values as indicators of effect size at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the effect size, which is another way of saying “determining the level of practical signficance”: R2 Description .01 Small .09 Medium .25 Large To estimate the 95% confidence intervals, we can apply the confint function from base R, and enter the name of the regression model (reg.mod1) as the sole argument. # Estimate 95% confidence intervals confint(reg.mod1) ## 2.5 % 97.5 % ## (Intercept) 0.1226692 1.4369466 ## Conscientiousness 0.4461298 0.8165403 The 95% confidence interval ranges from .446 to .817 (i.e., 95% CI[.446, .817]), which indicates that the true population parameter for the association likely falls somewhere between those two values. As a direct indicator of the effect size (practical significance), we can calculate the zero-order (Pearson product-moment) correlation between Conscientiousness and Performance. Keep the method=\"pearson\" as is to request a Pearson product-moment correlation, and be sure to include the name of the data frame object (Selection) followed by the $ operator in front of each variable. # Estimate zero-order correlation (effect size) cor(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4686663 The correlation coefficient is medium-large in magnitude (r = .47), which indicates a moderate-strong, positive, linear association between Conscientiousness and Performance. r Description .10 Small .30 Medium .50 Large Sample Technical Write-Up of Results: A concurrent validation study was conducted to assess the criterion-related validity of a conscientiousness personality test in relation to the criterion of job performance. A simple linear regression model was estimated based on a sample of 163 job incumbents’ scores on the conscientiousness test and job performance. A statistically significant association was found between scores on the conscientiousness test and job performance, such that for every one point increase in conscientiousness, job performance tended to increase by .631 points (b = .631, p &lt; .001, 95% CI[.446, .817]. The unadjusted R2 value of .220 indicated that the model fit the data reasonably well, as evidenced by what can be described as a medium or medium-large effect by conventional standards. Specifically, the adjusted R2 value of .220 indicates that scores on the conscientiousness test explained 22.0% of the variability in job performance scores. In corroboration, the zero-order correlation coefficient for conscientiousness test in relation to the criterion of job performance was medium or medium-large in magnitude (r = .47), where the correlation coefficient can be referred to as a validity coefficient in this context. Obtaining Standardized Coefficients: If you would like to estimate the same simple linear regression model but view the standardized regression coefficients, just do small tweaks to the lm code/script that we specified above. First, let’s name the model something different given that it will include standardized coefficients; here, I decided to name the model st_reg.mod1. Next, within the lm function, apply the scale function from base R to the predictor and outcome variables as shown; doing so will standardize our variables and center them around zero. # Estimate simple linear regression model with standardized coefficient estimates st_reg.mod1 &lt;- lm(scale(Performance) ~ scale(Conscientiousness), data=df) summary(st_reg.mod1) ## ## Call: ## lm(formula = scale(Performance) ~ scale(Conscientiousness), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27833 -0.51458 -0.00982 0.68429 1.95341 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000000000000000661 0.06940583793586863059 0.000 1 ## scale(Conscientiousness) 0.46866634251609101680 0.06961972392188997549 6.732 0.00000000028 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8861 on 161 degrees of freedom ## Multiple R-squared: 0.2196, Adjusted R-squared: 0.2148 ## F-statistic: 45.32 on 1 and 161 DF, p-value: 0.0000000002801 In the Coefficients portion of the output, note that the intercept value is virtually zeroed out due to the standardization, and the regression coefficient associated with Conscientiousness is now in standardized units (\\(\\beta\\) = .47, p &lt; .001). The reason that the intercept value is not exactly zero is rounding error. Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: For every one standardized unit increase in Conscientiousness, Performance increases by .47 standardized units. Note that in the case of a simple linear regression model, the standardized regression coefficient is equal to the corresponding correlation coefficient between the same two variables. 38.3.4 predict Function from Base R The predict function from base R pairs nicely with the lm function, and it makes it easier to take an estimated regression model – and associated regression equation – and apply that model to new (“fresh”) predictor variable data. Prior to applying the predict function, we must specify a regression model. Using the lm (linear model) function from base R, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variable (Conscientiousness) to the right of the ~ operator. We are telling the function to “regress Performance on Conscientiousness.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify simple linear regression model and assign to object reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) Next, as we did above in the Initial Steps section, let’s read in the data file called “ApplicantData.csv” and assign it to a new object. Here I call this new object new_df. Let’s pretend this new data frame object contains data from future applicants who have completed the Conscientiousness personality measure as part of the organization’s selection process. We will ultimately “plug” these applicants’ Conscientiousness scores into our regression model equation to predict the applicants’ scores on the criterion variable called Performance. # Read in data new_df &lt;- read.csv(&quot;ApplicantData.csv&quot;) Next, as the first argument in the predict function, type object= followed by the estimated model object that we named reg.mod1. As the second argument, type newdata= followed by the name of the data frame object that contains new data on the predictor variable. It’s important to make sure that the predictor variable’s name (Conscientiousness) in our new data is exactly the same as the predictor variable’s name (Conscientiousness) in our original data that we used to estimate the model. If that were not the case, we would want to rename the predictor variable in the new data frame object to match the corresponding name in the original data frame object. # Predict scores on the criterion (outcome) variable predict(object=reg.mod1, newdata=new_df) ## 1 2 3 4 5 6 7 8 9 10 ## 3.494549 2.736947 1.474276 1.411143 2.295012 2.610680 3.557682 3.873350 3.242015 3.747083 In your console, you should see a vector of scores – these are the predicted criterion scores. In many cases, we might want to append this vector as a variable to our new data frame object. To do so, we just need to apply the &lt;- operator and, to the left of it, specify the name of the new data frame object (new_df) followed by the $ operator and a name for the new variable that will contain the predicted criterion scores (Perf_Predict_Consc). # Predict scores on the criterion (outcome) variable # and append as new variable in data frame new_df$Perf_Predict_Consc &lt;- predict(object=reg.mod1, newdata=new_df) We can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 4 AA4 1.0 3.3 1.411143 ## 3 AA3 1.1 2.5 1.474276 ## 5 AA5 2.4 4.4 2.295012 ## 6 AA6 2.9 3.3 2.610680 ## 2 AA2 3.1 4.9 2.736947 ## 9 AA9 3.9 4.2 3.242015 ## 1 AA1 4.3 4.1 3.494549 ## 7 AA7 4.4 5.0 3.557682 ## 10 AA10 4.7 4.6 3.747083 ## 8 AA8 4.9 4.5 3.873350 38.3.5 APA-Style Results Table If you want to present the results of your simple linear regression to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package (Stanley 2021). Using the lm function from base R, as we did above, let’s begin by estimating a simple linear regression model and naming the model object (reg.mod1). # Estimate simple linear regression model reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) If you haven’t already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (reg.mod1) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(reg.mod1) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 0.78* [0.12, 1.44] ## Conscientiousness 0.63** [0.45, 0.82] 0.47 [0.33, 0.61] .22 [.12, .32] .47** ## R2 = .220** ## 95% CI[.12,.32] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file “APA Simple Linear Regression Table.doc”. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(reg.mod1, filename=&quot;APA Simple Linear Regression Table.doc&quot;) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 0.78* [0.12, 1.44] ## Conscientiousness 0.63** [0.45, 0.82] 0.47 [0.33, 0.61] .22 [.12, .32] .47** ## R2 = .220** ## 95% CI[.12,.32] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table simple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. References "],["incrementalvalidity.html", "Chapter 39 Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression 39.1 Conceptual Overview 39.2 Tutorial 39.3 Chapter Supplement", " Chapter 39 Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression In this chapter, we will learn how to estimate a multiple linear regression model in order to investigate whether a selection tool shows evidence of incremental validity with respect to other selection tools in the model. We’ll begin with a conceptual overview of multiple linear regression, and we’ll conclude with a tutorial. 39.1 Conceptual Overview Multiple linear regression models allow us to apply statistical control and to examine whether a predictor variable shows incremental validity relative to other predictor variables with respect to an outcome variable. In this section, I will begin by reviewing fundamental concepts related to multiple linear regression and incremental validity. 39.1.1 Review of Multiple Linear Regression Link to conceptual video: https://youtu.be/1-chi0VaQ7w Like simple linear regression, multiple linear regression can provide us with information about the strength and sign of a linear association between a predictor variable and an outcome variable; however, unlike a simple linear regression model, a multiple linear regression model allows us to assess the strength and sign of the associations between two or more predictor variables and a single outcome variable. In doing so, using multiple linear regression, we can infer the association between a predictor variable and and outcome variable while statistically controlling for the associations between other predictor variables and the same outcome variable. When we statistically control for the effects of other predictor variables in a model, we are able to evaluate whether evidence of incremental validity exists for each predictor variable. Incremental validity refers to instances in which a predictor variable explains significant amounts of variance in the outcome variable even when statistically controlling for the effects of other predictor variables in the model. When a predictor variable (or a block of predictor variables) shows evidence of incremental validity, sometimes we use language like: “Over and beyond the variance explained by Predictors W and Z, Predictor X explained significant variance in Outcome Y.” In the context of employee selection, evaluating whether a selection tool shows evidence of incremental validity can be of value, as evidence of incremental validity can signify that a selection tool explains unique variance in the criterion (i.e., outcome) when accounting for the effects of other selection tools. In other words, if a selection tool shows evidence of incremental validity, we can be more confident that it contributes uniquely to the prediction of criterion scores and thus is not overly redundant with the other selection tools. In this chapter, we will learn how to estimate an ordinary least squares (OLS) multiple linear regression model, where OLS refers to the process of estimating the unknown components (i.e., parameters) of the regression model by attempting to minimize the sum of squared residuals. The sum of the squared residuals are the result of a process in which the differences between the observed outcome variable values and the predicted outcome variable values are calculated, squared, and then summed in order to identify a model with the least amount of error (residuals). This is where the concept of “best fit” comes into play, as the model is estimated to most closely fit the available data such that the error (residuals) between the predicted and observed outcome variable values are minimized. In other words, the goal is to find the linear model that best fits the data at hand; with that said, a specific type of regression called polynomial regression can be used to test nonlinear associations. Let’s consider a scenario in which we estimate a multiple linear regression model with two predictor variables and a single outcome variable. The equation for such a model with unstandardized regression coefficients (\\(b\\)) would be as follows: \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables \\(X_1\\) and \\(X_2\\) are equal to zero, \\(b_{1}\\) and \\(b_{2}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_1\\) and \\(X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. Importantly, the unstandardized regression coefficients \\(b_{1}\\) and \\(b_{2}\\) represent the “raw” slopes (i.e., weights, coefficients) – or rather, how many unstandardized units of \\(\\hat{Y}\\) increase or decrease as a result of a single unit increase in either \\(X_1\\) or \\(X_2\\) when controlling for the effect of the other predictor variable. That is, unstandardized regression coefficients reflect the nature of the association between two variables when the variables retain their original scaling. Often this is why we choose to use the unstandardized regression coefficients when making predictions about \\(\\hat{Y}\\), as the predicted scores will be have the same scaling as the outcome variable in its original form. If we wanted to estimate a multiple linear regression model with three predictor variables, our equation would change as follows. \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + b_{3}X_3 + e\\) We can also obtain standardized regression coefficients. To do so, the predictor variables (e.g., \\(X_1\\), \\(X_2\\)) and outcome variable (\\(Y\\)) scores must be standardized. To standardize variables, we convert the predictor and outcome variables to z-scores, such that their respective means are standardized to 0 and their variances and standard deviations are standardized to 1. When standardized, our multiple linear regression model equation will have a \\(\\hat{Y}\\)-intercept value equal to zero (and thus is not typically reported) and the standardized regression coefficient is commonly signified using the Greek letter \\(\\beta\\): \\(\\hat{Y} = \\beta_{1}X_1 + \\beta_{2}X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted standardized score on the outcome variable (\\(Y\\)), \\(\\beta_{1}\\) and \\(\\beta_{2}\\) represent the standardized coefficients (i.e., weights, slopes) of the association between the predictor variables \\(X_1\\) and \\(X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. A standardized regression coefficient (\\(\\beta\\)) allows us to also compare the relative magnitude of one \\(\\beta\\) to another \\(\\beta\\); with that being said, in the case of a multiple linear regression model, comparing \\(\\beta\\) coefficients is only appropriate when the predictor variables in the model share little to no intercorrelation (i.e., have low collinearity). Given that, I recommend that you proceed with caution should you choose to make such comparisons. In terms of interpretation, in a multiple linear regression model, the standardized regression coefficient (\\(\\beta_{1}\\)) indicates the standardized slope when controlling for the effects of other predictor variables in the model – or rather, how many standard units of \\(\\hat{Y}\\) increase or decrease as a result of a single standard unit increase in \\(X_1\\) when accounting for the effects of other predictor variables in the model. 39.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a multiple linear regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of multivariate outliers; The association between the predictor and outcome variables is linear; There is no (multi)collinearity between predictor variables; Average residual error value is zero for all levels of the predictor variables; Variances of residual errors are equal for all levels of the predictor variables, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for all levels of the predictor variables. The fourth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so let’s take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple linear regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights – and even the signs – of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 39.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of the other predictor variables in the model. In other words, if a regression coefficient’s p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent when controlling for the effects of other predictor variables in the model. In contrast, if the regression coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of other predictor variables in the model. Put differently, if a regression coefficient’s p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population – when controlling for the effects of other predictor variables in the model. I should note that it is entirely possible for a predictor variable and outcome variable to show a statistically significant association in a simple linear regression but for that same association to be not statistically significant when one or more predictor variables are added to the model. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 39.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a multiple linear regression model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. A standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) given that it is standardized. With that being said, I suggest doing so with caution as collinearity (i.e., correlation) between predictor variables in the model can bias our interpretation of \\(\\beta\\) as an effect size. Thus, if your goal is just to understand the bivariate association between a predictor variable and an outcome variable (without introducing statistical control), then I recommend to just estimate a correlation coefficient as an indicator of practical significance, which I discuss in the chapter on estimating criterion-related validity using correlations. In a multiple linear regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variables (i.e., R2). That is, in a multiple linear regression model, R2 represents the proportion of collective variance explained in the outcome variable by all of the predictor variables. Conceptually, we can think of the overlap between the variability in the predictor variables and and outcome variable as the variance explained (R2), and R2 is a way to evaluate how well a model fits the data (i.e., model fit). I’ve found that the R2 is often readily interpretable by non-analytics audiences. For example, an R2 of .25 in a multiple linear regression model can be interpreted as: the predictor variable scores explain 25% of the variability in scores on the outcome variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large As an effect size, R2 indicates the proportion of variance explained by the predictor variables in relation to the outcome variable – or in other words, the shared variance between the variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 39.1.1.4 Sample Write-Up A team of researchers is interested in whether a basketball player’s height and intelligence predict the number of points scored during a 10-game season. In this case, the predictor variables are the basketball players’ heights (in inches) and levels of intelligence, and the outcome variable is the number of points scored by the basketball players. Let’s imagine that the researchers collected data on these variables from a sample of 100 basketball players. Our hypothesis for such a situation might be: Basketball players’ heights and intelligence scores will both be positively related to the number of points players score in a 10-game season, such that taller and more intelligent players will tend to score more points in a season. We find that the unstandardized regression coefficients associated with player height (b = 2.31, p = .02) and player intelligence (b = .59, p = .01) in relation to points scored are statistically significant and positive, that the R2 value is .24 (p &lt; .01), and that tolerance estimates for both predictor variables are .91 and .91. We can summarize the findings as follows: Based on a sample of 100 basketball players, basketball player height was found to predict points scored in a 10-game season, after controlling for player intelligence, such that taller players tended to score more points (b = 2.31, p = .02). Specifically, for every 1-inch increase in height, players tended to score 2.31 additional points during the season, when controlling for player intelligence. In addition, basketball player intelligence was found to predict points scored, when controlling for player height, such that more intelligent players tended to score more points (b = .59, p = .01). Specifically, for every 1 additional point scored on the intelligence test, players tended to score .59 additional points during the season. Further, this reflects a large collective effect, as approximately 24% of the variability in points scored was explained by players’ heights and intelligence, collectively (R2 = .24, p &lt; .01). Finally, collinearity was not a concern as both predictor variables (i.e., height, intelligence) had tolerance estimates of .91, which are both close to 1.0 and well-above the .20 threshold (below which tolerance is a major concern). 39.2 Tutorial This chapter’s tutorial demonstrates how to estimate a multiple linear regression model and interpret incremental validity. We also learn how to present the results in writing. 39.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/AOeJ2byUJdw Additionally, in the following video tutorial, I go into greater depth on how to test the statistical assumptions of a multiple linear regression model – just as I do in the written tutorial below. Link to video tutorial: https://youtu.be/zyEZop-5K9Q 39.2.2 Functions &amp; Packages Introduced Function Package Regression lessR 39.2.3 Initial Steps If you haven’t already, save the file called “ConcurrentValidation.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “ConcurrentValidation.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## Rows: 300 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (4): SJT, EI, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [300 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 The data frame contains 5 variables and 300 cases (i.e., employees): EmployeeID, SJT, EI, Interview, and Performance. Let’s assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., SJT, EI, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The SJT variable contains the scores on a situational judgment test designed to “tap into” the psychological concepts of emotional intelligence and empathy; potential scores on this variable could range from 1 (low emotional intelligence &amp; empathy) to 10 (emotional intelligence &amp; empathy). The EI variable contains scores on an emotional intelligence assessment; potential scores on this variable could range from 1 (low emotional intelligence) to 10 (emotional intelligence). The Interview variable contains the scores for a structured interview designed to assess interviewees’ level of interpersonal skills; potential scores on this variable could range from 1 (poor interpersonal skills) to 15 (interpersonal skills). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 30 (exceeds performance standards). 39.2.4 Estimate Multiple Linear Regression Model Let’s assume that evidence of criterion-related validity was already found for the three selection tools (SJT, EI, Interview) using correlations; that is, the validity coefficient associated with each selection tool and the criterion (Performance) was statistically significant. If you want, you can run the correlations to verify this – or you can trust me on this. For a review of correlation in this context, check out the chapter on estimating criterion-related validity using a correlation. Given that evidence of criterion-related validity was already found for the three selection tools, our next step is to include all three selection tools as predictors in a multiple linear regression model. The criterion (Performance) variable will serve as our sole outcome variable in the model. In doing so, we can evaluate which selection tools show evidence of incremental validity. 39.2.4.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a multiple linear regression model, we need to first test the statistical assumptions. Fortunately, the Regression function from the lessR package (Gerbing, Business, and University 2021) automatically produces common tests of statistical assumptions. So to get started, let’s install and access the lessR package using the install.packages and library functions, respectively. In the chapter supplement, you can learn how to carry how the same tests using the lm function from base R. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the names of the predictor variables (SJT, EI, Interview) to the right of the ~ operator; note that we use the + to add additional predictor variables to the model. We are telling the function to “regress Performance on SJT, EI, and Interview.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate multiple linear regression model Regression(Performance ~ SJT + EI + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + EI + Interview, data=df, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: EI ## Predictor Variable 3: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.602 0.574 9.756 0.000 4.472 6.732 ## SJT 0.485 0.231 2.099 0.037 0.030 0.939 ## EI 0.091 0.237 0.385 0.700 -0.376 0.559 ## Interview 0.378 0.067 5.686 0.000 0.247 0.509 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.996 for 296 degrees of freedom ## 95% range of residual variation: 11.794 = 2 * (1.968 * 2.996) ## ## R-squared: 0.265 Adjusted R-squared: 0.257 PRESS R-squared: 0.233 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 35.507 df: 3 and 296 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.101 0.000 ## EI 1 36.702 36.702 4.088 0.044 ## Interview 1 290.301 290.301 32.331 0.000 ## ## Model 3 956.435 318.812 35.507 0.000 ## Residuals 296 2657.762 8.979 ## Performance 299 3614.197 12.088 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT EI Interview ## Performance 1.00 0.42 0.43 0.39 ## SJT 0.42 1.00 0.93 0.24 ## EI 0.43 0.93 1.00 0.32 ## Interview 0.39 0.24 0.32 1.00 ## ## Tolerance VIF ## SJT 0.127 7.887 ## EI 0.121 8.278 ## Interview 0.873 1.146 ## ## SJT EI Interview R2adj X&#39;s ## 1 0 1 0.259 2 ## 1 1 1 0.257 3 ## 0 1 1 0.249 2 ## 1 1 0 0.179 2 ## 0 1 0 0.178 1 ## 1 0 0 0.171 1 ## 0 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 300 rows of data, or do n_res_rows=&quot;all&quot;] ## --------------------------------------------------------------------------- ## SJT EI Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 8.000 1.000 24.000 11.074 12.926 4.516 0.765 0.137 ## 70 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 170 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 270 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 233 10.000 9.000 6.000 27.000 13.542 13.458 4.693 0.654 0.100 ## 1 9.000 8.000 2.000 22.000 11.452 10.548 3.634 0.556 0.074 ## 101 9.000 8.000 2.000 22.000 11.452 10.548 3.634 0.556 0.074 ## 69 2.000 1.000 1.000 18.000 7.041 10.959 3.772 0.505 0.061 ## 169 2.000 1.000 1.000 18.000 7.041 10.959 3.772 0.505 0.061 ## 92 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 192 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 292 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 20 2.000 1.000 8.000 18.000 9.689 8.311 2.837 0.422 0.044 ## 120 2.000 1.000 8.000 18.000 9.689 8.311 2.837 0.422 0.044 ## 283 9.000 7.000 5.000 22.000 12.496 9.504 3.249 0.418 0.042 ## 269 2.000 1.000 5.000 18.000 8.554 9.446 3.225 0.385 0.036 ## 97 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 197 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 297 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 82 2.000 1.000 4.000 14.000 8.176 5.824 1.966 0.230 0.013 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT EI Interview Performance pred s_pred pi.lwr pi.upr width ## 69 2.000 1.000 1.000 18.000 7.041 3.023 1.092 12.990 11.898 ## 169 2.000 1.000 1.000 18.000 7.041 3.023 1.092 12.990 11.898 ## 41 1.000 2.000 3.000 8.000 7.404 3.052 1.398 13.410 12.012 ## ... ## 210 8.000 7.000 2.000 10.000 10.876 3.021 4.930 16.822 11.892 ## 4 6.000 5.000 5.000 11.000 10.859 3.002 4.951 16.766 11.815 ## 47 6.000 5.000 5.000 11.000 10.859 3.002 4.951 16.766 11.815 ## ... ## 225 7.000 6.000 9.000 16.000 12.948 3.011 7.022 18.874 11.852 ## 28 4.000 7.000 14.000 12.000 13.477 3.149 7.278 19.675 12.396 ## 128 4.000 7.000 14.000 12.000 13.477 3.149 7.278 19.675 12.396 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- By default, the output for the Regression function produces three plots that are useful for assessing statistical assumptions and for interpreting the results, as well as text output. Let’s begin by reviewing the first two plots depicting the residuals and the section of the text called Residuals and Influence. Let’s take a look at the second plot in your Plot window; you may need to hit the back arrow button to review the three plots. Fitted Values &amp; Residuals Plot: The second plot is scatterplot that shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimates – or in other words, how much our predicted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for across levels of the predictor variables (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for all levels of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to deviate slightly from zero, which may indicate some degree of heteroscedasticity, and one case that is flagged as a potential multivariate outlier (case associated with row number 201). This case was flagged based on an outlier/influence statistic called Cook’s distance (D). So we may have a slight violation of the homscedasticity of residuals assumption. Residuals &amp; Influence Output: Moving to the text output section called Residuals and Influence, we see a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: (a) studentized residual (rstdnt), (b) number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and (c) Cook’s distance (cooks). Corroborating what we saw in the plot, the case associated with row number 201 has the highest Cook’s distance value (.137), followed closely by the cases associated with row numbers 70, 170, and 270. There are several different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cook’s distance values exceed 1, where values in excess of 1 would indicate a problematic case. Regardless, as a sensitivity analysis, we would perhaps want to estimate our model once more after removing the cases associated with row number 201, 70, 170, and 270 from our data frame. Overall, we may have found what seems to be four potentially influential multivariate outlier cases, and we should be a bit concerned about satisfying the homoscedasticity of residuals, as the average residuals deviated slightly from zero. It’s possible that removing the four aforementioned cases might also help to address the slight violation of the homoscedasticity of residuals assumption. Next, let’s consider the following statistical assumption: Residual errors are normally distributed for each level of the predictor variable. Distribution of Residuals Plot: Moving on to the first plot, which displays the distribution of the residuals, we can use the display to determine whether or not we have satisfied the statistical assumption that the residuals are normally distributed. We see a histogram and density distribution of our residuals with the shape of a normal distribution superimposed. As you can see, our residuals show a mostly normal distribution, which is great and in line with the assumption. Collinearity: Moving onto the third diagnostic plot containing the scatterplot matrix, let’s see if we might have any collinearity concerns. Specifically, let’s focus in on the correlations between the predictor variables (i.e., selection tool variables), as these can be indicators of whether collinearity might be of concern in this model. The correlation between SJT and Interview is .24, and the correlation between Interview and EI is .32 – both of which are acceptable from a collinearity perspective. In contrast, the correlation between SJT and EI is HUGE at .93. Generally speaking, a correlation that is .85 or higher can indicate that two variables are practically indistinguishable from one another, and here the correlation between SJT and EI is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error. To further explore potential collinearity in a different way, let’s check out the table in our text output called Collinearity. The corresponding table shows two indices of collinearity: tolerance and valence inflation factor (VIF). Because the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), let’s focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned about collinearity when the tolerance statistics falls below .20, with a tolerance of .00 being the worst and thus the strongest level of collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. In the table, we can see that SJT has a tolerance statistic of .127, which is lower than .20 and indicates that SJT overlaps considerably with the other predictor variables (EI, Interview) in the model in terms of shared variance. Similarly, EI has a troublesome tolerance statistic of .121, which suggests a similar problem. The tolerance for Interview is .873, which suggests low levels of collinearity (which is a good thing). Because SJT and EI both have low tolerance statistics, in this situation, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the correlation matrix (see above). As such, in the current model, we definitely have a collinearity problem. Summary of Statistical Assumptions Tests: Given the apparent slight violation of the assumption of homoscedasticity, the four potential multivariate outlier cases (i.e., 201, 70, 170, and 270, which correspond to EmployeeID values of EE223, EE92, EE292, EE192, respectively), and the very concerning levels of collinearity involving our SJT and EI variables, we should avoid interpreting the model as currently specified. In fact, we should at the very least remove either the SJT or EI predictor variable from the model to address the most concerning statistical assumption violation. So how do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource-intensive tool. In addition, we might consider applicants’ reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, let’s assume that we have a compelling reason for dropping EI and retaining SJT. Re-Specify Model to Address Collinearity Issue: Given our decision to drop EI as a selection tool due to a severe violation of the collinearity assumption (see above), let’s re-specify and re-estimate our multiple linear regression model with just SJT and Interview as predictor variables. # Re-estimate multiple linear regression model # (with EI variable dropped due to collinearity) Regression(Performance ~ SJT + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + Interview, data=df, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.527 0.539 10.249 0.000 4.465 6.588 ## SJT 0.567 0.085 6.714 0.000 0.401 0.734 ## Interview 0.385 0.064 6.031 0.000 0.260 0.511 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.992 for 297 degrees of freedom ## 95% range of residual variation: 11.777 = 2 * (1.968 * 2.992) ## ## R-squared: 0.264 Adjusted R-squared: 0.259 PRESS R-squared: 0.236 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 53.339 df: 2 and 297 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.303 0.000 ## Interview 1 325.670 325.670 36.375 0.000 ## ## Model 2 955.102 477.551 53.339 0.000 ## Residuals 297 2659.095 8.953 ## Performance 299 3614.197 12.088 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT Interview ## Performance 1.00 0.42 0.39 ## SJT 0.42 1.00 0.24 ## Interview 0.39 0.24 1.00 ## ## Tolerance VIF ## SJT 0.944 1.060 ## Interview 0.944 1.060 ## ## SJT Interview R2adj X&#39;s ## 1 1 0.259 2 ## 1 0 0.171 1 ## 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 300 rows of data, or do n_res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## SJT Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 1.000 24.000 11.019 12.981 4.537 0.736 0.169 ## 70 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 170 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 270 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 233 10.000 6.000 27.000 13.513 13.487 4.709 0.645 0.129 ## 1 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 101 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 69 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 169 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 92 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 192 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 292 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 20 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 120 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 283 9.000 5.000 22.000 12.560 9.440 3.226 0.372 0.045 ## 269 2.000 5.000 18.000 8.588 9.412 3.216 0.371 0.045 ## 97 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 197 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 297 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 82 2.000 4.000 14.000 8.203 5.797 1.959 0.224 0.017 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT Interview Performance pred s_pred pi.lwr pi.upr width ## 69 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 169 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 41 1.000 3.000 8.000 7.250 3.021 1.305 13.195 11.891 ## ... ## 210 8.000 2.000 10.000 10.837 3.015 4.903 16.771 11.868 ## 4 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## 47 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## ... ## 222 9.000 9.000 11.000 14.102 3.015 8.168 20.035 11.867 ## 92 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## 192 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- In a real-world situation, we would once again work through the statistical assumption tests that we did above; however, for sake of brevity, we will assume that the statistical assumptions have been reasonably satisfied in this re-specified model in which only the SJT and Interview variables are included as predictors. Thus, we will feel confident that we can interpret our statistical tests, confidence intervals, and prediction intervals in a meaningful way, beginning with the Background section of the output. 39.2.4.2 Interpret Multiple Linear Regression Model Results Background: The Background section of the text output section shows which data frame object was used to estimate the model, the name of the response (outcome, criterion) variable, and the name of the predictor variable. In addition, it shows the number of cases in the data frame as well as how many were used in the estimation of the model; by default, the Regression function uses listwise deletion when one or more of the variables in the model has a missing value, which means that a case with any missing value on one of the focal variables is removed as part of the analysis. Here we can see that all 300 cases in the data frame were retained for the analysis, which means that none of the variables in the model had any missing values. Basic Analysis: The Basic Analysis section of the output first displays a table containing the estimated regression model (Estimated Model for [INSERT OUTCOME VARIABLE NAME]), including the regression coefficients (slopes, weights) and their standard errors, t-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (more on that later). The regression coefficients associated with the predictor variables (SJT and Interview) in relation to the outcome variable (Performance) are, however, of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .567, and its associated p-value is less than .001 (b = .567, p &lt; .001). [NOTE: Because the regression coefficient is unstandardized, its practical significance cannot be directly interpreted, and it is not a standardized effect size like a correlation coefficient.] Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero when statistically controlling for the effects of Interview. Further, the 95% confidence interval ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one point increase in situational judgment test (SJT) scores, job performance (Performance) scores increase by .567 points when controlling for the effect of structured interview (Interview) scores. Next, the unstandardized regression coefficient for Interview is .385, and its associated p-value is less than .001 (b = .385, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero when statistically controlling for the effects of SJT. Further, the 95% confidence interval ranges from .260 to .511 (i.e., 95% CI[.260, .511]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one point increase in structured interview (Interview) scores, job performance (Performance) scores increase by .385 points when controlling for the effect of situational judgment test (SJT) scores. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Let’s assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance – something we’ll cover in greater depth in the next chapter. The Model Fit section of the output appears below the table containing the regression coefficient estimates. In this section, you will find the (unadjusted) R-squared (R2) estimate, which is an indicator of the model’s fit to the data as well as the extent to which the predictor variable explains variance (i.e., variability) in the outcome variable. The R-squared (R2) value of .264 indicates the extent to which the predictor variables collectively explain variance in the outcome variable in this sample – or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in Performance is explained by SJT and Interview collectively. You can also think of the R2 values as effect sizes (i.e., indicators of practical significance) at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the R2 effect size, which is another way of saying “determining the level of practical significance”: R2 Description .01 Small .09 Medium .25 Large The raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .259 (or 25.9%). If space permits, it’s a good idea to report both values, but given how close the unadjusted and adjusted R2 estimates tend to be, reporting and interpreting just the unadjusted R2 is usually fine – and is typically customary. The Model Fit section also contains the sum of squares, F-value, and p-value includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS).. In this table, we are mostly interested in the F-value and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variables. In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.317 and that its associated p-value is less than .001 – the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. Again, you can think of the R2 value as an indicator of effect size at the model level, and in the table above, you will find the conventional thresholds for qualitatively describing a small, medium, or large R2 value. Relations Among the Variables: The section called Relations Among the Variables displays the zero-order (Pearson product-moment) correlation between the predictor variable and outcome variable. This correlation can be used to gauge the effect size of a predictor variable in relation to the outcome variable, as a correlation coefficient is a standardized metric that can be compared across samples - unlike an unstandardized regression coefficient; however, it’s important to note that such a correlation represents a bivariate (i.e., two-variable) association – and thus doesn’t involve statistical control like our multiple linear regression model. Collinearity: The Colinearity section displays the tolerance statistics. As you can see, both tolerance statistics are .944 and thus close to 1.00, which indicates very low levels of collinearity. We would be concerned if a tolerance statistic fell below .20, which is not the case for this model when applied to this sample of employees. Prediction Error: In the output section called Prediction Error, information about the forecasting error and prediction intervals. This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. What lessR is doing in the Prediction Error section is taking the model you estimated using the focal dataset, which we could call our training dataset, and then it takes the values for our predictor and outcome variables from our sample and plugs them into the model and accounts for forecasting error for each set of values. Specifically, the standard error of forecast (sf) for each set of values is base on a combination of the standard deviation of the residuals for the entire model (modeling error) and the sampling error for the value on the regression line. Consequently, each set of values is assigned a lower and upper bound of a prediction interval for the outcome variable. The width of the prediction interval is specific to the values used to test the model, so the widths vary across the values. In fact, the further one gets from the mean of the outcome variable (in either direction), the wider the prediction intervals become. The 95% prediction intervals, along with the 95% confidence intervals and regression line of best fit, are plotted on the third and final plot of the function output. As you can, see the prediction intervals are the outermost lines as they include both sampling error and the modeling error, whereas the confidence intervals are the inner lines, as they reflect just the sampling error. Sample Technical Write-Up of Results: A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (SJT), an emotional intelligence assessment (EI), and a structured interview (Interview) in relation to job performance (Performance). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (b = .567, p &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (b = .385, p &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (R2 = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion. 39.2.4.3 Optional: Obtaining Standardized Coefficients As an optional detour, if you would like to estimate the same simple linear regression model but view the standardized regression coefficients, simply add the argument new_scale=\"z\" to your previous Regression function; that argument rescales your outcome and predictor variables to z-scores prior to estimating the model, which in effect produces standardized coefficients. # Estimate multiple linear regression model with standardized coefficients Regression(Performance ~ SJT + EI, data=df, new_scale=&quot;z&quot;) ## ## Rescaled Data, First Six Rows ## Performance SJT EI ## 1 22 1.632 1.609 ## 2 11 1.158 0.657 ## 3 5 0.683 0.657 ## 4 11 0.209 0.181 ## 5 12 0.209 0.181 ## 6 12 -0.266 0.181 ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + EI, data=df, new_scale=&quot;z&quot;, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: EI ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## Data are Standardized ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 10.737 0.182 59.025 0.000 10.379 11.095 ## SJT 0.549 0.504 1.088 0.277 -0.443 1.541 ## EI 0.968 0.504 1.920 0.056 -0.024 1.960 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 3.151 for 297 degrees of freedom ## 95% range of residual variation: 12.401 = 2 * (1.968 * 3.151) ## ## R-squared: 0.184 Adjusted R-squared: 0.179 PRESS R-squared: 0.165 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 33.550 df: 2 and 297 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.491 629.491 63.416 0.000 ## EI 1 36.576 36.576 3.685 0.056 ## ## Model 2 666.066 333.033 33.550 0.000 ## Residuals 297 2948.130 9.926 ## Performance 299 3614.197 12.088 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT EI ## Performance 1.00 0.42 0.43 ## SJT 0.42 1.00 0.93 ## EI 0.43 0.93 1.00 ## ## Tolerance VIF ## SJT 0.131 7.655 ## EI 0.131 7.655 ## ## SJT EI R2adj X&#39;s ## 1 1 0.179 2 ## 0 1 0.178 1 ## 1 0 0.171 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 300 rows of data, or do n_res_rows=&quot;all&quot;] ## ----------------------------------------------------------------- ## SJT EI Performance fitted resid rstdnt dffits cooks ## 233 2.107 2.086 27.000 13.912 13.088 4.316 0.594 0.111 ## 70 2.107 1.609 5.000 13.450 -8.450 -2.741 -0.406 0.054 ## 170 2.107 1.609 5.000 13.450 -8.450 -2.741 -0.406 0.054 ## 270 2.107 1.609 5.000 13.450 -8.450 -2.741 -0.406 0.054 ## 201 1.632 1.609 24.000 13.189 10.811 3.518 0.395 0.050 ## 283 1.632 1.133 22.000 12.729 9.271 3.007 0.385 0.048 ## 20 -1.689 -1.724 18.000 8.142 9.858 3.199 0.373 0.045 ## 69 -1.689 -1.724 18.000 8.142 9.858 3.199 0.373 0.045 ## 120 -1.689 -1.724 18.000 8.142 9.858 3.199 0.373 0.045 ## 169 -1.689 -1.724 18.000 8.142 9.858 3.199 0.373 0.045 ## 269 -1.689 -1.724 18.000 8.142 9.858 3.199 0.373 0.045 ## 1 1.632 1.609 22.000 13.189 8.811 2.848 0.320 0.033 ## 101 1.632 1.609 22.000 13.189 8.811 2.848 0.320 0.033 ## 31 -2.164 -0.771 6.000 8.803 -2.803 -0.917 -0.229 0.018 ## 131 -2.164 -0.771 6.000 8.803 -2.803 -0.917 -0.229 0.018 ## 231 -2.164 -0.771 6.000 8.803 -2.803 -0.917 -0.229 0.018 ## 82 -1.689 -1.724 14.000 8.142 5.858 1.880 0.219 0.016 ## 97 -1.689 -1.724 14.000 8.142 5.858 1.880 0.219 0.016 ## 182 -1.689 -1.724 14.000 8.142 5.858 1.880 0.219 0.016 ## 197 -1.689 -1.724 14.000 8.142 5.858 1.880 0.219 0.016 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT EI Performance pred s_pred pi.lwr pi.upr width ## 20 -1.689 -1.724 18.000 8.142 3.172 1.900 14.383 12.484 ## 40 -1.689 -1.724 5.000 8.142 3.172 1.900 14.383 12.484 ## 55 -1.689 -1.724 11.000 8.142 3.172 1.900 14.383 12.484 ## ... ## 211 0.683 -0.295 10.000 10.826 3.193 4.542 17.110 12.568 ## 4 0.209 0.181 11.000 11.027 3.156 4.815 17.238 12.422 ## 5 0.209 0.181 12.000 11.027 3.156 4.815 17.238 12.422 ## ... ## 294 0.209 0.181 10.000 11.027 3.156 4.815 17.238 12.422 ## 28 -0.740 1.133 12.000 11.427 3.290 4.953 17.901 12.948 ## 128 -0.740 1.133 12.000 11.427 3.290 4.953 17.901 12.948 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- In the Estimated Model section of the output, note that the intercept value is zeroed out due to the standardization, and the regression coefficients associated with SJT and Interview are now in standardized units (\\(\\beta\\) = .344, p &lt; .001 and \\(\\beta\\) = .309, p &lt; .001, respectively). Note that the p-values are the same as the unstandardized regression coefficients we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When statistically controlling for the effect of Interview, for every one standardized unit increase in SJT, Performance increases by .344 standardized units, and when statistically controlling for the effect of SJT, for every one standardized unit increase in Interview, Performance increases by .309 standardized units. 39.2.5 Summary In this chapter, we learned how to estimate a multiple linear regression model using the Regression function from the lessR package in order to estimate whether evidence of incremental validity exists. 39.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a multiple linear regression model. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 39.3.1 Functions &amp; Packages Introduced Function Package lm base R print base R vif car plot base R cooks.distance base R sort base R head base R summary base R confint base R cor base R scale base R apa.reg.table apaTables 39.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## Rows: 300 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (4): SJT, EI, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [300 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 39.3.3 lm Function from Base R In the following section, we will learn how to apply the lm function from base R to estimate a multiple linear regression model. Please note that we are doing some of the operations in a different order than what we did with the Regression function from lessR. Why? Well, the Regression function from lessR generates a number of diagnostics automatically (by default), and thus we took advantage of that in the previous section. With the lm function from base R, we have to piece together the diagnostics the old-fashioned way, which also happens to mean that we have more control over the order in which we do things. As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variables (SJT, EI, Interview) to the right of the ~ operator. We are telling the function to “regress Performance on SJT, EI, and Interview.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify multiple linear regression model reg.mod1 &lt;- lm(Performance ~ SJT + EI + Interview, data=df) Note: You won’t see any output in your console by specifying the regression model above. If you print the model (reg.mod1) to your console using the print function from base R, you only get the regression coefficients (but no statistical tests or model fit information). Later on, we’ll apply a different function to obtain the full model results. # Print very brief model information (NOT NECESSARY) print(reg.mod1) ## ## Call: ## lm(formula = Performance ~ SJT + EI + Interview, data = df) ## ## Coefficients: ## (Intercept) SJT EI Interview ## 5.60170 0.48470 0.09147 0.37827 Now that we have specified our model, let’s see if we might have any worrisome collinearity among our predictors, which if you recall, a key statistical assumption is that the model is free of collinearity. Identifying problematic collinearity typically means we need to re-specify our model by perhaps dropping a predictor variable or two. Thus, let’s start with testing the assumption related to collinearity. Creating a correlation matrix is one great way to identify possible collinearity issues, so let’s start with a correlation matrix of the variables in our data frame. Let’s use the cor function from base R. Before doing, so let’s create a temporary data frame object (temp) in which we drop the EmployeeID variable from the original df data frame object; we need to do this because the cor function only accepts numeric variables, and the EmployeeID variable is non-numeric. , and enter the name of our data frame as the sole argument. To learn more about how to remove variables, feel free to check out the chapter on filtering # Create temporary data frame object # with EmployeeID variable removed temp &lt;- subset(df, select=-EmployeeID) # Create basic correlation matrix cor(temp) ## SJT EI Interview Performance ## SJT 1.0000000 0.9324020 0.2373394 0.4173192 ## EI 0.9324020 1.0000000 0.3175592 0.4255306 ## Interview 0.2373394 0.3175592 1.0000000 0.3906502 ## Performance 0.4173192 0.4255306 0.3906502 1.0000000 High correlations between pairs of predictor variables are indicative of high collinearity. The correlation between SJT and Interview is approximately .24, and the correlation between EI and Interview is .32, both of which are in the acceptable range. The correlation between SJT and EI is HUGE at .93. Generally speaking, a correlation that is .85 or higher can indicate that two predictors are practically distinguishable from one another, and here the correlation between SJT and EmotionalIntelligence is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error. To further explore this collinearity issue, let’s estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you haven’t already, install the car package (Fox and Weisberg 2019) which contains the vif function we will use. # Install car package if you haven&#39;t already install.packages(&quot;car&quot;) Now, access the car package using the library function. # Access car package library(car) # Compute VIF statistic vif(reg.mod1) ## SJT EI Interview ## 7.887216 8.277683 1.145830 Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide 1 by the VIF. # Compute tolerance statistic 1 / vif(reg.mod1) ## SJT EI Interview ## 0.1267875 0.1208068 0.8727299 In the table, we can see that SJT has a tolerance statistic of .127, which is lower than .20 and indicates that SJT overlaps considerable with the other predictor variables (EI, Interview) in terms of shared variance. Similarly, the EI has a tolerance statistic of .121, which suggests a similar problem. Because SJT and EI both have very low tolerance statistics that fall below .20, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the scatterplot matrix plot. As such, in the current model, we definitely have a collinearity problem involving SJT and EI, which means interpreting the rest of the output would not be appropriate. When we face severe collinearity between scores on two selection tools, as we do in this scenario, we need to make a thoughtful decision. Given that the correlation between SJT and EI is extremely high at .93, these variables are from a statistical perspective essentially redundant. How do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource intensive tool. In addition, we might consider applicants’ reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, let’s assume that we have a compelling reason for dropping EI and retaining SJT. Given our decision to drop EI as a selection tool due to a violation of the collinearity assumption, let’s re-specify and re-estimate our multiple linear regression model with just SJT and Interview as predictor variables. This time, let’s name our model object reg.mod2. # Re-specify multiple linear regression model # with just SJT &amp; Interview as predictor variables reg.mod2 &lt;- lm(Performance ~ SJT + Interview, data=df) Let’s quickly run the VIF and tolerance statistics to see if collinearity is an issue with the two predictors in this new model. # Compute VIF statistic vif(reg.mod2) ## SJT Interview ## 1.059692 1.059692 # Compute tolerance statistic 1 / vif(reg.mod2) ## SJT Interview ## 0.94367 0.94367 Both tolerance statistics are identical (because we only have two predictors in the model), and both values are .943, which is nearly 1.00. Accordingly, we don’t have any concerns with collinearity in this new model. We’re ready to move on to evaluating other important statistical assumptions. Statistical Assumptions: Let’s look at some diagnostics to determine whether we have reason to believe we have met the other statistical assumptions described at the beginning of this tutorial. We will generate plots and other output to inform our conclusions whether we have satisfied certain statistical assumptions. We will begin by generating a scatterplot displaying the association between the fitted (predicted) values and residuals. To do so, we will use the plot function from base R. As the first argument, enter the name of the regression model object you created above (reg.mod2). As the second argument, type the numeral 1, which will request the first of four possible diagnostic plots, of which we will review three. # Diagnostics plot: fitted values &amp; residuals plot(reg.mod2, 1) The resulting plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable, but the language “fitted” is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be three cases that are flagged as a potential multivariate outlier (i.e., row numbers 69, 201, and 233). As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the plot script from above, but this time, enter the numeral 2 (instead of 1) to request the second diagnostic plot. # Diagnostics plot: normal Q-Q plot plot(reg.mod2, 2) Normally distributed residuals will fall along the dotted diagonal line. As you can see, many of the residuals fall on or near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers 69, 201, and 233. We also see some deviations from the dotted line at the low and high ends of the theoretical quantiles, which shows some departure from normality in the residuals. As the last diagnostic plot, let’s look at Cook’s distance (D) across cases. Once again, adapt the plot script from above, but this time, enter the numeral 4 to request the fourth diagnostic plot. We’re skipping the third plot. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.mod2, 4) Here we see case associated with row number 201 again, but we also see cases associate with row numbers 70, 170, and 201 based on Cook’s distance. We can also grab the cases with the highest Cook’s distances. Let’s create an object called cooksD that we will assign a vector of Cook’s distance values to using the cooks.distance function from base R. Just enter the name of the regression model object (reg.mod1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.mod2) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 201 70 170 270 233 101 1 69 169 92 192 292 ## 0.16917874 0.14329599 0.14329599 0.14329599 0.12946844 0.09214623 0.09214623 0.08140835 0.08140835 0.06114519 0.06114519 0.06114519 ## 20 120 283 269 97 197 297 82 ## 0.05121764 0.05121764 0.04481510 0.04456115 0.01981777 0.01981777 0.01981777 0.01656404 Here we get the numeric values associated with Cook’s distances, which corroborates what we saw in the plot of Cook’s distances above. Again, the case associated with row 201 is by far the largest, and the next largest values are clustered closer together, which include the cases associated with row numbers 70, 170, and 270. There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). Across the three diagnostic plots and the vector of Cook’s distances, 201 appears to be consistently the most concerning case, and cases associated with row numbers 69, 70, 170, 270, and 233 may also be problematic. For the most part, the distribution of the residuals look mostly normal. As such, as a sensitivity analysis and for the sake of demonstration, we will estimate our model once more after removing the cases associated with row numbers 201, 69, 70, 170, 270, and 233 from our data frame. If you’ve completed the entire tutorial thus far, you may have noted that some of the plots we are using with the lm function are different than those that come with the Regression function from lessR (see above), and further, you may have noticed that we have flagged more potential problematic cases in using these plots than the ones for the Regression function. This goes to show, again, how thoughtful we must be with the tools that we use, and again, I recommend erring on the side of caution when it comes to removing cases for subsequent analyses. And, remember, no model will ever be perfect. For now, let’s go ahead and interpret the results of the multiple linear regression model. Obtaining the Model Results: Type the name of the summary function from base R and include whatever you named your regression model (reg.mod1) as the sole parenthetical argument; we specified the regression model object called reg.mod2) earlier in the tutorial. The summary function simply returns a summary of your estimated regression model results. # Get summary of multiple linear regression model results summary(reg.mod2) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8249 -1.5198 -0.0401 1.0396 13.4868 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.52654 0.53925 10.249 &lt; 0.0000000000000002 *** ## SJT 0.56748 0.08453 6.714 0.0000000000964 *** ## Interview 0.38530 0.06388 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.992 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called Coefficients contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, t-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficients for the predictor variables in relation to the outcome variable are often of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .567, and its associated p-value is less than .001 (b = .567, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: Controlling for the effects of Interview, for every one point increase in SJT, Performance increases by .567 points Let’s move on to the regression coefficient for Interview, which is statistically significant and positive too (b = .385, p &lt; .001). Because these two regression coefficients are unstandardized, it would not be appropriate to compare their magnitudes. We can interpret the significant regression coefficient as follows: Controlling for the effects of SJT, for every one point increase in Interview, Performance increases by .385 points. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) If we plug in, for example, the value 6 as an observed value of SJT and the value 7 as an observed value of Interview, then we get 11.624, as shown below: \\(11.624 = 5.527 + (.567 * 6) + (.385 * 7)\\) Thus, we are able to predict future values of Performance based on our estimated regression model. Below the table containing the regression coefficient estimates, the (unadjusted multiple) R-squared (R2) and adjusted R2 values appear, which are indicators of the model’s fit to the data as well as the extent to which the predictor variables collectively explain variability in the outcome variable. First, the (multiple) R-squared (R2) value of .264 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in Performance is explained by SJT. This raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .259 (or 25.9%). Typically, it’s a good idea to report both values. The table containing the sum of squares, F-values, and p-values includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. In this table, we are mostly interested in the F-value associated with the overall model and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variables. In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 53.733 and its associated p-value is less than .001, the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. You can think of the R2 values as indicators of effect size at the model level. I provide some rules of thumb for qualitatively interpreting the magnitude of the effect size, which is another way of saying “determining the level of practical significance” (see table below). As you can see, our statistically significant unadjusted and adjusted R2 value can both be described as large by conventional standards. Thus, we seem to have a good model here. R2 Description .01 Small .09 Medium .25 Large To estimate the 95% confidence intervals, we can apply the confint function from base R, and enter the name of the regression model (reg.mod1) as the sole argument. # Estimate 95% confidence intervals confint(reg.mod2) ## 2.5 % 97.5 % ## (Intercept) 4.4652963 6.5877787 ## SJT 0.4011348 0.7338281 ## Interview 0.2595753 0.5110242 The 95% confidence interval for the SJT regression coefficient ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for association likely falls somewhere between those two values. The 95% confidence interval for the Interview regression coefficient ranges from .260 to .511. (i.e., 95% CI[.260, .511.]). As a direct indicator of the effect size (practical significance), we can calculate the zero-order (Pearson product-moment) correlations between SJT and Performance and between Interview and Performance. Keep the method=\"pearson\" as is to request a Pearson product-moment correlation, and be sure to include the name of the data frame object (df) followed by the $ operator in front of each variable. # Estimate zero-order correlations (effect sizes) cor(df$SJT, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4173192 cor(df$Interview, df$Performance, method=&quot;pearson&quot;) ## [1] 0.3906502 The correlation coefficients between SJT and Performance and between Interview and Performance are medium-large in magnitude (r = .42 and r = .39, respectively), which indicates a moderate-strong, positive, effects for each of these selection tools with respect to Performance. r Description .10 Small .30 Medium .50 Large Sample Technical Write-Up of Results: A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (SJT), an emotional intelligence assessment (EI), and a structured interview (Interview) in relation to job performance (Performance). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (b = .567, p &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (b = .385, p &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (R2 = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion. Obtaining Standardized Coefficients: If you would like to estimate the same multiple linear regression model but view the standardized regression coefficients, just do some small tweaks to the lm code/script that we specified above. First, let’s name the model something different given that it will include standardized coefficients; here, I decided to name the model st_reg.mod2. Next, within the lm function, apply the scale function from base R to the predictor and outcome variables as shown; doing so will standardize our variables and center them around zero. # Estimate multiple linear regression model # with standardized coefficient estimates st_reg.mod2 &lt;- lm(scale(Performance) ~ scale(SJT) + scale(Interview), data=df) summary(st_reg.mod2) ## ## Call: ## lm(formula = scale(Performance) ~ scale(SJT) + scale(Interview), ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1135 -0.4371 -0.0115 0.2990 3.8792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.000000000000000002772 0.049688712929418246689 0.000 1 ## scale(SJT) 0.343978856791381015778 0.051235702957663477319 6.714 0.0000000000964 *** ## scale(Interview) 0.309010486383703208979 0.051235702957663498136 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8606 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 In the Coefficients portion of the output, note that the intercept value is virtually zeroed out due to the standardization, and the regression coefficients associated with SJT and Interview are now in standardized units (\\(\\beta\\) = .340, p &lt; .001 and \\(\\beta\\) = .309, p &lt; .001, respectively). The reason that the intercept value is not exactly zero (in scientific notation) is rounding error. Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When controlling for Interview, for every one standardized unit increase in SJT, Performance increases by .344 standardized units, and when controlling for SJT, for every one standardized unit increase in Interview, Performance increases by .309 standardized units. Dealing with Multivariate Outliers: If you recall above, we found that the case associated with row numbers 201, 70, 270, and 170 in this sample may be candidates for removal. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison cases, unless the cases appear to have a dramatic influence on the estimated regression line. Some might argue that we should retain cases with row numbers 201, 70, 270, and 170, and others might argue that we should remove them; really, it comes down to your own logic, rationale, and justification, and I recommend pulling in other information you might have about these cases (even beyond data contained in this dataset) to inform your decision. If you were to decide to remove these cases, here’s what you would do. First, look at the data frame (using the View function) and determine which cases row number 201, 70, 270, and 170 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that row numbers 201, 70, 270, and 170 in our data frame are associated with EmployeeIDs EE223, EE92, EE292, and EE192, respectively. Next, with respect to estimating the regression model, I suggest naming the unstandardized regression model something different, and here I name it reg.mod3. The model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: subset=(!EmployeeID %in% c(\"EE223\", \"EE92\", \"EE292\", \"EE192\")); the subset argument subsets the data frame within the lm function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which EmployeeID is not equal to EE223, EE92, EE292, and EE192. Remember, the logical operator ! means “not” and the %in% operator means “within”. Revisit the chapter on filtering data if you want to see the full list of logical operators and more information on removing multiple cases. # Estimate multiple linear regression model # but remove row numbers 201, 70, 270, and 170 # which correspond to EmployeeID numbers of # EE223, EE92, EE292, and EE192 reg.mod3 &lt;- lm(Performance ~ SJT + Interview, data=df, subset=(!EmployeeID %in% c(&quot;EE223&quot;, &quot;EE92&quot;, &quot;EE292&quot;, &quot;EE192&quot;))) summary(reg.mod3) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df, subset = (!EmployeeID %in% ## c(&quot;EE223&quot;, &quot;EE92&quot;, &quot;EE292&quot;, &quot;EE192&quot;))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1832 -1.2913 -0.0509 0.8278 13.1206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70328 0.49639 9.475 &lt; 0.0000000000000002 *** ## SJT 0.61531 0.07723 7.968 0.000000000000036184 *** ## Interview 0.50383 0.05873 8.579 0.000000000000000567 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.677 on 293 degrees of freedom ## Multiple R-squared: 0.371, Adjusted R-squared: 0.3667 ## F-statistic: 86.41 on 2 and 293 DF, p-value: &lt; 0.00000000000000022 The pattern of results remains mostly the same; however, the model fit (as evidenced R2) improved after removing these outliers. Even with an improved model fit, We should remain hesitant to use the model based on the reduced sample (without these multivariate outliers), as this could potentially hurt our ability to generalize our findings to future samples of applicants. Then again, if the outlier cases really to do seem to be atypical for the population of interest, then you might make a case for removing them. Basically, it’s up to you to justify your decision and document it. If your data frame does not have a unique identifier variable, you could remove the outlier by referencing the outlier by row number. Instead of using the subset= argument, you would follow up your data=df argument with [-c(201, 70, 270, 170),] to indicate that you wish to remove row numbers 201, 70, 270, and 170. The minus sign (-) signals “not.” # Estimate multiple linear regression model # but remove row numbers 201, 70, 270, and 170 reg.mod3 &lt;- lm(Performance ~ SJT + Interview, data=df[-c(201, 70, 270, 170),]) summary(reg.mod3) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df[-c(201, ## 70, 270, 170), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1832 -1.2913 -0.0509 0.8278 13.1206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70328 0.49639 9.475 &lt; 0.0000000000000002 *** ## SJT 0.61531 0.07723 7.968 0.000000000000036184 *** ## Interview 0.50383 0.05873 8.579 0.000000000000000567 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.677 on 293 degrees of freedom ## Multiple R-squared: 0.371, Adjusted R-squared: 0.3667 ## F-statistic: 86.41 on 2 and 293 DF, p-value: &lt; 0.00000000000000022 39.3.4 APA-Style Results Table If you want to present the results of your multiple linear regression to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package (Stanley 2021). Using the lm function from base R, as we did above, let’s begin by estimating a multiple linear regression model and naming the model object (reg.mod1). # Estimate multiple linear regression model reg.mod1 &lt;- lm(Performance ~ SJT + Interview, data=df) If you haven’t already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (reg.mod1) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(reg.mod1) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 5.53** [4.47, 6.59] ## SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42** ## Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39** ## R2 = .264** ## 95% CI[.18,.34] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file “APA Multiple Linear Regression Table.doc”. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(reg.mod1, filename=&quot;APA Multiple Linear Regression Table.doc&quot;) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 5.53** [4.47, 6.59] ## SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42** ## Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39** ## R2 = .264** ## 95% CI[.18,.34] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table multiple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. References "],["compensatory.html", "Chapter 40 Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression 40.1 Conceptual Overview 40.2 Tutorial 40.3 Chapter Supplement", " Chapter 40 Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression In this chapter, we will learn how to apply a compensatory approach to making selection decisions by using multiple linear regression. We’ll begin with a conceptual overview of the compensatory approach, and we’ll conclude with a tutorial. 40.1 Conceptual Overview Just as we can use multiple linear regression to evaluate whether evidence of incremental validity exists for a selection tool, we can also use multiple linear regression to apply a compensatory approach to making selection decisions. In general, there are three overarching (and mutually non-exclusive) approaches to making selection decisions: (a) compensatory (e.g., multiple linear regression), (b) noncompensatory (e.g., multiple-cutoff), and (c) multiple-hurdle. These three approaches can be mixed and matched to fit the selection-decision needs of an organization; however, in this chapter, we’ll focus specifically on the compensatory approach, whereas in the next chapter, we will focus on the multiple-cutoff noncompensatory approach. 40.1.1 Review of Multiple Linear Regression This chapter is an extension of the chapter on estimating the incremental validity of a selection tool using multiple linear regression. Thus, in this chapter, I will forgo a review of ordinary least squares (OLS) multiple linear regression, as that is already covered in the previous chapter, along with a discussion of statistical assumptions, statistical significance, and a sample write-up. 40.1.2 Review of Compensatory Approach Link to conceptual video: https://youtu.be/kL0YxGswbIc In the context of employee selection, a compensatory approach refers to the process of using applicants’ scores on multiple selection tools to predict who will have the highest scores on the criterion. In doing so, applicants can compensate for lower scores on one selection tool with higher scores on one or more other selection tools – hence the name “compensatory approach.” One of the simpler ways to apply a compensatory approach is to apply equal weights to scores from the various selection tools when predicting future criterion scores; however, such an approach does not acknowledge that some selection tools may explain more variance in the criterion (e.g., job performance) than others. Using regression coefficients from a multiple linear regression model, we can weight scores on selection tools differentially when predicting criterion scores. Notably, these weights are empirically driven (e.g., data informed) as opposed to judgment informed (e.g., subject matter expert ratings). In essence, applying a compensatory approach using multiple linear regression allows us to apply our estimated regression model equation to predict criterion scores for applicants. Subsequently, we can sort the predicted criterion scores to determine which applicants are most promising and to inform selection decisions. To understand how a compensatory approach works, let’s consider a hypothetical scenario in which we administer three selection tools to applicants: cognitive ability test (CA), work simulation (WS), and structured interview (SI). Let’s assume that a predictive validation study was conducted previously to estimate the criterion-related validities of these three selection tools and whether they each showed evidence of incremental validity with respect to each other – and let’s imagine that our criterion of interest is job performance. Finally let’s assume that the following equation represents the general framework for the multiple linear regression model we estimated for investigating incremental validity: \\(\\hat{Y}_{Performance} = b_{Intercept} + b_{CA}X_{CA} + b_{WS}X_{WS} + b_{SI}X_{SI}\\) where \\(\\hat{Y}_{Performance}\\) represents the predicted score on the performance criterion variable, \\(b_{Intercept}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables are equal to zero, \\(b_{CA}\\), \\(b_{WS}\\), and \\(b_{SI}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_{CA}\\) (cognitive ability), \\(X_{WS}\\) (work simulation), and \\(X_{SI} (structured interview)\\), respectively, and the outcome variable \\(\\hat{Y}_{Performance}\\). Now, let’s plug some hypothetical intercept and regression coefficient estimates into the equation, and assume that these estimates were pulled from our unstandardized multiple linear regression output. \\(\\hat{Y}_{Performance} = 1.32 + .42_{CA}X_{CA} + .50_{WS}X_{WS} + .22_{SI}X_{SI}\\) Finally, let’s imagine that we administered the cognitive ability test, work simulation, and structured interview to two applicants with the goal of predicting their future job performance scores. The first applicant received the following scores on the selection tools, where all tests happened to be out of a possible 10 points: cognitive ability (CA) = 5, work simulation (WS) = 7, and structured interview (SI) = 9. If we plug those values into the regression equation, we get a predicted job performance score of 8.90. \\(8.90_{Performance} = 1.32 + .42_{CA}*5_{CA} + .50_{WS}*7_{WS} + .22_{SI}*9_{SI}\\) In contrast, the second applicant received the following scores on the selection tools: cognitive ability (CA) = 7, work simulation (WS) = 5, and structured interview (SI) = 9. Notice that the structured interview score was the same for both applicants but that the cognitive ability and work simulation scores were flipped-flopped. In this hypothetical example, if you were to add up each applicants total score across the three selection tools, the sum would be the same: 21 points. Because each test in this example has maximum potential score of 10, the total possible points is 30 points. Thus, if we were to simply add up both applicants’ scores, these applicants would be “tied.” Fortunately, just like we did for the first applicant, we can apply our regression coefficients (i.e., weights) to these three scores to generated a score that reflects a weighted compensatory approach. For the second applicant, the predicted performance score is 8.74. \\(8.74_{Performance} = 1.32 + .42_{CA}*7_{CA} + .50_{WS}*5_{WS} + .22_{SI}*9_{SI}\\) As you can see, the first applicant has a higher predicted performance score (8.90) than the second applicant (8.74), so based purely off of this information, we would rank the first applicant higher. Hopefully, you can see how the selection tool scores are differentially weighted based on their corresponding regression coefficient estimates. 40.1.2.1 Sample Write-Up Our team designed a new selection system for customer service representatives (CSRs) with the following selection tools administered within a single hurdle: personality inventory, work sample, and structured interview. The three selection tools were validated using the concurrent validation design, such that the selection tools were administered to a sample of 200 job incumbents (i.e., current CSRs), and at about the same time, criterion scores were collected for those same job incumbents in the form of job performance ratings. First, to assess the criterion-related validity of the three selection tools in relation to the criterion of job performance ratings, correlation coefficients were estimated, which in turn served as validity coefficients. The personality inventory, work sample, and structured interview each showed evidence of criterion-related validity, as evidenced by the statistically significant and large-sized correlation coefficients (r = .51, p &lt; .01; r = .60, p &lt; .01; r = .54, p &lt; .01). Second, because all three selection tools showed evidence of criterion-related based on their respective correlations with job performance ratings, we included all three selection tools as predictor variables in a multiple linear regression model with job performance rating as the outcome variable. We did so to assess the incremental validity of the three selection tools. On one hand, when statistically controlling for the effects of the work sample and structured interview, the personality inventory did not show evidence of incremental validity, as the corresponding regression coefficient was not statistically significantly different from zero (b = .88, p = .21). On the other hand, when statistically controlling for the effects of the personality inventory and structured interview, the work sample showed evidence of incremental validity, as the corresponding regression coefficient was statistically significantly different from zero (b = 2.10, p = .01); specifically, for every one unit increase in work sample scores, job performance ratings increased by 2.10 units. Similarly, when statistically controlling for the effects of the personality inventory and work sample, the structured interview showed evidence of incremental validity, as the corresponding regression coefficient was statistically significantly different from zero (b = 1.56, p = .02); specifically, for every one unit increase in structured interview scores, job performance ratings increased by 1.56 units. The model fit the data reasonably well; collectively, the three selection tools explained 31% of the variance in job performance (R2 = .31, p &lt; .01), which is a large effect at the model level. Collinearity was not a concern, as the tolerance estimates for the personality inventory, work sample, and structured interview were all above the .20 threshold and approached 1.00 (i.e., .72, .85, .81, respectively). Given that the personality inventory did not show evidence of incremental validity relative to the other two selection tools, we decided to drop the personality inventory from the single-hurdle selection system, leaving us with the work sample and structured interview selection tools. Third, we estimated a follow-up multiple linear regression model with just the two selection tools that showed evidence of incremental validity in our initial multiple linear regression model. When statistically controlling for the effects of the structured interview, the work sample showed evidence of incremental validity, as the corresponding regression coefficient was statistically significantly different from zero (b = 2.31, p &lt; .01); specifically, for every one unit increase in work sample scores, job performance ratings increased by 2.31 units. Similarly, when statistically controlling for the effects of the work sample, the structured interview showed evidence of incremental validity, as the corresponding regression coefficient was statistically significantly different from zero (b = 1.65, p = .01); specifically, for every one unit increase in structured interview scores, job performance ratings increased by 1.65 units. This follow-up model also fit the data reasonably well; collectively, the three selection tools explained 29% of the variance in job performance (R2 = .29, p &lt; .01), which is a large effect at the model level. Collinearity was not a concern for this model, as the tolerance estimates for the work sample and structured interview were both above the .20 threshold and approached 1.00 (i.e., .88 and .88, respectively). We concluded that both the work sample and structured interview showed evidence of being job-related and evidence of offering unique contributions to the prediction of the job performance criterion. Fourth, based on the model estimates from our follow-up multiple linear regression model, where just work sample and structured interview served as the predictor variables, we applied a compensatory approach to informing selection decisions. Specifically, we used the unstandardized regression coefficients from the follow-up model (see previous step) to weight applicants’ scores on the work sample and structured interview selection tools. In doing so, we were able to estimated applicants’ predicted scores on job performance should they be hired. To create the prediction equation, we extracted the model Y-intercept and the regression coefficients associated with the two selection tools; the resulting prediction equation was: \\(JobPerformance = 4.12 + 2.31(WorkSample) + 1.65(StructuredInterview)\\). Fifth, we administered the work sample and structured interview selection tools to 10 new applicants who applied to three CSR job vacancies. Using the above prediction equation above, we estimated each applicant’s predicted job performance score based on their work sample and structured interview scores. The top-three predicted job performance scores were associated with applicant IDs AA5, AA3, and AA6, and their predicted job performance scores were 14.11, 13.71, and 13.71, respectively. We decided to extend job offers to those three applicants. 40.2 Tutorial This chapter’s tutorial demonstrates how to apply a compensatory approach to making selection decisions by estimate a multiple linear regression model. 40.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/vtnYvcs1yUQ 40.2.2 Functions &amp; Packages Introduced Function Package Regression lessR order base R 40.2.3 Initial Steps If you haven’t already, save the file called “ConcurrentValidation.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “ConcurrentValidation.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## Rows: 300 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (4): SJT, EI, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [300 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 The data frame contains 5 variables and 300 cases (i.e., employees): EmployeeID, SJT, EI, Interview, and Performance. Let’s assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., SJT, EI, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The SJT variable contains the scores on a situational judgment test designed to “tap into” the psychological concepts of emotional intelligence and empathy; potential scores on this variable could range from 1 (low emotional intelligence &amp; empathy) to 10 (emotional intelligence &amp; empathy). The EI variable contains scores on an emotional intelligence assessment; potential scores on this variable could range from 1 (low emotional intelligence) to 10 (emotional intelligence). The Interview variable contains the scores for a structured interview designed to assess interviewees’ level of interpersonal skills; potential scores on this variable could range from 1 (poor interpersonal skills) to 15 (interpersonal skills). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 30 (exceeds performance standards). 40.2.4 Estimate Multiple Linear Regression Model We’ll begin by first assuming that that evidence of criterion-related validity was already found for the three selection tools (SJT, EI, Interview) using correlations. Second, we’ll assume that the EI selection tool was not found to have incremental validity when all three selection tools were included in a multiple linear regression model, thereby leaving us to estimate a second multiple linear regression model in which only SJT and Interview were included as predictors. For reviews of the aforementioned assumed prior steps, please refer to the previous chapters on estimating criterion-related validity using a correlation and estimating the incremental validity of a selection tool using multiple linear regression. In this chapter, we’ll begin by estimating the multiple linear regression model with SJT and Interview specified as predictor variables and Performance specified as the outcome variable. There are different functions we could use to estimate a multiple linear regression model, and in this tutorial we’ll focus on just one: the Regression function from lessR (Gerbing, Business, and University 2021). Note that this is the same function that we can use to estimate a simple linear regression model. If you haven’t already install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome (i.e., criterion) variable (Performance) to the left of the tilde (~) operator and the names of the predictor (e.g., selection tool) variables (SJT, Interview) separated by the + operator to the right of the ~ operator. We are telling the function to “regress Performance on SJT and Interview.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate multiple linear regression model Regression(Performance ~ SJT + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + Interview, data=df, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.527 0.539 10.249 0.000 4.465 6.588 ## SJT 0.567 0.085 6.714 0.000 0.401 0.734 ## Interview 0.385 0.064 6.031 0.000 0.260 0.511 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.992 for 297 degrees of freedom ## 95% range of residual variation: 11.777 = 2 * (1.968 * 2.992) ## ## R-squared: 0.264 Adjusted R-squared: 0.259 PRESS R-squared: 0.236 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 53.339 df: 2 and 297 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.303 0.000 ## Interview 1 325.670 325.670 36.375 0.000 ## ## Model 2 955.102 477.551 53.339 0.000 ## Residuals 297 2659.095 8.953 ## Performance 299 3614.197 12.088 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT Interview ## Performance 1.00 0.42 0.39 ## SJT 0.42 1.00 0.24 ## Interview 0.39 0.24 1.00 ## ## Tolerance VIF ## SJT 0.944 1.060 ## Interview 0.944 1.060 ## ## SJT Interview R2adj X&#39;s ## 1 1 0.259 2 ## 1 0 0.171 1 ## 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## -- Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [n_res_rows = 20, out of 300 rows of data, or do n_res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## SJT Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 1.000 24.000 11.019 12.981 4.537 0.736 0.169 ## 70 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 170 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 270 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 233 10.000 6.000 27.000 13.513 13.487 4.709 0.645 0.129 ## 1 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 101 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 69 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 169 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 92 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 192 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 292 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 20 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 120 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 283 9.000 5.000 22.000 12.560 9.440 3.226 0.372 0.045 ## 269 2.000 5.000 18.000 8.588 9.412 3.216 0.371 0.045 ## 97 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 197 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 297 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 82 2.000 4.000 14.000 8.203 5.797 1.959 0.224 0.017 ## ## ## PREDICTION ERROR ## ## -- Data, Predicted, Standard Error of Prediction, 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals add n_pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT Interview Performance pred s_pred pi.lwr pi.upr width ## 69 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 169 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 41 1.000 3.000 8.000 7.250 3.021 1.305 13.195 11.891 ## ... ## 210 8.000 2.000 10.000 10.837 3.015 4.903 16.771 11.868 ## 4 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## 47 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## ... ## 222 9.000 9.000 11.000 14.102 3.015 8.168 20.035 11.867 ## 92 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## 192 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## ---------------------------------- Given that the output for this model has already been reviewed in detail in the chapter on estimating the incremental validity of a selection tool using multiple linear regression, we’ll focus only on the information that is relevant for applying a compensatory model – namely the regression coefficients. For the estimated model, we find that both selection tools (i.e., predictor variables) show evidence of incremental validity; that is, both selection tools are significantly associated with the criterion of job performance when statistically controlling for the effects of each other. Specifically, the association between SJT and Performance (when controlling for Interview) is positive and statistically significant (b = .567, p &lt; .001), and the association between Interview and Performance (when controlling for SJT) is also positive and statistically significant (b = .385, p &lt; .001). If you recall from the chapter on predicting criterion scores on using simple linear regression, the intercept and predictor variable regression coefficients can be used to construct an equation, and this equation serves as the basis for making criterion-score predictions. In fact, when an equation comes from estimated multiple linear regression coefficients, the equation can serve as the basis for applying a compensatory approach to selection decisions. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Let’s assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance. We’ll bring this process to life in the following section in order to apply a compensatory approach to selection decisions. 40.2.5 Predict Criterion Scores Using the multiple linear regression model we estimated above, we can write code that generates predicted criterion scores based on new data we feed into the model. In fact, we can even add these predictions to the new data frame to which the new data belong. We’ll begin by specifying the same multiple linear regression model as above – except this time we will assign the estimated model to an object that we can subsequently reference. To do so, we’ll use the &lt;- operator. In this example, I’m naming the model reg_mod. # Assign multiple linear regression model to object reg_mod &lt;- Regression(Performance ~ SJT + Interview, data=df) Now that because we have assigned the multiple linear regression model to the object reg_mod, we can reference specific elements from that model, such as the regression coefficients. We’re going to do so to build our regression model equation as an R formula. Let’s begin by referencing the model intercept value and assigning it to an object called b0. I’m calling this object b0 because conventionally the “b” or “B” notation is used to signify regression coefficients and because the “0” (zero) is often used as a subscript to signify our intercept value in a regression model equation. To “pull” or reference the intercept value from our regression model, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients. If we were to run this by itself, it would print all of the regression coefficient values estimated for the model; we, however, want just the intercept value. Given that, immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact text: \"(Intercept)\". This will call up the intercept coefficient for any model estimated using the Regression function from lessR. # Reference estimated model intercept and assign to object b0 &lt;- reg_mod$coefficients[&quot;(Intercept)&quot;] Next, let’s “pull” the regression coefficients associated with the two predictor variables: SJT and Interview. Let’s assign these regression coefficients to objects called b1 and b2, respectively, which adhere to conventional notation. To reference the coefficient associated with SJT, we will specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients – just as we did above with the model intercept. Immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact name of the predictor variable associated with the coefficient you wish to reference (within quotation marks): \"SJT\". # Reference estimated model regression coefficient for SJT and assign to object b1 &lt;- reg_mod$coefficients[&quot;SJT&quot;] Let’s repeat the process for the Interview variable, except name this object b2. # Reference estimated model regression coefficient for Interview and assign to object b2 &lt;- reg_mod$coefficients[&quot;Interview&quot;] Now that we’ve pulled the coefficients and assigned them to objects (b0, b1, b2) that we can reference in our regression model equation, let’s read in new data so that we can reference new applicants’ scores on the same conscientiousness selection tool. As we did above in the Initial Steps section, let’s read in the data file called “NewApplicants.csv” and assign it to a new object. Here I call this new object new_df. # Read in data new_df &lt;- read.csv(&quot;NewApplicants.csv&quot;) Let’s take a peek at first six rows of the new_df data frame object. # Print first six rows head(new_df) ## ApplicantID SJT Interview ## 1 AA1 6 9 ## 2 AA2 2 7 ## 3 AA3 6 3 ## 4 AA4 10 5 ## 5 AA5 7 13 ## 6 AA6 4 10 Note that the data frame object includes an ApplicantID unique identifier variable as well as variables associated with two selection tools: SJT and Interview. Note that we don’t have any criterion (Performance) scores here because these folks are still applicants. Our goal then is to predict their future criterion scores using our regression model from above. To make criterion-score predictions based on the new applicant data for the SJT and Interview selection tools, we’ll need to create a regression equation based on estimates from the multiple linear regression model. Fortunately, we’ve already created objects corresponding to our model intercept (b0), SJT coefficient (b1), and Interview coefficient (b2). Using these values, we’ll specify a linear equation (e.g., Y = b0 + b1 * X + b2 * W). In our model equation, we’ll start by specifying a new variable containing what will be our vector of criterion-score predictions based on the SJT and Interview selection tool variables. I’m going to call this new variable Perf_Predict, as hopefully that name signals that the variable contains performance predictions. Using the $ operator, we can attach this new variable to the new data frame object we read in called new_df. To the right of the &lt;- operator, we’ll specify the rest of the equation; in this context, you can think of the &lt;- operator as being the equal sign in our equation; in fact, you could replace the &lt;- operator with = if you wanted to. First, type in the object associated with model intercept (b0) followed by the + operator. Second, type in the object associated with the SJT coefficient (b1), and follow that with the multiplication operator (*) so that we can multiple the coefficient by the new values for the selection tool called SJT; after the * operator, type in the name of the corresponding selection tool variable from the new data frame object: new_df$SJT; follow this with the + operator. Third, type in the object associated with the Interview coefficient (b2), and follow that with the multiplication operator (*) so that we can multiple the coefficient by the new values for the selection tool called Interview; after the * operator, type in the name of the corresponding selection tool variable from the new data frame object: new_df$Interview. # Assemble regression equation and assign to new variable in second data frame new_df$Perf_Predict &lt;- b0 + b1 * new_df$SJT + b2 * new_df$Interview Let’s take a look at the new_df data frame object to verify that the new Perf_Predict variable (containing the predicted criterion scores) was added successfully. # View data frame object View(new_df) In the viewer tab, we can use the up and down arrows to sort by variable scores. Alternatively and optionally, we can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 2 AA2 2 7 9.358599 ## 7 AA7 2 8 9.743899 ## 3 AA3 6 3 10.087326 ## 10 AA10 8 2 10.836989 ## 8 AA8 2 11 10.899798 ## 15 AA15 6 6 11.243225 ## 6 AA6 4 10 11.649461 ## 11 AA11 5 10 12.216943 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 4 AA4 10 5 13.127851 ## 5 AA5 7 13 14.507805 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 14 AA14 6 15 14.710923 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. when making selection decisions using a compensatory approach (like we are doing in this tutorial), it often makes most sense to sort in descending order. # Sort data frame by new variable in default descending order new_df[order(-new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 14 AA14 6 15 14.710923 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 5 AA5 7 13 14.507805 ## 4 AA4 10 5 13.127851 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 11 AA11 5 10 12.216943 ## 6 AA6 4 10 11.649461 ## 15 AA15 6 6 11.243225 ## 8 AA8 2 11 10.899798 ## 10 AA10 8 2 10.836989 ## 3 AA3 6 3 10.087326 ## 7 AA7 2 8 9.743899 ## 2 AA2 2 7 9.358599 As you can see, applicants AA14, AA9, and AA12 have the highest predicted criterion scores, where the criterion in this context is future job performance. If we had three open positions in need of filling, using a compensatory approach, we could extend offers first to these three applicants. If any one of them were to decline, we could start working our way down the list. In some instances, we might set a “floor” for the lowest acceptable predicted score and not consider any applicants with scores below that threshold. Alternatively, we could use a banding approach to identify groups of applicants for which we treat their predicted criterion scores as essentially equivalent (Mueller et al. 2007). One potential drawback to using a compensatory approach is that all applicants needs need to participate in all selection tools used in the multiple linear regression model, which can add expense and time. 40.2.6 Summary In this tutorial, we learned how to estimate a multiple linear regression model using the Regression function from the lessR package in order to apply a compensatory approach to selection decisions. 40.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm and predict functions from base R to estimate a multiple linear regression model and predict future criterion scores. Because this function comes from base R, we do not need to install and access an additional package. 40.3.1 Functions &amp; Packages Introduced Function Package lm base R summary base R predict base R order base R 40.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## Rows: 300 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (4): SJT, EI, Interview, Performance ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [300 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 40.3.3 lm &amp; predict Functions from Base R In the following section, we will learn how to apply the lm and predict functions from base R to estimate a multiple linear regression model and predict future criterion scores. As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variables (SJT, Interview) to the right of the ~ operator. We are telling the function to “regress Performance on SJT and Interview.” As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify multiple linear regression model reg.mod &lt;- lm(Performance ~ SJT + Interview, data=df) To view the model estimation results, we’ll use the summary function and specify the name of our regression model object as the sole parenthetical argument. # Print summary of multiple linear regression model results summary(reg.mod) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8249 -1.5198 -0.0401 1.0396 13.4868 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.52654 0.53925 10.249 &lt; 0.0000000000000002 *** ## SJT 0.56748 0.08453 6.714 0.0000000000964 *** ## Interview 0.38530 0.06388 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.992 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 Given that the output for this model has already been reviewed in detail in the chapter on estimating the incremental validity of a selection tool using multiple linear regression, we’ll focus only on the information that is relevant for applying a compensatory model – namely the regression coefficients. For the estimated model, we find that both selection tools (i.e., predictor variables) show evidence of incremental validity; that is, both selection tools are significantly associated with the criterion of job performance when statistically controlling for the effects of each other. Specifically, the association between SJT and Performance (when controlling for Interview) is positive and statistically significant (b = .567, p &lt; .001), and the association between Interview and Performance (when controlling for SJT) is also positive and statistically significant (b = .385, p &lt; .001). If you recall from the chapter on predicting criterion scores on using simple linear regression, the intercept and predictor variable regression coefficients can be used to construct an equation, and this equation serves as the basis for making criterion-score predictions. In fact, when an equation comes from estimated multiple linear regression coefficients, the equation can serve as the basis for applying a compensatory approach to selection decisions. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Let’s assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance. Next, we’ll bring this process to life in order to apply a compensatory approach to selection decisions. The predict function from base R pairs nicely with the lm function, and it makes it easier to take an estimated regression model – and associated regression equation – and apply that model to new (“fresh”) predictor variable data. Before applying the predict function, let’s read in the data file called “NewApplicants.csv” and assign it to a new object. Here I call this new object new_df. Let’s pretend this new data frame object contains data from future applicants who have completed the SJT and Interview tools as part of the organization’s selection process. We will ultimately plug these applicants’ SJT and Interview scores into our multiple linear regression model equation to predict the applicants’ scores on the criterion variable called Performance. # Read in data new_df &lt;- read.csv(&quot;NewApplicants.csv&quot;) Next, as the first argument in the predict function, type object= followed by the estimated model object that we named reg.mod above. As the second argument, type newdata= followed by the name of the data frame object that contains new data on the predictor variable. It’s important to make sure that the predictor variables’ names (SJT, Interview) in our new data are exactly the same as the predictor variables’ names (SJT, Interview) in our original data that we used to estimate the model. If that were not the case, we would want to rename the predictor variable in the new data frame object to match the corresponding name in the original data frame object, which is covered in the chapter on adding/removing variable names and renaming specific variables. # Predict scores on the criterion (outcome) variable predict(object=reg.mod, newdata=new_df) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 12.399124 9.358599 10.087326 13.127851 14.507805 11.649461 9.743899 10.899798 14.689987 10.836989 12.216943 14.689987 12.399124 ## 14 15 ## 14.710923 11.243225 In your console, you should see a vector of scores – these are the predicted criterion scores. In many cases, we might want to append this vector as a variable to our new data frame object. To do so, we just need to apply the &lt;- operator and, to the left of it, specify the name of the new data frame object (new_df) followed by the $ operator and a name for the new variable that will contain the predicted criterion scores (Perf_Predict). # Predict scores on the criterion (outcome) variable # and append as new variable in data frame new_df$Perf_Predict &lt;- predict(object=reg.mod, newdata=new_df) We can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 2 AA2 2 7 9.358599 ## 7 AA7 2 8 9.743899 ## 3 AA3 6 3 10.087326 ## 10 AA10 8 2 10.836989 ## 8 AA8 2 11 10.899798 ## 15 AA15 6 6 11.243225 ## 6 AA6 4 10 11.649461 ## 11 AA11 5 10 12.216943 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 4 AA4 10 5 13.127851 ## 5 AA5 7 13 14.507805 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 14 AA14 6 15 14.710923 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. when making selection decisions using a compensatory approach (like we are doing in this tutorial), it often makes most sense to sort in descending order. # Sort data frame by new variable in default descending order new_df[order(-new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 14 AA14 6 15 14.710923 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 5 AA5 7 13 14.507805 ## 4 AA4 10 5 13.127851 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 11 AA11 5 10 12.216943 ## 6 AA6 4 10 11.649461 ## 15 AA15 6 6 11.243225 ## 8 AA8 2 11 10.899798 ## 10 AA10 8 2 10.836989 ## 3 AA3 6 3 10.087326 ## 7 AA7 2 8 9.743899 ## 2 AA2 2 7 9.358599 As you can see, applicants AA14, AA9, and AA12 have the highest predicted criterion scores, where the criterion in this context is future job performance. If we had three open positions in need of filling, using a compensatory approach, we could extend offers first to these three applicants. If any one of them were to decline, we could start working our way down the list. In some instances, we might set a “floor” for the lowest acceptable predicted score and not consider any applicants with scores below that threshold. Alternatively, we could use a banding approach to identify groups of applicants for which we treat their predicted criterion scores as essentially equivalent (Mueller et al. 2007). One potential drawback to using a compensatory approach is that all applicants needs need to participate in all selection tools used in the multiple linear regression model, which can add expense and time. References "],["multiplecutoff.html", "Chapter 41 Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method 41.1 Conceptual Overview 41.2 Tutorial", " Chapter 41 Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method In this chapter, we will learn how to apply a noncompensatory approach to making selection decisions by using the Angoff Method. We’ll begin with a conceptual overview of the noncompensatory approach and Angoff Method, and we’ll conclude with a tutorial. 41.1 Conceptual Overview In general, there are three overarching (and mutually non-exclusive) approaches to making selection decisions: (a) compensatory (e.g., multiple linear regression), (b) noncompensatory (e.g., multiple-cutoff), and (c) multiple-hurdle. These three approaches can be mixed and matched to fit the selection-decision needs of an organization. In the previous chapter, we focused a compensatory approach using multiple linear regression, and in this chapter, we will focus on the multiple-cutoff noncompensatory approach using the Angoff Method. 41.1.1 Review of Noncompensatory Approach Link to conceptual video: https://youtu.be/2BMJNLptVPw A noncompensatory approach to making selection decisions involves the application of multiple cutoff scores, where cutoff scores are sometimes referred to as critical scores or just cut scores. A noncompensatory approach signals that an applicant’s score on one selection tool cannot compensate for their score on another selection tool. A common noncompensatory approach involves setting a separate cutoff score for each selection tool. Naive, judgmental, and empirical methods exist for setting cutoff scores (Mueller et al. 2007). Of the judgmental methods, the Angoff Method (Angoff 1971) is one of the more well-known. Briefly, this method requires that a set of subject matter experts (SMEs) estimate the probability that a minimally qualified applicant would respond correctly to an item (e.g., question) from a selection tool (e.g., assessment, test). Assuming a multi-item selection tool, the SMEs’ probability estimates are then averaged for each item, and the cutoff score is then computed by calculating the sum of the average SME probability estimates across items. Conceptually, the resulting cutoff score is thought to represent the mean of the distribution of selection tool overall scores. Only relatively simple arithmetic (e.g., mean, sum) is needed when applying the Angoff Method for two or more selection tools as part of a multiple-cutoff approach to making selection decisions. That being said, sometimes it helps to practice prepping the data and applying such arithmetic along with logical expressions to construct a simple algorithm. 41.2 Tutorial This chapter’s tutorial demonstrates how to apply a noncompensatory approach to making selection decisions by using the Angoff Method. 41.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/OBiY73Pruao 41.2.2 Functions &amp; Packages Introduced Function Package colMeans base R sum base R c base R rowSums base R ifelse base R is.na base R 41.2.3 Initial Steps If you haven’t already, save the file called “angoff_sme.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “angoff_sme.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df1 &lt;- read_csv(&quot;angoff_sme.csv&quot;) ## Rows: 5 Columns: 9 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): SME, CogAb_i1, CogAb_i2, CogAb_i3, CogAb_i4, CogAb_i5, KnowTest_i1, KnowTest_i2, KnowTest_i3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df1) ## [1] &quot;SME&quot; &quot;CogAb_i1&quot; &quot;CogAb_i2&quot; &quot;CogAb_i3&quot; &quot;CogAb_i4&quot; &quot;CogAb_i5&quot; &quot;KnowTest_i1&quot; &quot;KnowTest_i2&quot; &quot;KnowTest_i3&quot; # View variable type for each variable in data frame str(df1) ## spc_tbl_ [5 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ SME : num [1:5] 1 2 3 4 5 ## $ CogAb_i1 : num [1:5] 0.78 0.85 0.76 0.75 0.81 ## $ CogAb_i2 : num [1:5] 0.72 0.72 0.7 0.69 0.69 ## $ CogAb_i3 : num [1:5] 0.67 0.6 0.59 0.59 0.63 ## $ CogAb_i4 : num [1:5] 0.53 0.61 0.52 0.56 0.59 ## $ CogAb_i5 : num [1:5] 0.44 0.46 0.49 0.47 0.45 ## $ KnowTest_i1: num [1:5] 0.69 0.8 0.69 0.67 0.75 ## $ KnowTest_i2: num [1:5] 0.7 0.72 0.69 0.69 0.79 ## $ KnowTest_i3: num [1:5] 0.72 0.76 0.78 0.77 0.73 ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. SME = col_double(), ## .. CogAb_i1 = col_double(), ## .. CogAb_i2 = col_double(), ## .. CogAb_i3 = col_double(), ## .. CogAb_i4 = col_double(), ## .. CogAb_i5 = col_double(), ## .. KnowTest_i1 = col_double(), ## .. KnowTest_i2 = col_double(), ## .. KnowTest_i3 = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df1) ## # A tibble: 5 × 9 ## SME CogAb_i1 CogAb_i2 CogAb_i3 CogAb_i4 CogAb_i5 KnowTest_i1 KnowTest_i2 KnowTest_i3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.78 0.72 0.67 0.53 0.44 0.69 0.7 0.72 ## 2 2 0.85 0.72 0.6 0.61 0.46 0.8 0.72 0.76 ## 3 3 0.76 0.7 0.59 0.52 0.49 0.69 0.69 0.78 ## 4 4 0.75 0.69 0.59 0.56 0.47 0.67 0.69 0.77 ## 5 5 0.81 0.69 0.63 0.59 0.45 0.75 0.79 0.73 The data frame contains 9 variables and 5 observations (i.e., SMEs). The SME variable contains unique identifiers for the subject matter experts (SMEs) who estimated the probability (for each item) that a minimally qualified applicant would answer correctly (i.e., Angoff Method). The variables labeled CogAb_i1 through CogAb_i5 correspond to five cognitive ability test items that were designed to have increasing levels of difficulty, with the first item (CogAb_i1) being the easiest and the fifth item (CogAb_i5) being the most difficult. The variables labeled KnowTest_i1 through KnowTest_i3 correspond to three knowledge test items. 41.2.4 Create Cutoff Scores To create cutoff scores for the cognitive ability and knowledge tests, we’ll nest the colMeans function within the sum function, where both functions are from base R. Let’s begin by creating a cutoff score for the cognitive ability test based on its five items: CogAb_i1, CogAb_i2, CogAb_i3, CogAb_i4, and CogAb_i5. Specify a unique name for an object that we can subsequently assign the cognitive ability test cutoff score to. Below, I name this object CogAb_cutoff, but you could name it whatever makes sense to you. To the right of the object name you specified, type the &lt;- operator, which will allow us to assign to the object the value that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the sum function. The sum function computes the sum for a vector of scores. As the sole argument within the sum function specify the colMeans function. The colMeans function computes the means for all columns within a data frame – or a subset of columns if we reference specific variables. As the sole argument in the colMeans function, we’ll use bracket (i.e., matrix) notation to specify the data frame name and the specific variables we wish to estimate the column means for. First, type the name of the data frame object (df1), and follow that object name up with brackets ([ ]). Using bracket notation, we can reference columns by typing a comma (,), followed by a vector containing the columns (i.e., variables) we wish to reference. To specify that vector of column (variable) names, we can use the c function from base R, and within that function, specify each variable’s name in quotation marks (\" \"), and separate variable names using commas. # Create cutoff score for cognitive ability test CogAb_cutoff &lt;- sum( colMeans( df1[, c(&quot;CogAb_i1&quot;,&quot;CogAb_i2&quot;,&quot;CogAb_i3&quot;,&quot;CogAb_i4&quot;,&quot;CogAb_i5&quot;)] ) ) Next, let’s apply the same process as above to the three knowledge test items in order to create a cutoff score for the knowledge test: KnowTest_i1, KnowTest_i2, and KnowTest_i3. # Create cutoff score for knowledge test KnowTest_cutoff &lt;- sum( colMeans( df1[, c(&quot;KnowTest_i1&quot;,&quot;KnowTest_i2&quot;,&quot;KnowTest_i3&quot;)] ) ) 41.2.5 Apply Cutoff Scores to Make Selection Decisions After creating the cutoff scores for our two selection tools, we’re ready to apply these cutoff scores to data collected from applicants, which will result in an algorithm of sorts. The name of the data file containing the applicant data on the cognitive ability and knowledge tests is “angoff_applicant.csv”. Below, I read in the data as a data frame and assign the data frame to an object that I’m calling df2. # Read in data df2 &lt;- read.csv(&quot;angoff_applicant.csv&quot;) Let’s print this data frame to our Console window by using the print function from base R. # Print data frame object print(df2) ## Applicant_ID CogAb_i1 CogAb_i2 CogAb_i3 CogAb_i4 CogAb_i5 KnowTest_i1 KnowTest_i2 KnowTest_i3 ## 1 AA1 0 0 1 0 0 0 0 1 ## 2 AA2 1 1 1 1 0 1 1 1 ## 3 AA3 1 1 0 0 0 1 0 1 ## 4 AA4 1 0 0 1 0 0 0 0 ## 5 AA5 1 0 1 0 1 1 1 0 ## 6 AA6 1 1 1 0 0 0 1 1 ## 7 AA7 0 0 0 0 0 0 1 1 ## 8 AA8 1 1 0 1 0 0 0 1 ## 9 AA9 0 1 1 0 0 1 1 1 ## 10 AA10 1 1 1 1 1 1 1 1 ## 11 AA11 1 0 0 0 0 1 0 1 ## 12 AA12 1 0 1 0 0 1 0 0 ## 13 AA13 1 1 1 0 1 1 1 1 ## 14 AA14 1 1 0 0 0 0 0 0 ## 15 AA15 1 0 1 0 0 0 1 1 ## 16 AA16 1 1 0 0 1 1 0 1 ## 17 AA17 1 1 0 1 0 1 1 1 ## 18 AA18 1 1 0 0 1 1 0 1 ## 19 AA19 1 0 0 0 0 1 0 1 ## 20 AA20 0 1 0 1 0 1 1 0 The data frame contains 9 variables and 20 observations (i.e., applicants). The Applicant_ID variable contains unique identifiers for the applicants who completed each of the items for the cognitive ability and knowledge tests. The variables labeled CogAb_i1 through CogAb_i5 correspond to five cognitive ability test items that were designed to have increasing levels of difficulty, with the first item (CogAb_i1) being the easiest and the fifth item (CogAb_i5) being the most difficult. The variables labeled KnowTest_i1 through KnowTest_i3 correspond to three knowledge test items. Each of the items consist of scores of 1 and 0, where 1 indicates that an applicant answered the item correctly and 0 indicates that an applicant answered the item incorrectly. Using the applicant data (df2), we need to compute the overall score for each applicant on each of the two selection tools. Let’s begin with the cognitive ability test items: CogAb_i1, CogAb_i2, CogAb_i3, CogAb_i4, and CogAb_i5. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the cognitive ability test overall scores. Because we’re going to compute an overall score based on the sum of the number of items each applicant answered correctly, I decided to name this variable CogAb_sum. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the rowSums function from base R. The rowSums function computes the sum for each row. As the first argument in the rowSums function, we’ll use bracket (i.e., matrix) notation to specify the data frame name and the specific variables we wish to estimate the column means for. First, type the name of the data frame object (df2), and follow that object name up with brackets ([ ]). Using bracket notation, we can reference columns by typing a comma (,), followed by a vector containing the columns (i.e., variables) we wish to reference. To specify that vector of column (variable) names, we can use the c function from base R, and within that function, specify each variable’s name in quotation marks (\" \"), and separate variable names using commas. As the second argument in the rowSums function, type the argument na.rm=TRUE, which will tell the function to ignore missing data when computing the sum for each row. # Compute the overall (sum) cognitive ability test score for each applicant df2$CogAb_sum &lt;- rowSums( df2[,c(&quot;CogAb_i1&quot;,&quot;CogAb_i2&quot;,&quot;CogAb_i3&quot;,&quot;CogAb_i4&quot;,&quot;CogAb_i5&quot;)], na.rm=TRUE) Next, repeat the same process for the three knowledge test items: KnowTest_i1, KnowTest_i2, and KnowTest_i3. # Compute the overall (sum) knowledge test score for each applicant df2$KnowTest_sum &lt;- rowSums( df2[,c(&quot;KnowTest_i1&quot;,&quot;KnowTest_i2&quot;,&quot;KnowTest_i3&quot;)], na.rm=TRUE) We’re now ready to apply the cutoff scores to applicants’ scores on the two selection tools. Let’s begin by applying the cutoff score for the cognitive ability test to the applicants’ overall scores on the cognitive ability test. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the cognitive ability test pass vs. fail scores (CogAb_pass). To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the ifelse function from base R. The ifelse function allows us to apply if/else logical arguments to a vector of values (such as a variable). As the first argument in the ifelse function, specify a logical argument that applies the cutoff score to the applicants overall scores for the cognitive ability test. Specifically, we want to make a logical expression in which the cognitive ability overall scale variable scores are greater than or equal to the cutoff score object we created previously: df2$CogAb_sum &gt;= CogAb_cutoff. As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicant’s overall cognitive ability test score is greater than or equal to the cutoff score, then let’s type \"Pass\" as the resulting value. As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be false for a particular applicant. If an applicant’s overall cognitive ability test score is not greater than or equal to the cutoff score, then let’s type \"Fail\" as the resulting value. # Create variable containing cognitive ability test pass/fail scores df2$CogAb_pass &lt;- ifelse( df2$CogAb_sum &gt;= CogAb_cutoff, &quot;Pass&quot;, &quot;Fail&quot; ) Next, let’s apply the same process to the knowledge test overall scores. # Create variable containing knowledge test pass/fail scores df2$KnowTest_pass &lt;- ifelse( df2$KnowTest_sum &gt;= KnowTest_cutoff, &quot;Pass&quot;, &quot;Fail&quot; ) We are now ready to start making some overall selection decisions using what’s referred to as a multiple-cutoff approach. Our goal is to create a new variable that indicates which applicants passed both selection tools based on the cutoff scores we previously applied. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the overall pass vs. fail scores (Overall_pass) based on our multiple-cutoff approach. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the ifelse function from base R. As the first argument in the ifelse function, specify a logical argument with a logical &amp; (AND) operator to specify that in order for an applicant to pass both selection tools, they need to earn passing scores on both. Specifically, we want to make a logical expression in which applicants’ scores on pass/fail variables we created above are used to flag those individuals who earned a score of “Pass” on both: df2$CogAb_pass == \"Pass\" &amp; df2$KnowTest_pass == \"Pass\". As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicant satisfies both of those logical expressions in the previous argument, let’s assign them a \"Pass\" score. As the second argument in the ifelse function, provide a value that will be generated should one or both of the logical expressions in the first argument be false for a particular applicant. If an applicant does not satisfy both of those logical expressions in the first argument, let’s assign them a \"Fail\" score. # Create variable containing overall pass/fail scores based on both tools df2$Overall_pass &lt;- ifelse( df2$CogAb_pass == &quot;Pass&quot; &amp; df2$KnowTest_pass == &quot;Pass&quot;, &quot;Pass&quot;, &quot;Fail&quot; ) Using the ifelse function once more, let’s create a vector containing the applicant unique identifier numbers for those who passed the multiple-cutoff selection process and NA (missing) for everyone else who failed. Come up with a unique name for an object to which we can assign the vector of applicant unique ID numbers and NAs so that we can reference it later. Here, I name the object Applicant_ID_pass. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the ifelse function. Type the name of the ifelse function from base R. As the first argument in the ifelse function, specify a logical argument in which scores on the Overall_pass variable we created above is equal to the value “Pass”: df2$Overall_pass == \"Pass\". As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicant satisfies both of those logical expressions in the previous argument, let’s reference their applicant unique identifier number (Applicant_ID) from the applicant data frame object. As the second argument in the ifelse function, simply enter NA. Should the logical expression in the first argument be false, then the applicant will receive a missing value. # Create a vector of the applicant unique identifiers for those who passed # and NAs for those who failed the multiple-cutoff selection process Applicant_ID_pass &lt;- ifelse( df2$Overall_pass == &quot;Pass&quot;, df2$Applicant_ID, NA) Finally, from the Applicant_ID_pass vector object we created above, let’s drop all NA values. Type name of the vector we created above that contains the applicant unique identifier numbers for those who passed the multiple-cutoff selection process and NAs for those who failed (Applicant_ID_pass). Following the name of the Applicant_ID_pass vector object, insert brackets ([ ]). Within the brackets ([ ]), type the not (!) operator followed by the is.na function from base R. The is.na function returns a TRUE if a value is NA and a FALSE if the value is not NA. By preceding that function with the not (!) operator, we can flip that logic, such that those with an NA will effectively receive a FALSE value. As the sole parenthetical argument within the is.na function, type the exact name of the same vector object from the first step (Applicant_ID_pass). # Retain only the applicant unique identifer numbers for those who passed # the multiple-cutoff selection process Applicant_ID_pass[!is.na(Applicant_ID_pass)] ## [1] &quot;AA2&quot; &quot;AA10&quot; &quot;AA13&quot; In your Console, you should see three applicant unique identifier numbers: AA2, AA10, and AA13. These are the applicants who passed the multiple-cutoff selection process. Depending on the overall design of the selection system, these are the three applicants who should be given either job offers or passed along to the next phase of the selection system (e.g., interview). 41.2.6 Summary In this chapter, we learned how to apply common functions from base R to make selection decisions from a multiple-cutoff selection process that uses the Angoff Method, where the application of multiple cutoffs is a noncompensatory approach. References "],["differentialprediction.html", "Chapter 42 Testing for Differential Prediction Using Moderated Multiple Linear Regression 42.1 Conceptual Overview 42.2 Tutorial 42.3 Chapter Supplement", " Chapter 42 Testing for Differential Prediction Using Moderated Multiple Linear Regression In this chapter, we will learn how to apply test whether a selection tool shows evidence of differential prediction by using moderating multiple linear regression. We’ll begin with conceptual overviews of the moderated multiple linear regression and differential prediction, and we’ll conclude with a tutorial. 42.1 Conceptual Overview In this chapter, we will learn how to use an ordinary least squares (OLS) moderated multiple linear regression model to test for differential prediction of an employee selection tool due to protected class membership. Thus, the way in which we build and interpret our moderated multiple regression model will be couched in the context of differential prediction. Nonetheless, this tutorial should still provide you with the technical and interpretation skills to understand how to apply moderated multiple linear regression in other non-selection contexts. 42.1.1 Review of Moderated Multiple Linear Regression Link to conceptual video: https://youtu.be/mkia_KLjJ0M Moderated multiple linear regression (MMLR) is a specific type of multiple linear regression, which means that MMLR involves multiple predictor variables and a single outcome variable. For more information on multiple linear regression, check out the chapter on estimating incremental validity. In MMLR, we are in effect determining whether the association between a predictor variable and the outcome variable is conditional upon the level/value of another predictor variable, the latter of which we often refer to as a moderator or moderator variable. Moderators: A moderator (i.e., moderator variable) is a predictor variable upon which an association between two other variables is conditional. A moderator variable can be continuous (e.g., age: measured in years) or categorical (e.g., location: Portland vs. Beaverton). When we find that one variable moderates the association between two other variables, we often refer to this as an interaction or interaction effect. For example, let’s imagine that we are interested in the association between job satisfaction and job performance in our organization. By introducing the categorical moderator variable of employee location (Portland vs. Beaverton), we can determine whether the magnitude and/or sign of the association between job satisfaction and job performance varies by the location at which employees work. Perhaps we find that the association between job satisfaction and job performance is significantly stronger for employees who work in Beaverton as compared to employees who work in Portland (see figure below), and thus we would conclude that there is a significant interaction between job satisfaction and location in relation to job performance. This line and scatter plot illustrates pictorially how a variable like employee location might moderate the association between job satisfaction and job performance. When investigating whether an interaction exists between two variables relative to the outcome variable, we need to create a new variable called an interaction term (i.e., product term). An interaction term is created by multiplying the scores of two predictor variables – or in other words, the computing the product of two variables’ respective scores. In fact, the presence of an in interaction term – and associated multiplication – signals why MMLR is referred to as a multiplicative model. In terms of the language we use to describe an interaction and the associated interaction term, we often refer to one of the predictor variables involved in an interaction as the moderator. For an illustration of how an interaction term is created, please refer to the table below. To create an interaction term (i.e., product term), we simply multiply the associated predictor variables’ scores (e.g., predictor variable and moderator variable) for each case (e.g., employee). Centering Predictor &amp; Moderator Variables: To improve the interpretability of the main effects (but not the interaction term) and to reduce collinearity between the predictor variables involved in the interaction term, we typically grand-mean center any continuous (interval, ratio) predictor variables; we do not need to center the outcome variable or a predictor or moderator variable that is categorical (e.g., dichotomous). As discussed in the chapter on centering and standardizing variables, grand-mean centering refers to the process of subtracting the overall sample mean for a variable from each case’s score on that variable. We center the predictor variables (i.e., predictor and moderator variables) before computing the interaction term based on those variables. To illustrate how grand-mean centering works, take a look at the table below, and note that we are just subtracting the (grand) mean for the sample from the specific value on the predictor variable for each case. To grand-mean center a variable, we subtract the overall sample mean of the variable from each case’s score on that variable. When we grand-mean center the predictor variables in a regression model, it also changes our interpretation of the \\(\\hat{Y}\\)-intercept value. If you recall from prior reviews of simple linear regression and multiple linear regression, the \\(\\hat{Y}\\)-intercept value is the mean of the outcome variable when each predictor variable(s) in the model is/are set to zero. Thus, when we center each predictor variable, we shift the mean of each variable to zero, which means that after grand-mean centering, the \\(\\hat{Y}\\)-intercept value is the mean of the outcome variable when each predictor variable is set to its mean. Beyond moderated multiple linear regression, grand-mean centering can be useful in simple and multiple linear regression models when interpreting the regression coefficients associated with the intercept and the slope(s), as in some cases, a score of zero on the predictor variable is theoretically meaningless, implausible, or both. For example, if one of our predictor variables has an interval measurement scale that ranges from 10-100, then a score of zero is not even part of the scale range, which would make shifting the zero value to the grand-mean a more meaningful value. Regardless of your decision to grand-mean center or to not grand-mean center the predictor variable and moderator variables, the interaction term’s significance level (p-value) and model fit (i.e., R2) will remain the same. In fact, sometimes it makes more sense to plot the findings of your model based on the uncentered version of the variables. That choice is really up to you and should be based on the context at hand. Moderated Multiple Linear Regression: In moderated multiple linear regression (MMLR), we have three or more predictor variables and a single outcome variable, where one of the predictor variables is the interaction term (i.e., product term) between two of the predictor variables (e.g., predictor and moderator variables). Just like multiple linear regression, MMLR allows for a series of covariates (control variables) to be included in the model; accordingly, each regression coefficient (i.e., slope, weight) represents the relationship between the associated predictor and the outcome variable when statistically controlling for all other predictors in the model. To better understand what MMLR is and how it works, I find that it’s useful to consider a MMLR model in equation form. Specifically, let’s consider a scenario in which we estimate a MMLR with two predictor variables, their interaction term (i.e., product term), and a single outcome variable. The equation for such a model with unstandardized regression coefficients (\\(b\\)) would be as follows: \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + b_{3}X_1*X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables \\(X_1\\) and \\(X_2\\) and interaction term \\(X_1*X_2\\) are equal to zero, \\(b_{1}\\), \\(b_{2}\\), and \\(b_{3}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_1\\) and \\(X_2\\) and interaction term \\(X_1*X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. If the regression coefficient associated with the interaction term (\\(b_{3}\\)) is found to be statistically significant, then we would conclude that there is evidence of a significant interaction effect. If we conceptualize one of the predictor variables as a moderator variable, then we might re-write the equation from above by swapping out \\(X_2\\) with \\(W\\) (or \\(Z\\)). The meaning of the equation remains the same, but changing the notation in this way may signal more clearly which variable we are labeling a moderator. \\(\\hat{Y} = b_{0} + b_{1}X + b_{2}W + b_{3}X*W + e\\) Unstandardized regression coefficient indicates the “raw” regression coefficient (i.e., weight, slope) when controlling for the associations between all other predictors in the model and the outcome variable; accordingly, each unstandardized regression coefficient represents how many unstandardized units of \\(\\hat{Y}\\) increase/decrease as a result of a single unit increase in a predictor variable, when statistically controlling for the effects of other predictors in the regression model. Typically, we are most interested in whether slope differences exist; with that being said, in the context of testing for differential prediction (described below), we are also interested in whether intercept differences exist. In essence, a significant interaction term indicates the presence of slope differences, where slopes differences refer to significant variation in the association (i.e., slope) between the predictor variable and outcome variable by levels/values of the moderator variable. In the context of our first MMLR equation (see above), if the regression coefficient (\\(b_{3}\\)) associated with the interaction term (\\(X_1*X_2\\)) is statistically significant, we conclude that there is significant interaction and thus significant slope differences. In fact, we might even us language like, “\\(X_2\\) moderates the association between \\(X_1\\) and \\(Y\\).” When we find a statistically significant interaction, it is customary to plot the simple slopes and estimate whether each of the simple slopes is significantly different from zero. Simple slopes allow us to probe the form of the significant interaction so that we can more accurately describe it. Plus, the follow-up simple slopes analysis helps us understand at which levels of the moderator variable the association between the predictor and outcome variables is statistically significant. For a continuous moderator variable, we typically plot and estimate the simple slopes at 1 standard deviation (SD) above and below the mean for the variable – and sometimes at the mean of the variable as well; with that being said, if we want, we can choose more theoretically or empirically meaningful values for the moderator variable. For a categorical variable, we plot and estimate the simple slopes based on the different discrete levels (categories) of the variable. For example, if our moderator variable captures gender identity operationalized as man (0) and woman (1), then we would plot the simple slopes when sex is equal to man (0) and when sex is equal to woman (1). 42.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a moderated multiple linear regression model are the same as those for conventional multiple linear regression include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of multivariate outliers; The association between the predictor and outcome variables is linear; There is no (multi)collinearity between predictor variables; Average residual error value is zero for all levels of the predictor variables; Variances of residual errors are equal for all levels of the predictor variables, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for all levels of the predictor variables. The fourth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so let’s take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple linear regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights – and even the signs – of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 42.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of the other predictor variable(s) and interaction term variable in the model. In other words, if a regression coefficient’s p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent when controlling for the effects of other predictor variables in the model. In contrast, if the regression coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of other predictor variable(s) and interaction term variable in the model. Put differently, if a regression coefficient’s p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable (or interaction term variable) and the outcome variable in the population – when controlling for the effects of other predictor variables and interaction term variable in the model. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 42.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a moderated multiple linear regression (MMLR) model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. A standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) given that it is standardized. With that being said, I suggest doing so with caution as collinearity (i.e., correlation) between predictor variables in the model can bias our interpretation of \\(\\beta\\) as an effect size. Thus, if your goal is just to understand the bivariate association between a predictor variable and an outcome variable (without introducing statistical control), then I recommend to just estimate a correlation coefficient as an indicator of practical significance, which I discuss in the chapter on estimating criterion-related validity using correlations. In a MMLR model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variables (i.e., R2). That is, in a multiple linear regression model, R2 represents the proportion of collective variance explained in the outcome variable by all of the predictor variables. Conceptually, we can think of the overlap between the variability in the predictor variables and and outcome variable as the variance explained (R2), and R2 is a way to evaluate how well a model fits the data (i.e., model fit). I’ve found that the R2 is often readily interpretable by non-analytics audiences. For example, an R2 of .25 in a MMLR model can be interpreted as: the predictor variable and interaction term variable scores explain 25% of the variability in scores on the outcome variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large 42.1.2 Review of Differential Prediction Differential prediction refers to differences in intercepts, slopes, or both based on subgroup (projected class) membership (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968), and sometimes differential prediction is referred to as predictive bias. Assuming a single regression line is used to predict the criterion (e.g., job performance) scores of applicants, differential prediction is concerned with overpredicting or underpredicting criterion scores for one subgroup relative to another (Cascio and Aguinis 2005). A common regression line would be inappropriate when evidence of intercept differences, slope differences, or both exist. In the U.S., the use of separate regression lines – one for each subgroup – is legally murky, with some interpreting separate regression lines as violation of federal law and others cautioning that the practice might be interpreted as score adjustment, thereby running afoul of the Civil Rights Act of 1991 (Berry 2015; Saad and Sackett 2002). Specifically, Section 106 of the Civil Rights Act of 1991 amends Section 703 of the Civil Rights Act of 1964 to prohibit the discriminatory use of selection test scores: “It shall be an unlawful employment practice for a respondent, in connection with the selection or referral of applicants or candidates for employment or promotion, to adjust the scores of, use different cutoff scores for, or otherwise alter the results of, employment related tests on the basis of race, color, religion, sex, or national origin.” — Section 106, U.S. Civil Rights Act of 1991 Thus, a legally cautious approach would be to discontinue the use of a selection tool in its current form should evidence of differential prediction come to light. We’ll begin by considering an example of intercept differences when predicting scores on the criterion (e.g., job performance) based on scores on a selection tool. Imagine a situation in which scores on a selection tool differentially predict scores on job performance for men and women, such that the slopes are the same, but the intercepts of each slope are different (see figure below). The dashed black line indicates what a common regression line would look like if it were used. As you can see, if we were to use a single common regression line when intercept differences exist between men and women, then we would tend to underpredict job performance scores for men and overpredict for women. This simple slopes plot illustrates intercept differences by group but not slope differences. The dashed line indicates what a single regression line would like if it were used. Next, we’ll consider an example involving slope differences in the context of differential prediction. In the figure below, the slopes (e.g., criterion-related validities) are different when the selection tool is used for men as compared to women. Again, the dashed black line represents what would happen if a common regression line were used, which would clearly be inappropriate. Note that, in this particular example, the job performance scores would generally be underpredicted for men and overpredicted for women when using the same regression. Further, the selection tool appears to show criterion-related validity (i.e., a significant slope) for men but not for women. Accordingly, based on this sample, the selection tool might only be a valid predictor of job performance for men. Clearly this would be a problem. This simple slopes plot illustrates slope differences by group (i.e., different criterion-related validities) but not intercept differences. The dashed line indicates what a single regression line would like if it were used. Finally, we’ll consider an example involving both intercept differences and slope differences. The figure below shows that use of a common regression line would pose problems in terms of predictive bias. In this example, for men, using of a common regression line would overpredict job performance scores for a certain range of selection tool scores and underpredict job performance scores for another range of selection tool scores; the opposite would be true for women. This simple slopes plot illustrates both intercept differences and slope differences by group. The dashed line indicates what a single regression line would like if it were used. Words of Caution: In this tutorial, we will learn how to apply MMLR in the service of detecting differential predication using an approach that is generally supported by the U.S. court system and researchers. With that said, using MMLR to detect differential prediction should be used cautiously with small samples, as failure to detect differential prediction that actually exists (i.e., Type II Error, false negative) could be the result of low statistical power, and relatedly, unreliability or bias in the selection tool or criterion and violation of certain statistical assumptions (e.g., homoscedasticity of residual error variances) can affect our ability to find differential prediction that actually exists (Aguinis and Pierce 1998). Further, when one subgroup is less than 30% of the total sample size, statistical power is also diminished (Stone-Romero, Alliger, and Aguinis 1994). The take-home message is to remain cautious and thoughtful when investigating differential prediction. Multistep MMLR Process for Differential Prediction: We will use a three-step “step-up” process for investigating differential prediction using MMLR (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). This process uses what is referred to as hierarchical regression, as additional predictor variables are added at each subsequent step, and their incremental validity above and beyond the other predictors in the model is assessed. In the first step, we will regress the criterion variable on the selection tool variable, and in this step, we will verify whether the selection tool shows criterion-related validity by itself. If we find evidence of criterion-related validity for the selection tool in this first step, then we proceed to the second and third steps. In the second step, we include the selection tool variable as before but add the dichotomous (binary) protected class/group variable to the model. If the protected class variable is statistically significant when controlling for the selection tool variable, then there is evidence of intercept differences. You might be asking yourself: What happens when a protected class variable is not inherently categorical or, specifically, dichotomous? When it comes to the protected class of race, many organizations have more than two races represented among their employees. In this situation, it is customary (at least within the U.S.) to compare two subgroups at time (e.g., White and Black) or to create a dichotomy consisting of the majority race and members of other non-majority races (e.g., White and Non-White). For a inherently continuous variable like age, classically, the standard procedure was to dichotomize the age variable by those who are 40 years and older and those who are younger than 40 years, as this is consistent with the U.S. Age Discrimination in Employment Act (ADEA) of 1967. There are two potential issues with this: (a) Some states have further age protections in place that protect workers under 40 as well, and (b) Some jobs in organizations have very young workforces, leaving the question of whether people in their 30s are being favored over people in their 20s, or vice versa (for example). In addition, when we look at organizations with workers who are well into their 50s, 60s, and 70s, then there might be evidence of age discrimination and differential prediction for those who are in their 50s compared to those who are in their 60s. Dichotomizing the age variable using a 40 and above vs. under 40 split might miss these issues and generally nuance. (For a review of this topic, see Fisher et al. 2017.) As such, with a variable like age, there might be some advantages to keeping it as a continuous variable, as MMLR can handle this. In the third step, we include the selection tool and protected class variables as before but add the interaction term between the selection tool and protected class variables. If the interaction term is statistically significant, then there is evidence of differential prediction in the form of slope differences. 42.1.2.1 Sample Write-Up Our HR analytics team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction for the structured interview based on the U.S. protected class of gender. Based on a sample of 370 individuals, we applied a three-step “step up” process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence of differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .192, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, but we did not find evidence of intercept differences (b = .166, p = .146). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between interview scores and job performance scores to a statistically significant extent (b = -.373, p &lt; .001). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant and positive for men (b = .39, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .39 units; in contrast, the association between structured interview scores and job performance scores for women was nonsignificant (b = .02, p = .72). In sum, we found evidence of slope differences, which means that this interview tool shows evidence of predictive bias with respect to gender, making the application of a common regression line (i.e., equation) egally problematic within the United States. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we caution against using separate regression lines for men and women, as this may be interpreted as running afoul of the U.S. Civil Rights Act of 1991 (Saad and Sackett 2002). A more conservative approach would be to develop a structured interview process that allows for the use of a common regression line across legally protected groups. 42.2 Tutorial This chapter’s tutorial demonstrates how to use moderated multiple linear regression in the service of detecting differential prediction in a selection tool. 42.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/3n2vvMc4yHs 42.2.2 Functions &amp; Packages Introduced Function Package mean base R Regression lessR reg_brief lessR probe_interaction interactions 42.2.3 Initial Steps If you haven’t already, save the file “DifferentialPrediction.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file “DifferentialPrediction.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object DiffPred &lt;- read_csv(&quot;DifferentialPrediction.csv&quot;) ## Rows: 320 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): emp_id, gender, race ## dbl (3): perf_eval, interview, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(DiffPred) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;gender&quot; &quot;age&quot; &quot;race&quot; # View variable type for each variable in data frame str(DiffPred) ## spc_tbl_ [320 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:320] &quot;EE200&quot; &quot;EE202&quot; &quot;EE203&quot; &quot;EE206&quot; ... ## $ perf_eval: num [1:320] 6.2 6.6 5 4 5.3 5.9 5.5 3.8 5.7 3.8 ... ## $ interview: num [1:320] 8.5 5.7 7 6.8 9.6 6 7.5 5.9 9.6 8.3 ... ## $ gender : chr [1:320] &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; ... ## $ age : num [1:320] 30 45.6 39.2 36.9 24.5 41.5 35.1 43.2 25.7 27.8 ... ## $ race : chr [1:320] &quot;black&quot; &quot;black&quot; &quot;asian&quot; &quot;black&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. race = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(DiffPred) ## # A tibble: 6 × 6 ## emp_id perf_eval interview gender age race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EE200 6.2 8.5 woman 30 black ## 2 EE202 6.6 5.7 woman 45.6 black ## 3 EE203 5 7 man 39.2 asian ## 4 EE206 4 6.8 woman 36.9 black ## 5 EE209 5.3 9.6 woman 24.5 white ## 6 EE214 5.9 6 woman 41.5 asian There are 6 variables and 370 cases (i.e., employees) in the DiffPred data frame: emp_id, perf_eval, interview, age, gender, and race. Per the output of the str (structure) function above, the variables perf_eval, interview, and age are of type numeric (continuous: interval/ratio), and the variables emp_id, gender, and race are of type character (nominal/categorical). The variable emp_id is the unique employee identifier. Imagine that these data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered a rated structured interview (interview) 90 days after entering the organization. The structured interview (interview) variable was designed to assess individuals’ interpersonal skills, and ratings can range from 1 (very weak interpersonal skills) to 10 (very strong interpersonal skills). The interviews were scored by untrained raters who were often the hiring managers but not always. The perf_eval variable is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, with possible ratings ranging from 1-7, with 7 indicating high performance. The age variable represents the job incumbents’ ages (in years). The gender variable represents the job incumbents’ tender identify and is defined by two levels/categories/values: man and woman. Finally, the race variable represents the job incumbents’ race/ethnicity and is defined by three levels/categories/values: asian, black, and white. 42.2.4 Grand-Mean Center Continuous Predictor Variables To improve the interpretability of our model and to reduce the collinearity between the predictor and moderator variables with their interaction term, we will grand-mean center continuous predictor and moderator variables. [Note: A moderator variable is just a predictor variable that we are framing as a moderator.] With regard to interpretability, grand-mean centering aids in our interpretation of the regression coefficients associated with our predictor and moderator variables in our MMLR by reducing collinearity between these variables and their interaction term and by estimating the y-intercept when all predictors are at their respective means (averages), as opposed to when they are at zero (which is not always a plausible or meaningful value for some variables). Further, when interpreting the association between one predictor variable and the outcome variable while statistically controlling for other variables in the model, grand-mean centering allows us to interpret the effect of the focal predictor variable when the other predictor variables are at their respective means (as opposed to zero). For more detailed information on centering, check out the chapter on centering and standardizing variables. We only center predictor and moderator variables that are of type numeric and that we conceptualize as having a continuous (interval or ratio) measurement scale. In our current data frame, we will grand-mean center just the interview and age variables. We will use what is perhaps the most intuitive approach for grand-mean centering variables. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variable’s names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (DiffPred), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each case’s value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction operator (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (DiffPred), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. Now repeat those same steps for the age variable. # Grand-mean centering: Using basic arithmetic and the mean function from base R DiffPred$c_interview &lt;- DiffPred$interview - mean(DiffPred$interview, na.rm=TRUE) DiffPred$c_age &lt;- DiffPred$age - mean(DiffPred$age, na.rm=TRUE) 42.2.5 Estimate Moderated Multiple Linear Regression Model As described above, we will use the standard three-step differential prediction assessment using MMLR. We will repeat this three step process for all three protected class variables in our data frame: gender, c_age, and race. [Note that we are using the grand-mean centered age variable we created above.]. Our selection tool of interest is our c_interview (which is the grand-mean centered version of the variable we created above), and our criterion of interest is perf_eval. I will show you how to test differential prediction for each of these protected class variables using the Regression (or reg_brief) function from lessR (Gerbing, Business, and University 2021). Note: To reduce the overall length of this tutorial, I have omitted the diagnostic tests of the statistical assumptions, and thus for the purposes of this tutorial, we will assume that the statistical assumptions have been met. For your own work, be sure to run diagnostic tests to evaluate whether your model meets these statistical assumptions. For more information on statistical assumption testing in this context, check out the chapter on estimating increment validity using multiple linear regression. If you haven’t already install and access the lessR package using the install.packages and library functions, respectively (see above). This will allow us to access the Regression and reg_brief functions. # Install lessR package if you haven&#39;t already install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) 42.2.5.1 Test Differential Prediction Based on gender Let’s start with investigating whether differential prediction exists for our interview selection tool in relation to perf_eval based gender subgroup membership (i.e., man, woman). Step One: Let’s being by regressing the criterion variable perf_eval on the selection tool variable c_interview. Here we are just establishing whether the selection tool shows evidence of criterion-related validity. See the chapters on predicting criterion scores using simple linear regression if you would like more information on regression-based approaches to assessing criterion-related validity, including determining whether statistical assumptions have been met. You can use either the Regression or reg_brief function from lessR, where the latter provides less output. To keep the amount of output in this tutorial to a minimum, I will use the reg_brief function, but when you evaluate whether statistical assumptions have been met, be sure to use the full Regression function. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.708 0.054 86.472 0.000 4.600 4.815 ## c_interview 0.192 0.041 4.629 0.000 0.110 0.273 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.974 for 318 degrees of freedom ## 95% range of residual variation: 3.832 = 2 * (1.967 * 0.974) ## ## R-squared: 0.063 Adjusted R-squared: 0.060 PRESS R-squared: 0.051 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 21.425 df: 1 and 318 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 20.319 20.319 21.425 0.000 ## Residuals 318 301.583 0.948 ## perf_eval 319 321.902 1.009 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The unstandardized regression coefficient for c_interview is statistically significant and positive (b = .192, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small-medium in magnitude (R2 = .063, adjusted R2 = .060, p &lt; .001); that is, approximately 6% of the variability in perf_eval can be explained by c_interview. See the table below for common rules of thumbs for qualitatively describing R2 values. R2 Description .01 Small .09 Medium .25 Large Because Step One revealed a statistically significant association between c_interview and perf_eval – and thus evidence of criterion-related validity for the selection tool – we will proceed to Step Two and Step Three. Had we not found evidence of criterion-related validity in Step One, we would have stopped at that step; after all, it won’t be worthwhile to look for evidence of differential prediction if we won’t be using the selection tool moving forward (given that it’s not job related). Step Two: Next, let’s add the protected class variable gender to our model to investigate whether intercept differences exist. For more information on how to interpret and test the statistical assumptions of a multiple linear regression model, check out the chapter on estimating incremental validity using multiple linear regression. Note that this model can be called an additive model. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (gender) reg_brief(perf_eval ~ c_interview + gender, data=DiffPred) ## ## &gt;&gt;&gt; gender is not numeric. Converted to indicator variables. ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + gender, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: genderwoman ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.630 0.076 60.719 0.000 4.480 4.780 ## c_interview 0.211 0.043 4.861 0.000 0.125 0.296 ## genderwoman 0.166 0.114 1.458 0.146 -0.058 0.391 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.972 for 317 degrees of freedom ## 95% range of residual variation: 3.825 = 2 * (1.967 * 0.972) ## ## R-squared: 0.069 Adjusted R-squared: 0.063 PRESS R-squared: NA ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 11.813 df: 2 and 317 p-value: 0.000 ## ## -- Analysis of Variance from Type II Sums of Squares ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 22.328 22.328 23.626 0.000 ## genderwoman 1 2.009 2.009 2.125 0.146 ## Residuals 317 299.574 0.945 ## ## -- Test of Interaction ## ## c_interview:gender df: 1 df resid: 316 SS: 17.514 F: 19.622 p-value: 0 ## ## -- Assume parallel lines, no interaction of gender with c_interview ## ## Level man: y^_perf_eval = 4.630 + 0.211(x_c_interview) ## Level woman: y^_perf_eval = 4.796 + 0.211(x_c_interview) ## ## -- Visualize Separately Computed Regression Lines ## ## Plot(c_interview, perf_eval, by=gender, fit=&quot;lm&quot;) ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The unstandardized regression coefficient associated with gender (or more specifically genderwoman) is not statistically significant (b = .166, p = .146) when controlling for c_interview, which indicates that there is no evidence of intercept differences. Note that the name of the regression coefficient for gender has woman appended to it; this means that the intercept represents the perf_eval scores of women minus those for men. Behind the scenes, the reg_brief function has converted the gender variable to a dichotomous factor, where alphabetically man becomes 0 and woman becomes 1. Because the coefficient is negative, it implies that the intercept value is higher for women relative to men; however, please note that we wouldn’t interpret the direction of this effect because it was not statistically significant. The amount of variance explained by c_interview and gender in perf_eval is small-medium in magnitude (R2 = .069, adjusted R2 = .063, p &lt; .001), which is slightly larger than when just c_interview was included in the model (see Step One above). Because there is no evidence of intercept differences for Step Two, we won’t plot/graph the our model, as doing so might mislead someone who sees such a plot without knowing or understanding the results of the model we just estimated. We will now proceed to Step Three. Step Three: As the final step, it’s time to add the interaction term (also known as a product term) between c_interview and gender to the model, which creates what is referred to as a multiplicative model. Fortunately, this is very easy to do in R. All we need to do is change the + operator from our previous regression model script to the *, where the * implies an interaction in the context of a model. Conceptually, the * is appropriate because we are estimating a multiplicative model containing an interaction term. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * gender, data=DiffPred) ## ## &gt;&gt;&gt; gender is not numeric. Converted to indicator variables. ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * gender, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: genderwoman ## Predictor Variable 3: c_interview.genderwoman ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.562 0.076 60.298 0.000 4.413 4.711 ## c_interview 0.394 0.059 6.672 0.000 0.278 0.511 ## genderwoman 0.155 0.111 1.397 0.163 -0.063 0.373 ## c_interview.genderwoman -0.373 0.084 -4.430 0.000 -0.539 -0.207 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.945 for 316 degrees of freedom ## 95% range of residual variation: 3.718 = 2 * (1.967 * 0.945) ## ## R-squared: 0.124 Adjusted R-squared: 0.115 PRESS R-squared: NA ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 14.879 df: 3 and 316 p-value: 0.000 ## ## -- Analysis of Variance from Type II Sums of Squares ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 22.328 22.328 23.626 0.000 ## genderwoman 1 2.009 2.009 2.250 0.135 ## c_interview.genderwoman 1 17.514 17.514 19.622 0.000 ## Residuals 316 282.060 0.893 ## ## -- Test of Interaction ## ## c_interview:gender df: 1 df resid: 316 SS: 17.514 F: 19.622 p-value: 0 ## ## -- Assume parallel lines, no interaction of gender with c_interview ## ## Level man: y^_perf_eval = 4.562 + 0.394(x_c_interview) ## Level woman: y^_perf_eval = 4.717 + 0.394(x_c_interview) ## ## -- Visualize Separately Computed Regression Lines ## ## Plot(c_interview, perf_eval, by=gender, fit=&quot;lm&quot;) ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR In our output, we now have an unstandardized regression coefficient called c_interview:gender (or c_interview.gender), which represents our interaction term for c_interview and gender. The regression coefficient associated with the interaction term (c_interview:gender) is negative and statistically significant (b = -.373, p &lt; .001), but we shouldn’t pay too much attention to the sign of the interaction term as it is difficult to interpret it without graphing the regression model in its entirety. We should, however, pay attention to the fact that the interaction term is statistically significant, which indicates that there is evidence of slope differences; in other words, there seems to be multiplicative effect between c_interview and gender. Although we didn’t find evidence of intercept differences in Step Two, here in Step Three, we do find evidence that the criterion-related validities for the selection tool (c_interview) in relation to the criterion (perf_eval) are significantly different from one another; in other words, we found evidence that the slopes for men and women differ. The amount of variance explained by c_interview, gender, and their interaction in relation to perf_eval is medium in magnitude (R2 = .124, adjusted R2 = .115, p &lt; .001), which is larger than when just c_interview and gender were included in the model (see Step Two above). Given the statistically significant interaction term, it is appropriate (and helpful) to probe the interaction by plotting it. This will help us understand the form of the interaction. We can do so by creating a plot of the simple slopes. To do so, we will use a package called interactions (Long 2019), so if you haven’t already, install and access the package. # Install interactions package if you haven&#39;t already install.packages(&quot;interactions&quot;) # Note: You may need to install the sandwich package independently if you # receive an error when attempting to run the probe_interaction function # install.packages(&quot;sandwich&quot;) # Access interactions package library(interactions) As input to the probe_interaction function from the interactions package, we will need to use the lm function base R to specify the same regression model we estimated above. We can just copy and paste the same arguments from the reg_brief function into the lm function parentheses. We need to name this model something so that we can reference it in the next function, so let’s name it Slope_Model (for slope differences model) using the &lt;- operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class Slope_Model &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) Now we’re ready to apply the probe_interaction in order to plot this multiplicative model. As the first argument, enter the name of the regression model we just created above (Slope_Model). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we aren’t requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Probe the significant interaction (slope differences) probe_interaction(Slope_Model, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.39 0.06 6.67 0.00 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.02 0.06 0.35 0.72 The plot output shows that the two slopes are indeed different, such that there doesn’t seem to be much of an association between the selection tool (c_interview) and the criterion (perf_eval) for women, but there seems to be a positive association for men. Now take a look at the text output in your console. The simple slopes analysis automatically estimates the regression coefficient (slope) for both levels of the moderator variable (gender: man, woman). Corroborating what we see in the plot, we find that the simple slope for men is indeed statistically significant and positive (b = .39, p &lt; .01), such that for every one unit increase in interview scores, job performance scores tend to go up by .39 units. In contrast, we find that the simple slope for women is nonsignificant (b = .02, p = .72). In sum, we found evidence of slope differences – but not intercept differences – for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the levels/subgroups of the protected class variable (gender). Thus, we can conclude that there is evidence of differential prediction based on gender for this interview selection tool, which means that it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. Sample Write-Up: Our HR analytics team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction of the structured interview based on the U.S. protected class of gender. Based on a sample of 370 individuals, we applied a three-step process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .192, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, but did not find evidence of intercept differences based on gender (b = .166, p = .146). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between structured interview scores and job performance scores to a statistically significant extent (b = -.373, p &lt; .001). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant for men (b = .39, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .39 units; in contrast, the associated between structured interview scores and job performance scores for women was nonsignificant (b = .02, p = .72). In sum, we found evidence slope differences, which means that this structured interview tool shows predictive bias with respect to gender, making the application of a common regression line (i.e., equation) legally problematic within the United States. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we caution against using separate regression lines for men and women, as this may be interpreted as running afoul of the U.S. Civil Rights Act of 1991 (Saad and Sackett 2002). A more conservative approach would be to develop a structured interview process that allows for the use of a common regression line across legally protected groups. 42.2.5.2 Test Differential Prediction Based on c_age Investigating whether differential prediction exists for c_interview in relation to perf_eval based on the continuous moderator variable c_age will unfold very similarly to the process we used for the dichotomous gender variable from above. The only differences will occur when it comes to estimating and visualizing the intercept differences and simple slopes (assuming we find statistically significant effects). Given this, we will breeze through the steps, which means I will provide less explanation. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.708 0.054 86.472 0.000 4.600 4.815 ## c_interview 0.192 0.041 4.629 0.000 0.110 0.273 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.974 for 318 degrees of freedom ## 95% range of residual variation: 3.832 = 2 * (1.967 * 0.974) ## ## R-squared: 0.063 Adjusted R-squared: 0.060 PRESS R-squared: 0.051 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 21.425 df: 1 and 318 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 20.319 20.319 21.425 0.000 ## Residuals 318 301.583 0.948 ## perf_eval 319 321.902 1.009 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The regression coefficient for c_interview is statistically significant and positive (b = .192, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small-medium in magnitude (R2 = .063, adjusted R2 = .060, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable c_age. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) reg_brief(perf_eval ~ c_interview + c_age, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + c_age, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: c_age ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.708 0.050 94.164 0.000 4.609 4.806 ## c_interview 0.978 0.108 9.028 0.000 0.765 1.191 ## c_age 0.129 0.017 7.752 0.000 0.096 0.161 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.894 for 317 degrees of freedom ## 95% range of residual variation: 3.519 = 2 * (1.967 * 0.894) ## ## R-squared: 0.212 Adjusted R-squared: 0.207 PRESS R-squared: 0.192 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 42.749 df: 2 and 317 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 20.319 20.319 25.406 0.000 ## c_age 1 48.059 48.059 60.092 0.000 ## ## Model 2 68.378 34.189 42.749 0.000 ## Residuals 317 253.524 0.800 ## perf_eval 319 321.902 1.009 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The regression coefficient associated with c_age is positive and statistically significant (b = .129, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is positive, it implies that the intercept value is significantly higher for older workers relative to younger workers. The amount of variance explained by c_interview and c_age in perf_eval is medium-large in magnitude (R2 = .212, adjusted R2 = .207, p &lt; .001), which is notably larger than when just c_interview was included in the model (see Step One above). Using the probe_interaction function from the interactions package, we will visualize the intercept differences. As input to the probe_interaction function, we will use the lm function base R to specify our regression model from above. Name this model something so that we can reference it in the next function (Int_Model). # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) Int_Model &lt;- lm(perf_eval ~ c_interview + c_age, data=DiffPred) Now, we will specify the arguments in the probe_interaction function. Note that the function automatically categorizes the continuous moderator variable by its mean and 1 standard deviation (SD) below and above the mean. As you can see in the plot, individuals whose age is 1 SD above the mean have the highest intercept value, where the intercept is the value of perf_eval (Job Performance) when interview (Interview) is equal to zero. # Visualize intercept differences probe_interaction(Int_Model, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.593122204199181268791 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 ## ## Slope of c_interview when c_age = -0.000000000000003164136 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 ## ## Slope of c_interview when c_age = 8.593122204199174163364 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 Step Three: Now, we will add the interaction term between c_interview and c_age to the model using the * operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (c_age), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * c_age, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * c_age, data=DiffPred, Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: c_age ## ## Number of cases (rows) of data: 320 ## Number of cases retained for analysis: 320 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.995 0.068 73.937 0.000 4.862 5.128 ## c_interview 1.711 0.160 10.700 0.000 1.397 2.026 ## c_age 0.263 0.027 9.590 0.000 0.209 0.317 ## c_interview:c_age 0.027 0.005 5.986 0.000 0.018 0.036 ## ## Standard deviation of perf_eval: 1.005 ## ## Standard deviation of residuals: 0.849 for 316 degrees of freedom ## 95% range of residual variation: 3.340 = 2 * (1.967 * 0.849) ## ## R-squared: 0.293 Adjusted R-squared: 0.286 PRESS R-squared: 0.275 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 43.573 df: 3 and 316 p-value: 0.000 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 20.319 20.319 28.198 0.000 ## c_age 1 48.059 48.059 66.694 0.000 ## c_interview:c_age 1 25.817 25.817 35.827 0.000 ## ## Model 3 94.195 31.398 43.573 0.000 ## Residuals 316 227.707 0.721 ## perf_eval 319 321.902 1.009 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The regression coefficient associated with the interaction term (c_interview:c_age) is positive and statistically significant (b = .027, p &lt; .001), which indicates that there is evidence of slope differences. The amount of variance explained by c_interview, c_age, and their interaction in relation to perf_eval is large in magnitude (R2 = .293, adjusted R2 = .286, p &lt; .001), which is larger than when just c_interview and c_age were included in the model. Given the statistically significant interaction term, we will use the probe_interaction function from the interactions package to probe the interaction. As input to the probe_interaction function from the interactions package, we will need to use the lm function base R to specify our regression model from above. We need to name this model something so that we can reference it in the next function, so let’s name it Slope_Model (for slope differences model) using the &lt;- operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (c_age), # and interaction between selection tool and protected class Slope_Model &lt;- lm(perf_eval ~ c_interview * c_age, data=DiffPred) We’re ready to use the probe_interaction to plot this multiplicative model. # Probe the significant interaction (slope differences) probe_interaction(Slope_Model, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.593122204199181268791 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.48 0.13 11.16 0.00 ## ## Slope of c_interview when c_age = -0.000000000000003164136 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.71 0.16 10.70 0.00 ## ## Slope of c_interview when c_age = 8.593122204199174163364 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.95 0.19 10.16 0.00 The plot and the simple slopes analyses output show that the two slopes are indeed different, such that the association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) is more positive for older workers (b = 1.95, p &lt; .01), than for average-aged workers (b = 1.71, p &lt; .01) and younger workers (b = 1.48, p &lt; .01). Notably, all three simple slopes are statistically significant and positive, but the slope is more positive for older workers. In sum, we found evidence of both intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the age protected class variable (c_age). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.2.5.3 Test Differential Prediction Based on race To investigate whether differential prediction exists for c_interview in relation to perf_eval based on race, we will follow mostly the same process as we did with the dichotomous variable gender above. However, we will filter our data within the regression function to compare just Black individuals with White individuals, as it is customary to compare just two races/ethnicities at a time. The decision to focus on only Black and White individuals is arbitrary and just for demonstration purposes. Note that in the DiffPred data frame object that both black and white are lowercase category labels in the race variable. Step one: Regress the criterion variable perf_eval on the selection tool variable c_interview. To subset the data such that only individuals who are Black or White are retained, we will add the following argument: rows=(race==\"black\" | race==\"white\"). This argument uses conditional statements to indicate that we want to retain those cases for which race is equal to Black or for which race is equal to White. We have effectively dichotomized the race variable using this approach. If you need to review conditional/logical statements when filtering data, check out the chapter on filtering data. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred, rows=(race==&quot;black&quot; | race==&quot;white&quot;)) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, rows=(race == &quot;black&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 212 ## Number of cases retained for analysis: 212 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.715 0.068 69.164 0.000 4.580 4.849 ## c_interview 0.140 0.052 2.672 0.008 0.037 0.243 ## ## Standard deviation of perf_eval: 1.004 ## ## Standard deviation of residuals: 0.989 for 210 degrees of freedom ## 95% range of residual variation: 3.901 = 2 * (1.971 * 0.989) ## ## R-squared: 0.033 Adjusted R-squared: 0.028 PRESS R-squared: 0.014 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 7.139 df: 1 and 210 p-value: 0.008 ## ## -- Analysis of Variance ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 6.990 6.990 7.139 0.008 ## Residuals 210 205.600 0.979 ## perf_eval 211 212.590 1.008 ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR We should begin by noting that our effective sample size is now 212, whereas previously it was 370 when we included individuals of all races. The regression coefficient for c_interview is statistically significant and positive (b = .140, p = .008) based on this reduced sample of 212 individuals, which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small in magnitude (R2 = .033, adjusted R2 = .028, p &lt; .001). Step Two: We will regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable race. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) reg_brief(perf_eval ~ c_interview + race, data=DiffPred, rows=(race==&quot;black&quot; | race==&quot;white&quot;)) ## ## &gt;&gt;&gt; race is not numeric. Converted to indicator variables. ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + race, data=DiffPred, rows=(race == &quot;black&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: racewhite ## ## Number of cases (rows) of data: 212 ## Number of cases retained for analysis: 212 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.911 0.089 54.922 0.000 4.735 5.087 ## c_interview 0.127 0.051 2.469 0.014 0.026 0.228 ## racewhite -0.440 0.134 -3.287 0.001 -0.705 -0.176 ## ## Standard deviation of perf_eval: 1.004 ## ## Standard deviation of residuals: 0.967 for 209 degrees of freedom ## 95% range of residual variation: 3.813 = 2 * (1.971 * 0.967) ## ## R-squared: 0.080 Adjusted R-squared: 0.072 PRESS R-squared: NA ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 9.139 df: 2 and 209 p-value: 0.000 ## ## -- Analysis of Variance from Type II Sums of Squares ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 5.700 5.700 6.094 0.014 ## racewhite 1 10.107 10.107 10.806 0.001 ## Residuals 209 195.493 0.935 ## ## -- Test of Interaction ## ## c_interview:race df: 1 df resid: 208 SS: 3.343 F: 3.618 p-value: 0.059 ## ## -- Assume parallel lines, no interaction of race with c_interview ## ## Level black: y^_perf_eval = 4.911 + 0.127(x_c_interview) ## Level white: y^_perf_eval = 4.470 + 0.127(x_c_interview) ## ## -- Visualize Separately Computed Regression Lines ## ## Plot(c_interview, perf_eval, by=race, fit=&quot;lm&quot;) ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The regression coefficient associated with race (or more specifically racewhite) is negative and statistically significant (b = -.440, p = .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is negative and because white is appended to the race variable name, it implies that the intercept value is significantly lower for White individuals compared to Black individuals. The amount of variance explained by c_interview and race in perf_eval is small-medium in magnitude (R2 = .080, adjusted R2 = .072, p &lt; .001). Using the probe_interaction function from the interactions package, we will visualize the intercept differences. As input to the probe_interaction function, use the lm function base R to specify our regression model from above. The subset= argument functions the same way as the rows= argument from the reg_brief and Regression functions from lessR, so we’ll use that instead for the lm function. Name this model something so that we can reference it in the next function (Int_Model). # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) Int_Model &lt;- lm(perf_eval ~ c_interview + race, data=DiffPred, subset=(race==&quot;black&quot; | race==&quot;white&quot;)) Let’s specify the arguments in the probe_interaction function to plot the intercept differences. # Visualize intercept differences probe_interaction(Int_Model, pred=c_interview, modx=race, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when race = white: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.13 0.05 2.47 0.01 ## ## Slope of c_interview when race = black: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.13 0.05 2.47 0.01 Step Three: Finally, let’s add the interaction term between c_interview and race to the model using the * operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (race), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * race, data=DiffPred, rows=(race==&quot;black&quot; | race==&quot;white&quot;)) ## ## &gt;&gt;&gt; race is not numeric. Converted to indicator variables. ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * race, data=DiffPred, rows=(race == &quot;black&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: racewhite ## Predictor Variable 3: c_interview.racewhite ## ## Number of cases (rows) of data: 212 ## Number of cases retained for analysis: 212 ## ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.912 0.089 55.275 0.000 4.737 5.087 ## c_interview 0.218 0.070 3.112 0.002 0.080 0.357 ## racewhite -0.463 0.134 -3.466 0.001 -0.727 -0.200 ## c_interview.racewhite -0.194 0.102 -1.902 0.059 -0.396 0.007 ## ## Standard deviation of perf_eval: 1.004 ## ## Standard deviation of residuals: 0.961 for 208 degrees of freedom ## 95% range of residual variation: 3.790 = 2 * (1.971 * 0.961) ## ## R-squared: 0.096 Adjusted R-squared: 0.083 PRESS R-squared: NA ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 7.375 df: 3 and 208 p-value: 0.000 ## ## -- Analysis of Variance from Type II Sums of Squares ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 5.700 5.700 6.094 0.014 ## racewhite 1 10.107 10.107 10.941 0.001 ## c_interview.racewhite 1 3.343 3.343 3.618 0.059 ## Residuals 208 192.150 0.924 ## ## -- Test of Interaction ## ## c_interview:race df: 1 df resid: 208 SS: 3.343 F: 3.618 p-value: 0.059 ## ## -- Assume parallel lines, no interaction of race with c_interview ## ## Level black: y^_perf_eval = 4.912 + 0.218(x_c_interview) ## Level white: y^_perf_eval = 4.448 + 0.218(x_c_interview) ## ## -- Visualize Separately Computed Regression Lines ## ## Plot(c_interview, perf_eval, by=race, fit=&quot;lm&quot;) ## ## ## K-FOLD CROSS-VALIDATION ## ## ## RELATIONS AMONG THE VARIABLES ## ## ## RESIDUALS AND INFLUENCE ## ## ## PREDICTION ERROR The regression coefficient associated with the interaction term (c_interview:racewhite) is not statistically significant (b = -.194, p = .059), which indicates that there is no evidence of slope differences. Given this, we won’t probe the interaction because, well, there is not an interaction to probe. In sum, we found evidence of intercept differences for the selection tool c_interview in relation to the criterion perf_eval that were conditional upon the protected class variable race. Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.2.6 Summary In this chapter, we learned how to use moderated multiple linear regression (MMLR) to test whether evidence of differential prediction exists. To do so, we learned how to use the Regression function (well, technically we used the reg_brief function) from lessR. In addition, we used the probe_interaction function from the interactions package to probe the intercept differences and slope differences visually and using simple slopes analyses. 42.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a moderated multiple linear regression (MMLR) model. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 42.3.1 Functions &amp; Packages Introduced Function Package mean base R lm base R summary base R anova base R probe_interaction interactions apa.reg.table apaTables 42.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object DiffPred &lt;- read_csv(&quot;DifferentialPrediction.csv&quot;) ## Rows: 320 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): emp_id, gender, race ## dbl (3): perf_eval, interview, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(DiffPred) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;gender&quot; &quot;age&quot; &quot;race&quot; # View variable type for each variable in data frame str(DiffPred) ## spc_tbl_ [320 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:320] &quot;EE200&quot; &quot;EE202&quot; &quot;EE203&quot; &quot;EE206&quot; ... ## $ perf_eval: num [1:320] 6.2 6.6 5 4 5.3 5.9 5.5 3.8 5.7 3.8 ... ## $ interview: num [1:320] 8.5 5.7 7 6.8 9.6 6 7.5 5.9 9.6 8.3 ... ## $ gender : chr [1:320] &quot;woman&quot; &quot;woman&quot; &quot;man&quot; &quot;woman&quot; ... ## $ age : num [1:320] 30 45.6 39.2 36.9 24.5 41.5 35.1 43.2 25.7 27.8 ... ## $ race : chr [1:320] &quot;black&quot; &quot;black&quot; &quot;asian&quot; &quot;black&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. race = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(DiffPred) ## # A tibble: 6 × 6 ## emp_id perf_eval interview gender age race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EE200 6.2 8.5 woman 30 black ## 2 EE202 6.6 5.7 woman 45.6 black ## 3 EE203 5 7 man 39.2 asian ## 4 EE206 4 6.8 woman 36.9 black ## 5 EE209 5.3 9.6 woman 24.5 white ## 6 EE214 5.9 6 woman 41.5 asian 42.3.3 lm Function from Base R We will use the lm function from base R to specify and estimate our regression functions. As I did above, I will show you how to apply moderated multiple linear regression (MMLR) in the service of detecting differential prediction of a selection tool based on protected class membership. Grand-Mean Center Variables: Before we estimate the regression models, however, we need to grand-mean center certain variables. We only center predictor and moderator variables that are of type numeric and that we conceptualize as having a continuous (interval or ratio) measurement scale. In our current data frame, we will grand-mean center just the interview and age variables. We will use what is perhaps the most intuitive approach for grand-mean centering variables. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variable’s names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (DiffPred), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each case’s value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction operator (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (DiffPred), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. Now repeat those same steps for the age variable. # Grand-mean centering: Using basic arithmetic and the mean function from base R DiffPred$c_interview &lt;- DiffPred$interview - mean(DiffPred$interview, na.rm=TRUE) DiffPred$c_age &lt;- DiffPred$age - mean(DiffPred$age, na.rm=TRUE) 42.3.3.1 Test Differential Prediction Based on gender Let’s start with investigating whether differential prediction exists for our interview selection tool in relation to perf_eval based gender subgroup membership (i.e., man, woman). Step One: Let’s being by regressing the criterion variable perf_eval on the selection tool variable c_interview. Here we are just establishing whether the selection tool shows evidence of criterion-related validity. See the chapters on criterion-related validity using correlation and predicting criterion scores using simple linear regression if you would like more information on regression-based approaches to assessing criterion-related validity, including determining whether statistical assumptions have been met. Let’s name our regression model for this first step model.1. Remember, we need to use the summary function from base R to view our model output. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7226 -0.6846 0.0007 0.7494 2.5842 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70750 0.05444 86.472 &lt; 0.0000000000000002 *** ## c_interview 0.19175 0.04143 4.629 0.00000537 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9738 on 318 degrees of freedom ## Multiple R-squared: 0.06312, Adjusted R-squared: 0.06018 ## F-statistic: 21.43 on 1 and 318 DF, p-value: 0.000005366 The regression coefficient for c_interview is statistically significant and positive (b = .192, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small-medium in magnitude (R2 = .063, adjusted R2 = .060, p &lt; .001). See the table below for common rules of thumbs for qualitatively describing R2 values. R2 Description .01 Small .09 Medium .25 Large Step Two: Next, let’s add the protected class variable gender to our model to investigate whether intercept differences exist. We’ll name this model model.2. For more information on how to interpret and test the statistical assumptions of a multiple linear regression model, check out the chapter on estimating incremental validity. In addition, let’s use the anova function from base R to do a nested model comparison between model.1 and model.2. This will tell us whether adding gender to the model significantly improved model fit. Finally, let’s reference the summary/output for each model and specifically the r.squared values to determine what the change in R2 was from one model to the next. The change in R2 gives as an idea of how much (in terms of practical significance) the model fit improved when adding the additional variable. Note that we use the $ operator to indicate that we want the r.squared value from the summary(model.2) and summary(model.2) output, and then we do a simple subtraction to get the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (gender) model.2 &lt;- lm(perf_eval ~ c_interview + gender, data=DiffPred) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + gender, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8125 -0.6629 0.0049 0.7578 2.5245 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.62953 0.07625 60.719 &lt; 0.0000000000000002 *** ## c_interview 0.21058 0.04332 4.861 0.00000184 *** ## genderwoman 0.16634 0.11409 1.458 0.146 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9721 on 317 degrees of freedom ## Multiple R-squared: 0.06936, Adjusted R-squared: 0.06349 ## F-statistic: 11.81 on 2 and 317 DF, p-value: 0.00001127 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 318 301.58 ## 2 317 299.57 1 2.0086 2.1255 0.1459 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.006239901 The unstandardized regression coefficient associated with gender (or more specifically genderwoman) is not statistically significant (b = .166, p = .146) when controlling for c_interview, which indicates that there is no evidence of intercept differences. Note that the name of the regression coefficient for gender has woman appended to it; this means that the intercept represents the perf_eval scores of women minus those for men. Behind the scenes, the reg_brief function has converted the gender variable to a dichotomous factor, where alphabetically man becomes 0 and woman becomes 1. Because the coefficient is negative, it implies that the intercept value is higher for women relative to men; however, please note that we wouldn’t interpret the direction of this effect because it was not statistically significant. The amount of variance explained by c_interview and gender in perf_eval is small-medium in magnitude (R2 = .069, adjusted R2 = .063, p &lt; .001), which is slightly larger than when just c_interview was included in the model (see Step One above). The nonsignificant F-value (2.126, p = .146) in from the nested model comparison indicates that adding gender to the model did not significantly improve model fit, and thus we will not interpret the change in R2. Because there is no evidence of intercept differences for Step Two, we won’t plot/graph the our model, as doing so might mislead someone who sees such a plot without knowing or understanding the results of the model we just estimated. We will now proceed to Step Three. Step Three: As the final step, it’s time to add the interaction term (also known as a product term) between c_interview and gender to the model, which creates what is referred to as a multiplicative model. Fortunately, this is very easy to do in R. All we need to do is change the + operator from our previous regression model script to the *, where the * implies an interaction in the context of a model. Conceptually, the * is appropriate because we are estimating a multiplicative model containing an interaction term. Let’s name this model model.3. As before, we’ll do a nested model comparison between the previous model and this model to determine whether there is a significant improvement in model fit. In addition, we’ll estimate change in R2. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class model.3 &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * gender, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.71851 -0.67772 0.01742 0.69477 2.48590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.56191 0.07566 60.298 &lt; 0.0000000000000002 *** ## c_interview 0.39426 0.05909 6.672 0.000000000113 *** ## genderwoman 0.15494 0.11091 1.397 0.163 ## c_interview:genderwoman -0.37306 0.08422 -4.430 0.000013026266 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9448 on 316 degrees of freedom ## Multiple R-squared: 0.1238, Adjusted R-squared: 0.1155 ## F-statistic: 14.88 on 3 and 316 DF, p-value: 0.000000004387 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + gender ## Model 2: perf_eval ~ c_interview * gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 317 299.57 ## 2 316 282.06 1 17.514 19.622 0.00001303 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.05440861 In our output, we now have an unstandardized regression coefficient called c_interview:gender (or c_interview.gender), which represents our interaction term for c_interview and gender. The regression coefficient associated with the interaction term (c_interview:gender) is negative and statistically significant (b = -.373, p &lt; .001), but we shouldn’t pay too much attention to the sign of the interaction term as it is difficult to interpret it without graphing the regression model in its entirety. We should, however, pay attention to the fact that the interaction term is statistically significant, which indicates that there is evidence of slope differences; in other words, there seems to be multiplicative effect between c_interview and gender. Although we didn’t find evidence of intercept differences in Step Two, here in Step Three, we do find evidence that the criterion-related validities for the selection tool (c_interview) in relation to the criterion (perf_eval) are significantly different from one another; in other words, we found evidence that the slopes for men and women differ. The amount of variance explained by c_interview, gender, and their interaction in relation to perf_eval is medium in magnitude (R2 = .124, adjusted R2 = .115, p &lt; .001), which is larger than when just c_interview and gender were included in the model (see Step Two above). The significant F-value (19.622, p &lt; .001) in from the nested model comparison indicates that adding the interaction term significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .054), which indicates that adding the interaction term to the model explained an additional 5.4% of the variance in perf_eval. Given the statistically significant interaction term, it is appropriate (and helpful) to probe the interaction by plotting it. This will help us understand the form of the interaction. We can do so by creating a plot of the simple slopes. To do so, we will use a package called interactions (Long 2019), so if you haven’t already, install and access the package. # Install interactions package if you haven&#39;t already install.packages(&quot;interactions&quot;) # Note: You may need to install the sandwich package independently if you # receive an error when attempting to run the probe_interaction function # install.packages(&quot;sandwich&quot;) # Access interactions package library(interactions) As the first argument in the probe_interaction function from the interactions package, enter the name of the regression model we just created above (model.3). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we aren’t requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Probe the significant interaction (slope differences) probe_interaction(model.3, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.39 0.06 6.67 0.00 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.02 0.06 0.35 0.72 The plot output shows that the two slopes are indeed different, such that there doesn’t seem to be much of an association between the selection tool (c_interview) and the criterion (perf_eval) for women, but there seems to be a positive association for men. Now take a look at the text output in your console. The simple slopes analysis automatically estimates the regression coefficient (slope) for both levels of the moderator variable (gender: man, woman). Corroborating what we see in the plot, we find that the simple slope for men is indeed statistically significant and positive (b = .39, p &lt; .01), such that for every one unit increase in interview scores, job performance scores tend to go up by .39 units. In contrast, we find that the simple slope for women is nonsignificant (b = .02, p = .72). In sum, we found evidence of slope differences – but not intercept differences – for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the levels/subgroups of the protected class variable (gender). Thus, we can conclude that there is evidence of differential prediction based on gender for this interview selection tool, which means that it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. Sample Write-Up: Our HR analytics team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction of the structured interview based on the U.S. protected class of gender. Based on a sample of 370 individuals, we applied a three-step process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .192, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, but did not find evidence of intercept differences based on gender (b = .166, p = .146). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between structured interview scores and job performance scores to a statistically significant extent (b = -.373, p &lt; .001). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant for men (b = .39, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .39 units; in contrast, the associated between structured interview scores and job performance scores for women was nonsignificant (b = .02, p = .72). In sum, we found evidence slope differences, which means that this structured interview tool shows predictive bias with respect to gender, making the application of a common regression line (i.e., equation) legally problematic within the United States. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we caution against using separate regression lines for men and women, as this may be interpreted as running afoul of the U.S. Civil Rights Act of 1991 (Saad and Sackett 2002). A more conservative approach would be to develop a structured interview process that allows for the use of a common regression line across legally protected groups. 42.3.3.2 Test Differential Prediction Based on c_age Investigating whether differential prediction exists for c_interview in relation to perf_eval based on the continuous moderator variable c_age will unfold very similarly to the process we used for the dichotomous gender variable from above. The only differences will occur when it comes to estimating and visualizing the intercept differences and simple slopes (assuming we find statistically significant effects). Given this, we will breeze through the steps, which means I will provide less explanation. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7226 -0.6846 0.0007 0.7494 2.5842 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70750 0.05444 86.472 &lt; 0.0000000000000002 *** ## c_interview 0.19175 0.04143 4.629 0.00000537 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9738 on 318 degrees of freedom ## Multiple R-squared: 0.06312, Adjusted R-squared: 0.06018 ## F-statistic: 21.43 on 1 and 318 DF, p-value: 0.000005366 The regression coefficient for c_interview is statistically significant and positive (b = .192, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small-medium in magnitude (R2 = .063, adjusted R2 = .060, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable c_age. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) model.2 &lt;- lm(perf_eval ~ c_interview + c_age, data=DiffPred) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + c_age, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2596 -0.6087 0.0444 0.6184 2.5522 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70750 0.04999 94.164 &lt; 0.0000000000000002 *** ## c_interview 0.97806 0.10833 9.028 &lt; 0.0000000000000002 *** ## c_age 0.12863 0.01659 7.752 0.000000000000124 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8943 on 317 degrees of freedom ## Multiple R-squared: 0.2124, Adjusted R-squared: 0.2075 ## F-statistic: 42.75 on 2 and 317 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + c_age ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 318 301.58 ## 2 317 253.52 1 48.059 60.092 0.0000000000001241 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.1492979 The regression coefficient associated with c_age is positive and statistically significant (b = .129, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is positive, it implies that the intercept value is significantly higher for older workers relative to younger workers. The amount of variance explained by c_interview and c_age in perf_eval is medium-large in magnitude (R2 = .212, adjusted R2 = .207, p &lt; .001), which is notably larger than when just c_interview was included in the model (see Step One above). The significant F-value (60.092, p &lt; .001) in from the nested model comparison indicates that adding c_age significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .149), which indicates that adding c_age to the model explained an additional 14.9% of the variance in perf_eval. Using the probe_interaction function from the interactions package, we will visualize the intercept differences of the model.2 model we specified above. Note that the function automatically categorizes the continuous moderator variable by its mean and 1 standard deviation (SD) below and above the mean. As you can see in the plot, individuals whose age is 1 SD above the mean have the highest intercept value, where the intercept is the value of perf_eval (Job Performance) when interview (Interview) is equal to zero. # Visualize intercept differences probe_interaction(model.2, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.593122204199181268791 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 ## ## Slope of c_interview when c_age = -0.000000000000003164136 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 ## ## Slope of c_interview when c_age = 8.593122204199174163364 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.98 0.11 9.03 0.00 Step Three: Now, we will add the interaction term between c_interview and c_age to the model using the * operator. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) model.3 &lt;- lm(perf_eval ~ c_interview * c_age, data=DiffPred) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * c_age, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6271 -0.5798 0.0261 0.5553 2.4924 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.995359 0.067563 73.937 &lt; 0.0000000000000002 *** ## c_interview 1.711274 0.159937 10.700 &lt; 0.0000000000000002 *** ## c_age 0.263061 0.027431 9.590 &lt; 0.0000000000000002 *** ## c_interview:c_age 0.027267 0.004555 5.986 0.00000000585 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8489 on 316 degrees of freedom ## Multiple R-squared: 0.2926, Adjusted R-squared: 0.2859 ## F-statistic: 43.57 on 3 and 316 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + c_age ## Model 2: perf_eval ~ c_interview * c_age ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 317 253.52 ## 2 316 227.71 1 25.817 35.827 0.000000005849 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.08020031 The regression coefficient associated with the interaction term (c_interview:c_age) is positive and statistically significant (b = .027, p &lt; .001), which indicates that there is evidence of slope differences. The amount of variance explained by c_interview, c_age, and their interaction in relation to perf_eval is large in magnitude (R2 = .293, adjusted R2 = .286, p &lt; .001), which is larger than when just c_interview and c_age were included in the model. The significant F-value (35.827, p &lt; .001) in from the nested model comparison indicates that adding the interaction term significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .080), which indicates that adding the interaction term to the model explained an additional 8.0% of the variance in perf_eval. Given the statistically significant interaction term, we will use the probe_interaction function from the interactions package to probe the interaction to plot this multiplicative model that we named model.3. # Probe the significant interaction (slope differences) probe_interaction(model.3, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.593122204199181268791 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.48 0.13 11.16 0.00 ## ## Slope of c_interview when c_age = -0.000000000000003164136 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.71 0.16 10.70 0.00 ## ## Slope of c_interview when c_age = 8.593122204199174163364 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 1.95 0.19 10.16 0.00 The plot and the simple slopes analyses output show that the two slopes are indeed different, such that the association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) is more positive for older workers (b = 1.95, p &lt; .01), than for average-aged workers (b = 1.71, p &lt; .01) and younger workers (b = 1.48, p &lt; .01). Notably, all three simple slopes are statistically significant and positive, but the slope is more positive for older workers. In sum, we found evidence of both intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the age protected class variable (c_age). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.3.3.3 Test Differential Prediction Based on race To investigate whether differential prediction exists for c_interview in relation to perf_eval based on race, we will follow mostly the same process as we did with the dichotomous variable gender above. However, we will filter our data within the regression function to compare just Black individuals with White individuals, as it is customary to compare just two races/ethnicities at a time. The decision to focus on only Black and White individuals is arbitrary and just for demonstration purposes. Note that in the DiffPred data frame object that both black and white are lowercase category labels in the race variable. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. Add the subset=(race==\"black\" | race==\"white\") argument to subset the data such that only those individuals who are Black and White are retained. In doing so, we effectively dichotomize the race variable within the lm function. If you need to review conditional/logical statements when filtering data, check out the chapter on filtering data. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred, subset=(race==&quot;black&quot; | race==&quot;white&quot;)) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred, subset = (race == ## &quot;black&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.72580 -0.66775 -0.00208 0.74761 2.49792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.71479 0.06817 69.164 &lt; 0.0000000000000002 *** ## c_interview 0.13983 0.05233 2.672 0.00813 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9895 on 210 degrees of freedom ## Multiple R-squared: 0.03288, Adjusted R-squared: 0.02827 ## F-statistic: 7.139 on 1 and 210 DF, p-value: 0.008133 We should begin by noting that our effective sample size is now 212 (which we can compute by adding the number of degrees of freedom in our model, 212, to the number of parameters estimated, 2), whereas previously it was 370 when we included individuals of all races. The regression coefficient for c_interview is statistically significant and positive (b = .140, p = .008) based on this reduced sample of 212 individuals, which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is small in magnitude (R2 = .033, adjusted R2 = .028, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable race. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) model.2 &lt;- lm(perf_eval ~ c_interview + race, data=DiffPred, subset=(race==&quot;black&quot; | race==&quot;white&quot;)) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + race, data = DiffPred, ## subset = (race == &quot;black&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51207 -0.64712 -0.03035 0.65707 2.28188 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.91079 0.08941 54.922 &lt; 0.0000000000000002 *** ## c_interview 0.12665 0.05131 2.469 0.01437 * ## racewhite -0.44041 0.13398 -3.287 0.00119 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 209 degrees of freedom ## Multiple R-squared: 0.08042, Adjusted R-squared: 0.07162 ## F-statistic: 9.139 on 2 and 209 DF, p-value: 0.0001567 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 210 205.60 ## 2 209 195.49 1 10.107 10.806 0.001187 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.04754406 The regression coefficient associated with race (or more specifically racewhite) is negative and statistically significant (b = -.440, p = .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is negative and because white is appended to the race variable name, it implies that the intercept value is significantly lower for White individuals compared to Black individuals. The amount of variance explained by c_interview and race in perf_eval is small-medium in magnitude (R2 = .080, adjusted R2 = .072, p &lt; .001). The significant F-value (10.806, p = .001) from the nested model comparison indicates that adding race significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .048), which indicates that adding race to the model explained an additional 4.8% of the variance in perf_eval. Using the probe_interaction function from the interactions package, we will visualize the statistically significant intercept differences. As input to the probe_interaction function, use the the model.2 model object from above, and specify the arguments in the probe_interaction function as you did before. # Visualize intercept differences probe_interaction(model.2, pred=c_interview, modx=race, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when race = white: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.13 0.05 2.47 0.01 ## ## Slope of c_interview when race = black: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.13 0.05 2.47 0.01 Step Three: Finally, let’s add the interaction term between c_interview and race to the model using the * operator. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (race), # and interaction between selection tool and protected class model.3 &lt;- lm(perf_eval ~ c_interview * race, data=DiffPred, subset=(race==&quot;black&quot; | race==&quot;white&quot;)) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * race, data = DiffPred, ## subset = (race == &quot;black&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7314 -0.6396 0.0227 0.6637 2.4205 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.91180 0.08886 55.275 &lt; 0.0000000000000002 *** ## c_interview 0.21843 0.07020 3.112 0.002122 ** ## racewhite -0.46345 0.13370 -3.466 0.000641 *** ## c_interview:racewhite -0.19428 0.10213 -1.902 0.058527 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9611 on 208 degrees of freedom ## Multiple R-squared: 0.09615, Adjusted R-squared: 0.08311 ## F-statistic: 7.375 on 3 and 208 DF, p-value: 0.0001015 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + race ## Model 2: perf_eval ~ c_interview * race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 209 195.49 ## 2 208 192.15 1 3.3426 3.6184 0.05853 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.0157234 The regression coefficient associated with the interaction term (c_interview:racewhite) is not statistically significant (b = -.194, p = .059), which indicates that there is no evidence of slope differences. Given this, we won’t probe the interaction because, well, there is not an interaction to probe. Further, adding the interaction term did not significantly improve model fit, so we won’t interpret change in R2. Given all this, we won’t probe the interaction because, well, there is not an interaction to probe. In sum, we found evidence of intercept differences for the selection tool c_interview in relation to the criterion perf_eval that were conditional upon the protected class variable race. Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.3.4 APA-Style Results Table If you want to present the results of your moderated multiple linear regression (MMLR) model to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package (Stanley 2021). Using the lm function from base R, as we did above, let’s begin by estimating a MMLR model and naming the model object (model.3). # Estimate multiple linear regression model model.3 &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) If you haven’t already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (model.3) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(model.3) ## ## ## Regression results using perf_eval as the criterion ## ## ## Predictor b b_95%_CI sr2 sr2_95%_CI Fit ## (Intercept) 4.56** [4.41, 4.71] ## c_interview 0.39** [0.28, 0.51] .12 [.06, .19] ## genderwoman 0.15 [-0.06, 0.37] .01 [-.01, .02] ## c_interview:genderwoman -0.37** [-0.54, -0.21] .05 [.01, .10] ## R2 = .124** ## 95% CI[.06,.19] ## ## ## Note. A significant b-weight indicates the semi-partial correlation is also significant. ## b represents unstandardized regression weights. ## sr2 represents the semi-partial correlation squared. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file “APA Multiple Linear Regression Table.doc”. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(model.3, filename=&quot;APA Moderated Multiple Linear Regression Table.doc&quot;) ## ## ## Regression results using perf_eval as the criterion ## ## ## Predictor b b_95%_CI sr2 sr2_95%_CI Fit ## (Intercept) 4.56** [4.41, 4.71] ## c_interview 0.39** [0.28, 0.51] .12 [.06, .19] ## genderwoman 0.15 [-0.06, 0.37] .01 [-.01, .02] ## c_interview:genderwoman -0.37** [-0.54, -0.21] .05 [.01, .10] ## R2 = .124** ## 95% CI[.06,.19] ## ## ## Note. A significant b-weight indicates the semi-partial correlation is also significant. ## b represents unstandardized regression weights. ## sr2 represents the semi-partial correlation squared. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table moderated multiple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. References "],["crossvalidation.html", "Chapter 43 Statistically &amp; Empirically Cross-Validating a Selection Tool 43.1 Conceptual Overview 43.2 Tutorial", " Chapter 43 Statistically &amp; Empirically Cross-Validating a Selection Tool In this chapter, we will learn how to perform both statistical and empirical cross-validation, and we will do so in the context of validating an employee selection tool by estimating a multiple linear regression model. For a review of multiple linear regression, please refer to the chapter on estimating incremental validity. 43.1 Conceptual Overview Link to conceptual video: https://youtu.be/DWUc5GBRFuA Cross-validation is a process through which we determine how well a regression model estimated using a sample from drawn from a particular population generalizes to another sample from that same population. More specifically, the term cross-validity “refers to whether the [regression coefficients] derived from one sample can predict outcomes to the same degree in the population as a whole or in other samples drawn from the same population” (Cascio and Aguinis 2005, 173). At its core, cross-validation represents a step toward true predictive analytics, as it involves training our data on one sample and then testing our data on one or more other samples from that population. In general, we can distinguish between two approaches to cross-validation: statistical and empirical. 43.1.1 Review of Statistical Cross-Validation Statistical cross-validation is performed without collecting additional data from the underlying population from which the original regression model was estimated. There are a couple of different statistical indicators of a model’s cross-validity, which can be used in tandem to inform judgments of regarding cross-validity: adjusted R2 and squared cross-validity (Raju et al. 1997). The adjusted R2 (i.e., \\(R^2_{adj}\\), squared multiple correlation) (Ezekiel 1930; Wherry 1931) appears in most statistical platforms’ regression output (including many R functions), and it provides a more accurate estimate of the amount of variance explained by the predictor variable(s) in the outcome variable than the unadjusted R2 (\\(R^2_{unadj}\\)). Specifically, the \\(R^2_{adj}\\) corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The \\(R^2_{adj}\\) is a better indicator of the magnitude of the association in the underlying population than the \\(R^2_{unadj}\\). The formula for \\(R^2_{adj}\\) is: \\(R^2_{adj} = 1-\\frac{N-1}{N-k-1}(1-R^2_{unadj})\\) where \\(N\\) refers to the sample size, \\(k\\) refers to the number of predictor variables in the model, and \\(R^2_{unadj}\\) is the unadjusted R2, which reflects the sample-specific variance explained by the predictor variables in relation to the outcome variable. Again, this \\(R^2_{adj}\\) is readily produced by most regression functions, so additional steps are not required to compute it. In contrast, the squared cross-validity (\\(\\rho^2_{c}\\)) (Browne 1975) often requires additional computation beyond what our regression function computes. As you can see in the formula below, the \\(\\rho^2_{c}\\) formula uses the \\(R^2_{adj}\\) as input. \\(\\rho^2_{c} = \\frac{(N-k-3)(R^2_{adj})^2+R^2_{adj}}{(N-2k-2)(R^2_{adj})+k}\\) Both \\(R^2_{adj}\\) and \\(\\rho^2_{c}\\) can be used inform our understanding of a model’s squared cross-validity; however, preference should be given to the \\(\\rho^2_{c}\\), as \\(R^2_{adj}\\) by itself tends to overestimate the squared cross-validity. If we take the square root of \\(\\rho^2_{c}\\), we get an estimate of cross-validity (\\(\\rho_{c}\\)). 43.1.2 Review of Empirical Cross-Validation Empirical cross-validation requires that we sample additional data from the same population, or that we split our original sample (if it’s large enough) into subsamples, where the subsamples are sometimes referred to as holdout samples. Regardless of which approach we use, the logic is similar: One (sub)sample serves as our training (i.e., initial, original, derivation) data, and the other (sub)sample serves as our test (i.e., cross-validation) data. Using either approach, we estimate the regression model from the first (sub)sample and then estimate the extent to which the model fits the second (sub)sample as well as its and predictive accuracy. 43.1.2.1 Sample Write-Up We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we first performed statistical cross-validation. Specifically, we found that the adjusted R2 was .348 and the squared cross-validity (\\(\\rho^2_{c}\\)) was .341. The latter indicates that in the population, the three selection tools collectively explain 34.1% of the variability in the criterion. Next, we performed empirical cross-validation by assessing the model’s performance in a new same of data drawn from the sample population. The squared cross-validity (\\(\\rho^2_{c}\\)) was .341, which indicates that the model explained 34.1% of the variance in the criterion when applied to the second sample, which was only a slight decrease in performance compared to the original sample (\\(R^2_{unadj}\\) = .359, \\(R^2_{adj}\\) = .348). Further, estimates of root mean square error (RMSE) indicated that the RMSE for the second sample is 13.6% larger than the RMSE for the first sample, which indicates that prediction errors didn’t increase substantially when applying the model to the second sample. Overall, the model performed relatively well applied to a second sample of data, and it doesn’t appear as though we over fit the model when estimating it using the first sample. 43.2 Tutorial This chapter’s tutorial demonstrates how to perform statistical and empirical cross-validation using R. 43.2.1 Functions &amp; Packages Introduced Function Package lm base R summary base R sqrt base R predict base R R2 caret mean base R RMSE caret abs base R MAE caret cbind base R 43.2.2 Initial Steps If you haven’t already, save the files called “Sample1.csv” and “Sample2.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data files for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called “Sample1.csv” and “Sample2.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects Sample1 &lt;- read_csv(&quot;Sample1.csv&quot;) ## Rows: 182 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): PerfEval, WorkSample, SJT, Consc ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Sample2 &lt;- read_csv(&quot;Sample2.csv&quot;) ## Rows: 166 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): PerfEval, WorkSample, SJT, Consc ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(Sample1) ## [1] &quot;PerfEval&quot; &quot;WorkSample&quot; &quot;SJT&quot; &quot;Consc&quot; names(Sample2) ## [1] &quot;PerfEval&quot; &quot;WorkSample&quot; &quot;SJT&quot; &quot;Consc&quot; # View number of rows/cases in data frame (tibble) objects nrow(Sample1) ## [1] 182 nrow(Sample2) ## [1] 166 # View variable type for each variable in data frame (tibble) objects str(Sample1) ## spc_tbl_ [182 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ PerfEval : num [1:182] 5.1 5 3.5 5 4 4.4 5.2 5.1 3.8 3.1 ... ## $ WorkSample: num [1:182] 7.5 8.1 7.6 6.6 6 6 8 8.8 6.1 6.5 ... ## $ SJT : num [1:182] 7.9 5.6 4.4 5.7 8.5 5.8 6.1 4.7 4.5 5.1 ... ## $ Consc : num [1:182] 4.9 2.4 3.1 4.5 3.6 4.8 4.7 4.9 3.2 4.6 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. PerfEval = col_double(), ## .. WorkSample = col_double(), ## .. SJT = col_double(), ## .. Consc = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; str(Sample2) ## spc_tbl_ [166 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ PerfEval : num [1:166] 4 4.3 3.9 5.6 4.5 4.9 4.2 4.7 4.5 5.6 ... ## $ WorkSample: num [1:166] 6.7 9 6.8 7.6 4.5 8.1 6.5 7.1 6.1 5.3 ... ## $ SJT : num [1:166] 4.4 3.7 5.6 6.8 6.9 7.4 6.4 7 7.2 10 ... ## $ Consc : num [1:166] 4.3 1.7 4.5 5.2 3.8 4.3 3.7 5.1 4.1 4.3 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. PerfEval = col_double(), ## .. WorkSample = col_double(), ## .. SJT = col_double(), ## .. Consc = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame (tibble) objects head(Sample1) ## # A tibble: 6 × 4 ## PerfEval WorkSample SJT Consc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 7.5 7.9 4.9 ## 2 5 8.1 5.6 2.4 ## 3 3.5 7.6 4.4 3.1 ## 4 5 6.6 5.7 4.5 ## 5 4 6 8.5 3.6 ## 6 4.4 6 5.8 4.8 head(Sample2) ## # A tibble: 6 × 4 ## PerfEval WorkSample SJT Consc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 6.7 4.4 4.3 ## 2 4.3 9 3.7 1.7 ## 3 3.9 6.8 5.6 4.5 ## 4 5.6 7.6 6.8 5.2 ## 5 4.5 4.5 6.9 3.8 ## 6 4.9 8.1 7.4 4.3 The first data frame (Sample1) has 182 cases and the following 4 variables: PerfEval, WorkSample, SJT, and Consc. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). Imagine that the Sample1 data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered the three selection tools 90 days after entering the organization. PerfEval is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, where scores could range from 1 (low performance) to 7 (high performance). The WorkSample variable contains scores from a work sample assessment, and scores could range from 1-10, with 10 indicating a strong performance on the assessment. SJT refers to a situational judgment test in which scores can range from 1-10, with 10 indicating higher proficiency. Consc refers to an a conscientiousness personality inventory in which scores can range from 1-7, with 7 indicating higher conscientiousness. Proactivity refers to a proactive personality inventory in which scores can range from 1-15, with 15 indicating higher proactive personality. 43.2.3 Perform Statistical Cross-Validation In order to perform statistical cross-validation, we first need to estimate our model. In this chapter, we’ll estimate a multiple linear regression in which we treat the three selection tool variables (WorkSample, SJT, Consc) as predictors and the variable PerfEval as our criterion (outcome). In other words, we are estimating the criterion-related and incremental validity of these three selection tools in relation to the outcome. In doing so, we will determine whether each selection to explains unique variance in the outcome, and how much variance, collectively, all three selection tools explain in the outcome. For more information on multiple linear regression, check out the chapter on estimating incremental validity. Note that in this chapter we will use the lm function from base R to estimate our multiple linear regression model, which is discussed in the chapter supplement to the aforementioned chapter. # Estimate multiple linear regression model model1 &lt;- lm(PerfEval ~ WorkSample + SJT + Consc, data=Sample1) # Print summary of results summary(model1) ## ## Call: ## lm(formula = PerfEval ~ WorkSample + SJT + Consc, data = Sample1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62345 -0.39876 0.01343 0.32258 1.51027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.30283 0.34833 3.740 0.000248 *** ## WorkSample 0.28430 0.03854 7.376 0.00000000000596 *** ## SJT 0.08118 0.03082 2.634 0.009190 ** ## Consc 0.23241 0.04652 4.996 0.00000139215819 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.589 on 178 degrees of freedom ## Multiple R-squared: 0.3592, Adjusted R-squared: 0.3484 ## F-statistic: 33.25 on 3 and 178 DF, p-value: &lt; 0.00000000000000022 In the output, you can see that each of the regression coefficients associated with the predictor variables (i.e., selection tools) are statistically significant, which indicates that they each explain unique variance in the outcome (PerfEval) when controlling for the predictors in the model. Further, the \\(R^2_{unadj}\\) (unadjusted R2) is .359, and the \\(R^2_{adj}\\) (adjusted R2) is .348, both of which provide us with estimates of model fit and the amount of variance explained in the outcome variable collectively by the three predictor variables. Next, let’s define a function that will compute the squared cross-validity (\\(\\rho^2_{c}\\)) based on the Browne (1975) formula when we type the name of a regression model object as the sole argument. Let’s name this function rho2 for \\(\\rho^2_{c}\\). As a reminder, the Browne formula is: \\(\\rho^2_{c} = \\frac{(N-k-3)(R^2_{adj})^2+R^2_{adj}}{(N-2k-2)(R^2_{adj})+k}\\) You don’t need to change anything in the function script below. Simply run it as is. # Write function for squared cross-validity formula (Browne, 1975) rho2 &lt;- function(x) { N &lt;- nobs(x) # model sample size k &lt;- length(coef(x)) - 1 # number of predictor variables R2_adj &lt;- summary(x)$adj.r.squared # adjusted R-squared ((N - k - 3) * (R2_adj^2) + R2_adj) / ((N - (2 * k) - 2) * R2_adj + k) # Browne formula } Using the new function called rho2, type the name of our regression model object as the sole parenthetical argument. # Estimate squared cross-validity rho2(model1) ## [1] 0.3412245 The output indicates that the squared cross-validity (\\(\\rho^2_{c}\\)) is .341, which indicates that, in the population, the predictor variables included in the model explain 34.1% of the variability in the outcome variable. If you recall, the \\(R^2_{unadj}\\) was .359, and the \\(R^2_{adj}\\) was .348. Thus, compared to \\(R^2_{unadj}\\), the model fit didn’t shrink too much. To estimate the cross-validity (\\(\\rho_{c}\\)), just take the square root of (\\(\\rho^2_{c}\\)). # Estimate cross-validity sqrt(rho2(model1)) ## [1] 0.5841442 Thus, our cross-validity (multiple correlation) is .584. Technical Write-Up: We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we performed statistical cross-validation. Specifically, we found that the adjusted R2 was .348 and the squared cross-validity (\\(\\rho^2_{c}\\)) was .341. The latter indicates that in the population, the three selection tools collectively explain 34.1% of the variability in the criterion. 43.2.4 Perform Empirical Cross-Validation There are different ways that we can go about performing empirical cross-validation, but regardless of the approach, we are entering the realm of predictive analytics. Why? Well, we are testing the model fit and predictive accuracy of our model estimated using one sample in a different sample from the same population. To get things started, let’s estimate our multiple linear regression model that contains the three selection tool variables (WorkSample, SJT, Consc) as predictors and PerfEval as the outcome (criterion) variable. We will estimate our model using the first sample data frame, which we named Sample1 above. # Estimate multiple linear regression model model1 &lt;- lm(PerfEval ~ WorkSample + SJT + Consc, data=Sample1) # Print summary of results summary(model1) ## ## Call: ## lm(formula = PerfEval ~ WorkSample + SJT + Consc, data = Sample1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62345 -0.39876 0.01343 0.32258 1.51027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.30283 0.34833 3.740 0.000248 *** ## WorkSample 0.28430 0.03854 7.376 0.00000000000596 *** ## SJT 0.08118 0.03082 2.634 0.009190 ** ## Consc 0.23241 0.04652 4.996 0.00000139215819 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.589 on 178 degrees of freedom ## Multiple R-squared: 0.3592, Adjusted R-squared: 0.3484 ## F-statistic: 33.25 on 3 and 178 DF, p-value: &lt; 0.00000000000000022 In the output, you can see that each of the regression coefficients associated with the predictor variables (i.e., selection tools) are statistically significant, which indicates that they each explain unique variance in the outcome (PerfEval) when controlling for the predictors in the model. Further, the \\(R^2_{unadj}\\) (unadjusted R2) is .359, and the \\(R^2_{adj}\\) (adjusted R2) is .348, both of which provide us with estimates of model fit and the amount of variance explained in the outcome variable collectively by the three predictor variables. Let’s kick things up a notch by applying our model that we named model1 to a new data frame called (Sample2). Using the predict function from base R, enter the name of the model object as the first argument and the name of the second data frame object as the second argument. Let’s assign these predicted values using the &lt;- assignment operator to an object called predictions. The predict function is using our regression model equation from the first sample to estimate the fitted (predicted) values of the outcome variable in the second sample based on the values for the different predictor variables. # Predict values on outcome using second sample predictions &lt;- predict(model1, Sample2) Next, we will evaluate the quality of our model’s fit to the new dataset (Sample2). We’ll use several functions from the caret package (Kuhn 2021), so be sure to install and access that package if you haven’t already. The caret package is renowned for its predictive modeling and machine learning functions. # Install caret package if you haven&#39;t already install.packages(&quot;caret&quot;) # Access caret package library(caret) For comparison, first, let’s extract the unadjusted r.squared value from the our regression model summary by appending r.squared to the end of summary(model1) using the $ operator. Second, using the R2 function from the caret package, enter the name of the predictions object we created above as the first argument, and as the second argument, enter the name of the second data frame (Sample2), followed by the the $ operator and the name of the outcome variable (PerfEval). The first operation will extract our \\(R^2_{unadj}\\) from the original sample, and the second operation will estimate our \\(\\rho^2_{c}\\) based on our new model and its performance in the second sample. # Unadjusted R-squared based on the first sample summary(model1)$r.squared ## [1] 0.359162 # Adjusted R-squared based on the first sample summary(model1)$adj.r.squared ## [1] 0.3483614 # Squared cross-validity based on the second sample R2(predictions, Sample2$PerfEval) ## [1] 0.3412069 As one would expect, the \\(\\rho^2_{c}\\) is slightly lower the \\(R^2_{unadj}\\) when the model is applied to new data, but fortunately, it doesn’t decrease that much. Further, the \\(\\rho^2_{c}\\) is only slightly lower than the \\(R^2_{adj}\\). This is our first indication that the model performed fairly well with the new data from the population. To calculate the multiple R and cross-validity (\\(\\rho_c\\)) values, we simply take the square root (sqrt) of the two values we calculated above. # Multiple R for first sample sqrt(summary(model1)$r.squared) ## [1] 0.5993013 # Cross-validity for second sample sqrt(R2(predictions, Sample2$PerfEval)) ## [1] 0.5841292 Similarly, the multiple R and cross-validity (\\(\\rho_c\\)) values are comparable, with the cross-validity value only slightly lower than the multiple R value. The mean squared error (MSE) is another way that we can test how well our model performed. As the label implies, the MSE is the mean of the error (residual) values squared. First, let’s estimate the MSE for the initial sample, which reflects the extent to which the fitted (predicted) values based on the model deviate from the actual outcome variables in the initial sample (Sample1) on which the model was estimated. To estimate the MSE for the model based on the initial sample, we extract the residuals from the model (model1), square them, and then take the mean using the mean function from base R. Second, let’s estimate the MSE for the second sample based on the extent to which the fitted (predicted) values from our model (model1) deviate from the actual outcome variables in the second sample (Sample2). To estimate the MSE for the model (model1) in relation to the second sample, we subtract the predicted values (predictions) from the outcome variable (PerfEval) values for the second sample (Sample2), square the difference, and then compute the mean using the mean function from base R. # MSE (Mean Squared Error) for first sample mean(model1$residuals^2) ## [1] 0.3393501 # MSE (Mean Squared Error) for second sample mean((Sample2$PerfEval - predictions)^2) ## [1] 0.4381229 In isolation, each MSE value does not mean much; however, each MSE value takes on mean when we compare it to another MSE value. As expected, the MSE is higher when we apply the model (model1) estimated from the first sample (Sample1) to the second sample (Sample2). With that said, we don’t want our MSE in the second, validation sample to be too much higher than the original, test sample. Often, however, we opt for the root mean squared error for evaluating the quality of our model’s fit because it is in the same units as the outcome variable. To calculate the root mean squared error (RMSE), we simply take the square root of the MSE. For our predicted (fitted) values based on our second, validation sample, we can simply apply the RMSE function from the caret package; as the first argument, type the name of the predictions object we created above, and as the second argument, type the name of the second sample (Sample2), followed by the $ operator and the name of the outcome variable (PerfEval). # RMSE (Root Mean Squared Error) for first sample sqrt(mean(model1$residuals^2)) ## [1] 0.5825376 # RMSE (Root Mean Squared Error) for second sample RMSE(predictions, Sample2$PerfEval) ## [1] 0.6619085 Again, we expect to see that the RMSE for our model (model1) when applied to the second sample to be larger, as we expect larger errors in prediction when we apply our model to new data. That said, we don’t want the differences in RMSE to be too large. But how much is too much you might ask? Good question, and there isn’t a great answer. What is considered a large difference depends in part on the context and what you’re predicting. That said, it’s sometimes helpful to calculate how much larger one RMSE is than the other. Below, I simply calculate how much larger (as a percentage) is the RMSE from the second sample than the first sample. # Percentage difference in RMSE (Root Mean Squared Error) values (RMSE(predictions, Sample2$PerfEval) - sqrt(mean(model1$residuals^2))) / sqrt(mean(model1$residuals^2)) * 100 ## [1] 13.62503 We see that the RMSE for the second, validation sample is 13.6% larger than the RMSE for the first, test sample, which is actually pretty good. If it were me, I would allow my concern to grow when that percentage difference hits 30% or higher, but that’s not a strict rule. The mean absolute error (MAE) is another common indicator of a model’s fit to data. The MAE is just the average of the errors’ (residuals’) absolute values. We want the difference between MAE values to be as small as possible. # MAE (Mean Absolute Error) for first sample mean(abs(model1$residuals)) ## [1] 0.4535265 # MAE (Mean Absolute Error) for second sample MAE(predictions, Sample2$PerfEval) ## [1] 0.5213717 As we did with the RMSE values, let’s calculate how much larger the MAE is when the model is applied to the second sample than the first sample. # Percentage difference in MAE (Mean Absolute Error) values (MAE(predictions, Sample2$PerfEval) - mean(abs(model1$residuals))) / mean(abs(model1$residuals)) * 100 ## [1] 14.95948 We see that the MAE for the second, validation sample is 15.0% larger than the MAE for the first, test sample, which again is actually pretty good. Finally, let’s estimate the 95% prediction intervals around each fitted (predicted) value based on our model (model1) when applied to the second, validation sample (Sample2). We will use the predict function from base R. As the first argument, enter the name of your model object (model1). As the second argument, type newdata= followed by the name of the data frame associated with the second sample (Sample2). As the third argument, type interval=\"prediction\" to indicate that you want to estimate prediction intervals; the default is the 95% prediction interval. Let’s assign the prediction interval and fitted values to an object called pred.int so that we can append those vectors to the second sample data frame, which we do in the following step using the cbind function from base R. Use the head function from base R to view the first 6 rows of your updated Sample2 data frame and to marvel at your work. # Estimate 95% prediction intervals pred.int &lt;- predict(model1, newdata=Sample2, interval=&quot;prediction&quot;) # Join fitted (predicted) values and upper and lower prediction interval values to data frame Sample2 &lt;- cbind(Sample2, pred.int) # View first 6 rows head(Sample2) ## PerfEval WorkSample SJT Consc fit lwr upr ## 1 4.0 6.7 4.4 4.3 4.564170 3.396242 5.732099 ## 2 4.3 9.0 3.7 1.7 4.556964 3.349621 5.764306 ## 3 3.9 6.8 5.6 4.5 4.736498 3.570208 5.902788 ## 4 5.6 7.6 6.8 5.2 5.224037 4.051437 6.396638 ## 5 4.5 4.5 6.9 3.8 4.025462 2.840158 5.210766 ## 6 4.9 8.1 7.4 4.3 5.205726 4.032809 6.378642 Unlike the confidence interval, the prediction interval is specific to each value and represents uncertainty around specific values of the outcome. In contrast, the confidence interval represents uncertainty around the point estimate of a regression coefficient associated with the relation between a predictor variable and the outcome variable. Thus, there is a single confidence interval for a regression coefficient, but there is a different prediction interval for each fitted (predicted) value of the outcome variable. Technical Write-Up: We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we performed empirical cross-validation by assessing the fit and predictive accuracy of the model in a second sample drawn from the sample population. The squared cross-validity (\\(\\rho^2_{c}\\)) was .341, which indicates that the model explained 34.1% of the variance in the criterion when applied to the second sample, which was only a slight decrease in performance compared to the original sample (\\(R^2_{unadj}\\) = .359, \\(R^2_{adj}\\) = .348). Further, estimates of root mean square error (RMSE) indicated that the RMSE for the second sample is 13.6% larger than the RMSE for the first sample, which indicates that were prediction errors didn’t increase substantially when applying the model to the second sample. Overall, the model performed relatively well applied to a second sample of data, and it doesn’t appear as though we overfit the model when estimating it using the first sample. 43.2.5 Summary In this chapter, we learned how to statistically and empirically cross-validate a regression model. To perform statistical cross-validation, we estimated the cross-validity based on the R-squared value, sample size, and number of predictor variables in the model. To perform empirical cross-validation, we fit our regression model using one sample, and then estimated how well that model fit data from a second sample. References "],["turnover.html", "Chapter 44 Introduction to Employee Separation &amp; Retention 44.1 Chapters Included", " Chapter 44 Introduction to Employee Separation &amp; Retention Employee separation and retention are often key behavioral outcomes within organizations – even for those outside of HR. Replacing employees who separate from the organization can be costly, particularly when the employees who have left are high performers. Additionally, retaining high-performing employees – as opposed to losing them to a competitor organization – can offer a strategic advantage to an organization. In the following conceptual video, I introduce the concepts of employee separation and employee retention. Link to conceptual video: https://youtu.be/I-kn24DQITI 44.1 Chapters Included In the following chapters, you will have opportunities to learn how to investigate the phenomenon of employee separation and retention. Computing Monthly &amp; Annual Turnover Rates Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence Identifying Predictors of Turnover Using Logistic Regression Applying k-Fold Cross-Validation to Logistic Regression Understanding Length of Service Using Survival Analysis "],["turnoverrate.html", "Chapter 45 Computing Monthly &amp; Annual Turnover Rates 45.1 Conceptual Overview 45.2 Tutorial", " Chapter 45 Computing Monthly &amp; Annual Turnover Rates In this chapter, we will learn how to compute monthly and annual turnover rates. 45.1 Conceptual Overview The turnover rate is a specific type of human resource (HR) metric, and allows us to describe past turnover in the organization relative to the total number of employees. In other words, the turnover rate is a specific type of descriptive analytics. A monthly turnover rate refers to the turnover rate for a given month. There are various considerations one can entertain when computing monthly turnover rates, such as how broadly to define separations. That is, do we consider all forms of separations or, perhaps, just voluntary turnover. In this chapter, we will focus on a very broad and simple conceptualization and approach to computing monthly turnover rates, but please note that there are other variations. We will focus on all separations that took place in the organization (i.e., both involuntary and voluntary turnover) and coarsely account for the average number of employees in a given month. Specifically, we will use the following formula to compute a monthly turnover rate: \\(TurnoverRate_{Monthly} = \\frac{Separations_{Total}}{Employees_{Average}} \\times 100\\) where \\(Separations_{Total}\\) refers to the total number of employee separations in a given month, and \\(Employees_{Average}\\) refers to the average number of employees who were working in a given month. Note that we multiply the initial fraction by 100 to convert the monthly turnover rate from a proportion to a percentage. An annual turnover rate refers to the turnover rate for a given year. Like monthly turnover rates, there are various considerations one can entertain when computing annual turnover rates, and in this tutorial we will keep things very simple by assuming that we are working with just one year’s worth of data and that we have access to separations data for all 12 months within that year. There are other ways in which we could compute annual turnover rates, but for the sake of learning, we will choose what is perhaps the most straight forward approach. Specifically, we will use the following formula to compute an annual turnover rate: \\(TurnoverRate_{Annual} = \\frac{Separations_{YearSumTotal}}{Employees_{AverageAcrossMonths}} \\times 100\\) where \\(Separations_{YearSumTotal}\\) refers to the sum total of employee separations for the year, and \\(Employees_{AverageAcrossMonths}\\) refers to the average number of employees who were working across the 12 months in the year. Note that we multiply the initial fraction by 100 to convert the annual turnover rate from a proportion to a percentage. 45.2 Tutorial This chapter’s tutorial demonstrates how to compute both monthly and annual turnover rates in R. 45.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/StdNp94V9Pk 45.2.2 Functions &amp; Packages Introduced Function Package sum base R mean base R plot base R 45.2.3 Initial Steps If you haven’t already, save the file called “turnover_rate.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “turnover_rate.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object tr &lt;- read_csv(&quot;turnover_rate.csv&quot;) ## Rows: 12 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (3): Month, Separations, Ave_Employees ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(tr) ## [1] &quot;Month&quot; &quot;Separations&quot; &quot;Ave_Employees&quot; # Print number of rows in data frame (tibble) object nrow(tr) ## [1] 12 # Print data frame (tibble) object print(tr) ## # A tibble: 12 × 3 ## Month Separations Ave_Employees ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12 127 ## 2 2 9 107 ## 3 3 6 119 ## 4 4 4 127 ## 5 5 2 113 ## 6 6 3 107 ## 7 7 5 110 ## 8 8 1 125 ## 9 9 6 116 ## 10 10 7 124 ## 11 11 4 117 ## 12 12 3 127 The data frame object we named tr contains three variables: Month, Separations, and Ave_Employees. The Month variable includes 12 unique values, which correspond to the 12 months of the year, where 1 refers to January, 2 refers to February, 3 refers to March, and so on. The Separations variable includes the total number of separations (i.e., number of individuals who turned over) from the organization in a given month. The Ave_Employees variable includes the average number of employees who worked in the organization in a given month; for this example, let’s assume that the average number of employees was calculated by adding together the number of employees at the beginning of the month and the number of employees at the end of the month, and then dividing that sum by 2. 45.2.4 Compute Monthly Turnover Rates When computing monthly turnover rates, we will create a new variable containing monthly turnover rates. To calculate the turnover rate for each month, we will use simple arithmetic by dividing the number of separations for the month divided by the average number of employees who worked in the organization that month; we can then multiply the resulting quotient by 100 to convert the proportion to a percentage. Because we are applying these arithmetic operations row by row (because in these data each row represents a unique month), our task will be relatively simple. To illustrate how the underlying math and the logic of the operations, let’s break this process of computing monthly turnover rates into three steps. If you’re feeling confident and already understand the logic, you can skip directly to the third step, as that is what we’re building up to. As the first step, let’s compute the monthly turnover rates as proportions and print them directly to our console. In other words, we won’t create a new variable just yet; rather, we will just peak at the monthly turnover rates (as proportions) as we get comfortable. Type in the name of the data frame object (tr) followed by the $ operator and the name of the separations variable (Separations); note that the $ operator is used to indicate which data frame object a variable belongs to. Next, type in the division symbol (/). After that, type in the name of the data frame object (tr) followed by the $ operator and the name of the average number of employees variable (Ave_Employees). Run that line of code. # Compute monthly turnover rates (as proportions) and print to the Console tr$Separations / tr$Ave_Employees ## [1] 0.09448819 0.08411215 0.05042017 0.03149606 0.01769912 0.02803738 0.04545455 0.00800000 0.05172414 0.05645161 0.03418803 0.02362205 In our console, we should see a vector of the monthly turnover rates as proportions. As the second step, if we wish to view the monthly turnover rates as percentages, we just need to multiply the quotient from above by 100. In the code below, I include parentheses around the chunk of code used to compute the proportions, as I think it’s easier to conceptualize what we’re doing. That being said, because we are working with division and multiplication here, mathematical orders of operation do not require the parentheses. It’s up to you whether you decide to include them. # Compute monthly turnover rates (as percentages) and print to the Console (tr$Separations / tr$Ave_Employees) * 100 ## [1] 9.448819 8.411215 5.042017 3.149606 1.769912 2.803738 4.545455 0.800000 5.172414 5.645161 3.418803 2.362205 In our console, we should see a vector of monthly turnover rates as percentages. As the third and final step, we will assign the vector of monthly turnover rate percentages we created above to a new variable in our data frame object. After you get the hang of computing monthly turnover rates, you can skip directly to this third step instead of building up to it using the first two steps I demonstrated above. We are simply taking the equation we used to compute monthly turnover rates as percentages and using the &lt;- operator to assign the resulting vector of values to a new variable in our data frame object. You can call the new variable whatever you’d like, and here I call it TR_Monthly and use the $ operator to signal that I wish to attach the vector/variable to the tr data frame object we have been working with thus far. # Compute monthly turnover rates (as percentages) and assign to variable tr$TR_Monthly &lt;- (tr$Separations / tr$Ave_Employees) * 100 To verify that we accomplished what we set out to do, let’s print the tr data frame object using the print function from base R. # Print data frame object to Console print(tr) ## # A tibble: 12 × 4 ## Month Separations Ave_Employees TR_Monthly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12 127 9.45 ## 2 2 9 107 8.41 ## 3 3 6 119 5.04 ## 4 4 4 127 3.15 ## 5 5 2 113 1.77 ## 6 6 3 107 2.80 ## 7 7 5 110 4.55 ## 8 8 1 125 0.8 ## 9 9 6 116 5.17 ## 10 10 7 124 5.65 ## 11 11 4 117 3.42 ## 12 12 3 127 2.36 We now have a new variable called TR_Monthly added to our tr data frame, and this new variable contains the monthly turnover rates as percentages. For example, we can see that for month 1 (January) the monthly turnover rate in this organization was approximately 9.45%. Often interpreting an HR metric like a turnover rate requires comparing it to other other turnover rates within the same organization (e.g., between units), to other time points (e.g., January compared to February), or to other organizations within the same industry (e.g., benchmarking). So what can we observe or describe about these monthly turnover rates? Well, we could state the following: Monthly turnover rates were highest in the first month (January) but then declined through the fifth month (May), increasing through the seventh month (July), dropping quickly to the eighth month (August), and then increasing, plateauing, and declining in the remaining months of the year. Optional: Given that we have access to an organization’s monthly turnover rates for an entire year, we can plot the turnover rates as a line graph to facilitate interpretation of trends. If you choose to create this optional data visualization, feel free to follow along; otherwise, you can skip down to the section called Compute Annual Turnover Rates. We will create a very simple plot using the aptly named plot function from base R. We will start with the most basic version of the line graph plot, and then add more arguments to refine it a bit. For the first argument in our plot function, let’s specify the Month variable from the tr data frame object as our x-axis variable using the x= argument: x=tr$Month. As the second argument, let’s specify the TR_Monthly variable (we just created) from the tr data frame object as our y-axis variable using the y= argument: y=tr$TR_Monthly. plot(x=tr$Month, y=tr$TR_Monthly) By default, the function as specified above provides us with scatter plot in our Plots window. If we wish to convert this plot to a line graph, we can add the following argument: type=\"l\". plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;) Next, we can create more meaningful x- and y-axis labels by using the xlab and ylab arguments. Note that anything we can write whatever we’d like in the quotation marks (\" \"). plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;, xlab=&quot;Month&quot;, ylab=&quot;Monthly Turnover Rate (%)&quot;) Finally, if we wish to adjust the y-axis limits, we can use the ylim argument followed by = and a vector of two values, where the first value is the lower limit of the y-axis and the second value is the upper limit. Here, we create a vector containing the lower- and upper-limits of 0 and 25 using the c (combine) function from base R. plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;, xlab=&quot;Month&quot;, ylab=&quot;Monthly Turnover Rate (%)&quot;, ylim=c(0,25)) The resulting plot allows us to potentially identify and describe any trends or patterns in monthly turnover rates over the course of an entire year. Here, we can see that monthly turnover rates were highest in the first month (January) but then declined through the fifth month (May), increasing through the seventh month (July), dropping quickly to the eighth month (August), and then increasing, plateauing, and declining in the remaining months of the year. If we were to have access to data over multiple years, it’s possible we might begin to see some seasonality to the turnover rates. With access to more years worth of data, we could also build forecasting models (e.g., perform time-series analyses). 45.2.5 Compute Annual Turnover Rate It’s time to shift gears and learn how to compute the annual turnover rate. Because we have 12 months of data from an organization for a single year, our task is relatively straight forward. When we have 12 months of turnover data for a single year, we can compute the annual turnover rate by simply summing the number of separations across the 12 months and then dividing that sum by the average number of employees across the 12 months. If we then multiply that quotient by 100, we can convert the annual turnover rate to a percentage. Because we will need to calculate the sum and average based on values within columns, we will use the sum and mean functions, respectively, from base R. As our numerator, we type the name of the sum function; as the first argument, we enter the name of our data frame object (tr) followed by the $ operator and the name of our separations variable (Separations), and as the second argument, we enter na.rm=TRUE, which would allow us to compute the sum even if there were missing data. [You don’t technically need the na.rm=TRUE argument here given that we have complete data, but it can be good to get in the habit of including it for other applications and contexts.] As the denominator, we enter the name of the mean function; as the first argument, we enter the name of our data frame object (tr) followed by the $ operator and the name of our separations variable (Ave_Employees), and as the second argument, we enter na.rm=TRUE. # Compute annual turnover rate (as proportion) and print to the Console sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE) ## [1] 0.5243129 As a proportion, our annual turnover rate is approximately .5243. If we wish to convert this to a percentage, we can simply multiply our equation from above by 100. # Compute annual turnover rate (as percentage) and print to the Console (sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE)) * 100 ## [1] 52.43129 After converting our annual turnover rate to a percentage, we see the value is approximately 52.43%. Finally, if we wish to assign the annual turnover rate to an object that we can subsequently reference in other functions and operations, we simply apply the &lt;- operator. Here, I’ve arbitrarily named this object TR_Annual. # Compute annual turnover rate and assign to object TR_Annual &lt;- (sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE)) * 100 We can print this object to our Console using the print function. # Print annual turnover rate object to the Console print(TR_Annual) ## [1] 52.43129 Like the monthly turnover rates, an annual turnover rate is typically evaluated by comparing it to prior years, comparing between units, or comparing to industry benchmarks. 45.2.6 Summary In this chapter, we learned how to compute monthly and annual turnover rates. For the annual turnover rates, we learned how to apply the sum and mean functions from base R. References "],["turnoverchisquare.html", "Chapter 46 Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence 46.1 Conceptual Overview 46.2 Tutorial 46.3 Chapter Supplement", " Chapter 46 Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence In this chapter, we will learn how to estimate whether theoretically or practically relevant categorical (nominal, ordinal) variables are associated with employees’ decisions to voluntarily turn over. In doing so, we will learn how to estimate the chi-square (\\(\\chi^2\\)) test of independence. In a previous chapter, we learned how to apply the \\(\\chi^2\\) test of independence to determine whether there was evidence of disparate impact. 46.1 Conceptual Overview Link to conceptual video: https://youtu.be/hsCW85LqpiE A chi-square (\\(\\chi^2\\)) test of independence is a type of categorical nonparametric statistical analysis that can be used to estimate the association between two categorical (nominal, ordinal) variables. As a reminder, categorical variables do not have inherent numeric values, and thus we often just describe how many cases belong to each category (i.e., counts, frequencies). It is very common to represent the association between two categorical variables as a contingency table, which is often referred to as a cross-tabulation. A \\(\\chi^2\\) test of independence is calculated by comparing the observed data in the contingency table (i.e., what we actually found in the sample) to a model in which the variables and data in another table are distributed to be independent of one another (i.e., expected data); there are other types of expected models, but we will focus on just an independent model in this review. The observed data includes the raw counts (i.e., frequencies) from the sample. The row and column marginals represent the sums of each row and column, respectively. The expected data includes the counts (i.e., frequencies) that would be expected if the two categorical variables were independent of one another (i.e., not associated). The expected values for each cell are calculated by multiplying the corresponding row and column marginals and dividing that product by the total sample size. In essence, a \\(\\chi^2\\) test of independence is used to test the null hypothesis that the two categorical variables are independent of one another (i.e., no association between them). If we reject the null hypothesis because the \\(\\chi^2\\) value relative to the degrees of freedom of the model is large enough, we conclude that the categorical variables are not in fact independent of one another but instead are contingent upon one another. In this chapter, we will focus specifically on a 2x2 \\(\\chi^2\\) test, which refers to the fact there are two categorical variables with two levels each (e.g., age: old vs. young; height: tall vs. short). As an added bonus, a 2x2 \\(\\chi^2\\) value can be converted to a phi (\\(\\phi\\)) coefficient, which can be interpreted as a Pearson product-moment correlation and thus as an effect size or indicator of practical significance. The formula for calculating \\(\\chi^2\\) is as follows: \\(\\chi^2 = \\sum_{i = 1}^{n} \\frac{(O_i - E_i)^2}{E_i}\\) where \\(O_i\\) refers to the observed values and \\(E_i\\) refers to expected values. The degrees of freedom (\\(df\\)) of a \\(\\chi^2\\) test is equal to the product of the number of levels of the first variable minus one and the number of levels of the second variable minus one. For example, a 2x2 \\(\\chi^2\\) test has 1 \\(df\\): \\((2-1)(2-1) = 1\\) 46.1.0.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; The nonoccurrences are included, where nonoccurrences refer to the instances in which an event or category does not occur. For example, when we investigate employee voluntary turnover, we often like to focus on the turnover event occurrences (i.e., quitting), but we also need to include nonoccurrences, which would be examples of the turnover event not occurring (i.e., staying). Failure to do so, could lead to misleading results. 46.1.0.2 Statistical Significance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning we reject the null hypothesis that the variables are independent. If the p-value associated with \\(\\chi^2\\) value is equal to or greater than .05 (or whatever two- or one-tailed alpha level we set), then we fail to reject the null hypothesis that the categorical variables are independent. Put differently, if the p-value associated with a \\(\\chi^2\\) value is equal to or greater than .05, we conclude that there is no association between the predictor variable and the outcome variable in the population. If we were to calculate the test by hand, then we might compare our calculated \\(\\chi^2\\) value to a table of critical values for a \\(\\chi^2\\) distribution. If our calculated value were larger than the critical value given the number of degrees of freedom (df) and the desired alpha level (i.e., significance level cutoff for p-value), we would conclude that there is evidence of an association between the categorical variables. Here is a website that lists critical values. Alternatively, we can calculate the exact p-value using statistical software if we know the exact \\(\\chi^2\\) value and the df, and most software programs compute the exact p-value automatically when estimating the \\(\\chi^2\\) test of independence. 46.1.0.3 Practical Significance By itself, the \\(\\chi^2\\) value is not an effect size; however, in the special case of a 2x2 \\(\\chi^2\\) test of independence, we can compute a phi (\\(\\phi\\)) coefficient, which can be interpreted as a Pearson product-moment correlation and thus as an effect size or indicator of practical significance. The size of a \\(\\phi\\) coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the \\(\\phi\\) values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. \\(\\phi\\) Description .10 Small .30 Medium .50 Large Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 46.1.0.4 Sample Write-Up Our organization created a new onboarding program a year ago. Today (a year later), we wish to investigate whether participating in the new vs. old onboarding program is associated with employees decisions to quit the organization for the subsequent six months. Based on a sample of 100 newcomers (N = 100) – half of whom went through the new onboarding program and half of whom went through the old onboarding program – we estimated a 2x2 chi-square (\\(\\chi^2\\)) test of independence. The \\(\\chi^2\\) test of independence was statistically significant (\\(\\chi^2\\) = 7.84, df = 1, p = .04), such that those who participated in the new onboarding program were less likely to quit in their first year. Note: If the p-value were equal to or greater than our alpha level (e.g., .05, two-tailed), then we would typically state that the association between the two variables is not statistically significant, and we would not proceed forward with interpreting the effect size (i.e., level of practical significance) because the test of statistical significance indicates that it is very unlikely based on our sample that a true association between these two variables exists in the population. 46.2 Tutorial This chapter’s tutorial demonstrates how a chi-square test of independence can be used to identify whether an association exists between a categorical (nominal, ordinal) variable and dichotomous turnover decisions (i.e., stay vs. quit). 46.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/l8v3kx6zNuQ 46.2.2 Functions &amp; Packages Introduced Function Package xtabs base R print base R xtabs base R chisq.test base R prop.table base R phi psych 46.2.3 Initial Steps If you haven’t already, save the file called “ChiSquareTurnover.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “ChiSquareTurnover.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object cst &lt;- read_csv(&quot;ChiSquareTurnover.csv&quot;) ## Rows: 75 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmployeeID, Onboarding, Turnover ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(cst) ## [1] &quot;EmployeeID&quot; &quot;Onboarding&quot; &quot;Turnover&quot; # Print number of rows in data frame (tibble) object nrow(cst) ## [1] 75 # Print structure of data frame (tibble) object str(cst) ## spc_tbl_ [75 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID: chr [1:75] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E8&quot; ... ## $ Onboarding: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ Turnover : chr [1:75] &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. Onboarding = col_character(), ## .. Turnover = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(cst) ## # A tibble: 6 × 3 ## EmployeeID Onboarding Turnover ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E1 No Quit ## 2 E2 No Quit ## 3 E3 No Quit ## 4 E8 No Quit ## 5 E14 No Quit ## 6 E22 No Quit There are 3 variables and 75 cases (i.e., employees) in the cst data frame: EmployeeID, Onboarding, and Turnover. Per the output of the str (structure) function above, all of the variables are of type character. EmployeeID is the unique employee identifier variable. Onboarding is a variable that indicates whether new employees participated in the onboarding program, and it has two levels: did not participate (No) and did participate (Yes) Turnover is a variable that indicates whether these individuals left the organization during their first year; it also has two levels: Quit and Stayed. 46.2.4 Create a Contingency Table for Observed Data Before running the chi-square test, we need to create a contingency (cross-tabulation) table for the Onboarding and Turnover variables. To do so, we will use the xtabs function from base R. In the function parentheses, as the first argument type the ~ operator followed by the two variables names, separated by the + operator (i.e., ~ Onboarding + Turnover). As the second argument, type data= followed by the name of the data frame to which both variables belong (cst). We will use the &lt;- assignment operator to name the table object (tbl). For more information on creating tables, check out the chapter on summarizing two or more categorical variables using cross-tabulations. # Create two-way contingency table tbl &lt;- xtabs(~ Onboarding + Turnover, data=cst) Let’s print the new tbl object to your Console. # Print table print(tbl) ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 As you can see, we have created a 2x2 contingency table, as it has two variables (Onboarding, Turnover) with each variable consisting of two levels (i.e., categories). This table includes our observed data. 46.2.5 Estimate Chi-Square (\\(\\chi^2\\)) Test of Independence Using the contingency table we created called tbl, we will insert it as the sole parenthetical argument in the chisq.test function from base R. # Estimate chi-square test of independence chisq.test(tbl) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tbl ## X-squared = 23.179, df = 1, p-value = 0.000001476 In the output, note that the \\(\\chi^2\\) value is 23.179 with 1 degree of freedom (df), and the associated p-value is less than .05, leading us to reject the null hypothesis that these two variables are independent of one another (\\(\\chi^2\\) = 23.176, df = 1, p &lt; .001). In other words, we have concluded that there is a statistically significant association between onboarding participation and whether someone quits in the first year. To understand the nature of this association, we need to look back to the contingency table values. In fact, sometimes it is helpful to look at the column (or row) proportions by applying the prop.table function from base R to the tbl object we created. As the second argument, type the numeral 2 to request the column marginal proportions. # Print column marginal proportions prop.table(tbl, 2) ## Turnover ## Onboarding Quit Stayed ## No 0.7272727 0.1320755 ## Yes 0.2727273 0.8679245 As you can see from the contingency table, of those employees who stayed, proportionally more employees had participated in the onboarding program (86.8%). Alternatively, for those who quit, proportionally more employees did not participate in the onboarding program (72.7%). Given our interpretation of the table along with the significant \\(\\chi^2\\) test, we can conclude that employees were significantly more likely to stay through the end of their first year if they participated in the onboarding program. In the special case of a 2x2 contingency table (or a 1x4 vector), we can compute a phi (\\(\\phi\\)) coefficient, which can be interpreted like a Pearson product-moment correlation coefficient and thus as an effect size. To calculate the phi coefficient, we will use the phi function from the psych package (Revelle 2023). If you haven’t already, install and access the psych package. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Within the phi function parentheses, insert the name of the 2x2 contingency table object (tbl) as the sole argument. # Estimate phi coefficient phi(tbl) ## [1] 0.587685 The \\(\\phi\\) coefficient for the association between Onboarding and Turnover is .59, which by conventional correlation standards is a large effect (\\(\\phi\\) = .59). Here is a table containing conventional rules of thumb for interpreting a Pearson correlation coefficient (r) or a phi coefficient (\\(\\phi\\)) as an effect size: \\(\\phi\\) Description .10 Small .30 Medium .50 Large Technical Write-Up: Based on a sample of 75 employees, we investigated whether participating in an onboarding program was associated with new employees’ decisions to quit in the first 12 months. Using a chi-square (\\(\\chi^2\\)) test of independence, we found that new employees who participated in the onboarding program were less likely to quit during their first 12 months on the job (\\(\\chi^2\\) = 23.176, df = 1, p &lt; .001). Specifically, of those employees who stayed at the organization, proportionally more employees had participated in the onboarding program (86.8%), and of those employees who quit the organization, proportionally more employees did not participate in the onboarding program (72.7%). The magnitude of this effect can be described as large (\\(\\phi\\) = .59). 46.2.5.1 Optional: Print Observed and Expected Counts In some instances, it might be helpful to view the observed and expected, as these are the basis for estimating the \\(\\chi^2\\) test of independence. First, we need to create an object that contains the information from the chisq.test function. Using the &lt;- assignment operator, let’s name this object chisq. # Create chi-square object chisq &lt;- chisq.test(tbl) We can request the observed counts/frequencies by typing the name of the chisq object, followed by the $ operator and observed. These will look familiar because they are the counts from our original contingency table. # Request observed counts/frequencies chisq$observed ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 To request the expected counts/frequencies, type the name of the chisq object, followed by the $ operator and expected. # Request expected counts/frequencies chisq$expected ## Turnover ## Onboarding Quit Stayed ## No 6.746667 16.25333 ## Yes 15.253333 36.74667 We can descriptively see departures from the observed and expected values in a systematic manner. To understand which cells were most influential in the calculation of the \\(\\chi^2\\) value, it is straightforward to request the Pearson residuals. Pearson residuals with larger absolute values have bigger contributions to the \\(\\chi^2\\) value. # Request Pearson residuals chisq$residuals ## Turnover ## Onboarding Quit Stayed ## No 3.562489 -2.295234 ## Yes -2.369277 1.526473 46.2.6 Summary In this chapter, we learned how to estimate a chi-square (\\(\\chi^2\\)) test of independence using the chisq.test function from base R. 46.3 Chapter Supplement In this chapter supplement, you will have an opportunity to learn how to compute and interpret an odds ratio and its 95% confidence interval, where the odds ratio is another method we can use to describe the association between two categorical variables from a 2x2 contingency table. 46.3.1 Functions &amp; Packages Introduced Function Package sqrt base R exp base R log base R print base R 46.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object cst &lt;- read_csv(&quot;ChiSquareTurnover.csv&quot;) ## Rows: 75 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): EmployeeID, Onboarding, Turnover ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(cst) ## [1] &quot;EmployeeID&quot; &quot;Onboarding&quot; &quot;Turnover&quot; # View variable type for each variable in data frame (tibble) object str(cst) ## spc_tbl_ [75 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID: chr [1:75] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E8&quot; ... ## $ Onboarding: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ Turnover : chr [1:75] &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. Onboarding = col_character(), ## .. Turnover = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame (tibble) object head(cst) ## # A tibble: 6 × 3 ## EmployeeID Onboarding Turnover ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E1 No Quit ## 2 E2 No Quit ## 3 E3 No Quit ## 4 E8 No Quit ## 5 E14 No Quit ## 6 E22 No Quit 46.3.3 Compute Odds Ratio for 2x2 Contingency Table When communicating the association between two categorical variables, sometimes it helps to the express the association as an odds ratio. An odds ratio allows us to describe the association between two variables as, “given X, the odds of experiencing the event are Y times greater.” To get started, let’s create a 2x2 contingency table object called tbl, which will house the counts for our two categorical variables (Onboarding, Turnover), where each variable has two levels. # Create two-way contingency table tbl &lt;- xtabs(~ Onboarding + Turnover, data=cst) # Print table print(tbl) ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 I’m going to demonstrate how to manually compute the odds ratio for a 2x2 contingency table, but note that you can estimate a logistic regression model to arrive at the same end, which is covered in the chapter on identifying predictors of turnover. I should also note that there are some great functions out there like the oddsratio.wald function from the epitools package that will do these calculations for us; nonetheless, I think it’s worthwhile to unpack how relatively simple these calculations are, as this can serve as a nice warm up for the following chapter. When we have a 2x2 contingency table of counts, the odds ratio is relatively straightforward to calculate if we conceptualize our contingency table as follows. Because we are expecting that those who participate in the treatment (i.e., onboarding program) will be more likely to stay in the organization, let’s frame the act of staying as the focal event, which means that those who quit did not experience the event of staying. Condition No Event (Quit) Event (Stayed) Control (No Onboarding) D C Treatment (Onboarding) B A The formula for computing an odds ratio is: \\(OR = \\frac{\\frac{A}{C}}{\\frac{B}{D}}\\) where: A is the number of individuals who participated in the treatment condition (e.g., onboarding program) and who also experienced the event in question (e.g., stayed); B is the number of individuals who participated in the treatment condition (e.g., onboarding program) but who did not experience the event in question (e.g., quit); C is the number of individuals who participated in the control condition (e.g., no onboarding program) but who also experienced the event in question (e.g., stayed); D is the number of individuals who participated in the control condition (e.g., no onboarding program) and who did not experience the event in question (e.g., quit). The formula can also be written in the following manner after cross-multiplication, which is what we’ll use when computing the odds ratio, as I think it is easier to read in R: \\(OR = \\frac{A \\times D}{B \\times C}\\) To make our calculations clearer, let’s reference specific cells from our contingency table object (tbl) using brackets ([ ]), where the first value within brackets references the row number and the second references the column number. We’ll assign the count value from each score to objects A, B, C, and D, to be consistent with the above formula. # Assign counts to objects A &lt;- tbl[2,2] B &lt;- tbl[2,1] C &lt;- tbl[1,2] D &lt;- tbl[1,1] Next, we will plug objects A, B, C, and D into the formula (see above) using the multiplication (*) and division (/) operators, and assign the result to an object called OR using the &lt;- assignment operator. # Calculate odds ratio of someone *staying* # who participated in the treatment (onboarding program) OR &lt;- (A * D) / (B * C) # Print odds ratio print(OR) ## [1] 17.52381 The odds ratio is 17.52, which indicates that for those who participated in the onboarding program the odds of staying was 17.52 greater. If an odds ratio is equal to 1.00, then it signifies no association – or equal odds of experiencing the focal event. If an odds ratio is greater than 1.00, then it signifies a positive association between the two variables, such that the higher level on the first variable (e.g., receiving treatment) is associated with the higher level on the second variable (e.g., experiencing event). Alternatively, if we would like to, instead, understand the odds of someone quitting after participating in the onboarding program, we would flip the numerator and denominator from our above code. Effectively, in this example, this simple operation switches our focal event from staying to quitting, thereby giving us a different perspective on the association between the two variables. # Calculate odds ratio of someone *quitting* # who participated in the treatment (onboarding program) OR_alt &lt;- (B * C) / (A * D) # flip numerator and denominator # Print odds ratio print(OR_alt) ## [1] 0.05706522 The new odds ratio (OR_alt) is approximately .06 (with rounding). Because an odds ratio that is less than 1.00 signifies a negative association, this odds ratio indicates that those who participated in the treatment (onboarding program) are less likely to have experienced the event of quitting. If we compute the reciprocal of the odds ratio, we arrive back at our original odds ratio. # Calculate reciprocal of odds ratio 1 / OR_alt ## [1] 17.52381 This takes us back to our original interpretation, which is that for those who participated in the onboarding program the odds of staying was 17.52 greater. I should note that an odds ratio of 17.52 is a HUGE effect. When working with a 2x2 contingency table, I recommend computing the reciprocal of any odds ratio less than 1.00, as it often leads to a more straightforward interpretation – just remember that the we have to conceptually flip the event and non-event when interpreting the now positive association between the two categorical variables. As a significance test for the odds ratio, we can compute the 95% confidence intervals. Because an odds ratio of 1.00 signifies no association between the two variables, any confidence interval that does not include 1.00 will be considered statistically significant. To compute the confidence interval, we first need to compute the standard error of the odds ratio using the following formula. \\(SE = \\sqrt{\\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\) # Calculate standard error (SE) of odds ratio SE &lt;- sqrt((1/A) + (1/B) + (1/C) + (1/D)) To construct the lower and upper limits of a 95% confidence interval, we need to first multiply the standard error (SE) by 1.96. To compute the lower limit, we subtract that product from the natural log of odds ratio (log(OR)) and then exponentiate the resulting value using the exp function from base R, where the exp function takes Euler’s constant (\\(e\\)) and adds a specified exponent to it. To compute the upper limit, we add that product to the natural log of odds ratio and then exponentiate the resulting value. # Calculate 95% confidence interval for odds ratio exp(log(OR) - 1.96*SE) # lower limit ## [1] 5.122538 exp(log(OR) + 1.96*SE) # upper limit ## [1] 59.94761 The 95% confidence interval ranges from 5.12 to 59.95 (95% CI[5.12, 59.95]). Because this interval does not include 1.00, we can conclude that the odds ratio of 17.52 is statistically significantly different from 1.00. References "],["logistic.html", "Chapter 47 Identifying Predictors of Turnover Using Logistic Regression 47.1 Conceptual Overview 47.2 Tutorial 47.3 Chapter Supplement", " Chapter 47 Identifying Predictors of Turnover Using Logistic Regression In this chapter, we will learn how to estimate a (binary) logistic regression model in order to identify potential predictors of employee voluntary turnover, when voluntary turnover is operationalized as a dichotomous (i.e., binary) variable (e.g., stay vs. quit). 47.1 Conceptual Overview Logistic regression (logit model) is part of the family of generalized linear models (GLMs). Assuming statistical assumptions have been satisfied, a binary logistic regression model is appropriate when the outcome variable of interest is dichotomous (i.e., binary) and when the predictor variable(s) of interest is/are continuous (interval, ratio) or categorical (nominal, ordinal). Unlike ordinary least squares (OLS) estimation, which was covered previously in the context of simple linear regression and multiple linear regression, logistic regression coefficients are typically estimated using maximum likelihood (ML). There are also extensions of the logistic regression like multinomial and ordinal logistic regression, where these extensions are appropriate when the categorical outcome variable is nominal or ordinal with three or more levels/categories. Logistic regression can be used to determine the odds that a dichotomous event occurs (e.g., stay vs. quit) given higher or lower values/levels on one or more predictor variables, where odds refers to the probability of an event occurring (p) relative to the probability of the event not occurring (1 - p). As such, an odds value of 1 can be interpreted as 1 to 1 odds; or in other words, the probability of the event occurring is equal to the probability of the event not occurring, which would be akin to flipping a fair coin. For example, if we find that the probability of quitting is .75 (p = .75; i.e., event occurring), then by extension, the probability of not quitting is .25 (1 - p = .25; i.e., event not occurring). Given these probabilities, the odds of quitting are 3 (.75 / .25 = 3), and the odds of not quitting are 1/3 or .33, which is calculated as: .25 / (1 - .25). A logit transformation of the odds of quitting and of the odds of not quitting will be symmetrical, where a logit transformation refers to taking the natural log (\\(\\ln\\)) of both values (i.e., logarithmic transformation). For example, a logit transformation of 3 and 1/3 yields 1.10 and -1.10, which are symmetrical: \\(\\ln(3) = 1.10\\) and \\(\\ln(1/3) = -1.10\\). 47.1.1 Review of Logistic Regression Just as there is a distinction between simple and multiple linear regression models, we can also draw a distinction between simple and multiple logistic regression models. When there is a single predictor variable and a dichotomous outcome variable, we can apply what is referred to as a simple logistic regression model. More specifically, a simple logistic regression refers to the bivariate linear association between a predictor variable and a dichotomous outcome variable that has undergone a logit transformation, as shown in the equation below. \\(logit(p) = \\log(odds) = \\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1)\\) where \\(logit(p)\\) represents the logit transformation of the outcome, \\(\\log(odds)\\) represents the log(arithmic) odds, \\(\\ln\\) represents the natural log, \\(p\\) represents the probability of an event occurring, \\(b_0\\) represents the intercept value, and \\(b_1\\) represents the regression coefficient (i.e., slope, weight) of the association between the predictor variable \\(X_1\\) and the logit transformation of the outcome variable. And when we have two or more predictor variables and a single dichotomous outcome variable, we estimate what is called a multiple logistic regression model, where an example of a multiple logistic regression model follows. \\(logit(p) = \\log(odds) = \\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1) + b_2(X_2)\\) where \\(b_2\\) represents the regression coefficient (i.e., slope, weight) of the association between the second predictor variable \\(X_2\\) and the logit transformation of the outcome variable. Logistic Function: Logistic regression is predicated on the logistic function. The scatter plot figure shown below illustrates the logistic function when there is a continuous (interval, ratio) predictor variable called \\(X\\) and a dichotomous outcome variable called \\(Y\\). Because a linear function would not not closely approximate the association between these two variables, we instead use a logistic function, which is a sigmoidal (or sigmoid) function and takes the visual form of an S-curve. The sigmoidal (sigmoid) function is shown in red and represents the probability of an event occurring at each level/value of the predictor variable. Just like simple and multiple linear regression models, regression coefficients are estimated in simple logistic regression models, but these coefficients are not perhaps as easily interpretable in their original form because the outcome variable undergoes a logarithmic transformation, as noted above. The regression coefficients in a logistic regression model represent the change in log odds (i.e., logit transformation) for every one unit change in the predictor variable. Odds Ratio: To make a statistically significant regression coefficient (\\(b_i\\)) easier to interpret, we often convert the coefficient to an odds ratio. To do so, we exponentiate the coefficient using Euler’s number (\\(e\\)). Euler’s number (\\(e\\)) is an irrational number and mathematical constant that is approximately equal to 2.71828. \\(e^{b_i}\\) An odds ratio that is less than 1.00 indicates a negative association between the predictor variable and outcome variable, and an odds ratio that is greater than 1.00 indicates a positive association. An odds ratio equal to 1.00 indicates that there is no association between the variables. Note: In the case of a multiple logistic regression model where we have two or more predictor variables, we would describe an odds ratios more accurately as an adjusted odds ratio because we are statistically controlling for the other predictor variable(s) in the model. Example of Interpreting a Negative Association: As an example, let’s imagine in a simple logistic regression model we find that the coefficient for the association between job satisfaction (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is -.42 – that is, we find a negative association. If we interpret the coefficient in its original form, we might say something like: “For every one unit increase in job satisfaction, there is as .42 unit reduction in log odds of quitting.” Such language will not typically be well-received by organizational stakeholders; however, if we convert the coefficient to an odds ratio by exponentiating it, we might find it easier to explain the finding to ourselves and others. \\(e^{-.42} = .66\\) In this example, the odds ratio of .66 is less than 1.00, which reflects back to us that the association is indeed negative, which we already knew from the original log odds coefficient value of -.42. We can interpret this odds ratio as follows: “For every unit increase in job satisfaction, the odds of quitting is reduced by 34% (1 - .66 = .34).” Alternatively, we can interpret the odds ratio from a different vantage point if we compute its reciprocal, which is 1.52 (1 / .66 = 1.52); in doing so, we can interpret the finding as: “For every unit increase in job satisfaction, the odds of quitting is reduced by 1 in 1.52.” Or, we could frame the finding in terms of not quitting: “For every unit increase in job satisfaction, the odds of not quitting is 1.52 times greater – or has a 1.52 times higher likelihood.” Example of Interpreting a Positive Association: Now that we’ve worked through an example of a negative association, let’s practice interpreting a positive association. Let’s imagine a different simple logistic regression model in which we find that the coefficient for the association between turnover intentions (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is .89. When interpreting the coefficient in its original form, we might say: “For every one unit increase in turnover intentions, there is as .89 unit increase in log odds of quitting.” Just as we did before, we can exponentiate the coefficient to find the odds ratio. \\(e^{.99} = 2.44\\) Because the odds ratio of 2.44 is greater than 1.00, it confirms to us that the association between turnover intentions and voluntary turnover is positive. We can interpret this finding as: “For every unit increase in turnover intentions, the odds of quitting is 2.44 times greater – or has a 2.44 times higher likelihood.” As another example, let’s imagine that we observe an odds ratio of 1.29 with respect to negative affectivity in relation to voluntary turnover. We could interpret this finding as: “For every unit increase in negative affectivity, the odds of quitting is 1.29 times greater.” Or, because 1.29 times greater is equivalent to saying that there was a 29% increase, we could say: “For every unit increase in negative affectivity, the odds of quitting increases by 29%.” Predicted Probabilities: Our logistic regression coefficients can also be used to predict the probability of the event occurring for different value(s) of the predictor variable(s). The equations that follow can help us to understand algebraically how we can determine the probability (\\(p\\)) of an event occurring based on the coefficients we might estimate for simple logistic regression model. \\(\\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1)\\) \\(\\frac{p}{1-p} = e^{b_0 + b_1(X_1)}\\) \\(p = \\frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}}\\) where \\(e\\) represents Euler’s number. As an example, let’s first imagine that our estimated intercept (\\(b_0\\)) is .32 and our estimated regression coefficient associated with the predictor variable (\\(b_1\\)) is -.09. Next, let’s imagine that someone has a score of 3 on the predictor variable (\\(X_1\\)). If we plug those values into the equation, we will find that the probability of a person with a score of 3 on the predictor variable is .51. \\(p = \\frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}} = \\frac{e^{.32 + (-.09 \\times 3)}}{1+e^{.32 + (-.09 \\times 3)}} = .51\\) If we set a probability threshold of .50 for experiencing the event, then we would classify anyone who scores a 3 on the predictor variable as having been predicted to experience the event in question. If however, the computed probability had been less than the probability threshold of .50, then we would have classified any associated with cases as having been predicted to not experience the event in question. 47.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple or multiple logistic regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of bivariate/multivariate outliers; The association between any continuous predictor variable(s) and the logit transformation of the outcome variable is linear; The outcome variable is dichotomous; For a multiple logistic regression model, there is no (multi)collinearity between predictor variables. The fifth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so let’s take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple logistic regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights – and even the signs – of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., multilevel logit model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 47.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficient’s p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficient’s p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. Keep in mind that in the context of a multiple logistic regression model, the association between each predictor variable and the logit transformation of the outcome variable must be interpreted with statistical control in mind, as we are effectively testing whether each predictor variable shows evidence of incremental validity in the presence of any other predictor variables. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. Note: In a logistic regression model, we may also construct confidence intervals around the odds ratios. 47.1.1.3 Practical Significance In its original form, a logistic regression coefficient is not an effect size; that is, it doesn’t provide an indication of practical significance. The odds ratio, however, can be conceptualized as an effect size. With that being said, there are some caveats. First, an odds ratio that is computed directly from an unstandardized logistic regression coefficient needs to be interpreted based on the raw scaling of the predictor variable, as the interpretation of the odds ratio has to do with the change in odds for unit change in the predictor variable. If we wish to compare odds ratios within or between models, we need to take this scaling issue into account. Second, in the case of a multiple logistic regression model, statistical control is at play, which means that the odds ratios are more accurately described as adjusted odds ratios. There are different thresholds we can apply when interpreting the magnitude of an odds ratio, and below I provide some thresholds that we will use in this tutorial. With that being said, thresholds for qualitatively interpreting effect sizes should be context dependent, and there are other thresholds we might apply. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large At the model level, we can’t compute a true R2 when estimating a logistic regression model. We can, however, compute a pseudo-R2, There are different formulas available for computing pseudo-R2 (e.g., Cox &amp; Snell, McFadden), and in this chapter we’ll focus on the following Nagelkerke (1991) formula: \\(pseudo-R^2 = \\frac{1 - (\\frac{L(M_{null})}{L(M_{full})})^{2/N}}{1-L(M_{null})^{2/N}}\\) where \\(L(M_{null})\\) is the likelihood of the outcome variable given a null, intercept-only model, \\(L(M_{full})\\) is the likelihood of the outcome variable given the predictor variable(s) in the model, and \\(N\\) is the sample size. It’s important to note, though, that as the name implies, a pseudo-R2 is not the same thing as a true R2. Thus, while we need to be cautious in our interpretations. In addition to pseudo-R2, we can also describe the classification accuracy of the model by using a confusion matrix (or classification table). A confusion matrix presents the percentage of correctly predicted values on the outcome variable; if a probability for a case based on the model is equal to or greater than .50, then it would be classified as a probability of 1, and all else would be classified as 0. For example, using a confusion matrix, we can make statements like: “The model correctly classified 55.9% of the employees as either stay or quit.” To indicate how well your model fit the data and performed, I recommend reporting either pseudo-R2, model classification accuracy, or both. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 47.1.1.4 Sample Write-Up A voluntary turnover study was conducted based on a sample 99 employees from the past year, some of whom quit the company and some of whom stayed. The focal outcome variable is turnover behavior (quit vs. stay), and because it is dichotomous, we used logistic regression. We were, specifically, interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]). For every one unit increase in negative affectivity, the odds of quitting were 3.304 times greater, when controlling for the other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]). For every one unit increase in turnover intentions, the odds of quitting were 2.451 times greater, when controlling for other predictor variables in the model. Both of these significant associations can be described as medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to correct classify 78.9% of employees from our sample using the estimated multiple logistic regression model. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. 47.2 Tutorial This chapter’s tutorial demonstrates how to estimate simple and multiple logistic regression models using R. 47.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/O7gRceyeyT8 47.2.2 Functions &amp; Packages Introduced Function Package Logit lessR log base R PseudoR2 DescTools exp base R glm base R merge base R data.frame base R mutate dplyr ifelse base R c base R predict base R detach base R 47.2.3 Initial Steps If you haven’t already, save the file called “Turnover.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “Turnover.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;Turnover.csv&quot;) ## Rows: 99 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): ID ## dbl (5): Turnover, JS, OC, TI, NAff ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;NAff&quot; # View variable type for each variable in data frame str(td) ## spc_tbl_ [99 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ... ## $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ... ## $ JS : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ... ## $ OC : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ... ## $ TI : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ... ## $ NAff : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_character(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. NAff = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(td) ## # A tibble: 6 × 6 ## ID Turnover JS OC TI NAff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EMP559 1 4.96 5.32 0.51 1.87 ## 2 EMP561 1 1.72 1.47 4.08 2.48 ## 3 EMP571 1 1.64 0.87 2.65 2.84 ## 4 EMP589 1 3.01 2.15 4.17 2.43 ## 5 EMP592 1 3.04 1.94 3.27 2.76 ## 6 EMP601 1 3.81 3.81 3.01 3.67 There are 5 variables and 99 cases (i.e., employees) in the td data frame: ID, Turnover, JS, OC, TI, and NAff. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio), except for the ID variable, which is of type character. ID is the unique employee identifier variable. Imagine that these data were collected as part of a turnover study within an organization to determine the drivers/predictors of turnover based on a sample of employees who stayed and leaved during the past year. The variables JS, OC, TI, and NAff were collected as part of an annual survey and were later joined with the Turnover variable. Survey respondents rated each survey item using a 7-point response scale, ranging from strongly disagree (0) to strongly agree (6). JS contains the average of each employee’s responses to 10 job satisfaction items. OC contains the average of each employee’s responses to 7 organizational commitment items. TI contains the average of each employee’s responses to 3 turnover intentions items, where higher scores indicate higher levels of turnover intentions. NAff contains the average of each employee’s responses to 10 negative affectivity items. Turnover is a variable that indicates whether these individuals left the organization during the prior year, with 1 = quit and 0 = stayed. Note: If the Turnover variable were to include the character values of quit and stay instead of 1 and 0, the functions covered in this tutorial would automatically convert the character values to 0 and 1 (behind the scenes), where 0 would be assigned to the character value that comes first alphabetically. 47.2.4 Estimate Simple Logistic Regression Model We’ll begin by specifying a simple logistic regression model, which means the model will include just one predictor variable. Let’s begin by regressing Turnover on JS using the Logit function from the lessR package (Gerbing, Business, and University 2021). If you haven’t already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) As the first argument in the Logit function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame to which both of the variables belong (td). Let’s begin by specifying JS as the predictor variable. # Estimate simple logistic regression model Logit(Turnover ~ JS, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 69 6.00 1 0 0.3162 0.1215 ## 97 5.59 0 0 0.3562 0.1120 ## 70 5.48 0 0 0.3673 0.1090 ## 73 5.48 1 0 0.3673 0.1090 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover label fitted std.err ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover label fitted std.err ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 4.238 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 59 60.2 10 49 83.1 ## Turnover 0 39 39.8 8 31 20.5 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Sensitivity: 83.05 ## Precision: 61.25 Note: In some instances, you might receive an error message like the one shown below. You can ignore such a message, as it just indicates that you have a very poor-performing model that results in predicted outcome-variable classifications that are all the same (e.g., all of the predicted values are 0). If you receive such a message, you can proceed forward with your interpretation of the output. \\(\\color{red}{\\text{Error:}}\\) \\(\\color{red}{\\text{All predicted values are 0.}}\\) \\(\\color{red}{\\text{Something is wrong here.}}\\) The output generates the model coefficient estimates, the odds ratios and their confidence intervals, model fit information (i.e., AIC), outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model (which we probably already knew), the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis after excluding cases with missing data (N = 98). 47.2.4.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Bivariate Outliers: To determine whether the data are free of bivariate outliers, let’s take a look at the text output section called Analysis of Residuals and Influence. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cook’s distance (cooks). The case associated with row number 66 has the highest Cook’s distance value (.085), followed by the cases associated with row numbers 67 and 71, which have Cook’s distance values of .062 and .049. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we may want to estimate our model once more after removing the cases associated with row numbers 66, 66, and 71 from our data frame; however, these Cook’s distance values don’t look too concerning or out of the ordinary, and thus I wouldn’t recommend removing the associated cases. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between our predictor variable JS and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of JS. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of the predictor variable JS followed by the + operator. After the + operator, type the name of the predictor variable JS, followed by the : operator and the log function from base R with the predictor variable JS as its sole parenthetical argument. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable Logit(Turnover ~ JS + JS:log(JS), data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 5.4461 3.2062 1.699 0.089 -0.8379 11.7300 ## JS -2.9840 2.1693 -1.376 0.169 -7.2357 1.2677 ## JS:log(JS) 1.1696 0.9778 1.196 0.232 -0.7467 3.0860 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 231.8457 0.4326 124248.7675 ## JS 0.0506 0.0007 3.5527 ## JS:log(JS) 3.2208 0.4739 21.8896 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 124.621 on 95 degrees of freedom ## ## AIC: 130.6211 ## ## Number of iterations to convergence: 5 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 7 1.38 0 0.8639 -0.8639 -2.105 -0.4827 0.158262 ## 69 6.00 1 0.5290 0.4710 1.221 0.5442 0.090322 ## 12 1.72 0 0.8029 -0.8029 -1.852 -0.3440 0.063819 ## 31 1.77 0 0.7936 -0.7936 -1.821 -0.3267 0.055946 ## 97 5.59 0 0.5044 -0.5044 -1.239 -0.3947 0.049267 ## 73 5.48 1 0.4993 0.5007 1.224 0.3561 0.039962 ## 70 5.48 0 0.4993 -0.4993 -1.221 -0.3554 0.039735 ## 58 5.43 1 0.4972 0.5028 1.224 0.3418 0.036908 ## 13 1.96 0 0.7577 -0.7577 -1.713 -0.2696 0.034590 ## 80 5.04 0 0.4853 -0.4853 -1.174 -0.2378 0.017543 ## 1 4.96 1 0.4840 0.5160 1.225 0.2327 0.017368 ## 33 4.88 1 0.4830 0.5170 1.225 0.2184 0.015319 ## 61 2.52 0 0.6572 -0.6572 -1.476 -0.1788 0.012429 ## 74 2.56 0 0.6506 -0.6506 -1.462 -0.1758 0.011886 ## 75 2.57 0 0.6490 -0.6490 -1.459 -0.1751 0.011761 ## 84 4.66 1 0.4823 0.5177 1.221 0.1854 0.011051 ## 63 4.65 1 0.4823 0.5177 1.221 0.1842 0.010899 ## 67 2.65 0 0.6363 -0.6363 -1.433 -0.1701 0.010877 ## 96 2.82 0 0.6108 -0.6108 -1.384 -0.1623 0.009530 ## 21 4.64 0 0.4824 -0.4824 -1.159 -0.1737 0.009334 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 84 4.66 1 0 0.4823 0.08526 ## 63 4.65 1 0 0.4823 0.08471 ## 21 4.64 0 0 0.4824 0.08416 ## 53 4.63 0 0 0.4824 0.08363 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover label fitted std.err ## 70 5.48 0 0 0.4993 0.15604 ## 73 5.48 1 0 0.4993 0.15604 ## 55 3.97 1 1 0.5005 0.06559 ## 16 3.95 0 1 0.5015 0.06547 ## 8 3.92 1 1 0.5031 0.06531 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover label fitted std.err ## 66 1.19 1 1 0.8945 0.08584 ## 48 1.05 1 1 0.9147 0.08191 ## 88 0.67 1 1 0.9582 0.06150 ## 24 0.23 1 1 0.9874 0.02972 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 1.825 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 59 60.2 9 50 84.7 ## Turnover 0 39 39.8 14 25 35.9 ## --------------------------------------------------- ## Total 98 65.3 ## ## Accuracy: 65.31 ## Sensitivity: 84.75 ## Precision: 66.67 Because the interaction term (JS:log(JS)) regression coefficient of 1.1696 in the Model Coefficients table is nonsignificant (p = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is nonlinear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (“Curvilinear Effects in Logistic Regression”) from Osborne (2015). Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well. Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. Just for the sake of demonstration, let’s transform the JS variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (JS_0) that is equal to JS minus the minimum value of JS. # ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero td$JS_0 &lt;- td$JS - min(td$JS, na.rm=TRUE) Now that we have a variable called JS_0 with at least one score equal to zero, let’s try the try the Box-Tidwell test. I’m going to add the argument brief=TRUE to reduce the amount of output generated by the function. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with predictor containing zero value(s)] Logit(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, brief=TRUE) If you ran the script above, you likely got an error message that looked like this: \\(\\color{red}{\\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\\) The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on JS_0, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above). Using the first approach, create a new variable in which the JS_0 variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the JS_0 variable, which results in the lowest score on the new variable (JS_1) being 1. As verification, I include the min function from base R. # Linear transformation that results in lowest score being 1 td$JS_1 &lt;- td$JS_0 + abs(min(td$JS_0, na.rm=TRUE)) + 1 # Verify that new lowest score is 1 min(td$JS_1, na.rm=TRUE) ## [1] 1 It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with transformed predictor = 1] Logit(Turnover ~ JS_1 + JS_1:log(JS_1), data=td, brief=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS_1 ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 8.0985 5.0081 1.617 0.106 -1.7171 17.9141 ## JS_1 -4.0594 2.9774 -1.363 0.173 -9.8950 1.7762 ## JS_1:log(JS_1) 1.5097 1.2264 1.231 0.218 -0.8939 3.9133 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 3289.6100 0.1796 60256846.1183 ## JS_1 0.0173 0.0001 5.9072 ## JS_1:log(JS_1) 4.5254 0.4090 50.0661 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 124.575 on 95 degrees of freedom ## ## AIC: 130.5747 ## ## Number of iterations to convergence: 4 There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (JS_1:log(JS_1)) is nonsignificant (b = 1.510, p = .218). Thus, we don’t see evidence that the assumption of linearity has been violated. We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the rows= argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that we’re back to using the variable called JS_0 that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with only cases with scores greater than zero] Logit(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, rows=(JS_0 &gt; 0), brief=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS_0 ## ## Number of cases (rows) of data: 97 ## Number of cases retained for analysis: 97 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 4.6652 2.8216 1.653 0.098 -0.8650 10.1953 ## JS_0 -2.6157 1.9936 -1.312 0.189 -6.5230 1.2916 ## JS_0:log(JS_0) 1.0392 0.9280 1.120 0.263 -0.7796 2.8579 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 106.1854 0.4211 26777.8281 ## JS_0 0.0731 0.0015 3.6387 ## JS_0:log(JS_0) 2.8269 0.4586 17.4256 ## ## ## -- Model Fit ## ## Null deviance: 130.725 on 96 degrees of freedom ## Residual deviance: 124.616 on 94 degrees of freedom ## ## AIC: 130.6162 ## ## Number of iterations to convergence: 4 We lost one case because that person had a score of zero on the JS_0 continuous predictor variable, and we see that the interaction term (JS_0:log(JS_0)) is non significant (b = 1.0392, p = .263). To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your continuous predictor variable has scores that are equal to or less than zero. Now that we’ve met the assumption of linearity, we’re finally ready to interpret the model results! 47.2.4.2 Interpret Model Results Basic Analysis: The Basic Analysis section of the original output first displays a table called the Model Coefficients, which includes the regression coefficients (slopes, weights) and their standard errors, z-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (JS) in relation to the outcome variable (Turnover) is often of substantive interest. Here, we see that the unstandardized regression coefficient for JS is -.438, and its associated p-value is less than .05 (b = -.438, p = .025). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. Further, the 95% confidence interval ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (JS), the logistic function decreases by .438 units – or that for every one unit increase in the predictor variable (JS) the logit transformation of the outcome variable decreases by .438. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) To that end, to aid our interpretation of the significant finding, we can move our attention to the table called Odds ratios and confidence intervals. To convert our regression coefficient to an odds ratio, the Logit function has already exponentiated it. Behind the scenes, this is what happened: \\(e^{-.438} = .646\\) We could also do this manually by using the exp function from base R. Any difference between the Logit output and the output below is attributable to rounding. # For the sake of demonstration: # Exponentiate logistic regression coefficient to convert to odds ratio # Note that the Logit function already does this for us exp(-.438) ## [1] 0.6453258 In the Odds ratios and confidence intervals table, we see that indeed the odds ratio is approximately .646. Because the odds ratio is less than 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (\\(100 \\times .354\\)) for every one unit increase in job satisfaction (JS). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of not quitting (i.e., staying): The odds of not quitting are 1.548 times as likely for every one unit increase in job satisfaction (JS). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large In the Model Fit table, note that we don’t have an estimate of R-squared (R2) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-R-squared (R2) values, but for now let’s focus on what is produced in the Logit function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data by looking at the Specified Confusion Matrices table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 39 (39.8%) and 59 (60.2%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%). Overall, we tend to be most interested in the overall percentage of correct classifications, which is 58.2% – so not a monumental amount of prediction accuracy when using just JS (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Forecasts: In the output section called Forecasts, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. Nagelkerke pseudo-R2: To compute Nagelkerke’s pseudo-R2, we will need to install and access the DescTools package (Andri et mult. al. 2021) so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) To use the function, we’ll need to re-estimate our simple logistic regression model using the glm function from base R. To request a logistic regression model as a specific type of generalized linear model, we’ll add the family=binomial argument. Using the &lt;- assignment operator, we will assign the resulting estimated model to an object that we’ll arbitrarily call model1. # Estimate simple logistic regression model and assign to object model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) In the PseudoR2 function, we will specify the name of the model object (model1) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerke’s formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model1, &quot;Nagel&quot;) ## Nagelkerke ## 0.07258382 The estimated Nagelkerke pseudo-R2 is .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS explains 7.3% of the variance in Turnover. Because the DescTools package also has a function called Logit, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (JS) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (b = -.438, p = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. Dealing with Bivariate Outliers: If you recall above, we found that the cases associated with row numbers 66 and 67 in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cook’s distance value greater than 1.0). If you were to decide to remove cases 66 and 67, here’s what you would do. First, look at the data frame (using the View function) and determine which cases row numbers 66 and 67 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that they are associated with ID equal to EMP861 and EMP862, respectively. Next, with respect to estimating the logistic regression model, the model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: rows=(!ID %in% c(\"EMP861\",\"EMP862\")); the rows argument subsets the data frame within the Logit function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which ID is not (!) within the vector containing EMP861 and EMP862. Please consider revisiting the chapter on filtering (subsetting) data if you would like to see the full list of logical operators or to review how to filter out cases from a data frame before specifying the model. # Simple logistic regression model with outlier/influential cases removed Logit(Turnover ~ JS, data=td, rows=(!ID %in% c(&quot;EMP861&quot;,&quot;EMP862&quot;))) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 97 ## Number of cases retained for analysis: 96 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8799 0.7067 2.660 0.008 0.4948 3.2649 ## JS -0.4388 0.1997 -2.198 0.028 -0.8301 -0.0475 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.5528 1.6402 26.1785 ## JS 0.6448 0.4360 0.9536 ## ## ## -- Model Fit ## ## Null deviance: 128.887 on 95 degrees of freedom ## Residual deviance: 123.664 on 94 degrees of freedom ## ## AIC: 127.6641 ## ## Number of iterations to convergence: 4 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 96 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3202 0.6798 1.5612 0.3758 0.08581 ## 7 1.38 0 0.7815 -0.7815 -1.7807 -0.2965 0.06696 ## 73 5.48 1 0.3717 0.6283 1.4394 0.2963 0.04900 ## 12 1.72 0 0.7549 -0.7549 -1.7039 -0.2564 0.04677 ## 58 5.43 1 0.3769 0.6231 1.4281 0.2889 0.04624 ## 31 1.77 0 0.7509 -0.7509 -1.6928 -0.2507 0.04425 ## 13 1.96 0 0.7349 -0.7349 -1.6508 -0.2291 0.03562 ## 1 4.96 1 0.4264 0.5736 1.3247 0.2239 0.02592 ## 33 4.88 1 0.4350 0.5650 1.3076 0.2136 0.02334 ## 61 2.52 0 0.6844 -0.6844 -1.5305 -0.1720 0.01814 ## 97 5.59 0 0.3605 -0.3605 -0.9631 -0.2060 0.01765 ## 84 4.66 1 0.4589 0.5411 1.2617 0.1870 0.01736 ## 74 2.56 0 0.6806 -0.6806 -1.5221 -0.1685 0.01729 ## 70 5.48 0 0.3717 -0.3717 -0.9809 -0.2021 0.01715 ## 63 4.65 1 0.4600 0.5400 1.2596 0.1858 0.01713 ## 75 2.57 0 0.6797 -0.6797 -1.5200 -0.1676 0.01708 ## 80 5.04 0 0.4178 -0.4178 -1.0538 -0.1840 0.01480 ## 77 4.46 1 0.4807 0.5193 1.2209 0.1649 0.01316 ## 96 2.82 0 0.6553 -0.6553 -1.4682 -0.1483 0.01283 ## 39 4.43 1 0.4840 0.5160 1.2149 0.1618 0.01262 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 69 6.00 1 0 0.3202 0.1234 ## 97 5.59 0 0 0.3605 0.1134 ## 70 5.48 0 0 0.3717 0.1103 ## 73 5.48 1 0 0.3717 0.1103 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover label fitted std.err ## 39 4.43 1 0 0.4840 0.07517 ## 83 4.41 0 0 0.4862 0.07449 ## 64 4.26 1 1 0.5027 0.06955 ## 27 4.15 0 1 0.5147 0.06614 ## 14 4.14 0 1 0.5158 0.06584 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover label fitted std.err ## 7 1.38 0 1 0.7815 0.07718 ## 48 1.05 1 1 0.8052 0.08013 ## 88 0.67 1 1 0.8300 0.08191 ## 24 0.23 1 1 0.8556 0.08194 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 4.284 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 58 60.4 9 49 84.5 ## Turnover 0 38 39.6 8 30 21.1 ## --------------------------------------------------- ## Total 96 59.4 ## ## Accuracy: 59.38 ## Sensitivity: 84.48 ## Precision: 62.03 47.2.4.3 Optional: Compute Predicted Probabilities Based on Sample Data The Logit function makes it easy to compute the probabilities of the even occurring based on the sample observations. In fact, all we have to do is add the pred_all=TRUE argument to the function. # Simple logistic regression model with predicted probabilities Logit(Turnover ~ JS, data=td, pred_all=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 69 6.00 1 0 0.3162 0.12145 ## 97 5.59 0 0 0.3562 0.11198 ## 70 5.48 0 0 0.3673 0.10900 ## 73 5.48 1 0 0.3673 0.10900 ## 58 5.43 1 0 0.3724 0.10758 ## 80 5.04 0 0 0.4131 0.09555 ## 1 4.96 1 0 0.4217 0.09291 ## 33 4.88 1 0 0.4302 0.09023 ## 84 4.66 1 0 0.4540 0.08274 ## 63 4.65 1 0 0.4551 0.08240 ## 21 4.64 0 0 0.4561 0.08206 ## 53 4.63 0 0 0.4572 0.08172 ## 45 4.59 0 0 0.4616 0.08036 ## 47 4.57 0 0 0.4638 0.07968 ## 77 4.46 1 0 0.4757 0.07597 ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## 29 4.11 0 1 0.5140 0.06491 ## 23 4.10 0 1 0.5151 0.06462 ## 89 4.07 0 1 0.5184 0.06376 ## 99 4.06 0 1 0.5195 0.06348 ## 94 4.03 0 1 0.5228 0.06265 ## 55 3.97 1 1 0.5293 0.06105 ## 16 3.95 0 1 0.5315 0.06054 ## 8 3.92 1 1 0.5348 0.05978 ## 42 3.90 0 1 0.5369 0.05929 ## 57 3.87 1 1 0.5402 0.05858 ## 40 3.83 0 1 0.5446 0.05767 ## 6 3.81 1 1 0.5467 0.05724 ## 50 3.79 0 1 0.5489 0.05681 ## 98 3.69 1 1 0.5597 0.05489 ## 86 3.63 1 1 0.5662 0.05390 ## 11 3.62 1 1 0.5672 0.05375 ## 76 3.50 0 1 0.5801 0.05222 ## 54 3.49 1 1 0.5812 0.05211 ## 93 3.45 0 1 0.5854 0.05174 ## 56 3.44 0 1 0.5865 0.05166 ## 87 3.42 1 1 0.5886 0.05151 ## 28 3.37 1 1 0.5939 0.05119 ## 18 3.35 1 1 0.5960 0.05109 ## 82 3.34 0 1 0.5971 0.05105 ## 92 3.33 1 1 0.5981 0.05101 ## 46 3.32 0 1 0.5992 0.05097 ## 22 3.27 1 1 0.6044 0.05085 ## 59 3.27 1 1 0.6044 0.05085 ## 36 3.26 0 1 0.6055 0.05084 ## 60 3.25 0 1 0.6065 0.05083 ## 71 3.23 0 1 0.6086 0.05082 ## 78 3.22 1 1 0.6096 0.05082 ## 91 3.21 1 1 0.6107 0.05083 ## 35 3.19 1 1 0.6127 0.05085 ## 32 3.15 0 1 0.6169 0.05094 ## 37 3.15 1 1 0.6169 0.05094 ## 43 3.05 1 1 0.6272 0.05141 ## 5 3.04 1 1 0.6282 0.05147 ## 4 3.01 1 1 0.6313 0.05168 ## 79 3.01 1 1 0.6313 0.05168 ## 20 2.98 0 1 0.6343 0.05192 ## 30 2.96 0 1 0.6364 0.05209 ## 25 2.94 1 1 0.6384 0.05228 ## 81 2.93 1 1 0.6394 0.05237 ## 96 2.82 0 1 0.6504 0.05359 ## 51 2.79 1 1 0.6534 0.05397 ## 26 2.73 1 1 0.6593 0.05478 ## 34 2.67 1 1 0.6652 0.05565 ## 67 2.65 0 1 0.6671 0.05595 ## 65 2.62 1 1 0.6700 0.05642 ## 75 2.57 0 1 0.6749 0.05721 ## 74 2.56 0 1 0.6758 0.05738 ## 61 2.52 0 1 0.6797 0.05804 ## 95 2.48 1 1 0.6835 0.05871 ## 17 2.46 1 1 0.6853 0.05905 ## 41 2.38 1 1 0.6928 0.06044 ## 9 2.35 1 1 0.6956 0.06096 ## 19 2.33 1 1 0.6975 0.06131 ## 68 2.28 1 1 0.7021 0.06220 ## 72 2.08 1 1 0.7201 0.06571 ## 15 2.03 1 1 0.7245 0.06657 ## 90 2.00 1 1 0.7271 0.06708 ## 13 1.96 0 1 0.7305 0.06775 ## 38 1.96 1 1 0.7305 0.06775 ## 62 1.92 1 1 0.7340 0.06842 ## 49 1.84 1 1 0.7407 0.06971 ## 31 1.77 0 1 0.7466 0.07079 ## 85 1.76 1 1 0.7474 0.07095 ## 2 1.72 1 1 0.7507 0.07154 ## 12 1.72 0 1 0.7507 0.07154 ## 10 1.69 1 1 0.7532 0.07198 ## 3 1.64 1 1 0.7572 0.07270 ## 44 1.43 1 1 0.7737 0.07541 ## 7 1.38 0 1 0.7775 0.07598 ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 4.238 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 59 60.2 10 49 83.1 ## Turnover 0 39 39.8 8 31 20.5 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Sensitivity: 83.05 ## Precision: 61.25 In the output under the Forecasts section, you should see a section called Data, Fitted Values, Standard Errors. This section now contains the predicted probabilities and associated classifications for all cases in the sample. The column labeled fitted contains the predicted probabilities, and the column labeled predict contains the probabilities when applied to a default probability threshold of .50, such that 1 indicates that the probability was .50 or greater (i.e., event predicted to occur) 0 indicates the probability was less than .50 (i.e., event not predicted to occur). If desired, we can also append the predicted probabilities (i.e., fitted values) to our existing data frame by referencing the row names (i.e., row numbers) of each object when we merge. Let’s overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (prob_JS), followed by the = operator and our simple logistic regression model from above with $fitted.values to the end. This will extract just the fitted values from the output and then convert the vector to a data frame object. As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( prob_JS = Logit(Turnover ~ JS, data=td, pred_all=TRUE)$fitted.values ), by=&quot;row.names&quot;, all=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 69 6.00 1 0 0.3162 0.12145 ## 97 5.59 0 0 0.3562 0.11198 ## 70 5.48 0 0 0.3673 0.10900 ## 73 5.48 1 0 0.3673 0.10900 ## 58 5.43 1 0 0.3724 0.10758 ## 80 5.04 0 0 0.4131 0.09555 ## 1 4.96 1 0 0.4217 0.09291 ## 33 4.88 1 0 0.4302 0.09023 ## 84 4.66 1 0 0.4540 0.08274 ## 63 4.65 1 0 0.4551 0.08240 ## 21 4.64 0 0 0.4561 0.08206 ## 53 4.63 0 0 0.4572 0.08172 ## 45 4.59 0 0 0.4616 0.08036 ## 47 4.57 0 0 0.4638 0.07968 ## 77 4.46 1 0 0.4757 0.07597 ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## 29 4.11 0 1 0.5140 0.06491 ## 23 4.10 0 1 0.5151 0.06462 ## 89 4.07 0 1 0.5184 0.06376 ## 99 4.06 0 1 0.5195 0.06348 ## 94 4.03 0 1 0.5228 0.06265 ## 55 3.97 1 1 0.5293 0.06105 ## 16 3.95 0 1 0.5315 0.06054 ## 8 3.92 1 1 0.5348 0.05978 ## 42 3.90 0 1 0.5369 0.05929 ## 57 3.87 1 1 0.5402 0.05858 ## 40 3.83 0 1 0.5446 0.05767 ## 6 3.81 1 1 0.5467 0.05724 ## 50 3.79 0 1 0.5489 0.05681 ## 98 3.69 1 1 0.5597 0.05489 ## 86 3.63 1 1 0.5662 0.05390 ## 11 3.62 1 1 0.5672 0.05375 ## 76 3.50 0 1 0.5801 0.05222 ## 54 3.49 1 1 0.5812 0.05211 ## 93 3.45 0 1 0.5854 0.05174 ## 56 3.44 0 1 0.5865 0.05166 ## 87 3.42 1 1 0.5886 0.05151 ## 28 3.37 1 1 0.5939 0.05119 ## 18 3.35 1 1 0.5960 0.05109 ## 82 3.34 0 1 0.5971 0.05105 ## 92 3.33 1 1 0.5981 0.05101 ## 46 3.32 0 1 0.5992 0.05097 ## 22 3.27 1 1 0.6044 0.05085 ## 59 3.27 1 1 0.6044 0.05085 ## 36 3.26 0 1 0.6055 0.05084 ## 60 3.25 0 1 0.6065 0.05083 ## 71 3.23 0 1 0.6086 0.05082 ## 78 3.22 1 1 0.6096 0.05082 ## 91 3.21 1 1 0.6107 0.05083 ## 35 3.19 1 1 0.6127 0.05085 ## 32 3.15 0 1 0.6169 0.05094 ## 37 3.15 1 1 0.6169 0.05094 ## 43 3.05 1 1 0.6272 0.05141 ## 5 3.04 1 1 0.6282 0.05147 ## 4 3.01 1 1 0.6313 0.05168 ## 79 3.01 1 1 0.6313 0.05168 ## 20 2.98 0 1 0.6343 0.05192 ## 30 2.96 0 1 0.6364 0.05209 ## 25 2.94 1 1 0.6384 0.05228 ## 81 2.93 1 1 0.6394 0.05237 ## 96 2.82 0 1 0.6504 0.05359 ## 51 2.79 1 1 0.6534 0.05397 ## 26 2.73 1 1 0.6593 0.05478 ## 34 2.67 1 1 0.6652 0.05565 ## 67 2.65 0 1 0.6671 0.05595 ## 65 2.62 1 1 0.6700 0.05642 ## 75 2.57 0 1 0.6749 0.05721 ## 74 2.56 0 1 0.6758 0.05738 ## 61 2.52 0 1 0.6797 0.05804 ## 95 2.48 1 1 0.6835 0.05871 ## 17 2.46 1 1 0.6853 0.05905 ## 41 2.38 1 1 0.6928 0.06044 ## 9 2.35 1 1 0.6956 0.06096 ## 19 2.33 1 1 0.6975 0.06131 ## 68 2.28 1 1 0.7021 0.06220 ## 72 2.08 1 1 0.7201 0.06571 ## 15 2.03 1 1 0.7245 0.06657 ## 90 2.00 1 1 0.7271 0.06708 ## 13 1.96 0 1 0.7305 0.06775 ## 38 1.96 1 1 0.7305 0.06775 ## 62 1.92 1 1 0.7340 0.06842 ## 49 1.84 1 1 0.7407 0.06971 ## 31 1.77 0 1 0.7466 0.07079 ## 85 1.76 1 1 0.7474 0.07095 ## 2 1.72 1 1 0.7507 0.07154 ## 12 1.72 0 1 0.7507 0.07154 ## 10 1.69 1 1 0.7532 0.07198 ## 3 1.64 1 1 0.7572 0.07270 ## 44 1.43 1 1 0.7737 0.07541 ## 7 1.38 0 1 0.7775 0.07598 ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 4.238 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 59 60.2 10 49 83.1 ## Turnover 0 39 39.8 8 31 20.5 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Sensitivity: 83.05 ## Precision: 61.25 If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 prob_JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 We can then take those predicted probabilities and apply a threshold by which scores higher than that threshold will indicate that the event is likely to happen. A common threshold for dichotomous outcomes is .50. We’ll use the mutate function from the dplyr package (Wickham et al. 2023) and the ifelse function base R to make this happen. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) # Classify probabilities as 1 (Quit) or 0 (Stay) # Using threshold of .50 td &lt;- mutate(td, Pred_Turnover = ifelse(prob_JS &gt;= .50, # apply threshold 1, # if logical statement is true (&quot;Quit&quot;) 0) # if logical statement is false (&quot;Stay&quot;) ) It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the JS predictor variable, which we could then use to plug into our model; to do this with a model estimated using the Logit model, setting up the model as follows may be the best course of action. # Assign simple logistic regression model to an object mod &lt;- Logit(Turnover ~ JS, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## -- Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 66 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 67 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 71 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 54 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 4 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 25 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 5 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 27 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 83 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 60 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 58 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 68 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 72 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 73 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 64 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 79 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 75 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 33 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS Turnover label fitted std.err ## 66 6.00 1 0 0.3162 0.1215 ## 97 5.59 0 0 0.3562 0.1120 ## 68 5.48 0 0 0.3673 0.1090 ## 71 5.48 1 0 0.3673 0.1090 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover label fitted std.err ## 33 4.43 1 0 0.4790 0.07497 ## 82 4.41 0 0 0.4812 0.07431 ## 61 4.26 1 0 0.4976 0.06946 ## 20 4.15 0 1 0.5097 0.06609 ## 6 4.14 0 1 0.5107 0.06579 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover label fitted std.err ## 63 1.19 1 1 0.7916 0.07790 ## 43 1.05 1 1 0.8015 0.07904 ## 87 0.67 1 1 0.8266 0.08096 ## 17 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## Corresponding cutoff threshold for JS: 4.238 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 59 60.2 10 49 83.1 ## Turnover 0 39 39.8 8 31 20.5 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Sensitivity: 83.05 ## Precision: 61.25 # Extract intercept and predictor coefficient b0 = unname(mod$coefficients[&quot;(Intercept)&quot;]) # extract intercept b1 = unname(mod$coefficients[&quot;JS&quot;]) # extract predictor coefficient # Specify equation to estimate predicted probabilities # using sample data on which model was estimated td$prob_JS_alt &lt;- exp(b0 + b1*td$JS) / (1 + exp(b0 + b1*td$JS)) If we so desired, we could plug in new values for JS from another data frame object like we did in the chapter on predicting criterion scores using simple linear regression. Or we could specify a vector of values for JS that we wish to compute the predicted probabilities of turnover. # Create vector of plausible values for JS vec_JS &lt;- c(0,1,2,3,4,5,6) # &quot;Plug in&quot; vector of values in equation exp(b0 + b1*vec_JS) / (1 + exp(b0 + b1*vec_JS)) ## [1] 0.8647542 0.8049594 0.7270717 0.6322888 0.5260465 0.4173924 0.3162077 The 7 predicted probabilities for turning over corresponding to the 7 values we listed in our vector have been printed in order to our Console. 47.2.5 Estimate Multiple Logistic Regression Model Now we will expand our initial model by specifying a multiple logistic regression model, which means the model will include more than one predictor variable. Let’s begin by regressing Turnover on JS, NAff, and TI using the Logit function from the lessR package. If you haven’t already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) As the first argument in the Logit function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables are typed to the right of the ~ symbol. Separate predictor variables with the + symbol. As the second argument, type data= followed by the name of the data frame to which both of the variables belong (td). Let’s estimate a model with JS, NAff, and TI as the predictor variables. # Estimate multiple logistic regression model Logit(Turnover ~ JS + NAff + TI, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## Predictor Variable 2: NAff ## Predictor Variable 3: TI ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 95 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) -3.9286 1.8120 -2.168 0.030 -7.4800 -0.3772 ## JS -0.2332 0.2215 -1.053 0.293 -0.6674 0.2010 ## NAff 1.1952 0.4996 2.392 0.017 0.2160 2.1743 ## TI 0.8967 0.3166 2.832 0.005 0.2761 1.5172 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 0.0197 0.0006 0.6858 ## JS 0.7920 0.5130 1.2227 ## NAff 3.3041 1.2411 8.7963 ## TI 2.4514 1.3180 4.5593 ## ## ## -- Model Fit ## ## Null deviance: 127.017 on 94 degrees of freedom ## Residual deviance: 105.229 on 91 degrees of freedom ## ## AIC: 113.2285 ## ## Number of iterations to convergence: 4 ## ## ## Collinearity ## ## Tolerance VIF ## JS 0.885 1.129 ## NAff 0.973 1.028 ## TI 0.903 1.108 ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 95 cases (rows) of data] ## -------------------------------------------------------------------- ## JS NAff TI Turnover fitted residual rstudent dffits cooks ## 6 4.14 3.07 4.33 0 0.93448 -0.9345 -2.4565 -0.4581 0.15185 ## 1 4.96 1.87 0.51 1 0.08371 0.9163 2.3396 0.4680 0.13427 ## 66 6.00 1.45 2.37 1 0.18699 0.8130 1.9301 0.5252 0.10092 ## 42 4.57 2.01 4.27 0 0.77499 -0.7750 -1.8127 -0.5016 0.08228 ## 96 2.82 2.05 4.25 0 0.84220 -0.8422 -1.9794 -0.3798 0.05870 ## 74 3.50 3.28 1.37 0 0.59961 -0.5996 -1.4140 -0.4473 0.04692 ## 27 4.88 1.76 1.81 1 0.20749 0.7925 1.8220 0.3639 0.04554 ## 26 3.15 2.84 3.48 0 0.86430 -0.8643 -2.0397 -0.3080 0.04250 ## 67 1.38 2.30 3.18 0 0.79411 -0.7941 -1.8204 -0.3391 0.03973 ## 61 4.26 1.73 2.10 1 0.27461 0.7254 1.6390 0.3009 0.02635 ## 68 5.48 2.27 2.78 0 0.49981 -0.4998 -1.2109 -0.3241 0.02182 ## 4 1.72 2.11 1.78 0 0.44724 -0.4472 -1.1203 -0.3091 0.01882 ## 25 1.77 2.24 1.16 0 0.34887 -0.3489 -0.9604 -0.3157 0.01803 ## 52 3.44 2.35 3.37 0 0.75019 -0.7502 -1.6856 -0.2364 0.01720 ## 54 5.43 3.01 2.43 1 0.64141 0.3586 0.9747 0.3071 0.01719 ## 69 3.23 2.84 2.18 0 0.66088 -0.6609 -1.4931 -0.2582 0.01719 ## 8 3.95 2.87 1.36 0 0.45014 -0.4501 -1.1216 -0.2899 0.01661 ## 75 4.46 2.81 1.80 1 0.50095 0.4991 1.2014 0.2785 0.01611 ## 77 3.01 1.53 3.01 1 0.47428 0.5257 1.2459 0.2729 0.01593 ## 64 2.65 1.94 3.27 0 0.66911 -0.6691 -1.5074 -0.2437 0.01552 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS NAff TI Turnover label fitted std.err ## 1 4.96 1.87 0.51 1 0 0.08371 0.05860 ## 97 5.59 1.60 1.62 0 0 0.13385 0.07684 ## 99 4.06 2.19 0.65 0 0 0.15774 0.08794 ## 66 6.00 1.45 2.37 1 0 0.18699 0.10944 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS NAff TI Turnover label fitted std.err ## 77 3.01 1.53 3.01 1 0 0.4743 0.11354 ## 53 3.87 1.72 3.05 1 0 0.4899 0.09955 ## 68 5.48 2.27 2.78 0 0 0.4998 0.13673 ## 75 4.46 2.81 1.80 1 1 0.5009 0.11987 ## 78 3.92 2.80 1.70 1 1 0.5070 0.11250 ## ## ... for the last 4 rows of sorted data ... ## ## JS NAff TI Turnover label fitted std.err ## 17 0.23 2.38 4.20 1 1 0.9327 0.04890 ## 6 4.14 3.07 4.33 0 1 0.9345 0.04905 ## 19 2.73 3.64 3.36 1 1 0.9426 0.04146 ## 95 2.48 3.18 4.74 1 1 0.9719 0.02322 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 58 61.1 6 52 89.7 ## Turnover 0 37 38.9 23 14 62.2 ## --------------------------------------------------- ## Total 95 78.9 ## ## Accuracy: 78.95 ## Sensitivity: 89.66 ## Precision: 78.79 Note: In some instances, you might receive the error message shown below. You can ignore this message, as it just indicates that you have a poor predictor variable in the model that results in fitted/predicted values that are all the same. If you get this message, proceed forward with your interpretation of the output. \\(\\color{red}{\\text{Error:}}\\) \\(\\color{red}{\\text{All predicted values are 0.}}\\) \\(\\color{red}{\\text{Something is wrong here.}}\\) The output generates the model coefficient estimates, the odds ratios and confidence intervals, model fit information (i.e., AIC), collinearity diagnostics, outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model, the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis (N = 95) after removing cases with missing data on any of the model variables. 47.2.5.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a multiple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Multivariate Outliers: To determine whether the data are free of multivariate outliers, let’s take a look at the text output section called Analysis of Residuals and Influence. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cook’s distance (cooks). The case associated with row number 14 has the highest Cook’s distance value (.152), followed by the cases associated with row numbers 1 (.134), 69 (.101), and 47 (.082). A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 14, 1, 69 and 47 from our data frame; though, I wouldn’t necessarily recommend this, as these Cook’s distance values aren’t too concerning. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so. No (Multi)Collinearity Between Predictor Variables: To assess whether (multi)collinearity might be an issue, check out the table called Collinearity in the output. This table shows two indices of collinearity: tolerance and valence inflation factor (VIF). Because the VIF is just the reciprocal of the tolerance (1/tolerance), let’s focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are very lower levels of collinearity. In the table, we can see that the tolerance statistics are all closer to 1.00 and well above .20, thereby indicating that collinearity is not likely an issue. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To investigate the assumption of linearity between continuous predictor variables and the logit transformation of the outcome variable, we can add the interaction between each continuous predictor variable and its logarithmic (i.e., natural log) transformation. We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between each predictor variable (i.e., JS, NAff, TI) and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of each predictor variable JS followed by the + operator. After the + operator, type the name of the same predictor variable JS, followed by the : operator and the log function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all continuous predictor variables in the model. # Box-Tidwell diagnostic test of linearity between continuous predictor variables and # logit transformation of outcome variable Logit(Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + TI + TI:log(TI), data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## Predictor Variable 2: NAff ## Predictor Variable 3: TI ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 95 ## ## ## BASIC ANALYSIS ## ## -- Estimated Model of Turnover for the Logit of Reference Group Membership ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 0.9206 9.8331 0.094 0.925 -18.3519 20.1932 ## JS -4.5845 2.6176 -1.751 0.080 -9.7149 0.5459 ## NAff -0.4107 6.4047 -0.064 0.949 -12.9636 12.1422 ## TI 3.2888 3.0158 1.091 0.275 -2.6221 9.1998 ## JS:log(JS) 2.0053 1.1844 1.693 0.090 -0.3160 4.3266 ## NAff:log(NAff) 0.9559 3.5321 0.271 0.787 -5.9670 7.8788 ## TI:log(TI) -1.2101 1.5232 -0.794 0.427 -4.1956 1.7754 ## ## ## -- Odds Ratios and Confidence Intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 2.5108 0.0000 588545072.9794 ## JS 0.0102 0.0001 1.7261 ## NAff 0.6632 0.0000 187626.1824 ## TI 26.8110 0.0726 9894.6983 ## JS:log(JS) 7.4284 0.7291 75.6872 ## NAff:log(NAff) 2.6010 0.0026 2640.6975 ## TI:log(TI) 0.2982 0.0151 5.9024 ## ## ## -- Model Fit ## ## Null deviance: 127.017 on 94 degrees of freedom ## Residual deviance: 100.854 on 88 degrees of freedom ## ## AIC: 114.8541 ## ## Number of iterations to convergence: 5 ## ## ## Collinearity ## ## Tolerance VIF ## JS 0.030 32.947 ## NAff 0.012 82.947 ## TI 0.073 13.775 ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 95 cases (rows) of data] ## -------------------------------------------------------------------- ## JS NAff TI Turnover fitted residual rstudent dffits cooks ## 1 4.96 1.87 0.51 1 0.03086 0.9691 3.2985 0.9563 0.63056 ## 6 4.14 3.07 4.33 0 0.91111 -0.9111 -2.3981 -0.6526 0.14155 ## 67 1.38 2.30 3.18 0 0.91516 -0.9152 -2.3760 -0.5617 0.10829 ## 68 5.48 2.27 2.78 0 0.73899 -0.7390 -1.7745 -0.6741 0.07686 ## 66 6.00 1.45 2.37 1 0.55250 0.4475 1.2378 0.7960 0.07043 ## 42 4.57 2.01 4.27 0 0.72166 -0.7217 -1.7245 -0.6507 0.06898 ## 74 3.50 3.28 1.37 0 0.50709 -0.5071 -1.3129 -0.6965 0.05735 ## 96 2.82 2.05 4.25 0 0.72223 -0.7222 -1.6950 -0.5521 0.04977 ## 79 5.04 1.22 2.92 0 0.42774 -0.4277 -1.1658 -0.6487 0.04597 ## 4 1.72 2.11 1.78 0 0.53924 -0.5392 -1.3118 -0.4774 0.02800 ## 27 4.88 1.76 1.81 1 0.25761 0.7424 1.6982 0.3893 0.02590 ## 25 1.77 2.24 1.16 0 0.31999 -0.3200 -0.9531 -0.5030 0.02531 ## 26 3.15 2.84 3.48 0 0.83033 -0.8303 -1.9278 -0.3366 0.02494 ## 81 3.34 1.27 2.96 0 0.33222 -0.3322 -0.9633 -0.4596 0.02133 ## 77 3.01 1.53 3.01 1 0.41376 0.5862 1.3781 0.4014 0.02101 ## 21 3.37 1.65 4.16 1 0.57404 0.4260 1.1068 0.4163 0.01894 ## 61 4.26 1.73 2.10 1 0.26659 0.7334 1.6620 0.3239 0.01759 ## 70 2.08 1.62 4.55 1 0.75821 0.2418 0.7970 0.3930 0.01463 ## 52 3.44 2.35 3.37 0 0.68034 -0.6803 -1.5371 -0.2832 0.01210 ## 58 2.52 1.30 2.13 0 0.24723 -0.2472 -0.7985 -0.3562 0.01207 ## ## ## PREDICTION ## ## Probability threshold for classification : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [pred_all=TRUE to see all intervals displayed] ## -------------------------------------------------------------------- ## JS NAff TI Turnover label fitted std.err ## 1 4.96 1.87 0.51 1 0 0.03086 0.05764 ## 99 4.06 2.19 0.65 0 0 0.04451 0.06863 ## 88 4.07 2.21 1.21 0 0 0.14009 0.09587 ## 46 3.79 1.45 1.83 0 0 0.15073 0.09141 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS NAff TI Turnover label fitted std.err ## 49 4.63 2.45 2.01 0 0 0.4820 0.1237 ## 86 3.42 2.23 2.57 1 0 0.4962 0.1032 ## 28 2.67 1.95 2.68 1 0 0.4997 0.1070 ## 74 3.50 3.28 1.37 0 1 0.5071 0.2402 ## 13 2.98 2.32 2.41 0 1 0.5145 0.1018 ## ## ... for the last 4 rows of sorted data ... ## ## JS NAff TI Turnover label fitted std.err ## 95 2.48 3.18 4.74 1 1 0.9499 0.063267 ## 19 2.73 3.64 3.36 1 1 0.9537 0.060780 ## 87 0.67 2.56 3.08 1 1 0.9890 0.020304 ## 17 0.23 2.38 4.20 1 1 0.9988 0.003638 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 1 58 61.1 8 50 86.2 ## Turnover 0 37 38.9 21 16 56.8 ## --------------------------------------------------- ## Total 95 74.7 ## ## Accuracy: 74.74 ## Sensitivity: 86.21 ## Precision: 75.76 Because the p-values associated with each of the interaction terms and their regression coefficients (JS:log(JS), NAff:log(NAff), TI:log(TI)) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio). Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a previous section on simple logistic regression. 47.2.5.2 Interpret Model Results Basic Analysis: The Basic Analysis section of the original output first displays a table called the Model Coefficients, which includes the regression coefficients (slopes, weights) and their standard errors, z-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (JS) in relation to the logit transformation of the outcome variable (Turnover) is not statistically significant when controlling for the other predictor variables in the model because the associated p-value is equal to or greater than .05 (b = -.233, p = .293, 95% CI[-0.667, .201]). The regression coefficient for NAff is 1.195 and its associated p-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (b = 1.195, p = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for TI is .897 and its associated p-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (b = .897, p = .005, 95% CI[.276, 1.517]). Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, in some contexts we might not write out the regression equation with that variable included; instead, we might re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) To that end, to aid our interpretation of the significant finding, we can move our attention to the table called Odds ratios and confidence intervals. To convert our regression coefficients to odds ratios, the function is simply exponentiating them. Behind the scenes, this is what happened: \\(e^{1.195} = 3.304\\) We can also do this manually using the exp function from base R. Any difference between the Logit output and the output below is attributable to rounding. # For the sake of demonstration: # Exponentiate logistic regression coefficient to convert to odds ratio # Note that the Logit function already does this for us exp(1.195) ## [1] 3.303558 In the Odds ratios and confidence intervals table, we see that the odds ratios are automatically computed for us. We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belong to NAff and TI. Regarding NAff, its odds ratio of 3.304 is greater than 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times greater for every one unit increase in NAff when controlling for other predictor variables in the model. Regarding TI, its odds ratio of 2.451 is also greater than 1, and thus, the odds of quitting are 2.451 times greater for every one unit increase in TI when controlling for other predictor variables in the model. Note that the odds ratio (OR) can be conceptualized as a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large In the Model Fit table, note that we don’t have an estimate of R-squared (R2) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-R-squared (R2) values, but for now let’s focus on what is produced in the Logit function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data by looking at the Specified Confusion Matrices table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 37 (38.9%) and 58 (61.1%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 89.7% accuracy. Overall, we tend to be most interested in the overall percentage of correct classifications, which is 78.9%. That is, based on the logistic regression model that included the predictor variables of JS, NAff, and TI, we were able to predict actual turnover behavior from the same sample 78.9% of the time, which was a big improvement over the simple logistic regression model in which we had just JS as a predictor variable. Forecasts: In the output section called Forecasts, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. Nagelkerke pseudo-R2: To compute Nagelkerke’s pseudo-R2, we will need to install and access the DescTools package (if we haven’t already) so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) To use the function, we’ll need to re-estimate our multiple logistic regression model using the glm function from base R. To request a logistic regression model as a specific type of generalized linear model, we’ll add the family=binomial argument. Using the &lt;- assignment operator, we will assign the resulting estimated model to an object that we’ll arbitrarily call model2. # Estimate simple logistic regression model and assign to object model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) In the PseudoR2 function, we will specify the name of the model object (model2) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerke’s formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model2, &quot;Nagel&quot;) ## Nagelkerke ## 0.2779516 The estimated Nagelkerke pseudo-R2 is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-R2 was just .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS, NAff, and TI explain 27.8% of the variance in Turnover. Because the DescTools package also has a function called Logit, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-R2 was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover. 47.2.6 Summary In this chapter, we learned how to estimate simple and multiple logistic regression models using the Logit function from lessR. 47.3 Chapter Supplement In addition to the Logit function from the lessR package covered above, we can use the glm function from base R to estimate a simple and multiple logistic regression models and the predict function from base R to predict probabilities of the event in question occurring. Because these functions come from base R, we do not need to install and access an additional package. 47.3.1 Functions &amp; Packages Introduced Function Package glm base R log base R exp base R plot base R cooks.distance base R summary base R confint base R / MASS coef base R subset base R na.omit base R xtabs base R print base R prop.table base R predict base R sum base R scale base R vif car PseudoR2 DescTools c base R merge base R data.frame base R mutate dplyr ifelse base R 47.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;Turnover.csv&quot;) ## Rows: 99 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): ID ## dbl (5): Turnover, JS, OC, TI, NAff ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(td) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;NAff&quot; # View variable type for each variable in data frame (tibble) object str(td) ## spc_tbl_ [99 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ... ## $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ... ## $ JS : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ... ## $ OC : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ... ## $ TI : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ... ## $ NAff : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_character(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. NAff = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame (tibble) object head(td) ## # A tibble: 6 × 6 ## ID Turnover JS OC TI NAff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EMP559 1 4.96 5.32 0.51 1.87 ## 2 EMP561 1 1.72 1.47 4.08 2.48 ## 3 EMP571 1 1.64 0.87 2.65 2.84 ## 4 EMP589 1 3.01 2.15 4.17 2.43 ## 5 EMP592 1 3.04 1.94 3.27 2.76 ## 6 EMP601 1 3.81 3.81 3.01 3.67 47.3.3 Simple Logistic Regression Model Using glm Function from Base R The glm function from base R stands for “generalized linear model (GLM),” making it an appropriate function for estimating logistic regression models and other models in the GLM family; logistic regression model is a GLM with a logit link function. Let’s begin by specifying JS as a predictor variable for Turnover. Using the &lt;- assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., model1). Type the name of the glm function. As the function’s first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. As the third argument, type family=binomial, which by default estimates a logit (logistic) model. # Estimate simple logistic regression model model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) Note: You won’t see a summary of the results in the Console, as we have not requested those. As a next step, we will use the model1 object to determine whether we have satisfied the statistical assumptions of a simple logistic regression model. 47.3.3.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Bivariate Outliers: To determine whether the data are free of bivariate outliers, let’s look at Cook’s distance (D) values across cases. Using the plot function from base R, type the name of our model object (model1) as the first argument, and as the second argument, enter the numeral 4 to request the fourth diagnostic plot, which is the Cook’s distance plot. # Diagnostics plot: Cook&#39;s Distance plot plot(model1, 4) The case associated with row number 66 has the highest Cook’s distance value, followed by the cases associated with row numbers 67 and 71 – although 66 and 67 seem to be clearest potential outliers of the three. We can also print the cases with the highest Cook’s distances. To do so, let’s create an object called cooksD to which we will assign a vector of Cook’s distance values using the cooks.distance function from base R. Just type the name of the logistic regression model object (model1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and typing the cooksD object name as the first object and decreasing=TRUE as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(model1) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top 20 Cook&#39;s distance values head(cooksD, n=20) ## 69 7 73 58 12 31 13 1 33 84 63 61 ## 0.08495602 0.06240864 0.04888996 0.04617863 0.04352935 0.04117397 0.03313842 0.02608930 0.02352896 0.01756755 0.01733141 0.01693066 ## 97 70 74 75 67 80 77 39 ## 0.01692984 0.01648318 0.01614505 0.01595550 0.01453516 0.01431210 0.01336026 0.01281733 Again, we see cases associated with row numbers 66 (.085), 67 (.062), and 71 (.049) having the highest Cook’s distance values, with 66 and 67 still looking like the most influential cases of the lot. As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 66 and 67 from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cook’s distance values are all well-below the more liberal threshold, so I would say we’re safe to leave the associated cases in the data. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between our predictor variable JS and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of JS. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of the predictor variable JS followed by the + operator. After the + operator, type the name of the predictor variable JS, followed by the : operator and the log function from base R with the predictor variable JS as its sole parenthetical argument. Finally, we will use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (boxtidwell). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable boxtidwell &lt;- glm(Turnover ~ JS + JS:log(JS), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS + JS:log(JS), family = binomial, ## data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.4461 3.2062 1.699 0.0894 . ## JS -2.9840 2.1693 -1.376 0.1690 ## JS:log(JS) 1.1696 0.9778 1.196 0.2316 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 124.62 on 95 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.62 ## ## Number of Fisher Scoring iterations: 5 Because the interaction term (JS:log(JS)) regression coefficient of 1.1696 in the Coefficients table is nonsignificant (p = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is nonlinear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (“Curvilinear Effects in Logistic Regression”) from Osborne (2015). Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well. Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. Just for the sake of demonstration, let’s transform the JS variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (JS_0) that is equal to JS minus the minimum value of JS. # ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero td$JS_0 &lt;- td$JS - min(td$JS, na.rm=TRUE) Now that we have a variable called JS_0 with at least one score equal to zero, let’s try the try the Box-Tidwell test. I’m going to add the argument brief=TRUE to reduce the amount of output generated by the function. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with predictor containing zero value(s)] boxtidwell &lt;- glm(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, family=binomial) # Print summary of results summary(boxtidwell) If you ran the script above, you likely got an error message that looked like this: \\(\\color{red}{\\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\\) The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on JS_0, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above). Using the first approach, create a new variable in which the JS_0 variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the JS_0 variable, which results in the lowest score on the new variable (JS_1) being 1. As verification, I include the min function from base R. # Linear transformation that results in lowest score being 1 td$JS_1 &lt;- td$JS_0 + abs(min(td$JS_0, na.rm=TRUE)) + 1 # Verify that new lowest score is 1 min(td$JS_1, na.rm=TRUE) ## [1] 1 It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with transformed predictor = 1] boxtidwell &lt;- glm(Turnover ~ JS_1 + JS_1:log(JS_1), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS_1 + JS_1:log(JS_1), family = binomial, ## data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.099 5.008 1.617 0.106 ## JS_1 -4.059 2.977 -1.363 0.173 ## JS_1:log(JS_1) 1.510 1.226 1.231 0.218 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 124.57 on 95 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.57 ## ## Number of Fisher Scoring iterations: 4 There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (JS_1:log(JS_1)) is nonsignificant (b = 1.510, p = .218). Thus, we don’t see evidence that the assumption of linearity has been violated. We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the rows= argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that we’re back to using the variable called JS_0 that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with only cases with scores greater than zero] boxtidwell &lt;- glm(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, family=binomial, subset=(JS_0 &gt; 0)) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS_0 + JS_0:log(JS_0), family = binomial, ## data = td, subset = (JS_0 &gt; 0)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.665 2.822 1.653 0.0982 . ## JS_0 -2.616 1.994 -1.312 0.1895 ## JS_0:log(JS_0) 1.039 0.928 1.120 0.2628 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 130.72 on 96 degrees of freedom ## Residual deviance: 124.62 on 94 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.62 ## ## Number of Fisher Scoring iterations: 4 We lost one case because that person had a score of zero on the JS_0 continuous predictor variable, and we see that the interaction term (JS_0:log(JS_0)) is non significant (b = 1.039, p = .263). To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your continuous predictor variable has scores that are equal to or less than zero. Now that we’ve met the assumption of linearity, we’re finally ready to interpret the model results! 47.3.3.2 Interpret Model Results Now we’re ready to interpret the results of the simple logistic regression model. For clarity, let’s re-specify our original simple logistic regression model from above. To apply the glm function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. Third, type the argument family=binomial, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the &lt;- assignment operator; here, I again arbitrarily named the object model1. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (model1). # Estimate simple logistic regression model model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) # Print summary of results summary(model1) ## ## Call: ## glm(formula = Turnover ~ JS, family = binomial, data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.8554 0.6883 2.695 0.00703 ** ## JS -0.4378 0.1958 -2.236 0.02538 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 126.34 on 96 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.34 ## ## Number of Fisher Scoring iterations: 4 The output generates the model coefficient estimates and model fit (i.e., AIC) information. Coefficients: The Coefficients section of the output displays the regression coefficients (slopes, weights) and their standard errors, z-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (JS) in relation to the outcome variable (Turnover) is often of substantive interest. Here, we see that the unstandardized regression coefficient for JS is -.438, and its associated p-value is less than .05 (b = -.438, p = .025). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (JS), the logit transformation of the outcome variable (Turnover) decreases by .438 units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. Confidence Intervals: To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the confint function from base R with the name of our model as the sole argument (model1). # Estimate 95% confidence intervals confint(model1) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.5644186 3.28485115 ## JS -0.8412956 -0.06705288 The 95% confidence interval for JS ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values. Odds Ratios: If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it - for example: \\(e^{-.438} = .646\\) We can also do this using R by applying the exp function from base R. Specifically, within the exp function parentheses, type the name of the coef function from base R, which extracts the regression coefficients from the model object. As the sole argument within the coef function parentheses, enter the name of the model we previously specified (model1). # Exponentiate logistic regression coefficient to convert to odds ratio exp(coef(model1)) ## (Intercept) JS ## 6.3939475 0.6454756 In the output, we see that indeed the odds ratio is approximately .646. Because the odds ratio is less than 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (100 \\(\\times\\) .354) for every one unit increase in job satisfaction (JS). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of not quitting (i.e., staying): The odds of not quitting are 1.548 times as likely for every one unit increase in job satisfaction (JS). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large To get the 95% confidence intervals for the odds ratio, we can nest the confint function within the exp function. # Exponentiate logistic regression coefficient to convert to odds ratio exp(confint(model1)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 1.7584252 26.7050090 ## JS 0.4311515 0.9351458 Model Fit &amp; Performance: Returning to the original output of the simple logistic regression model, let’s examine the section that contains information about model fit. Note that we don’t have an estimate of R-squared (R2) like we would with a traditional linear regression model. Later on, we will compute the pseudo-R-squared (R2) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (N = 98). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data initially by creating a confusion matrix in which we display the model’s accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 98 for the analysis. Thus, we should drop the case with missing data prior to estimating our baseline data and confusion matrix. We’ll start by using the subset function from base R to select just the two variables from our data frame that we used in our logistic regression model: Turnover and JS. As the first argument, type the name of the data frame (td). As the second argument, type select= followed by the c (combine) function from base R. Within the c function parentheses, enter the names of the two variables we wish to retain (Turnover, JS). Using the &lt;- assignment operator, create a new data frame object, and here I arbitrarily call this object td_short. # Create new short data frame with just the retained variables td_short &lt;- subset(td, select=c(Turnover, JS)) Next, apply the na.omit function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (td_short). Using the &lt;- assignment operator, overwrite the td_short data frame by entering its name to the left of the &lt;- assignment operator. # Drop cases with missing data in the short data frame td_short &lt;- na.omit(td_short) There are different ways we can estimate our baseline, but let’s keep it simple and use the xtabs function from base R. As the first argument, type the ~ followed the name of the short data frame td_short, followed by the $ and the outcome variable (Turnover). Use the &lt;- operator to create and name a new table object, where here I arbitrarily call it table1. Use the print function to view the table object (table1). Remember that for the Turnover variable quit = 1 and stay = 0. # Number of cases at each level of outcome variable table1 &lt;- xtabs(~ td_short$Turnover) # Print the table print(table1) ## td_short$Turnover ## 0 1 ## 39 59 As you can see, 39 people actually stayed and 59 people actually quit. To convert these to proportions, apply the prop.table function from base R to the table object we created in the previous step (table1). # Proportion of cases at each level of outcome variable prop.table(table1) ## td_short$Turnover ## 0 1 ## 0.3979592 0.6020408 Based on this output, we see that 39.8% of people stayed and 60.2% quit. Now we’re ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities pred.prob &lt;- predict(model1, type=&quot;response&quot;) We now need to dichotomize the vector of predicted probabilities (pred.prob), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new vector called dich.pred.prob (or whatever you would like to call it) based on the values from the pred.prob vector you created in the previous step. Next, for the dich.pred.prob, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) dich.pred.prob &lt;- pred.prob dich.pred.prob[dich.pred.prob &gt;= .50] &lt;- 1 dich.pred.prob[dich.pred.prob &lt; .50] &lt;- 0 Building on the xtabs function from above, use the + symbol to add the new dich.pred.prob vector as the column variable in the table. Also, create a new table object using the &lt;- operator called table2 (or whatever you would like to call it). Use the print function to print the table2 object. # Number of cases at each level of outcome variable table2 &lt;- xtabs(~ td_short$Turnover + dich.pred.prob) print(table2) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 8 31 ## 1 10 49 This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. By applying the prop.table function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (table2), and as the second argument, enter the numeral 1 to indicate that we want the row proportions. # Row proportions prop.table(table2, 1) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 0.2051282 0.7948718 ## 1 0.1694915 0.8305085 Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%). Finally, let’s determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, we’re interested in the cells that correspond to 0 on the Turnover variable and 0 on the dich.pred.prob vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (“0”), followed by a comma and the name of the column label (“0”). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (“1”), followed by a comma and the name of the column label (“1”). Now add those together to form the numerator. In the denominator, apply the sum function from base R to the table object (table2) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (JS) and the outcome variable (Turnover) in our original model. # Estimate overall percentage of correct classifications (table2[&quot;0&quot;,&quot;0&quot;] + table2[&quot;1&quot;,&quot;1&quot;]) / sum(table2) ## [1] 0.5816327 The overall percentage of correct classifications 58.2%, which is not a monumental amount of prediction accuracy when using just JS (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Nagelkerke pseudo-R2: To compute Nagelkerke’s pseudo-R2, we will need to install and access the DescTools package so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) In the PseudoR2 function, we will specify the name of the model object (model1) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerke’s formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model1, &quot;Nagel&quot;) ## Nagelkerke ## 0.07258382 The estimated Nagelkerke pseudo-R2 is .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS explains 7.3% of the variance in Turnover. Because the DescTools package also has a function called Logit, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (JS) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (b = -.438, p = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. Dealing with Bivariate Outliers: If you recall above, we found that the cases associated with row numbers 66 and 67 in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cook’s distance value greater than 1.0). If you were to decide to remove cases 66 and 67, here’s what you would do. First, look at the data frame (using the View function) and determine which cases row numbers 66 and 67 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that they are associated with ID equal to EMP861 and EMP862, respectively. Next, with respect to estimating the logistic regression model, I suggest naming the unstandardized regression model something different, and here I name it model1_b. The model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: subset=(!ID %in% c(\"EMP861\",\"EMP862\")); the subset argument subsets the data frame within the glm function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which ID is not (!) within the vector containing EMP861 and EMP862. Please consider revisiting the chapter on filtering (subsetting) data if you would like to see the full list of logical operators or to review how to filter out cases from a data frame before specifying the model. # Simple logistic regression model with outlier/influential cases removed model1_b &lt;- glm(Turnover ~ JS, data=td, family=binomial, subset=(!ID %in% c(&quot;EMP861&quot;,&quot;EMP862&quot;))) # Print summary of results summary(model1_b) ## ## Call: ## glm(formula = Turnover ~ JS, family = binomial, data = td, subset = (!ID %in% ## c(&quot;EMP861&quot;, &quot;EMP862&quot;))) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.8799 0.7067 2.660 0.00781 ** ## JS -0.4388 0.1997 -2.198 0.02797 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 128.89 on 95 degrees of freedom ## Residual deviance: 123.66 on 94 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 127.66 ## ## Number of Fisher Scoring iterations: 4 Standardizing Continuous Predictor Variable: Optionally, sometimes we may decide to standardize the continuous predictor variable in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the scale function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the chapter on centering and standardizing variables. # Estimate simple logistic regression model # with standardized continuous predictor variable model1_c &lt;- glm(Turnover ~ scale(JS), data=td, family=binomial) # Print summary of results summary(model1_c) ## ## Call: ## glm(formula = Turnover ~ scale(JS), family = binomial, data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4385 0.2132 2.057 0.0397 * ## scale(JS) -0.5022 0.2247 -2.236 0.0254 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 126.34 on 96 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.34 ## ## Number of Fisher Scoring iterations: 4 47.3.3.3 Compute Predicted Probabilities When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didn’t append this new vector to of values to our data frame object (td). For the sake of clarity, we will re-do this step using the predict function from base R once more. We’ll begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities based on # sample used to estimate the logistic regression model pred.prob &lt;- predict(model1, type=&quot;response&quot;) Now let’s add the pred.prob vector as a new variable called pred.prod.JS in the td data frame object. Let’s overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (pred.prod.JS), followed by the = operator and the name of the vector object containing the predicted probabilities (pred.prod). As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( pred.prob.JS = pred.prob), by=&quot;row.names&quot;, all=TRUE) If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 We can now dichotomize the predicted probabilities (pred.prob.JS), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new variable called dich.pred.prob.JS (or whatever we would like to call it) based on the values from the pred.prob.JS variable we just created. Next, for the dich.pred.prob.JS, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) td$dich.pred.prob.JS &lt;- td$pred.prob.JS td$dich.pred.prob.JS[td$dich.pred.prob.JS &gt;= .50] &lt;- 1 td$dich.pred.prob.JS[td$dich.pred.prob.JS &lt; .50] &lt;- 0 # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 1 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 1 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 1 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 1 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 1 It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the JS predictor variable, which we could then use to plug into our model; to do this with a model estimated using the glm model, will create a toy data frame with “fresh” JS values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object – just like we did in the chapter on predicting criterion scores using simple linear regression. In this toy data frame that we’re naming fresh_td, we use the data.frame function from base R. As the first argument, we’ll create an ID variable with some fictional employee ID numbers, and as the second argument, we’ll create a JS variable with some fictional job satisfaction scores. # Create toy data frame object for illustration purposes fresh_td &lt;- data.frame(ID=c(&quot;EMP1000&quot;,&quot;EMP1001&quot;,&quot;EMP1002&quot;, &quot;EMP1003&quot;,&quot;EMP1004&quot;,&quot;EMP1005&quot;), JS=c(4.5, 1.6, 0.7, 5.9, 2.1, 3.0)) When estimating the predicted probabilities of turnover based on these new JS scores, we’ll do the following: Using the &lt;- assignment operator, we will create a new variable called prob_JS that we will append to the fresh_td toy data frame object (using the $ operator). To the right of the &lt;- assignment operator, type the name of the predict function from base R. As the first argument, type the name of the logistic regression model object we estimated using the td (original) data frame. As the second argument, type newdata= followed by the name of the data frame object containing “fresh” data (fresh_td). It’s important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated. As the third argument, insert type=\"response\", which will request the predicted probabilities. # Use simple logistic regression model to estimate # predicted probabilities of turnover for &quot;fresh&quot; data fresh_td$prob_JS &lt;- predict(model1, # name of logistic regression model newdata=fresh_td, # name of &quot;fresh&quot; data frame type=&quot;response&quot;) # Print data frame object print(fresh_td) ## ID JS prob_JS ## 1 EMP1000 4.5 0.4713805 ## 2 EMP1001 1.6 0.7604090 ## 3 EMP1002 0.7 0.8247569 ## 4 EMP1003 5.9 0.3257483 ## 5 EMP1004 2.1 0.7182989 ## 6 EMP1005 3.0 0.6322888 47.3.4 Multiple Logistic Regression Using glm Function from Base R Just as we did for simple logistic regression, we can estimate a multiple logistic regression model using the glm function from base R. Let’s specify a multiple logistic regression model with JS, NAff, and TI as predictor variables and Turnover as the dichotomous outcome variable. Using the &lt;- assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., model2). Type the name of the glm function. As the function’s first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables, separated by the + operator, are typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. As the third argument, type family=binomial, which by default estimates a logit (logistic) model. # Estimate multiple logistic regression model model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) Note: You won’t see a summary of the results in the Console, as we have not requested those. As a next step, we will use the model1 object to determine whether we have satisfied the statistical assumptions of a multiple logistic regression model. 47.3.4.1 Test Statistical Assumptions To determine whether it’s appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Multivariate Outliers: To determine whether the data are free of multivariate outliers, let’s look at Cook’s distance (D) values across cases. Using the plot function from base R, type the name of our model object (model1) as the first argument, and as the second argument, enter the numeral 4 to request the fourth diagnostic plot, which is the Cook’s distance plot. # Diagnostics plot: Cook&#39;s Distance plot plot(model2, 4) The case associated with row number 6 has the highest Cook’s distance value, followed by the cases associated with row numbers 1 and 66. The cases associated with row numbers 6 and 1 seem to be clearest potential outliers of the three. We can also print the cases with the highest Cook’s distances. Let’s create an object called cooksD that we will assign a vector of Cook’s distance values to using the cooks.distance function from base R. Just enter the name of the logistic regression model object (model1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(model2) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top 20 Cook&#39;s distance values head(cooksD, n=20) ## 6 1 66 42 96 74 27 26 67 61 68 4 ## 0.15184792 0.13426790 0.10091817 0.08227834 0.05869871 0.04691783 0.04554332 0.04249992 0.03973226 0.02634920 0.02182251 0.01882168 ## 25 52 54 69 8 75 77 64 ## 0.01802907 0.01720366 0.01718997 0.01718628 0.01661234 0.01611474 0.01593173 0.01552323 Again, we see cases associated with row numbers 6 (.152), 1 (.134), and 66 (.101) having the highest Cook’s distance values. As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 6 and 1 (and maybe even 66) from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cook’s distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cook’s distance values are all well-below the more liberal threshold, so I would say we’re safe to leave the associated cases in the data. No (Multi)Collinearity Between Predictor Variables: To assess whether (multi)collinearity might be an issue, let’s estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you haven’t already, install the car package (Fox and Weisberg 2019) which contains the vif function we will use to compute tolerance. # Install car package if you haven&#39;t already install.packages(&quot;car&quot;) Now, access the car package using the library function. # Access car package library(car) Apply the vif function to the model object (model2). # Compute VIF statistic vif(model2) ## JS NAff TI ## 1.068428 1.020933 1.067496 Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approach .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide 1 by the VIF. # Compute tolerance statistic 1 / vif(model2) ## JS NAff TI ## 0.9359549 0.9794966 0.9367719 This table indicates that there are low levels of collinearity. Specifically, we can see that the tolerance statistics are all closer to 1.00 and definitely above .20, thereby indicating that collinearity is not likely an issue. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add to the model the interaction term between any continuous predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between each predictor variable and its logarithmic transformation to our logistic regression model – but not the main effect for the logarithmic transformation of the predictor variable. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ symbol. To the right of the ~ symbol, type the name of each predictor variable JS followed by the + symbol. After the + symbol, type the name of the same predictor variable JS, followed by the : symbol and the log function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all continuous predictor variables in the model. Be sure to name the model object, and here I name it arbitrarily boxtidwell. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (boxtidwell). # Box-Tidwell diagnostic test of linearity between continuous predictor variables and # logit transformation of outcome variable boxtidwell &lt;- glm(Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + TI + TI:log(TI), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + ## TI + TI:log(TI), family = binomial, data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.9206 9.8331 0.094 0.9254 ## JS -4.5845 2.6176 -1.751 0.0799 . ## NAff -0.4107 6.4047 -0.064 0.9489 ## TI 3.2888 3.0158 1.091 0.2755 ## JS:log(JS) 2.0053 1.1844 1.693 0.0904 . ## NAff:log(NAff) 0.9559 3.5321 0.271 0.7867 ## TI:log(TI) -1.2101 1.5232 -0.794 0.4269 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 100.85 on 88 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 114.85 ## ## Number of Fisher Scoring iterations: 5 Because the p-values associated with each of the interaction terms and their regression coefficients (JS:log(JS), NAff:log(NAff), TI:log(TI)) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio). Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a previous section on simple logistic regression. 47.3.4.2 Interpret Model Results Now we’re ready to interpret the results of the multiple logistic regression model. For clarity, let’s re-specify our original simple logistic regression model from above. To apply the glm function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables are typed to the right of the ~ symbol, with each predictor variable separated by the + operator. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. Third, type the argument family=binomial, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the &lt;- assignment operator; here, I again arbitrarily named the object model2. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (model2). # Estimate multiple logistic regression model model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) # Print summary of results summary(model2) ## ## Call: ## glm(formula = Turnover ~ JS + NAff + TI, family = binomial, data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.9286 1.8120 -2.168 0.03015 * ## JS -0.2332 0.2215 -1.053 0.29253 ## NAff 1.1952 0.4996 2.392 0.01674 * ## TI 0.8967 0.3166 2.832 0.00462 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 105.23 on 91 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 113.23 ## ## Number of Fisher Scoring iterations: 4 The output generates the model coefficient estimates and model fit (i.e., AIC) information. To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the confint function from base R with the name of our model as the sole argument (model2). # Estimate 95% confidence intervals confint(model2) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -7.6905274 -0.5144811 ## JS -0.6793832 0.1980706 ## NAff 0.2606377 2.2375600 ## TI 0.3091687 1.5633542 Coefficients: The Coefficients section of the output from the summary of the glm function generates the model coefficient estimates and model fit (i.e., AIC) information, and the output from the confint provides the confidence intervals. To begin, the Coefficients section displays the regression coefficients (slopes, weights) and their standard errors, z-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (JS) in relation to the logit transformation of the outcome variable (Turnover) is not statistically significant when controlling for the other predictor variables in the model because the associated p-value is equal to or greater than .05 (b = -.233, p = .293, 95% CI[-0.667, .201]). The regression coefficient for NAff is 1.195 and its associated p-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (b = 1.195, p = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for TI is .897 and its associated p-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (b = .897, p = .005, 95% CI[.276, 1.517]). Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, we typically would not write out the regression equation with that variable included. Instead, we would typically re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it – for example: \\(e^{1.195} = 3.304\\) We can also do this using R by applying the exp function from base R. Specifically, within the exp function parentheses, type the name of the coef function from base R, which extracts the regression coefficients from the model object. As the sole argument within the coef function parentheses, enter the name of the model we previously specified (model2). # Exponentiate logistic regression coefficient to convert to odds ratio exp(coef(model2)) ## (Intercept) JS NAff TI ## 0.0196714 0.7919969 3.3041473 2.4513941 We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belonging to NAff and TI. Regarding NAff, its odds ratio of 3.304 is greater than 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times as likely for every one unit increase in NAff when controlling for other predictor variables in the model. Regarding TI, its odds ratio of 2.451 is also greater than 1, and thus, the odds of quitting are 2.451 times as likely for every one unit increase in TI when controlling for other predictor variables in the model. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large To get the 95% confidence intervals for the odds ratio, we can nest the confint function within the exp function. # Exponentiate logistic regression coefficient to convert to odds ratio exp(confint(model2)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.000457137 0.5978107 ## JS 0.506929579 1.2190484 ## NAff 1.297757439 9.3704394 ## TI 1.362292195 4.7748100 Model Fit &amp; Performance: Returning to the original output of the multiple logistic regression model, let’s examine the section that contains information about model fit. Note that we don’t have an estimate of R-squared (R2) like we would with a traditional linear regression model. Later on, we will compute the pseudo-R-squared (R2) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (N = 95). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the model’s fit to the data initially by creating a confusion matrix in which we display the model’s accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 95 for the analysis. Thus, we should drop the cases with missing data prior to estimating our baseline data and confusion matrix. We’ll start by using the subset function from base R to select just the variables from our data frame that we used in our logistic regression model: Turnover, JS, NAff, and TI. As the first argument, type the name of the data frame (td). As the second argument, type select= followed by the c (combine) function from base R. Within the c function parentheses, enter the names of the two variables we wish to retain (Turnover, JS). Using the &lt;- assignment operator, create a new data frame object, and here I arbitrarily call this object td_short. # Create new short data frame with just the retained variables td_short &lt;- subset(td, select=c(Turnover, JS, NAff, TI)) Next, apply the na.omit function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (td_short). Using the &lt;- assignment operator, overwrite the td_short data frame by entering its name to the left of the &lt;- assignment operator. # Drop cases with missing data in the short data frame td_short &lt;- na.omit(td_short) There are different ways we can estimate our baseline, but let’s keep it simple and use the xtabs function from base R. As the first argument, type the ~ followed the name of the short data frame td_short, followed by the $ and the outcome variable (Turnover). Use the &lt;- operator to create and name a new table object, where here I arbitrarily call it table1. Use the print function to view the table object (table1). Remember that for the Turnover variable quit = 1 and stay = 0. # Number of cases at each level of outcome variable table1 &lt;- xtabs(~ td_short$Turnover) # Print the table print(table1) ## td_short$Turnover ## 0 1 ## 37 58 As you can see, 37 people actually stayed and 58 people actually quit. To convert these to proportions, apply the prop.table function from base R to the table object we created in the previous step (table1). # Proportion of cases at each level of outcome variable prop.table(table1) ## td_short$Turnover ## 0 1 ## 0.3894737 0.6105263 Based on this output, we see that 38.9% of people stayed and 61.1% quit. Now we’re ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model2). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities pred.prob &lt;- predict(model2, type=&quot;response&quot;) We now need to dichotomize the vector of predicted probabilities (pred.prob), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new vector called dich.pred.prob (or whatever you would like to call it) based on the values from the pred.prob vector you created in the previous step. Next, for the dich.pred.prob, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) dich.pred.prob &lt;- pred.prob dich.pred.prob[dich.pred.prob &gt;= .50] &lt;- 1 dich.pred.prob[dich.pred.prob &lt; .50] &lt;- 0 Building on the xtabs function from above, use the + symbol to add the new dich.pred.prob vector as the column variable in the table. Also, create a new table object using the &lt;- operator called table2 (or whatever you would like to call it). Use the print function to print the table2 object. # Number of cases at each level of outcome variable table2 &lt;- xtabs(~ td_short$Turnover + dich.pred.prob) print(table2) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 23 14 ## 1 6 52 This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. By applying the prop.table function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (table2), and as the second argument, enter the numeral 1 to indicate that we want the row proportions. # Row proportions prop.table(table2, 1) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 0.6216216 0.3783784 ## 1 0.1034483 0.8965517 Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model even better, as we were able to predict that outcome with 89.7% accuracy. Not too shabby. Finally, let’s determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, we’re interested in the cells that correspond to 0 on the Turnover variable and 0 on the dich.pred.prob vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (“0”), followed by a comma and the name of the column label (“0”). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (“1”), followed by a comma and the name of the column label (“1”). Now add those together to form the numerator. In the denominator, apply the sum function from base R to the table object (table2) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (JS) and the outcome variable (Turnover) in our original model. # Estimate overall percentage of correct classifications (table2[&quot;0&quot;,&quot;0&quot;] + table2[&quot;1&quot;,&quot;1&quot;]) / sum(table2) ## [1] 0.7894737 The overall percentage of correct classifications 78.9%, which is a pretty good accuracy in prediction when using the three predictor variables in our model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Nagelkerke pseudo-R2: To compute Nagelkerke’s pseudo-R2, we will need to install and access the DescTools package so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) In the PseudoR2 function, we will specify the name of the model object (model2) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerke’s formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model2, &quot;Nagel&quot;) ## Nagelkerke ## 0.2779516 The estimated Nagelkerke pseudo-R2 is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-R2 was just .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS, NAff, and TI explain 27.8% of the variance in Turnover. Because the DescTools package also has a function called Logit, let’s detach the package before moving forward so that we don’t inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees’ self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-R2 was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover. Standardizing Continuous Predictor Variables: Optionally, sometimes we may decide to standardize the continuous predictor variables in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the scale function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the chapter on centering and standardizing variables. # Estimate simple logistic regression model # with standardized continuous predictor variable model2_b &lt;- glm(Turnover ~ scale(JS) + scale(NAff) + scale(TI), data=td, family=binomial) # Print summary of results summary(model2_b) ## ## Call: ## glm(formula = Turnover ~ scale(JS) + scale(NAff) + scale(TI), ## family = binomial, data = td) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.5669 0.2415 2.348 0.01888 * ## scale(JS) -0.2675 0.2542 -1.053 0.29253 ## scale(NAff) 0.6190 0.2588 2.392 0.01674 * ## scale(TI) 0.7685 0.2713 2.832 0.00462 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 105.23 on 91 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 113.23 ## ## Number of Fisher Scoring iterations: 4 47.3.4.3 Compute Predicted Probabilities When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didn’t append this new vector to of values to our data frame object (td). For the sake of clarity, we will re-do this step using the predict function from base R once more. We’ll begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities based on # sample used to estimate the logistic regression model pred.prob &lt;- predict(model2, type=&quot;response&quot;) Now let’s add the pred.prob vector as a new variable called pred.prod.allpred in the td data frame object. Let’s overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (pred.prod.allpred), followed by the = operator and the name of the vector object containing the predicted probabilities (pred.prod). As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( pred.prod.allpred = pred.prob), by=&quot;row.names&quot;, all=TRUE) If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS pred.prod.allpred ## 1 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 0.08371016 ## 2 10 18 EMP643 1 3.35 2.94 3.60 2.08 3.12 4.12 0.5960009 1 0.73187048 ## 3 11 19 EMP644 1 2.33 0.80 3.22 2.45 2.10 3.10 0.6974856 1 0.79306191 ## 4 12 2 EMP561 1 1.72 1.47 4.08 2.48 1.49 2.49 0.7507079 1 0.90827169 ## 5 13 20 EMP647 0 2.98 2.16 2.41 2.32 2.75 3.75 0.6343220 1 0.57694329 ## 6 14 21 EMP677 0 4.64 2.92 1.48 2.43 4.41 5.41 0.4561403 0 0.31447250 We can now dichotomize the predicted probabilities (pred.prod.allpred), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, we’re setting our threshold for experiencing the event in question as .50. As the first step, let’s create a new variable called dich.pred.prod.allpred (or whatever we would like to call it) based on the values from the pred.prod.allpred variable we just created. Next, for the dich.pred.prod.allpred, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) td$dich.pred.prod.allpred &lt;- td$pred.prod.allpred td$dich.pred.prod.allpred[td$dich.pred.prod.allpred &gt;= .50] &lt;- 1 td$dich.pred.prod.allpred[td$dich.pred.prod.allpred &lt; .50] &lt;- 0 # Print first 6 rows of data frame object head(td) ## Row.names Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS pred.prod.allpred ## 1 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 0.08371016 ## 2 10 18 EMP643 1 3.35 2.94 3.60 2.08 3.12 4.12 0.5960009 1 0.73187048 ## 3 11 19 EMP644 1 2.33 0.80 3.22 2.45 2.10 3.10 0.6974856 1 0.79306191 ## 4 12 2 EMP561 1 1.72 1.47 4.08 2.48 1.49 2.49 0.7507079 1 0.90827169 ## 5 13 20 EMP647 0 2.98 2.16 2.41 2.32 2.75 3.75 0.6343220 1 0.57694329 ## 6 14 21 EMP677 0 4.64 2.92 1.48 2.43 4.41 5.41 0.4561403 0 0.31447250 ## dich.pred.prod.allpred ## 1 0 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 0 It’s important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain “fresh” data on the predictor variables, which we could then use to plug into our model; to do this with a model estimated using the glm model, will create a toy data frame with “fresh” predictor variable values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object – just like we did in the chapter on applying a compensatory approach to selection decisions using multiple linear regression. In this toy data frame that we’re naming fresh_td, we use the data.frame function from base R. As the first argument, we’ll create an ID variable with some fictional employee ID numbers. As the second argument, we’ll create a JS variable with some fictional job satisfaction scores. As the third argument, we’ll create a NAff variable with some fictional negative affectivity scores. As the fourth argument, we’ll create a TI variable with some fictional turnover intentions scores. # Create toy data frame object for illustration purposes fresh_td &lt;- data.frame(ID=c(&quot;EMP1000&quot;,&quot;EMP1001&quot;,&quot;EMP1002&quot;, &quot;EMP1003&quot;,&quot;EMP1004&quot;,&quot;EMP1005&quot;), JS=c(4.5, 1.6, 0.7, 5.9, 2.1, 3.0), NAff=c(0.9, 3.3, 6.0, 4.2, 4.0, 1.9), TI=c(6.0, 5.5, 4.8, 3.0, 0.1, 1.1)) When estimating the predicted probabilities of turnover based on these new predictor variable scores, we’ll do the following: Using the &lt;- assignment operator, we will create a new variable called prob_allpred that we will append to the fresh_td toy data frame object (using the $ operator). To the right of the &lt;- assignment operator, type the name of the predict function from base R. As the first argument, type the name of the logistic regression model object we estimated using the td (original) data frame. As the second argument, type newdata= followed by the name of the data frame object containing “fresh” data (fresh_td). It’s important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated. As the third argument, insert type=\"response\", which will request the predicted probabilities. # Use multiple logistic regression model to estimate # predicted probabilities of turnover for &quot;fresh&quot; data fresh_td$prob_allpred &lt;- predict(model2, # name of logistic regression model newdata=fresh_td, # name of &quot;fresh&quot; data frame type=&quot;response&quot;) # Print data frame object print(fresh_td) ## ID JS NAff TI prob_allpred ## 1 EMP1000 4.5 0.9 6.0 0.8142131 ## 2 EMP1001 1.6 3.3 5.5 0.9897887 ## 3 EMP1002 0.7 6.0 4.8 0.9993788 ## 4 EMP1003 5.9 4.2 3.0 0.9172278 ## 5 EMP1004 2.1 4.0 0.1 0.6111323 ## 6 EMP1005 3.0 1.9 1.1 0.2024548 References "],["kfold.html", "Chapter 48 Applying k-Fold Cross-Validation to Logistic Regression 48.1 Conceptual Overview 48.2 Tutorial", " Chapter 48 Applying k-Fold Cross-Validation to Logistic Regression In this chapter, we will learn how to apply k-fold cross-validation to logistic regression. As a specific type of cross-validation, k-fold cross-validation can be a useful framework for training and testing models. 48.1 Conceptual Overview In general, cross-validation is an integral part of predictive analytics, as it allows us to understand how a model estimated on one data set will perform when applied to one or more new data sets. Cross-validation was initially introduced in the chapter on statistically and empirically cross-validating a selection tool using multiple linear regression. k-fold cross-validation is a specific type of cross-validation that is commonly applied when carrying out predictive analytics. 48.1.1 Review of Predictive Analytics True predictive analytics involves training (i.e., estimating, building) a model using one or more samples of data and then evaluating (i.e., testing, validating) how well the model performs when applied to a separate sample of data drawn from the same population. Assuming we have a large enough data set to begin with, we often assign 80% of the cases to a training data set and 20% to a test data set. The term predictive analytics is a big umbrella term, and predictive analytics can be applied using many different types of models, including common regression model types like ordinary least squares (OLS) multiple linear regression or maximum likelihood logistic regression. When building a model using a predictive analytics framework, one of our goals is to minimize prediction errors (i.e., improve prediction accuracy) when the model is applied to “fresh” or “new” data (e.g., test data). Fewer prediction errors when applying the model to new data indicate that the model makes more accurate predictions. At the same time, we want to avoid overfitting our model to the data on which it is trained, as this can lead to a model that fits our training data well but that doesn’t generalize to our test data; applying a trained model to new data can help us evaluate the extent to which we might have overfit a model to the original data. Fortunately, there are multiple cross-validation frameworks that we can apply when training a model, and these frameworks can help us improve a model’s predictive performance/accuracy while also reducing the occurrence of overfitting. One such cross-validation framework is k-fold cross-validation. Predictive analytics involves estimating a model based on past data, and then applying that model to new data in order to evaluate the accuracy of the model’s predictions. 48.1.2 Review of k-Fold Cross-Validation Often, when applying a predictive analytics process, we do some sort of variation of the two-phase process described above: (a) train the model on one set of data, and (b) test the model on a separate set of data. We typically put a lot of thought and consideration into how to train a model using the training data. Not only do we need to identify an appropriate type of statistical model, but we also need to consider whether we will incorporate validation into the model-training process. A common validation approach is k-fold cross-validation. To perform k-fold cross-validation, we often do the following: We split the data into a training data set and a test data set. Commonly, 80% of the data are randomly selected for the training data set, while the remaining 20% end up in the test data set. We randomly split the training data into two or more (sub)samples, which will allow us to train the model in an iterative process. The total number of samples we split the data into will be equal to k, where k refers to the number of folds (i.e., resamples). For example, if we randomly split the training data into five samples, then our k will be 5, and thus we will be performing 5-fold cross-validation. We then use the k samples to train (i.e., estimate, build) our model, which is accomplished through iterating the model-estimation process across k folds of data. For each fold, we estimate the model based on pooled data from k-1 samples. That is, if we split our training data into 5 samples, then for the first fold we might estimate the model based on data pooled together from the first four samples (out of five); we then use the fifth “holdout” (i.e., kth) sample as the validation sample, which means we assess the estimated model’s predictive performance/accuracy on data that weren’t used for that specific model estimation process. This process repeats until every sample has served as the validation sample, for a total of k folds. The models’ estimated performance across the k folds is then synthesized. Specifically, the k cross-validated models are then tossed out, but the estimated error from those models can be synthesized (e.g., averaged) to arrive at a performance estimate for the final model. With some types of models, the estimated error (e.g., root mean-squared error, R2) from those models is used to inform the final model estimation. Note that the final model’s parameter estimates are typically estimated using all of the available training data. The k-fold cross-validation framework can be extended by taking the final trained model and evaluating its performance based on the test data set that – to this point – has yet to be used; this is how k-fold cross-validation can be used in a broader predictive analytics (i.e., predictive modeling) context. It is important to note, however, that there are other cross-validation approaches we might choose, such as traditional empirical cross-validation (holdout method) and leave-one-out cross-validation (LOOCV) – but those are beyond the scope of this chapter. James, Witten, Hastie, and Tibshirani (2013) provide a nice introduction to LOOCV; though, in short, LOOCV is a specific instance of k-fold cross-validation in which k is equal to the number of observations in the training data. The figure below provides a visual overview of the k-fold cross-validation process, which we will bring to life in the tutorial portion of this chapter. In this chapter, we will focus on applying k-fold cross-validation to the estimation and evaluation of a multiple logistic regression model. The use of logistic regression implies that our outcome variable will be dichotomous. For background information on logistic regression, including the statistical assumptions that should be satisfied, check out the prior chapter on logistic regression. I must note, however, that other types of statistical models can be applied within a k-fold cross-validation framework – or any cross-validation framework for that matter. For example, assuming the appropriate statistical assumptions have been met, we might also apply more traditional regression-based approaches like ordinary least squares (OLS) multiple linear regression, or we could apply supervised statistical learning models like Lasso regression, which will be covered in a later chapter. Overview of a k-fold cross-validation process 48.1.3 Conceptual Video For a more in-depth review of k-fold cross-validation, please check out the following conceptual video. Link to conceptual video: https://youtu.be/kituDjzXwfE 48.2 Tutorial This chapter’s tutorial demonstrates how to estimate and evaluate a multiple logistic regression model using k-fold cross-validation. 48.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/BQ1VAZ7jNYQ 48.2.2 Functions &amp; Packages Introduced Function Package as.factor base R set.seed base R nrow base R createDataPartition caret as.data.frame base R trainControl caret train caret print base R summary base R varImp caret predict base R confusionMatrix caret RMSE caret abs base R MAE caret cbind base R drop_na tidyr 48.2.3 Initial Steps If you haven’t already, save the file called “PredictiveAnalytics.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PredictiveAnalytics.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;PredictiveAnalytics.csv&quot;) ## Rows: 1000 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): ID, Turnover, JS, OC, TI, Naff ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;Naff&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spc_tbl_ [1,000 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:1000] 1 2 3 4 5 6 7 8 9 10 ... ## $ Turnover: num [1:1000] 1 0 1 1 0 1 1 1 0 1 ... ## $ JS : num [1:1000] 3.78 4.52 2.26 2.48 4.12 2.99 2.44 2.31 3.87 4.62 ... ## $ OC : num [1:1000] 4.41 2.38 3.17 2.06 6 0.4 4.07 3.04 3.02 1.19 ... ## $ TI : num [1:1000] 4.16 2.74 2.76 4.32 1.37 3.24 2.63 1.94 3.69 1.96 ... ## $ Naff : num [1:1000] 1.82 2.28 2.25 2.11 2.01 2.73 2.91 2.62 2.89 2.45 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_double(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. Naff = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 6 ## ID Turnover JS OC TI Naff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.78 4.41 4.16 1.82 ## 2 2 0 4.52 2.38 2.74 2.28 ## 3 3 1 2.26 3.17 2.76 2.25 ## 4 4 1 2.48 2.06 4.32 2.11 ## 5 5 0 4.12 6 1.37 2.01 ## 6 6 1 2.99 0.4 3.24 2.73 # Print number of rows in data frame (tibble) object nrow(df) ## [1] 1000 The data frame (df) has 1000 cases and the following 5 variables: ID, Turnover, JS, OC, TI, and NAff. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). ID is the unique employee identifier variable. Imagine that these data were collected as part of a turnover study within an organization to determine the drivers/predictors of turnover based on a sample of employees who stayed and leaved during the past year. The variables JS, OC, TI, and Naff were collected as part of an annual survey and were later joined with the Turnover variable. Survey respondents rated each survey item using a 7-point response scale, ranging from strongly disagree (0) to strongly agree (6). JS contains the average of each employee’s responses to 10 job satisfaction items. OC contains the average of each employee’s responses to 7 organizational commitment items. TI contains the average of each employee’s responses to 3 turnover intentions items, where higher scores indicate higher levels of turnover intentions. Naff contains the average of each employee’s responses to 10 negative affectivity items. Turnover is a variable that indicates whether these individuals left the organization during the prior year, with 1 = quit and 0 = stayed. 48.2.4 Apply k-Fold Cross-Validation Using Logistic Regression In this tutorial, we will apply k-fold cross-validation to estimate and evaluate a multiple logistic regression model. You can think of k-fold cross-validation as an enhanced version of the conventional empirical cross-validation process described and demonstrated in previous chapter. We can perform k-fold cross-validation with different types of regression models, and in this tutorial, we will focus on applying it using multiple logistic regression. Because we have access to 1,000 cases, we will randomly split our data into training and test data frames, and then within the training data frame, we will perform k-fold cross-validation to come up with a more accurate model. We will then we will apply the final model to the test data frame data frame to see how well it predicts with a completely new set of data. Note: The type of model we will apply in our k-fold cross-validation approach will delete any cases with missing data on any variables in the model (i.e., listwise deletion). We don’t have any missing data in df data frame object, but if we did, I would recommend deleting cases with missing data on the focal variables prior to randomly splitting the data. The drop_na function from the tidyr package (Wickham, Vaughan, and Girlich 2023) works well for this purpose. # In the event of missing data on focal variables, # perform listwise deletion prior to randomly splitting/partitioning # the data # Install tidyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;tidyr&quot;) # Access readr package library(tidyr) # Drop rows with missing data on focal variables df &lt;- df %&gt;% drop_na(Turnover, JS, Naff, TI, OC) # Print number of rows in updated data frame (tibble) object nrow(df) ## [1] 1000 Because we didn’t have missing data for any of the focal variables, the number of rows (i.e., cases) remains at 1000. Now we’re ready to install and access the caret package (Kuhn 2021), if we haven’t already. The caret package has a number of different functions that are useful for running predictive analytics and various machine learning models/algorithms. # Install caret package if you haven&#39;t already install.packages(&quot;caret&quot;) # Access caret package library(caret) Using the createDataPartition function from the caret package, we will split the data such that 80% of the cases will be randomly assigned to one split, and the remaining 20% of cases will be assigned to the other split. Before doing so, let’s use the set.seed function from base R and include a number of your choosing to create “random” operations that are reproducible. # Set random seed for subsequent random selection and assignment operations set.seed(1985) Next, let’s perform the data split/partition. We’ll start by creating a name for the index of unique cases that will identify who will be included in the 80% split of the original data frame; here, I call this object index and assign values to it using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the createDataPartition function. As the first argument, type the name of the data frame (df), followed by the $ operator and the name of the outcome variable (Turnover). As a the second argument, set p=.8 to note that you wish to randomly select 80% (.8) of the cases from the data frame. As the third argument, type list=FALSE to indicate that we want the values in matrix form to facilitate, which will facilitate our ability to reference them in a subsequent step. As the fourth argument, type times=1 to indicate that we only want to request a single partition of the data. # Partition data and create index matrix of selected values index &lt;- createDataPartition(df$Turnover, p=.8, list=FALSE, times=1) Note: I have noticed that the createDataPartition sometimes adds an extra case to the partitioned data when the outcome variable is of type factor as opposed to numeric, integer, or character. An extra case added to the training data frame is not likely to be of much consequence, but if if you would like to avoid this, try converting the outcome variable from type factor to type numeric, integer, or character, whichever makes the most sense given the variable. You can always convert the outcome back to type factor after this process, which I demonstrate in this tutorial. Sometimes this works, and sometimes it doesn’t. We’ll use the index matrix object we created above to assign cases to the training data frame (which we will name train_df) and to the test data frame (which we will name test_df). Beginning with the training data frame, we can assign the random sample of 80% of the original cases to the train_df object by using the &lt;- assignment operator. Specifically, using matrix notation, we will specify the name of the original data frame (df), followed by brackets ([ ]). In the brackets, include the name of the index object we created above, followed by a comma (,). The placement of the index object before the comma indicates that we are selecting rows with the selected unique row numbers (i.e., index) from the matrix. Next, we do essentially the same thing with the test_df, except that we insert a minus (-) symbol before index to indicate that we don’t want cases associated with the unique row numbers contained in the index object. # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] If you received the error message shown below, then you will want to convert the original data frame df from a tibble to a conventional data frame; after that, repeat the steps above. If you did not receive this message, then you can skip this step and proceed on to the step in which you verify the number of rows in each data frame. \\(\\color{red}{\\text{Error: `i` must have one dimension, not 2.}}\\) # Convert data frame object (df) to a conventional data frame object df &lt;- as.data.frame(df) # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] To check our work and to be sure that 80% of the cases ended up in the train_df data frame and the remaining 20% ended up in the test_df data frame, let’s apply the nrow function from base R to each data frame object. # Verify number of rows (cases) in each data frame nrow(train_df) ## [1] 800 nrow(test_df) ## [1] 200 Indeed, 800 (80%) of the original 1,000 cases ended up in the train_df data frame, and 200 (20%) of the original 1,000 cases ended up in the test_df data frame. For the train_df and test_df data frames we’ve created, let’s re-label our dichotomous outcome variable’s (Turnover) levels from 1 to “quit” and from 0 to “stay”. This will make our interpretations of the results a little bit easier later on. # Re-label values of outcome variable for train_df train_df$Turnover[train_df$Turnover==1] &lt;- &quot;quit&quot; train_df$Turnover[train_df$Turnover==0] &lt;- &quot;stay&quot; # Re-label values of outcome variable for test_df test_df$Turnover[test_df$Turnover==1] &lt;- &quot;quit&quot; test_df$Turnover[test_df$Turnover==0] &lt;- &quot;stay&quot; In addition, let’s convert the outcome variable to a variable of type factor. To do so, we apply the as.factor function from base R. # Convert outcome variable to factor for each data frame train_df$Turnover &lt;- as.factor(train_df$Turnover) test_df$Turnover &lt;- as.factor(test_df$Turnover) Now we’re ready to specify the type of training method we wish to apply, the number of folds, and that we wish to save all of our predictions. Let’s name this object containing these specifications as ctrlspecs using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the trainControl function from the caret package. As the first argument in the trainControl function, specify the method by typing method=\"cv\", where \"cv\" represents cross-validation. As the second argument, type number=10 to indicate that we want 10 folds (i.e., 10 resamples), which means we will specifically use 10-fold cross-validation. As the third argument, type savePredictions=\"all\" to save all of the hold-out predictions for each of our resamples from the train_df data frame, where hold-out predictions refer to those cases that weren’t used in training each iteration of the model based on each fold of data but were used for validating the model trained at each fold. As the fourth argument, type classProbs=TRUE, as this tells the function to compute class probabilities (along with predicted values) for classification models in each resample. # Specify type of training method used and the number of folds ctrlspecs &lt;- trainControl(method=&quot;cv&quot;, number=10, savePredictions=&quot;all&quot;, classProbs=TRUE) To train (i.e., estimate, build) our multiple logistic regression model, we will use the train function from the caret package. To begin, come up with a name for your model object; here, I name the model object model and use the &lt;- assignment operator to assign the model information to it. Type the name of the train function to the right of the &lt;- operator. As the first argument in the train function, we will specify our multiple logistic regression model. To the left of the tilde (~) operator, type the name of the dichotomous outcome variable called Turnover. To the right of the tilde (~) operator, type the names of the predictor variables we wish to include in the model, each separated by a + operator. As the second argument, type data= followed by the name of the training data frame object (train_df). As the third and fourth arguments, specify that we want to estimate a estimate a logistic regression model which is part of the generalized linear model (glm) and binomial family of analyses. To do so, type the argument method=\"glm\" and the argument family=binomial. As the fifth argument, type trControl= followed by the name of the object that includes your model specifications, which we created above (ctrlspecs). Finally, if you so desire, you can use the print function from base R to request some basic information about the training process. # Set random seed for subsequent random selection and assignment operations set.seed(1985) # Specify logistic regression model to be estimated using training data # and k-fold cross-validation process model1 &lt;- train(Turnover ~ JS + Naff + TI + OC, data=train_df, method=&quot;glm&quot;, family=binomial, trControl=ctrlspecs) # Print information about model print(model1) ## Generalized Linear Model ## ## 800 samples ## 4 predictor ## 2 classes: &#39;quit&#39;, &#39;stay&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 721, 720, 721, 719, 720, 721, ... ## Resampling results: ## ## Accuracy Kappa ## 0.6987682 0.3890541 In the general information provided in our output about our model/algorithm (i.e., generalized linear model), we find out which method was applied (i.e., cross-validated), the number of folds (i.e., 10), and the size of the same sizes in training each iteration of the model (e.g., 721, 720, 721, etc.). The function created 10 roughly equal-sized and randomly-assigned folds (per our request) from the train_df data frame by resampling from the original 800 cases. In total, the model was cross-validated 10 times across 10 folds, such that in each instance 9 samples (e.g., samples 1-9; i.e., k-1) were used to estimate the model and, more importantly, its classification/prediction error when applied to the validation kth sample (e.g., holdout data, out-of-sample data). This was repeated until every sample served as the validation sample once, resulting in k folds (i.e., 10 folds in this tutorial). These cross-validated models were then tossed out, but the estimated error, which by default is classification accuracy for this type of model, was averaged across the folds to provide an estimate of the overall predictive performance of the final model. The final model and its parameters (e.g., regression coefficients) were estimated based on the entire training data frame (train_df). In this way, the final model was built and trained to improve accuracy and, in the case of logistic regression, the percentage of correct classifications for the dichotomous outcome variable (Turnover). In the output, we also received estimates of the model’s performance in terms of the average accuracy and Cohen’s kappa values across the folds (i.e., resamples). Behind the scenes, the train function has done a lot for us. The accuracy statistic represents the proportion of total correctly classified cases out of all classification instances; in the case of a logistic regression model with a dichotomous outcome variable, a correctly classified case would be one in which the actual observed outcome for the case (e.g., observed quit) matches what was predicted based on the estimated model (e.g., predicted quit). Cohen’s kappa (\\(\\kappa\\)) is another classification accuracy statistic, but it differs from the accuracy statistic in that it accounts for the baseline probabilities from your null model that contains no predictor variables. In other words, it accounts for the proportion of cases in your data that were observed to have experienced one level of the dichotomous outcome (e.g., quit) versus the other level (e.g., stay). This is useful when there isn’t a 50%/50% split in the observed levels/classes of your dichotomous outcome variable. Like so many effect size metrics, Cohen’s (\\(\\kappa\\)) has some recommended thresholds for interpreting the classification strength different \\(\\kappa\\) values, as shown below (Landis and Koch 1977). Cohen’s \\(\\kappa\\) Description .81-1.00 Almost perfect .61-.80 Substantial .41-60 Moderate .21-.40 Fair .00-.20 Slight &lt; .00 Poor Based on our k-fold cross-validation framework, model accuracy was .70 (70%) and Cohen’s kappa was .39, which would be considered “fair” given the thresholds provided above. Thus, our model did an okay job of classifying cases correctly estimated using k-fold cross-validation and when tested using the training data frame (train_df). We can also use the summary function from base R to view the results of our final model, just as we would if we were estimating a multiple logistic regression model without cross-validation when using the glm function from base R. If you so desire, you could also specify the equation necessary to predict the likelihood that an individual quits; if you need a refresher, refer to the chapter on logistic regression and specifically the chapter supplement, which demonstrates how to estimate a multiple logistic regression using the glm function from base R. # Print results of final model estimated using training data summary(model1) ## ## Call: ## NULL ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.17201 0.61856 3.511 0.000446 *** ## JS 0.14929 0.07355 2.030 0.042386 * ## Naff -0.42540 0.15747 -2.702 0.006902 ** ## TI -0.99246 0.11133 -8.915 &lt; 0.0000000000000002 *** ## OC 0.25999 0.06994 3.717 0.000201 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1102.55 on 799 degrees of freedom ## Residual deviance: 907.54 on 795 degrees of freedom ## AIC: 917.54 ## ## Number of Fisher Scoring iterations: 4 As a next step, we will evaluate the importance of different predictor variables in our model. Variables with higher importance values contribute more to model estimation – and in this case, model classification. # Estimate the importance of different predictors varImp(model1) ## glm variable importance ## ## Overall ## TI 100.000 ## OC 24.509 ## Naff 9.758 ## JS 0.000 As you can see, TI was the most important variable when it came to predicting Turnover (1.00), and it was followed by OC (24.51) and then Naff (9.86). JS was deemed an unimportant predictor, as shown in the model summary above, and it had 0.00 importance based on this estimation process. Remember way back when in this tutorial when we partitioned our data into the train_df and test_df? Well, now we’re going to apply the final multiple logistic regression model we estimated using k-fold cross-validation and the train_df data frame to our test_df data frame. First, using our final model, we need to estimate predicted classes/values for individuals’ outcomes (Turnover) based on their predictor variables’ scores in the test_df data frame. Let’s call the object to which we will assign these predictions predictions by using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the predict function from base R. As the first argument in the predict function, type the name of the model object we built using the train function (model1), and as the second argument, type newdata= followed by the name of the testing data frame (test_df). # Predict outcome using model from training data based on testing data predictions &lt;- predict(model1, newdata=test_df) Using the confusionMatrix function from the caret package, we will assess how well our model fit the test data. Type the name of the confusionMatrix function. As the first argument, type data= followed by the name of the object to which you assigned the predicted values/classes on the outcome in the previous step (predictions). As the second argument, type the name of the test data frame (test_df), followed by the $ operator and the name of the dichotomous outcome variable (Turnover). # Create confusion matrix to assess model fit/performance on test data confusionMatrix(data=predictions, test_df$Turnover) ## Confusion Matrix and Statistics ## ## Reference ## Prediction quit stay ## quit 85 37 ## stay 17 61 ## ## Accuracy : 0.73 ## 95% CI : (0.6628, 0.7902) ## No Information Rate : 0.51 ## P-Value [Acc &gt; NIR] : 0.0000000001757 ## ## Kappa : 0.4576 ## ## Mcnemar&#39;s Test P-Value : 0.009722 ## ## Sensitivity : 0.8333 ## Specificity : 0.6224 ## Pos Pred Value : 0.6967 ## Neg Pred Value : 0.7821 ## Prevalence : 0.5100 ## Detection Rate : 0.4250 ## Detection Prevalence : 0.6100 ## Balanced Accuracy : 0.7279 ## ## &#39;Positive&#39; Class : quit ## The output provides us with a lot of useful information that can help us assess how well the model fit and performed with the test data, which I review below. Contingency Table: The output first provides a contingency table displaying the 2x2 table of observed quit vs. stay in relation to predicted quit vs. stay. Model Accuracy: We can see that the accuracy is .73 (73%) (95% CI[.66, .79]). The p-value is a one-sided test that helps us understand if the model is more accurate than a model with no information (e.g., predictors); here, we see that that p-value is less than .05, which would lead us to conclude the model is more accurate to a statistically significant extent. Cohen’s \\(\\kappa\\): Cohen’s \\(\\kappa\\) is .46, which would be considered “moderate” classification strength by the thresholds shown above. Sensitivity: Sensitivity represents the number of cases in which the “positive” class (e.g., quit) was accurately predicted divided by the total number of cases whose observed data fall into the “positive” class (e.g., quit). In other words, sensitivity represents the proportion of people who were correctly classified as having quit relative to the total number of people who actually quit. In this case, sensitivity is .83. Specificity: Specificity can be conceptualized as the opposite of sensitivity in that it represents the number of cases in which the “negative” class (e.g., stay) was accurately predicted divided by the total number of cases whose observed data fall into the “negative” class (e.g., stay). In other words, specificity represents the proportion of people who were correctly classified as having stayed relative to the total number of people who actually stayed. In this case, sensitivity is .62. Prevalence: Prevalence represents the number of cases whose observed data indicate that they were in the “positive” class (e.g., quit) divided by the total number of cases in the sample. In other words, prevalence represents the proportion of people who actually quit relative to the entire sample. In this example, prevalence is .51. When interpreting sensitivity, it is useful to compare it to prevalence. Balanced Accuracy: Balanced accuracy is calculated by adding sensitivity to specificity and then dividing that sum by 2. In this example, the balanced accuracy is .73, and thus provides another estimate of the model’s overall classification accuracy. To learn about some of the other information in the output, check out the documentation for the confusionMatrix function. # View documentation for confusionMatrix function from caret package ?caret::confusionMatrix Summary of Results: Overall, the multiple logistic regression model we trained using k-fold cross-validation performed reasonably well when applied to the test data. Adding additional predictors to the model or measuring the predictors with great reliability could improve the predictive and classification performance of the model. 48.2.5 Summary In this chapter, we dipped our toes into predictive analytics by using the k-fold cross-validation approach in which we trained our model using one set of data and then applied to a separate set of data to evaluate the extent to which it correctly classified cases. References "],["survival.html", "Chapter 49 Understanding Length of Service Using Survival Analysis 49.1 Conceptual Overview 49.2 Tutorial", " Chapter 49 Understanding Length of Service Using Survival Analysis In this chapter, we will learn how to perform a survival analysis in order to understand employees length of service prior to leaving the organization. 49.1 Conceptual Overview The delightfully named survival analysis refers to various statistical techniques that can be used to investigate how long it takes for an event to occur. How did it get that name? Well, the term survival comes from medical and health studies that predicted, well, survival time. Of course, survival analysis can also be applied to other phenomena of interest. For example, in organizations, we might wish to understand the probability of individuals voluntarily leaving the organization by certain time points and the factors, predictors, or drivers (i.e., covariates) that might help us understand how long certain individuals are likely stay – or their expected length of service. I should note that survival analysis also goes by other names, such as failure analysis, time-to-event analysis, duration analysis, event-history analysis, and accelerated failure time analysis. 49.1.1 Censoring Due to normal data acquisition processes, we often collect data at a specific point in time, where historical data might only be available for certain individuals (or based on whether they meet the inclusions criteria to be part of our sample). Further, for some individuals, the event in question (e.g., voluntary turnover) will not have occurred (at least in the context of the acquired data), which means that their length of survival until the event remains unknown. Alternatively, other individuals may have experienced another event (e.g., involuntary turnover) that was the reason for their exit from the study. Individuals who fall into these categories are referred to as censored cases. There are different types of censoring, such as right censoring, left censoring, and interval censoring. Right-censored cases are those cases for which the event has not yet occurred when data acquisition is completed and the study has ended, or those cases that dropped out of study data before the end for reasons other than the focal event; for example, if we would like to understand the survival rate for individuals who voluntarily turn over (i.e., focal event), then examples of right-censored individuals include those individuals who are still employed at the organization by the end of the study and those individuals who dropped out of the study prematurely due because they were fired (i.e., involuntarily turned over). Left-censored cases are those cases for which the focal event occurred prior to the study commencing. In organizational research, this would often mean that a case was mistakenly included in the study sample, when the case should have been excluded. An example of a left-censored case - albeit an unlikely one - would be if for some reason it was later determined that an employee included a study sample had voluntarily turned over (i.e., focal event) prior to the beginning of the 5-year study period. In organizational research, usually such cases will just be removed from the sample prior to analysis. Interval-censored cases are those cases for which the focal event occurred between two time points but where the exact time of the event remains unknown. As an example, suppose that for some reason, the exact day on which an employee voluntarily turned over (i.e., focal event) is unknown, but organizational records indicate that the individual was employed at the end of the prior month but not employed by the end of the subsequent month. This individual would be interval censored. In this chapter, we will focus on right-censored cases, as this is perhaps the most common type of censoring we concern ourselves with in organizational research. 49.1.2 Types of Survival Analysis There are different techniques we can use to carry out survival analysis, and different techniques are suited to answering different questions. The following are survival analyses that you might encounter: Kaplan-Meier analysis, Cox proportional hazards model (i.e., Cox regression), log-logistic parametric model, Weibull model, survival tree analysis, and survival random forest analysis. In this tutorial, we will focus on implementing Kaplan-Meier analysis and the Cox proportional hazards model. Before doing so, however, we will learn about what is referred to as the life table. 49.1.2.1 Life Table A life table is a descriptive tool and contains important information regarding length of survival and survival rate, and it is descriptive in nature. A life table displays the proportion of “survivors” at specific time points and, specifically, between two time points, where the latter is referred to as a time interval (\\(i\\)). The time interval width chosen affects the survival probabilities, so it’s important to choose the size of the interval wisely and thoughtfully. One approach is to create a new time interval at each subsequent time at which the event occurs; this is usually the default approach. Another approach is to create time intervals that are meaningful for the context; for example, if the focal event is voluntary turnover, then we might decide that the survival probability/rate should be assessed at 30, 60, and 90 days post hire and then 6 months, 1 year, 2 years, 3 years, and so forth and so on. Let’s consider an example of a life table. Imagine you wish to know how many people remained with the organization between their 61st day of work until the end of their probationary period at 89 days post hire. First, we need to estimate \\(r_i\\), where \\(r_i\\) refers to the adjusted number of individuals at risk during the time interval in question, which takes into account how many individuals entered the interval and how many individuals were censored. \\(r_i = n_i - \\frac{1}{2}c_i\\) where \\(n_i\\) refers to the number of individuals who entered the interval (e.g., 60-89 days), and \\(c_i\\) refers to the number of individuals who were censored in the time interval. For example, if 100 individuals entered the interval and 10 individuals were censored, then \\(r_i\\) is equal to 95. \\(r_i = 100 - \\frac{1}{2}*10 = 95\\) Next, we can calculate \\(q_i\\), which refers to the proportion of individuals who experienced the event during the time interval in question (i.e., did not “survive”). \\(q_i = \\frac{d_i}{r_i}\\) where \\(d_i\\) refers to the number of individuals who experienced the event during the time interval (i.e., did not “survive”). Let’s assume that 15 individuals experienced the event during this interval; this means that \\(q_i\\) is equal to .16. \\(q_i = \\frac{15}{95} = .16\\) To calculate \\(p_i\\), which is the proportion of individuals who did not experience the event during the time interval, we just subtract \\(q_i\\) from \\(1\\). You can think of \\(p_i\\) as the survival rate for a specific time interval. \\(p_i = 1 - q_i\\) If we plug our \\(q_i\\) value of .14 into the formula, we arrive at a \\(p_i\\) equal to .84. \\(p_i = 1 - .16 = .84\\) Finally, to calculate \\(s_i\\), which refers to the proportion of individuals who survived past/through the time interval in question (i.e., cumulative survival rate or cumulative survival proportion/probability), we use the following formula. \\(s_i = s_{i-1} * p_i\\) where \\(s_{i-1}\\) refers to the proportion of individuals who survived past/through the previous time interval. Let’s assume in this example that \\(s_{i-1}\\) is equal to .86, which would indicate that 86% of individuals survived past the previous time interval (i.e., did not experience the event). \\(s_i = .86 * .84 = .72\\) As you can see, in this example, the proportion of individuals who survived past/through the focal time interval is .72. In other words, the cumulative survival rate is .72 for that time interval. When we assemble all of these estimates together, we end up with a life table like the one shown below. Interval (\\(i\\)) \\(n_i\\) \\(c_i\\) \\(r_i\\) \\(d_i\\) \\(q_i\\) \\(p_i\\) \\(s_i\\) 0-29 days 120 2 119 15 .13 .87 .87 30-59 days 103 1 102.5 2 .02 .98 .85 60-89 days 100 10 95 15 .16 .84 .71 49.1.2.2 Kaplan-Meier Analysis Kaplan-Meier analysis is a nonparametric method, which means that it does not have the same distributional assumptions as a parametric method. This analysis allows us to estimate the standard error and confidence interval of the survival rate (\\(s_i\\)) at each time interval and across the entire study, which helps us to account for and understand uncertainty owing to sampling error. Using Kaplan-Meier analysis, we can also understand how independent groups of cases (e.g., race/ethnicity groups) differ with respect to their survival rates and their overall curves/trajectories. Kaplan-Meier analysis is an example of a descriptive approach to survival analysis, and it is also referred to as the product-limit method/estimator. 49.1.2.3 Cox Proportional Hazards Model The Cox proportional hazards model is a semiparametric method in that it makes a distributional assumption about the covariates in the model and their log-linear relation to the hazard (i.e., instantaneous rate of experiencing focal event), but does not make a distributional assumption about the hazard function itself. This analysis allows us to investigate how categorical and continuous covariates (e.g., predictor variables) might affect individuals’ length of survival. 49.1.2.4 Statistical Assumptions Given that Kaplan-Meier analysis and the Cox proportional hazards model are examples of nonparametric and semiparametric methods, respectively, we don’t need to meet many of the statistical assumptions commonly associated with traditional parametric methods. Kaplan-Meier analysis makes the following assumptions: Experiencing the focal event and being censored are mutually exclusive and independent; (b) censoring is similar between independent groups (i.e., by levels of categorical covariate); No interventions or trends were introduced during the study time frame that may have influenced the probability of experiencing the focal event. As described above, the Cox proportional hazards model is a semiparametric method and has a distributional assumption about the covariates in the model and their relation to the hazard. In addition, this type of model makes the following assumptions: Time-to-event values of cases are independent of one another; Hazard ratio is constant over time; Model covariates have multiplicative effect on the hazard (i.e., association are not linear). 49.1.2.5 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the coefficient is equal to zero. In other words, if a coefficient’s p-value is less than .05, we conclude that the coefficient differs from zero to a statistically significant extent. In contrast, if the coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the coefficient is equal to zero. Put differently, if a coefficient’s p-value is equal to or greater than .05, we conclude that the coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. 49.1.3 Conceptual Video For a more in-depth conceptual review of survival, please check out the following conceptual video. Link to conceptual video: https://youtu.be/874ZcEMlUas 49.2 Tutorial This chapter’s tutorial demonstrates how to perform survival analysis using R. 49.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/Bubo_7R7h0Q 49.2.2 Functions &amp; Packages Introduced Function Package survfit survival Surv survival hist base R subset base R print base R summary base R c base R plot base R ggsurvplot survminer coxph survival scale base R factor base R drop_na tidyr anova base R 49.2.3 Initial Steps If you haven’t already, save the file called “Survival.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “Survival.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object survdat &lt;- read_csv(&quot;Survival.csv&quot;) ## Rows: 701 Columns: 9 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Start_date, Gender, Race, Turnover_date ## dbl (5): id, Pay_hourly, Pay_sat, Turnover, LOS ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(survdat) ## [1] &quot;id&quot; &quot;Start_date&quot; &quot;Gender&quot; &quot;Race&quot; &quot;Pay_hourly&quot; &quot;Pay_sat&quot; &quot;Turnover&quot; &quot;Turnover_date&quot; ## [9] &quot;LOS&quot; # View variable type for each variable in data frame (tibble) object str(survdat) ## spc_tbl_ [701 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:701] 1073 1598 1742 1125 1676 ... ## $ Start_date : chr [1:701] &quot;5/4/2014&quot; &quot;9/30/2016&quot; &quot;3/14/2018&quot; &quot;9/7/2014&quot; ... ## $ Gender : chr [1:701] &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; ... ## $ Race : chr [1:701] &quot;Black&quot; &quot;Black&quot; &quot;Black&quot; &quot;Black&quot; ... ## $ Pay_hourly : num [1:701] 12.6 17.7 12.6 15.2 14.7 ... ## $ Pay_sat : num [1:701] 4.67 3 4.33 3 3.33 ... ## $ Turnover : num [1:701] 1 1 0 1 0 1 2 1 1 0 ... ## $ Turnover_date: chr [1:701] &quot;1/14/2017&quot; &quot;5/7/2018&quot; NA &quot;4/13/2017&quot; ... ## $ LOS : num [1:701] 986 584 293 949 622 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. Start_date = col_character(), ## .. Gender = col_character(), ## .. Race = col_character(), ## .. Pay_hourly = col_double(), ## .. Pay_sat = col_double(), ## .. Turnover = col_double(), ## .. Turnover_date = col_character(), ## .. LOS = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame (tibble) object head(survdat) ## # A tibble: 6 × 9 ## id Start_date Gender Race Pay_hourly Pay_sat Turnover Turnover_date LOS ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1073 5/4/2014 Man Black 12.6 4.67 1 1/14/2017 986 ## 2 1598 9/30/2016 Man Black 17.7 3 1 5/7/2018 584 ## 3 1742 3/14/2018 Man Black 12.6 4.33 0 &lt;NA&gt; 293 ## 4 1125 9/7/2014 Man Black 15.2 3 1 4/13/2017 949 ## 5 1676 4/19/2017 Man Black 14.7 3.33 0 &lt;NA&gt; 622 ## 6 1420 12/25/2015 Man Black 15.9 4.33 1 7/31/2018 949 There are 9 variables and 701 cases (i.e., individuals) in the survdat data frame: id, Start_date, Gender, Race, Pay_hourly, Pay_sat, Turnover, Turnover_date, and LOS. In this tutorial we will focus on just Race, Pay_hourly, Pay_sat, Turnover, and LOS. the Race variable is categorical (nominal) and contains information about the race for each case in the sample. The Pay_hourly variable contains numeric data regarding the hourly pay rate of each individual when they either last worked in the organization or their current hourly pay rate. The Pay_sat variable comes from a pay satisfaction survey, and represents the overall perceived satisfaction with pay, ranging from 1 (low) to 5 (high). The Turnover variable indicates whether a case stayed in the organization (0), voluntarily quit (1), or involuntarily left. Finally, the LOS variable stands for length of service, and it represents how many days an individual worked in the organization prior to leaving or how long those who were employed at the time of the data collection had worked in the organization thus far. 49.2.4 Create a Censoring Variable Let’s assume that our focal event for which we wish to estimate survival is voluntary turnover. Note that our Turnover variable has three levels: 0 = stayed, 1 = voluntary turnover, 2 = involuntary turnover. Given that, we need to create a variable that can be used to classify the right-censored cases, which are those individuals who are still employed at the organization at the end of the study (Turnover = 0) and those who left the organization for reasons other than voluntary turnover (Turnover = 2). Now we’re ready to create a new variable called censored in which cases with Turnover equal to 1 are set to 1; this will indicate the individuals who experienced the event (i.e., voluntary turnover) within the study. Next cases with Turnover equal to 0 or 2 are set to 1 for the censored variable, and these cases represented are our right-censored cases. # Create censoring variable survdat$censored[survdat$Turnover == 1] &lt;- 1 survdat$censored[survdat$Turnover == 0 | survdat$Turnover == 2] &lt;- 0 49.2.5 Inspect Distribution of Length of Service To better understand our data, let’s run a simple histogram for length of service (LOS) for all cases in our data frame (survdat) using the hist function from base R. # Inspect distribution for LOS for all cases hist(survdat$LOS) Interestingly, length of service looks normally distributed in this sample. Let’s drill down a bit deeper by looking at length of service for just those individuals who stayed in the organization for the duration of the study. To do so, let’s bring in the subset function from base R and integrate it within our hist function. # Inspect distribution for LOS for those who stayed hist(subset(survdat, Turnover==0)$LOS) This distribution also looks relatively normal. Next, let’s inspect the distributions for those who voluntarily turned over. # Inspect distribution for LOS for those who voluntarily turned over hist(subset(survdat, Turnover==1)$LOS) Both of these distributions seem to be relatively normal as well, although the distribution for just those individuals who involuntarily turned over has many fewer cases. Please note that it is not unusual to see nonnormal distributions of length of service; for example, in some industries, there tends to be a large number of “first day no shows” for certain jobs and a higher turnover during the first 90 days on-the-job than after the first 90 days. This can result in a positively skewed distribution. 49.2.6 Conduct Kaplan-Meier Analysis &amp; Create Life Table We’ll begin by taking a descriptive approach to survival analysis. That is, we’ll creating a life table and conduct a Kaplan-Meier analysis. If you haven’t already, be sure to install and access the survival package (Therneau 2021; Terry M. Therneau and Patricia M. Grambsch 2000). # Install survival package if you haven&#39;t already install.packages(&quot;survival&quot;) # Access survival package library(survival) To begin, we will use the survfit and Surv functions from the survival package, which allow us to fit a survival model based on a survival object. In this case, a survival object contains a vector of censored and non-censored values for the cases in our data frame, where the censored values are followed by a plus (+) sign. [Remember, we created a variable called censored above, which will serve as the basis for distinguishing between censored and non-censored cases in our data frame (survdat).] That is, the behind-the-scenes vector shows how long each person survived before the event in question, and if they were still surviving at the time the data were acquired throughout the study or left the study for some other reason, then they are considered to be (right) censored. Let’s name our Kaplan-Meier analysis model km_fit1 using the &lt;- operator. Next, type the name of the survfit function from the survival package. As the first argument, specify the survival model by specifying the outcome to the left of the ~ (tilde) symbol and the predictor to the right. As the outcome, we type the Surv function with the survival length variable (length of service: LOS) as the first argument, and the variable that indicates which cases should be censored (censored) should be typed as the second argument; cases with a zero (0) on the second variable (censored) are treated as censored because they did not experience the event in question (i.e., voluntary turnover). As our predictor, we will specify the numeral 1 because we want to begin by estimating a null model (i.e., no predictor variables); this value goes to the right of the ~ (tilde) symbol. As the second argument in the survfit function, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. As the final argument, type type=\"kaplan-meier\" to specify that we wish to use Kaplan-Meier estimation. Use the print function from base R to view the basic descriptive survival information for the entire sample. # Conduct Kaplan-Meier analysis km_fit1 &lt;- survfit(Surv(LOS, censored) ~ 1, data=survdat, type=&quot;kaplan-meier&quot;) # Print basic descriptive survival information print(km_fit1) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 701 463 1059 1022 1095 In the output, we find some basic descriptive information about our survival model. The n column indicates the number of total cases in our sample (e.g., 701). The events column indicates how many cases experienced the event over the course of the study (e.g., 463), where in this example, the event in question is voluntary turnover. The median column indicates the median length of time (e.g., 1059), where time in this example is measured in days of length of service before someone experiences the event; this is determined based on the time to event when the cumulative survival rate is .50. The 0.95LCL and 0.95UCL contain the upper and lower limits of the 95% confidence interval surrounding the median value (e.g., 1022-1095). Based on this output, we can concluded that 479 individuals out of 701 individuals voluntarily turned over over the course of the study time frame, and the median length of service prior to leaving was 1059 days (95% CI[1022, 1095]). Note: If the overall median length of survival (i.e., time to event) and/or the associated lower/upper confidence interval limits is/are NA, it indicates a cumulative survival rate of .50 was not reached for those estimates. Usually, you can see this visually in a survival plot, as the survival curve and/or its confidence interval limits will not cross a cumulative survival rate/probability of .50. Next, let’s summarize the Kaplan-Meier analysis object we created (km_fit1) using the summary function from base R. Just enter the name of the model object as the sole parenthetical argument in the summary function. # Summarize results of Kaplan-Meier analysis using default time intervals # and create a life table summary(km_fit1) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 701 12 0.9829 0.00490 0.9733 0.9925 ## 110 682 1 0.9814 0.00510 0.9715 0.9915 ## 146 681 3 0.9771 0.00566 0.9661 0.9883 ## 183 677 4 0.9713 0.00632 0.9590 0.9838 ## 219 672 3 0.9670 0.00677 0.9538 0.9804 ## 256 667 4 0.9612 0.00732 0.9470 0.9757 ## 292 659 1 0.9597 0.00745 0.9453 0.9745 ## 329 655 1 0.9583 0.00758 0.9435 0.9733 ## 365 652 6 0.9495 0.00832 0.9333 0.9659 ## 402 641 7 0.9391 0.00911 0.9214 0.9571 ## 438 633 8 0.9272 0.00991 0.9080 0.9469 ## 475 616 4 0.9212 0.01030 0.9012 0.9416 ## 511 610 8 0.9091 0.01101 0.8878 0.9310 ## 548 601 13 0.8895 0.01205 0.8662 0.9134 ## 584 584 11 0.8727 0.01284 0.8479 0.8982 ## 621 566 11 0.8557 0.01357 0.8296 0.8828 ## 657 549 12 0.8370 0.01431 0.8095 0.8656 ## 694 532 16 0.8119 0.01520 0.7826 0.8422 ## 730 506 11 0.7942 0.01577 0.7639 0.8257 ## 767 487 25 0.7534 0.01694 0.7210 0.7874 ## 803 454 20 0.7203 0.01774 0.6863 0.7559 ## 840 428 12 0.7001 0.01818 0.6653 0.7366 ## 876 406 24 0.6587 0.01897 0.6225 0.6969 ## 913 366 20 0.6227 0.01956 0.5855 0.6622 ## 949 339 19 0.5878 0.02004 0.5498 0.6284 ## 986 313 22 0.5465 0.02047 0.5078 0.5881 ## 1022 281 23 0.5017 0.02081 0.4626 0.5442 ## 1059 247 18 0.4652 0.02101 0.4258 0.5082 ## 1095 224 25 0.4133 0.02107 0.3740 0.4567 ## 1132 189 16 0.3783 0.02103 0.3392 0.4218 ## 1168 163 15 0.3435 0.02092 0.3048 0.3870 ## 1205 138 14 0.3086 0.02077 0.2705 0.3521 ## 1241 114 13 0.2734 0.02057 0.2359 0.3169 ## 1278 96 10 0.2449 0.02030 0.2082 0.2882 ## 1314 84 8 0.2216 0.01997 0.1857 0.2644 ## 1351 69 6 0.2023 0.01973 0.1672 0.2449 ## 1387 60 6 0.1821 0.01941 0.1478 0.2244 ## 1424 51 10 0.1464 0.01860 0.1141 0.1878 ## 1460 32 10 0.1007 0.01753 0.0715 0.1416 ## 1497 21 4 0.0815 0.01661 0.0546 0.1215 ## 1533 15 1 0.0760 0.01637 0.0499 0.1159 ## 1570 14 1 0.0706 0.01607 0.0452 0.1103 ## 1606 11 2 0.0578 0.01550 0.0341 0.0978 ## 1643 7 1 0.0495 0.01533 0.0270 0.0908 ## 1679 6 1 0.0413 0.01483 0.0204 0.0835 ## 1935 1 1 0.0000 NaN NA NA By default, this function begins each new time interval at the time in which a new event (i.e., voluntary turnover) occurs. This explains why the time intervals are not consistent widths in the time column of the life table output. The n.risk column corresponds to the the number of individuals who entered the time interval (\\(i\\)), which I introduced earlier in this tutorial using the notation \\(n_i\\). The n.event column contains the number of individuals who experienced the event during the time interval (i.e., did not “survive”), which I introduced earlier using the notation \\(d_i\\). The survival column contains the the proportion of individuals who survived past/through the time interval in question, which is referred to as the (cumulative) survival rate (\\(s_i\\)). The std.err column provides the standard errors (SEs) associated with the cumulative survival rate points estimates, and the lower 95% CI and upper 95% CI columns provide the 95% confidence interval around the point cumulative survival rate point estimate. Thus, the survfit function in combination with the summary function provides us with an abbreviated life table. We can, however, exert more control over the width of the time intervals in our life table output, and we might exert this control because we have business-related or theoretically meaningful time intervals that we would like to impose. Let’s adapt our previous summary function by adding an additional argument relating to the time intervals. Specifically, as the second argument, type times= followed by the c (combine) function from base R to specify a vector of different time intervals for which you would like to estimate survival rates. In this example, I set the intervals at 30 days, 60 days, 90 days post hire, and up to 30 subsequent 90-day intervals after the initial 90 days (by using the 90*(1:30) notation). # Summarize results of Kaplan-Meier analysis using pre-specified time intervals # and create a life table summary(km_fit1, times=c(30, 60, 90*(1:30))) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 701 0 1.0000 0.00000 1.0000 1.0000 ## 60 701 0 1.0000 0.00000 1.0000 1.0000 ## 90 682 12 0.9829 0.00490 0.9733 0.9925 ## 180 677 4 0.9771 0.00566 0.9661 0.9883 ## 270 659 11 0.9612 0.00732 0.9470 0.9757 ## 360 652 2 0.9583 0.00758 0.9435 0.9733 ## 450 616 21 0.9272 0.00991 0.9080 0.9469 ## 540 601 12 0.9091 0.01101 0.8878 0.9310 ## 630 549 35 0.8557 0.01357 0.8296 0.8828 ## 720 506 28 0.8119 0.01520 0.7826 0.8422 ## 810 428 56 0.7203 0.01774 0.6863 0.7559 ## 900 366 36 0.6587 0.01897 0.6225 0.6969 ## 990 281 61 0.5465 0.02047 0.5078 0.5881 ## 1080 224 41 0.4652 0.02101 0.4258 0.5082 ## 1170 138 56 0.3435 0.02092 0.3048 0.3870 ## 1260 96 27 0.2734 0.02057 0.2359 0.3169 ## 1350 69 18 0.2216 0.01997 0.1857 0.2644 ## 1440 32 22 0.1464 0.01860 0.1141 0.1878 ## 1530 15 14 0.0815 0.01661 0.0546 0.1215 ## 1620 7 4 0.0578 0.01550 0.0341 0.0978 ## 1710 4 2 0.0413 0.01483 0.0204 0.0835 ## 1800 3 0 0.0413 0.01483 0.0204 0.0835 ## 1890 1 0 0.0413 0.01483 0.0204 0.0835 The output includes a life table with the time intervals that we specified in the time column, along with the other columns that we reviewed in the previous output shown above. Looking at this life table, descriptively, we can see that the cumulative survival rate begins to drop more precipitously beginning at about 720 days post hire. We can create a data visualization (i.e., line chart) of our km_fit1 model by using the plot function from base R. Simply, enter the name of the model object as the sole parenthetical argument of the plot function. # Plot the cumulative survival rates (probabilities) plot(km_fit1) The data visualization shown above provides a visual depiction of the survival curve/trajectory along with the 95% confidence interval for each time interval. Corroborating what we saw in our first life table output using the default time-interval settings, the cumulative survival rate begins to drop more precipitously when length of service is between 500 and 100 days. Note that the x-axis shows the length of service (in days), and the y-axis shows the cumulative survival rate (i.e., probability). We can also create a more aesthetically pleasing data visualization of our km_fit1 model by using the ggsurvplot function from the survminer package (Kassambara, Kosinski, and Biecek 2021). If you haven’t already, install and access the survminer package. # Install survminer package if you haven&#39;t already install.packages(&quot;survminer&quot;) # Access survminer package library(survminer) Using the ggsurvplot function, type the name of the Kaplan-Meier analysis object (km_fit1) as the first argument. As the second argument, type data= followed by the name of the data frame object (survdat). The following arguments are optional. Setting risk.table=TRUE requests the risk table as output. Setting conf.int=TRUE requests that the 95% confidence intervals be displayed to show uncertainty. Finally, because the ggsurvplot function is built in part on the ggplot2 package, we can request different data visualization themes from ggplot2; by setting ggtheme=theme_minimal(), we are requesting a minimalist theme. # Plot the cumulative survival rates (probabilities) ggsurvplot(km_fit1, data=survdat, risk.table=TRUE, conf.int=TRUE, ggtheme=theme_minimal()) The plot graphically displays the cumulative survival rates/probabilities and the risk table. The shading around the red line represents the 95% confidence intervals. The risk table below the plot displays the average number of cases at risk of experiencing the event (\\(r_i\\)) at the time intervals displayed on the x-axis. Moving forward, let’s extend our Kaplan-Meier analysis model by adding a categorical covariate (i.e., predictor variable) to our Kaplan-Meier analysis model to see if we can observe differences between groups/categories in terms of survival curves. Specifically, let’s replace the numeral 1 from our previous model with the categorical Race variable, and let’s name this model object km_fit2. # Conduct Kaplan-Meier analysis with categorical covariate km_fit2 &lt;- survfit(Surv(LOS, censored) ~ Race, data=survdat, type=&quot;kaplan-meier&quot;) print(km_fit2) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## n events median 0.95LCL 0.95UCL ## Race=Black 283 190 1022 986 1095 ## Race=HispanicLatino 79 57 1022 876 1132 ## Race=White 339 216 1059 1022 1095 In the output, we find basic descriptive information about our survival model; however, because we added the categorical covariate variable called Race, we now see the overall survival information for the study time frame displayed by Race categories. For example, the n column indicates the number of total cases in our sample by Race categories (e.g., 283, 79, 339), and the events column indicates how many cases experienced the event over the course of the study for each Race category (e.g., 190, 57, 216). This information allows us to segment the survival information by levels/categories of the categorical covariate added to the model. As we did before, let’s summarize the updated model object (km_fit2) using the default time intervals. # Summarize results of Kaplan-Meier analysis using default time intervals # and create a life table summary(km_fit2) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## Race=Black ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 283 6 0.9788 0.00856 0.9622 0.9957 ## 146 272 3 0.9680 0.01049 0.9477 0.9888 ## 183 268 3 0.9572 0.01210 0.9337 0.9812 ## 256 264 1 0.9535 0.01258 0.9292 0.9785 ## 292 263 1 0.9499 0.01305 0.9247 0.9758 ## 329 261 1 0.9463 0.01350 0.9202 0.9731 ## 365 260 3 0.9354 0.01474 0.9069 0.9647 ## 402 255 7 0.9097 0.01724 0.8765 0.9441 ## 438 248 1 0.9060 0.01755 0.8723 0.9411 ## 475 244 3 0.8949 0.01848 0.8594 0.9318 ## 511 241 4 0.8800 0.01961 0.8424 0.9193 ## 548 236 7 0.8539 0.02136 0.8131 0.8968 ## 584 227 5 0.8351 0.02249 0.7922 0.8804 ## 621 219 3 0.8237 0.02313 0.7796 0.8703 ## 657 212 3 0.8120 0.02376 0.7668 0.8599 ## 694 205 4 0.7962 0.02458 0.7494 0.8458 ## 730 195 1 0.7921 0.02479 0.7450 0.8422 ## 767 191 8 0.7589 0.02638 0.7089 0.8124 ## 803 180 5 0.7378 0.02728 0.6862 0.7933 ## 840 173 5 0.7165 0.02811 0.6635 0.7738 ## 876 165 9 0.6774 0.02944 0.6221 0.7377 ## 913 152 12 0.6239 0.03090 0.5662 0.6875 ## 949 136 8 0.5872 0.03169 0.5283 0.6528 ## 986 126 12 0.5313 0.03253 0.4712 0.5990 ## 1022 110 8 0.4927 0.03291 0.4322 0.5616 ## 1059 98 8 0.4525 0.03315 0.3919 0.5223 ## 1095 89 9 0.4067 0.03312 0.3467 0.4771 ## 1132 74 8 0.3627 0.03299 0.3035 0.4335 ## 1168 63 6 0.3282 0.03272 0.2699 0.3990 ## 1205 53 2 0.3158 0.03264 0.2579 0.3867 ## 1241 47 5 0.2822 0.03244 0.2253 0.3535 ## 1278 41 6 0.2409 0.03177 0.1860 0.3120 ## 1314 33 3 0.2190 0.03130 0.1655 0.2898 ## 1351 26 3 0.1937 0.03090 0.1417 0.2648 ## 1387 21 3 0.1661 0.03034 0.1161 0.2376 ## 1424 18 4 0.1292 0.02866 0.0836 0.1995 ## 1460 11 6 0.0587 0.02336 0.0269 0.1281 ## 1497 5 1 0.0470 0.02144 0.0192 0.1149 ## 1606 3 2 0.0157 0.01464 0.0025 0.0979 ## 1935 1 1 0.0000 NaN NA NA ## ## Race=HispanicLatino ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 79 2 0.9747 0.0177 0.94065 1.000 ## 256 77 1 0.9620 0.0215 0.92079 1.000 ## 438 74 3 0.9230 0.0302 0.86569 0.984 ## 475 71 1 0.9100 0.0325 0.84859 0.976 ## 511 69 1 0.8968 0.0346 0.83159 0.967 ## 548 68 1 0.8836 0.0365 0.81496 0.958 ## 584 67 1 0.8705 0.0382 0.79863 0.949 ## 621 66 3 0.8309 0.0428 0.75112 0.919 ## 657 62 3 0.7907 0.0466 0.70444 0.887 ## 694 58 2 0.7634 0.0488 0.67351 0.865 ## 730 56 4 0.7089 0.0524 0.61330 0.819 ## 767 51 2 0.6811 0.0539 0.58324 0.795 ## 803 49 3 0.6394 0.0557 0.53901 0.758 ## 876 44 3 0.5958 0.0573 0.49341 0.719 ## 949 40 3 0.5511 0.0585 0.44753 0.679 ## 1022 34 5 0.4701 0.0601 0.36585 0.604 ## 1059 25 2 0.4325 0.0609 0.32815 0.570 ## 1095 23 3 0.3761 0.0610 0.27357 0.517 ## 1132 17 1 0.3539 0.0613 0.25201 0.497 ## 1168 16 3 0.2876 0.0606 0.19023 0.435 ## 1205 13 4 0.1991 0.0558 0.11491 0.345 ## 1241 8 1 0.1742 0.0541 0.09476 0.320 ## 1314 5 1 0.1394 0.0533 0.06582 0.295 ## 1351 4 1 0.1045 0.0501 0.04084 0.267 ## 1387 3 2 0.0348 0.0330 0.00545 0.223 ## 1533 1 1 0.0000 NaN NA NA ## ## Race=White ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 339 4 0.9882 0.00586 0.9768 1.000 ## 110 333 1 0.9852 0.00656 0.9725 0.998 ## 183 332 1 0.9823 0.00718 0.9683 0.996 ## 219 330 3 0.9733 0.00877 0.9563 0.991 ## 256 326 2 0.9674 0.00968 0.9486 0.987 ## 365 317 3 0.9582 0.01094 0.9370 0.980 ## 438 311 4 0.9459 0.01241 0.9219 0.971 ## 511 300 3 0.9364 0.01343 0.9105 0.963 ## 548 297 5 0.9207 0.01494 0.8918 0.950 ## 584 290 5 0.9048 0.01629 0.8734 0.937 ## 621 281 5 0.8887 0.01751 0.8550 0.924 ## 657 275 6 0.8693 0.01884 0.8332 0.907 ## 694 269 10 0.8370 0.02072 0.7973 0.879 ## 730 255 6 0.8173 0.02174 0.7758 0.861 ## 767 245 15 0.7673 0.02394 0.7217 0.816 ## 803 225 12 0.7263 0.02541 0.6782 0.778 ## 840 210 7 0.7021 0.02616 0.6527 0.755 ## 876 197 12 0.6594 0.02733 0.6079 0.715 ## 913 174 8 0.6290 0.02809 0.5763 0.687 ## 949 163 8 0.5982 0.02876 0.5444 0.657 ## 986 152 10 0.5588 0.02943 0.5040 0.620 ## 1022 137 10 0.5180 0.02998 0.4625 0.580 ## 1059 124 8 0.4846 0.03028 0.4287 0.548 ## 1095 112 13 0.4284 0.03052 0.3725 0.493 ## 1132 98 7 0.3978 0.03046 0.3423 0.462 ## 1168 84 6 0.3693 0.03041 0.3143 0.434 ## 1205 72 8 0.3283 0.03029 0.2740 0.393 ## 1241 59 7 0.2894 0.03007 0.2360 0.355 ## 1278 50 4 0.2662 0.02980 0.2138 0.332 ## 1314 46 4 0.2431 0.02937 0.1918 0.308 ## 1351 39 2 0.2306 0.02916 0.1800 0.295 ## 1387 36 1 0.2242 0.02905 0.1739 0.289 ## 1424 32 6 0.1822 0.02822 0.1345 0.247 ## 1460 20 4 0.1457 0.02784 0.1002 0.212 ## 1497 15 3 0.1166 0.02688 0.0742 0.183 ## 1570 10 1 0.1049 0.02660 0.0638 0.172 ## 1643 6 1 0.0874 0.02732 0.0474 0.161 ## 1679 5 1 0.0699 0.02687 0.0329 0.149 Just as we generated before when applying the summary function to a Kaplan-Meier analysis object, we find life table information. By default, the summary function begins each new time interval at the time in which a new event (i.e., voluntary turnover) occurs; however, now there are separate life tables for each level of the categorical covariate (i.e., Race). Note that the time intervals displayed in the time column of each table are specific to the cases found within each level/category of the categorical covariate. Just as we did before, we can also set our time intervals. Let’s choose the same time intervals as we did in our null model (without the covariate). # Summarize results of Kaplan-Meier analysis using pre-specified time intervals # and create a life table summary(km_fit2, times=c(30, 60, 90*(1:30))) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## Race=Black ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 283 0 1.0000 0.00000 1.0000 1.0000 ## 60 283 0 1.0000 0.00000 1.0000 1.0000 ## 90 272 6 0.9788 0.00856 0.9622 0.9957 ## 180 268 3 0.9680 0.01049 0.9477 0.9888 ## 270 263 4 0.9535 0.01258 0.9292 0.9785 ## 360 260 2 0.9463 0.01350 0.9202 0.9731 ## 450 244 11 0.9060 0.01755 0.8723 0.9411 ## 540 236 7 0.8800 0.01961 0.8424 0.9193 ## 630 212 15 0.8237 0.02313 0.7796 0.8703 ## 720 195 7 0.7962 0.02458 0.7494 0.8458 ## 810 173 14 0.7378 0.02728 0.6862 0.7933 ## 900 152 14 0.6774 0.02944 0.6221 0.7377 ## 990 110 32 0.5313 0.03253 0.4712 0.5990 ## 1080 89 16 0.4525 0.03315 0.3919 0.5223 ## 1170 53 23 0.3282 0.03272 0.2699 0.3990 ## 1260 41 7 0.2822 0.03244 0.2253 0.3535 ## 1350 26 9 0.2190 0.03130 0.1655 0.2898 ## 1440 11 10 0.1292 0.02866 0.0836 0.1995 ## 1530 4 7 0.0470 0.02144 0.0192 0.1149 ## 1620 1 2 0.0157 0.01464 0.0025 0.0979 ## 1710 1 0 0.0157 0.01464 0.0025 0.0979 ## 1800 1 0 0.0157 0.01464 0.0025 0.0979 ## 1890 1 0 0.0157 0.01464 0.0025 0.0979 ## ## Race=HispanicLatino ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 79 0 1.0000 0.0000 1.00000 1.000 ## 60 79 0 1.0000 0.0000 1.00000 1.000 ## 90 77 2 0.9747 0.0177 0.94065 1.000 ## 180 77 0 0.9747 0.0177 0.94065 1.000 ## 270 76 1 0.9620 0.0215 0.92079 1.000 ## 360 75 0 0.9620 0.0215 0.92079 1.000 ## 450 71 3 0.9230 0.0302 0.86569 0.984 ## 540 68 2 0.8968 0.0346 0.83159 0.967 ## 630 62 5 0.8309 0.0428 0.75112 0.919 ## 720 56 5 0.7634 0.0488 0.67351 0.865 ## 810 45 9 0.6394 0.0557 0.53901 0.758 ## 900 40 3 0.5958 0.0573 0.49341 0.719 ## 990 34 3 0.5511 0.0585 0.44753 0.679 ## 1080 23 7 0.4325 0.0609 0.32815 0.570 ## 1170 13 7 0.2876 0.0606 0.19023 0.435 ## 1260 5 5 0.1742 0.0541 0.09476 0.320 ## 1350 4 1 0.1394 0.0533 0.06582 0.295 ## 1440 1 3 0.0348 0.0330 0.00545 0.223 ## 1530 1 0 0.0348 0.0330 0.00545 0.223 ## ## Race=White ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 339 0 1.0000 0.00000 1.0000 1.000 ## 60 339 0 1.0000 0.00000 1.0000 1.000 ## 90 333 4 0.9882 0.00586 0.9768 1.000 ## 180 332 1 0.9852 0.00656 0.9725 0.998 ## 270 320 6 0.9674 0.00968 0.9486 0.987 ## 360 317 0 0.9674 0.00968 0.9486 0.987 ## 450 301 7 0.9459 0.01241 0.9219 0.971 ## 540 297 3 0.9364 0.01343 0.9105 0.963 ## 630 275 15 0.8887 0.01751 0.8550 0.924 ## 720 255 16 0.8370 0.02072 0.7973 0.879 ## 810 210 33 0.7263 0.02541 0.6782 0.778 ## 900 174 19 0.6594 0.02733 0.6079 0.715 ## 990 137 26 0.5588 0.02943 0.5040 0.620 ## 1080 112 18 0.4846 0.03028 0.4287 0.548 ## 1170 72 26 0.3693 0.03041 0.3143 0.434 ## 1260 50 15 0.2894 0.03007 0.2360 0.355 ## 1350 39 8 0.2431 0.02937 0.1918 0.308 ## 1440 20 9 0.1822 0.02822 0.1345 0.247 ## 1530 10 7 0.1166 0.02688 0.0742 0.183 ## 1620 6 1 0.1049 0.02660 0.0638 0.172 ## 1710 3 2 0.0699 0.02687 0.0329 0.149 ## 1800 2 0 0.0699 0.02687 0.0329 0.149 As you can see in the output, each life table now has the same pre-specified time intervals, which can make descriptive comparison of the table values to be a bit easier. Now let’s use the same arguments for the ggsurvplot function above, but this time, let’s add the arguments pval=TRUE and pval.method= to see, respectively, whether the survival rates differ to a statistically significant extent across levels of the categorical covariate and what statistical test is used to estimate whether such differences exist in the population. # Plot the cumulative survival rates (probabilities) ggsurvplot(km_fit2, data=survdat, risk.table=TRUE, pval=TRUE, pval.method=TRUE, conf.int=TRUE, ggtheme=theme_minimal()) In the plot, three survival rates are presented - one for each race. Note that the p-value is greater than or equal to .05, so we would not conclude that any differences we observed descriptively/visually between the three races are statistically significant. We can see that the log-rank test was used to estimate whether such differences existed across levels of the categorical covariate; we’ll revisit the log-rank test in the context of a Cox proportional hazards model below. In the table, we also find that the risk table is now stratified by levels (i.e., strata) of the categorical covariate. 49.2.7 Estimate Cox Proportional Hazards Model The Cox proportional hazards (Cox PH) model is a semiparametric method and is sometimes referred to as Cox regression. It estimates the log-linear association between a covariate (or multiple covariates) in relation to instantaneous rate of experiencing the focal event (i.e., hazard). This method allows us to introduce continuous covariates (as well as categorical covariates) in a regression-like model. To estimate a Cox PH model, we will use the coxph function from the survival package. Let’s name our model cox_reg1 using the &lt;- operator. Next, type the name of the coxph function. As the first argument, specify the survival model by specifying the outcome to the left of the ~ (tilde) symbol and the predictor(s) to the right. As the outcome, we type the Surv function with the survival length variable (LOS) as the first argument, and as the second argument, the censored variable (which indicates which cases should be censored) should be entered. Let’s keep this first model simple by simply applying Race as a categorical covariate, as we did above with the Kaplan-Meier analysis. As the second argument in the coxph function, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. Use the summary function from base R to summarize the results of the cox_reg1 object we specified. # Estimate Cox proportional hazards model with categorical covariate cox_reg1 &lt;- coxph(Surv(LOS, censored) ~ Race, data=survdat) # Print summary of results summary(cox_reg1) ## Call: ## coxph(formula = Surv(LOS, censored) ~ Race, data = survdat) ## ## n= 701, number of events= 463 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## RaceHispanicLatino 0.17936 1.19645 0.15147 1.184 0.236 ## RaceWhite -0.14365 0.86619 0.09998 -1.437 0.151 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## RaceHispanicLatino 1.1964 0.8358 0.8891 1.610 ## RaceWhite 0.8662 1.1545 0.7120 1.054 ## ## Concordance= 0.522 (se = 0.014 ) ## Likelihood ratio test= 5.13 on 2 df, p=0.08 ## Wald test = 5.28 on 2 df, p=0.07 ## Score (logrank) test = 5.3 on 2 df, p=0.07 In the output, we receive information about our sample size (n = 701) and the number of individuals in our sample who experienced the event of voluntary turnover (number of events = 463). Next, you will see a table with coefficients, along with their exponentiated values, their standard errors, z-test value (Wald statistic value), and p-value (Pr(&gt;|z|)). Because Race is a categorical variable with three levels/categories (i.e., Black, HispanicLatino, White), we see two lines in the table; these two lines reflect the dummy variables, which were created behind the scenes as part of the function. The Race category of Black comes first alphabetically (before HispanicLatino and White), and thus it serves as the default reference group. When comparing HispanicLatino individuals to Black individuals, we can see that there is not a difference in survival, as evidenced by the nonsignificant p-value (.236). Similarly, when comparing White individuals to Black individuals, there is not a significant difference in terms of survival (p = .151). Because these coefficients are nonsignificant, we will not proceed forward with interpretation of their respective hazard ratios, which appear in the subsequent table. We will, however, skip down to the last set of tests/values that appear in the output. The value labeled as Concordance in the output is an aggregate estimate of how well the model predicted individuals’ experience of the event in time (i.e., model fit). Specifically, pairs of individuals are compared to determine whether the individual with the temporally earlier experience of the event is correctly predicted by the model. If model is accurate, the pair is considered concordant. The overall concordance score represents the proportion of pairs of individuals whose experience of the event (e.g., voluntary turnover) was accurately predicted. Accordingly, a concordance value of .50 indicates that the model does no better than chance (i.e., 50-50 chance of correctly predicting which individual incurs the event before another). In our data, the concordance value is .522 (SE = .014), which indicates that including the Race covariate in the model slightly improves the accuracy of our predictions but not by much more than a model with no covariates (i.e., null model). The tests labeled as Likelihood ratio test (likelihood-ratio test), Wald test (Wald test), and Score (logrank) test (log-rank test) in the output all focus on whether the model with the covariates fits the data better than a model without the covariates (i.e., null model). Let’s focus first on the likelihood-ratio test (LRT), which is chi-square goodness-of-fit test that compares nested models. The LRT is not statistically significant (\\(\\chi^2\\) = 5.13, df = 2, p = .08), which indicates that the model with the Race covariate does not result in significantly better model fit than a null model without the covariate. Thus, we fail to reject the null hypothesis that our model with the covariates fits the same as the null model without the covariates. The Wald test and log-rank test can be interpreted similarly to the LRT and similarly have to do with the overall fit of the model. These three tests are referred to as asymptotically equivalent because their values should converge when the sample size is sufficiently high. If you’re working with a smaller sample size, the LRT tends to perform better. Given that, in general I recommend reporting and interpreting the log-rank test and, for space considerations, forgoing the reporting and interpretation of the other two tests. For example, in this example, the log-rank value is 5.3 (df = 2) with a p-value of .07, which indicates that the model with the Race covariate does not perform significantly better than a model with not covariates. Let’s extend our Cox PH model by addition two additional continuous covariates: Pay_hourly and Pay_sat. To add these covariates to our model, use the + symbol. Thus, as our first argument in the coxph function, enter the model specifications. As the second argument, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. Using the &lt;- operator, let’s name this new model cox_reg2. As before, use the summary function from base R to summarize the results of the cox_reg2 object. # Estimate Cox proportional hazards model with categorical &amp; continuous covariates cox_reg2 &lt;- coxph(Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, data=survdat) # Print summary of results summary(cox_reg2) ## Call: ## coxph(formula = Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, ## data = survdat) ## ## n= 663, number of events= 445 ## (38 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## RaceHispanicLatino 0.12087 1.12848 0.15796 0.765 0.444154 ## RaceWhite -0.04446 0.95651 0.10367 -0.429 0.668008 ## Pay_hourly -0.10971 0.89609 0.03276 -3.349 0.000812 *** ## Pay_sat 0.14135 1.15183 0.08135 1.738 0.082289 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## RaceHispanicLatino 1.1285 0.8862 0.8280 1.5380 ## RaceWhite 0.9565 1.0455 0.7806 1.1720 ## Pay_hourly 0.8961 1.1160 0.8404 0.9555 ## Pay_sat 1.1518 0.8682 0.9821 1.3509 ## ## Concordance= 0.55 (se = 0.016 ) ## Likelihood ratio test= 15.97 on 4 df, p=0.003 ## Wald test = 16 on 4 df, p=0.003 ## Score (logrank) test = 16.02 on 4 df, p=0.003 In the output, effective total sample size is 663, which is due to 38 cases/observations being listwise deleted from the sample size due to missing values on the Pay_sat covariate; you can look at the survdat data frame to verify this, if you would like. Of those 663 individuals included in the analysis, 445 experienced the event. Only the coefficient associated with the covariate Pay_hourly is statistically significant (b = -.110, p &lt; .001) when accounting for the other covariates in the model. Given that, let’s proceed forward to interpret its hazard ratio. The hazard ratio and its confidence interval can be found in the second table under the column exp(coef). (Note that exp stands for exponentiation, and for more information on exponentiation, check out the chapter covering logistic regression.) A hazard ratio is the ratio between the hazards (i.e., hazard rates) associated with two (adjacent) levels of a covariate. The hazard ratio associated with Pay_hourly is .896, which can be interpreted similar to an odds ratio from logistic regression. Because the hazard ratio is less than 1, it signifies a negative association, which we already knew from the regression coefficient. If we subtract .896 from 1, we get .104, which means we can interpret the association as follows: For every unit increase in the predictor variable, we can expect a 10.4% decrease in the expected hazard; or in other words, for every additional dollar in hourly pay earned, we can expect a 10.4% decrease in the likelihood of voluntary turnover. The column labeled exp(-coef) is just the reciprocal (i.e., 1 divided by hazard ratio) and thus allows us to interpret the association from a different perspective. Specifically, the reciprocal is 1.116, which can be interpreted as: For every dollar decrease in pay, the expected hazard will be 1.116 greater – or in other words, we expected to see a 11.6% increase in the likelihood of voluntary turnover for every additional dollar earned. It’s up to you to decide whether you want to report the hazard ratio, its reciprocal, or both. The columns labeled lower .95 and upper .95 contain the 95% confidence interval lower and upper limits for the hazard ratio, which for Pay_hourly are 95% CI[.840, .956]. Just for fun, let’s specify our Cox PH model in equation form to show how we can estimate the log of the overall risk score for survival time prior to experiencing the event (i.e., overall predicted hazard ratio for an individual). Given the nonsignificant of the other covariates in the model, in real life, we might choose to run the model once more with the nonsignificant covariates excluded. For this example, however, I’ll include nonsignificant covariates in the equation as well. \\(Log(Overall Risk) = .121*(Race[HL]) - .044*(Race[W]) - .110*(Pay_{hourly}) + .141*(Pay_{sat})\\) For example, if an individual is HispanicLatino, has an hourly pay rate of $16.00, and pay satisfaction equal to 4.00, then we would predict the log of their overall risk score for survival time prior to experiencing the event to be -1.075. \\(Log(Overall Risk) = .121*(1) - .044*(0) - .110*(16.00) + .141*(4.00)\\) \\(Log(Overall Risk) = .121 - 0 - 1.760 + .564\\) \\(Log(Overall Risk) = -1.075\\) If we exponentiate the log of the overall risk score, we arrive at .341. \\(e^{-1.075} = .341\\) Thus, the overall risk of voluntarily turning over for this individual is .341. This means that the expected hazard is 65.9% less when compared to an individual with scores of zero on each of the covariates in the model. This might sound great, but think about what this actually means. For our equation, an individual who earns zero on the covariates is Black, earns .00 dollars/hour, and has a score of .00 on the pay satisfaction scale. For this survival study, the minimum pay earned by an individual is 10.00 dollars/hour (mean = 14.11), the minimum score on the pay satisfaction scale is 2.00, and the sample is 40.4% Black, 11.3% HispanicLatino, and 48.4% White. This is why we might decide to grand-mean center our continuous covariates and specify a different reference group for our categorical covariate - for interpretation purposes. Let’s go ahead and grand-mean center the continuous covariates and set the HispanicLatino individuals as the reference group. If you would like a deeper dive into how to grand-mean center variables, check out the chapter on centering and standardizing variables. # Grand-mean center continuous covariates survdat$c_Pay_hourly &lt;- scale(survdat$Pay_hourly, center=TRUE, scale=FALSE) survdat$c_Pay_sat &lt;- scale(survdat$Pay_sat, center=TRUE, scale=FALSE) # Change reference group to HispanicLatino for categorical covariate by re-ordering levels survdat$HL_Race &lt;- factor(survdat$Race, levels=c(&quot;HispanicLatino&quot;, &quot;Black&quot;, &quot;White&quot;)) # Estimate Cox proportional hazards model with categorical &amp; continuous covariates cox_reg3 &lt;- coxph(Surv(LOS, censored) ~ HL_Race + c_Pay_hourly + c_Pay_sat, data=survdat) # Print summary of results summary(cox_reg3) ## Call: ## coxph(formula = Surv(LOS, censored) ~ HL_Race + c_Pay_hourly + ## c_Pay_sat, data = survdat) ## ## n= 663, number of events= 445 ## (38 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## HL_RaceBlack -0.12087 0.88615 0.15796 -0.765 0.444154 ## HL_RaceWhite -0.16533 0.84761 0.15773 -1.048 0.294567 ## c_Pay_hourly -0.10971 0.89609 0.03276 -3.349 0.000812 *** ## c_Pay_sat 0.14135 1.15183 0.08135 1.738 0.082289 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## HL_RaceBlack 0.8862 1.1285 0.6502 1.2077 ## HL_RaceWhite 0.8476 1.1798 0.6222 1.1547 ## c_Pay_hourly 0.8961 1.1160 0.8404 0.9555 ## c_Pay_sat 1.1518 0.8682 0.9821 1.3509 ## ## Concordance= 0.55 (se = 0.016 ) ## Likelihood ratio test= 15.97 on 4 df, p=0.003 ## Wald test = 16 on 4 df, p=0.003 ## Score (logrank) test = 16.02 on 4 df, p=0.003 Using our new output, let’s construct the latest incarnation of the equation. \\(Log(Overall Risk) = -.121*(Race[B]) - .165*(Race[W]) - .110*(Pay_{hourly}) + .141*(Pay_{sat})\\) As you can see, the coefficients for Pay_hourly and Pay_sat remain the same, but now we have the HispanicLatino group set as the reference group and the coefficients of the dummy coded variables changed. Let’s imagine the same individual as before for whom we would like to estimate the log of overall risk. For the two continuous covariates, we will subtract the grand mean of each of those aforementioned values (i.e., 16.00, 4.00) to arrive at their grand-mean centered values (i.e., 1.89, .32). \\(Log(Overall Risk) = -.121*(0) - .165*(0) - .110*(1.89) + .141*(.32)\\) \\(Log(Overall Risk) = -.162\\) Now let’s exponentiate the log of the overall risk. \\(e^{-.284} = .850\\) Keeping in mind that we have changed the reference group for the Race covariate to HispanicLatino and have grand-mean centered the Pay_hourly and Pay_sat covariates, the individual in question’s overall risk score is .850. Thus, the expected hazard is 15.0% less for this individual when compared to individuals with scores of zero on each of the covariates in the model – or in other words, when compared to individuals who are HispanicLatino, earn average hourly pay, and have average levels of pay satisfaction. Let’s now focus on the model-level performance and fit information in the output. Note that the concordance is .55 (SE = .016), which is an improvement from the concordance for the Cox PH model with just Race as a covariate. This indicates that including the two additional continuous covariates in the model improves the accuracy of our predictions. The significant log-rank test (16.02, df = 4, p = .003) indicates that the this model fits significant better than a null model. We can also perform nested model comparisons between two non-null models. Before doing so, we need to make sure both models are estimated using the same data, which is required for the following comparison when there is missing data on one or more of the variables in the models. (Note: This can change the original model results for a model that was originally estimated with a larger sample because it had less missing data than the the model to which we wish to compare it.) First, we will pull in the drop_na function from the tidyr package to do the heavy lifting for us. Specifically, within the function, we note the name of the data frame followed by the names of all of the variables from the full model. Second, we will use the anova function from base R to perform the nested model comparison between cox_reg1 and cox_reg2. # Access tidyr package # install.packages(&quot;tidyr&quot;) # install if necessary library(tidyr) # Re-estimate Cox PH nested models by dropping all cases with missing data on focal variables cox_reg1 &lt;- coxph(Surv(LOS, censored) ~ Race, data=drop_na(survdat, LOS, censored, Race, Pay_hourly, Pay_sat)) cox_reg2 &lt;- coxph(Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, data=drop_na(survdat, LOS, censored, Race, Pay_hourly, Pay_sat)) # Nested model comparison anova(cox_reg1, cox_reg2) ## Analysis of Deviance Table ## Cox model: response is Surv(LOS, censored) ## Model 1: ~ Race ## Model 2: ~ Race + Pay_hourly + Pay_sat ## loglik Chisq Df Pr(&gt;|Chi|) ## 1 -2459.6 ## 2 -2453.1 13.141 2 0.001401 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the chi-square test (\\(\\chi^2\\) = 13.141, df = 2, p = .001) indicates that the full model with all covariates included fits the data significantly better than the smaller nested model with only the Race covariate included. 49.2.8 Summary In this chapter, we learned the basics of how to conduct a Kaplan-Meier analysis, create a life table, and estimate a Cox proportional hazards (i.e., Cox regression) model. References "],["performancemanagement.html", "Chapter 50 Introduction to Employee Performance Management 50.1 Chapters Included", " Chapter 50 Introduction to Employee Performance Management Performance management entails the processes and systems designed to develop, management, and evaluate employees’ performance levels and contributions to the organization. Classically, performance appraisal (i.e., performance evaluation) often comes to mind for most people when they think of employee performance. Although performance appraisal is often useful for understanding and tracking employee performance, it represents just one component of a larger performance-management system. The term “performance management” carries a connotation of ongoing assessment and feedback, whereas “performance appraisal” is often used to describe a discrete, one-time performance-evaluation event. Just as reliability and validity are quality indicators when evaluating selection tools, for example, they are also important estimate within the context of performance management systems, particularly with respect to the measurement and/or evaluation components of the system. 50.1 Chapters Included In the following chapters, you will have opportunities to learn how to evaluate components of performance management system and how to determine factors that drive employee performance. Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations Investigating Nonlinear Associations Using Polynomial Regression Supervised Statistical Learning Using Lasso Regression Investigating Processes Using Path Analysis Estimating a Mediation Model Using Path Analysis "],["convergentdiscriminantvalidity.html", "Chapter 51 Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations 51.1 Conceptual Overview 51.2 Tutorial 51.3 Chapter Supplement", " Chapter 51 Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations In this chapter, we will learn how to generate scatter plots, estimate Pearson product-moment and point-biserial correlations, and create a correlation matrix to evaluate the convergent validity and discriminant validity of a performance measure. 51.1 Conceptual Overview In this section, first, we’ll begin by reviewing concurrent and discriminant validity. Second, we review the Pearson product-moment correlation and point-biserial correlation, and we’ll discuss the statistical assumptions that should be satisfied prior to estimating and interpreting both types of correlations as well as statistical significance and and practical significance in the context of these two correlations. The section will wrap up with a sample-write up of a correlation when used to estimate the criterion-related validity of a selection tool. Finally, we will describe the data visualization called a bivariate scatter plot and how it can be used to understand the association between two continuous (interval, ratio) variables. 51.1.1 Review of Concurrent &amp; Discriminant Validity To understand concurrent and discriminant validity, we first need to define criterion-related validity, where criterion-related validity (criterion validity) refers to the association between a variable and some outcome variable or correlate. The term criterion can be thought of as some outcome or correlate of practical or theoretical interest. Convergent validity is a specific type of criterion-related validity in which the criterion of interest is conceptually similar to the focal variable, such that we would expect for scores on two conceptually similar variables to be associated with one another. For example, imagine that supervisors customer service employees’ job performance using a behavioral performance evaluation tool. Given that these employees work in customer service, we would expect scores on the behavioral performance evaluation tool to correlate with a measure that assesses a conceptually similar construct like, say, customer satisfaction ratings. In other words, we would expect for the performance evaluation tool and customer satisfaction ratings to show evidence of convergent validity. Unlike convergent validity, we would find evidence of discriminant validity when scores on a conceptually dissimilar criterion are not associated with the focal variable. For example, we would not expect scores on a tool designed to measure job performance to correlate strongly with employee sex – assuming that the tool was intended to be bias free. 51.1.2 Review of Pearson Product-Moment &amp; Point-Biserial Correlation A correlation represents the sign (i.e., direction) and magnitude (i.e., strength) of an association between two variables. Correlation coefficients can range from -1.00 to +1.00, where zero (.00) represents no association, -1.00 represents a perfect negative (inverse) association, and +1.00 represents a perfect positive association. When estimated using data acquired from a criterion-related validation study, a correlation coefficient can be referred to as a validity coefficient. There are different types of correlations we can estimate, and their appropriateness will depend on the measurement scales of the two variables. For instance, the Pearson product-moment correlation is used when both variables are continuous (i.e., have interval or ratio measurement scales), whereas the point-biserial correlation is used when one variable is continuous and the other is truly dichotomous (e.g., has a nominal measurement scale with just two levels or categories) – and by truly dichotomous, I mean that there is no underlying continuum between the two components of the binary. If we assign numeric values to the two levels of the dichotomous variable (e.g., 0 and 1), then the point-biserial correlation will be mathematically equivalent to a Pearson product moment correlation. A Pearson product-moment correlation coefficient (\\(r_{pm}\\)) for a sample can be computed using the following formula: \\(r_{pm} = \\frac{\\sum XY - \\frac{(\\sum X)(\\sum Y)}{n}} {\\sqrt{(\\sum X^{2} - \\frac{\\sum X^{2}}{n}) (\\sum Y^{2} - \\frac{\\sum Y^{2}}{n})}}\\) where \\(X\\) refers to scores from one variable and \\(Y\\) refers to scores from the other variable, \\(n\\) refers to the sample size (i.e., the number of pairs of data corresponding to the number of cases – with complete data). If we assign numeric values to both levels of the dichotomous variable (e.g., 0 and 1), then we could use the formula for a Pearson product-moment correlation to calculate a point-biserial correlation. If, however, the two levels of the dichotomous variable are still non-numeric, then we can calculate a point-biserial correlation coefficient (\\(r_{pb}\\)) for a sample using the following formula: \\(r_{pb} = \\frac{M_1 - M_0}{s_N} \\sqrt{pq}\\) where \\(M_1\\) refers to mean score on the continuous (interval, ratio) variable for just the subset of cases with a score of 1 on the dichotomous variable; \\(M_0\\) refers to mean score on the continuous (interval, ratio) variable for just the subset of cases with a score of 0 on the dichotomous variable; \\(s_N\\) refers to the standard deviation for the continuous (interval, ratio) variable for the entire sample; \\(p\\) refers to the proportion of cases with a score of 0 on the dichotomous variable; and \\(q\\) refers to the proportion of cases with a score of 1 on the dichotomous variable. 51.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to estimating and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Each variable shows a univariate normal distribution; Each variable is free of univariate outliers, and together the variables are free of bivariate outliers; Variables demonstrate a bivariate normal distribution - meaning, each variable is normally distributed at each level/value of the other variable. Often, this roughly takes the form of an ellipse shape, if you were to superimpose an oval that would fit around most cases in a bivariate scatter plot; The association between the two variables is linear. The statistical assumptions that should be met prior to estimating and/or interpreting a point-biserial correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; One of the variables is continuous (i.e., has an interval or ratio measurement scale); One of the variables is dichotomous (i.e., binary); The continuous variable shows a univariate normal distribution at each level of the dichotomous variable; The variance of the continuous variable is approximately equal for each level of the dichotomous variable; The continuous variable is free of univariate outliers at each level of the dichotomous variable. Note: Regarding the first statistical assumption (i.e., cases randomly sampled from population), we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our variables, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 51.1.2.2 Statistical Significance The significance level of a correlation coefficient is determined by the sample size and the magnitude of the correlation coefficient. Specifically, a t-statistic with N-2 degrees of freedom (df) is calculated and compared to a Student’s t-distribution with N-2 df and a given alpha level (usually two-tailed, alpha = .05). If the calculated t-statistic is greater in magnitude than the chosen t-distribution, we conclude that the population correlation coefficient is significantly greater than zero. We use the following formula to calculate the t-statistic: \\(t = \\frac{r \\sqrt{N-2}}{1-r^{2}}\\) where \\(r\\) refers to the estimated correlation coefficient and \\(N\\) refers to the sample size. Alternatively, the exact p-value can be computed using a statistical software program like R if we know the df and t-value. In practice, however, we don’t always report the associated t-value; instead, we almost always report the exact p-value associated with the t-value when reporting information about statistical significance. When using null hypothesis significance testing, we interpret a p-value that is less than our chosen alpha level (which is conventionally .05, two-tailed) to meet the standard for statistical significance. This means that we reject the null hypothesis that the correlation is equal to zero. By rejecting this null hypothesis, we conclude that the correlation is significantly different from zero. If the p-value is equal to or greater than our chosen alpha level (e.g., .05, two-tailed), then we fail to reject the null hypothesis that the correlation is equal to zero; meaning, we conclude that there is no evidence of linear association between the two variables. 51.1.2.3 Practical Significance The size of a correlation coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large Some people like to also report the coefficient of determination as an indicator of effect size. The coefficient of determination is calculated by squaring the correlation coefficient to create r2. When multiplied by 100, the coefficient of determination (r2) can be interpreted as the percentage of variance/variability shared between the two variables, which is sometimes stated as follows: Variable \\(X\\) explains \\(X\\)% of the variance in Variable \\(Y\\) (or vice versa). Please note that we use the lower-case r in r2 to indicate that we are reporting the variance overlap between only two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 51.1.2.4 Sample Write-Up Based on a sample of 95 employees (N = 95), we evaluated the convergent and discriminant validity of a performance evaluation tool designed to measure the construct of job performance for sales professionals. With respect to convergent validity, we found that the correlation between overall performance evaluation scores and sales revenue generated is statistically significant, as the p-value is less than the conventional two-tailed alpha level of .05 (r = 0.509, p &lt; .001, 95% CI[.343, .644]). Further, because the correlation coefficient is positive, it indicates that there is a positive linear association between overall performance evaluation ratings and sales revenue generated. Using the commonly used thresholds in the table below, we can conclude that – in terms of practical significance – the correlation coefficient is large in magnitude. With respect to the 95% confidence interval, it is likely that the range from .343 to .644 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. Overall, the statistically significant and large-in-magnitude correlation between overall scores on the performance evaluation and sales revenue generated provides evidence of criterion-related validity – and specifically evidence of convergent validity, as these two conceptually similar variables were indeed correlated to a statistically significant extent. With respect to discriminant validity, we found that the correlation between overall performance evaluation scores and employee sex was not statistically significant, as the p-value was greater than the conventional two-tailed alpha level of .05 (r = -.135, p = .193, 95% CI[-.327, .069]). This leads us to conclude that employees’ overall performance evaluation ratings are not associated with their sex to a statistical significant extent and, therefore, that there is evidence of discriminant validity. 51.1.3 Review of Bivariate Scatter Plot The bivariate scatter plot (or scatterplot) is a useful data visualization display type when our goal is to visualize the association between two continuous (interval, ratio) variables. Each dot in a scatter plot represents an individual case (i.e., observation) from the sample, and the dot’s position on the graph represents the case’s scores on the each of the variables. A bivariate scatter plot can help us understand whether the statistical assumption of bivariate normality has been met (such as for a Pearson product-moment correlation), and it can help us determine whether the two variables have a linear or nonlinear association. The bivariate scatter plot is a type of data visualization that is indended to depict the nature of the association (or lack thereof) between two continuous (interval, ratio) variables. In this example, there appears to be a relatively strong, positive association between annual sales revenue generated and the amount of variable pay earned. If a bivariate scatter plot is applied to a continuous (interval, ratio) and dichotomous variable, it will take on a qualitatively different appearance and conveys different information. Namely, as shown in the figure below, when a dichotomous variable is involved, the bivariate scatter plot will display two columns (or rows) of dots, each corresponding to a level of the dichotomous variable. In fact, we can use the plot to infer visually whether there might be differences in means for the continuous variable based on the two levels of the dichotomous variable. The bivariate scatter plot can also be used to depict the nature of the association (or lack thereof) between a continuous (interval, ratio) variable and a dichotomous variable. In this example, one can imagine that the mean variable pay is higher for females compared to males. To play around with examples of bivariate scatter plots based on simulated data, check out this free tool. The tool also does a nice job of depicting the concept of shared variance in the context of correlation (i.e., coefficient of determination) using a Venn diagram. 51.2 Tutorial This chapter’s tutorial demonstrates how to evaluate evidence of concurrent and discriminant validity for a performance measure by using scatter plots, correlations, and a correlation matrix. 51.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorials below. Link to video tutorial: https://youtu.be/VQDZctJn43o Link to video tutorial: https://youtu.be/cEld0s-qpL4 Link to video tutorial: https://youtu.be/MfN8KGQU3MI 51.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR BoxPlot lessR drop_na tidyr nrow base R group_by dplyr summarize dplyr leveneTest car Correlation lessR as.numeric base R corr.test psych lowerCor psych write.csv base R 51.2.3 Initial Steps If you haven’t already, save the file called “PerfMgmtRewardSystemsExample.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PerfMgmtRewardSystemsExample.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object PerfRew &lt;- read_csv(&quot;PerfMgmtRewardSystemsExample.csv&quot;) ## Rows: 95 Columns: 11 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Sex ## dbl (10): EmpID, Perf_Qual, Perf_Prod, Perf_Effort, Perf_Admin, SalesRevenue, BasePay_2018, VariablePay_2018, Age, EducationLevel ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(PerfRew) ## [1] &quot;EmpID&quot; &quot;Perf_Qual&quot; &quot;Perf_Prod&quot; &quot;Perf_Effort&quot; &quot;Perf_Admin&quot; &quot;SalesRevenue&quot; &quot;BasePay_2018&quot; ## [8] &quot;VariablePay_2018&quot; &quot;Sex&quot; &quot;Age&quot; &quot;EducationLevel&quot; # Print variable type for each variable in data frame (tibble) object str(PerfRew) ## spc_tbl_ [95 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:95] 1 2 3 4 5 6 7 8 9 10 ... ## $ Perf_Qual : num [1:95] 3 1 2 2 1 2 5 2 2 2 ... ## $ Perf_Prod : num [1:95] 3 1 1 3 1 2 5 1 3 2 ... ## $ Perf_Effort : num [1:95] 3 1 1 3 1 2 5 1 3 3 ... ## $ Perf_Admin : num [1:95] 4 1 1 1 1 3 5 1 2 2 ... ## $ SalesRevenue : num [1:95] 57563 54123 56245 58291 58354 ... ## $ BasePay_2018 : num [1:95] 53791 52342 50844 52051 48061 ... ## $ VariablePay_2018: num [1:95] 6199 1919 7507 6285 4855 ... ## $ Sex : chr [1:95] &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Age : num [1:95] 39 48 38 35 32 34 57 43 35 47 ... ## $ EducationLevel : num [1:95] 2 4 4 4 2 4 3 2 4 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Perf_Qual = col_double(), ## .. Perf_Prod = col_double(), ## .. Perf_Effort = col_double(), ## .. Perf_Admin = col_double(), ## .. SalesRevenue = col_double(), ## .. BasePay_2018 = col_double(), ## .. VariablePay_2018 = col_double(), ## .. Sex = col_character(), ## .. Age = col_double(), ## .. EducationLevel = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(PerfRew) ## # A tibble: 6 × 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female 39 2 ## 2 2 1 1 1 1 54123 52342 1919 Male 48 4 ## 3 3 2 1 1 1 56245 50844 7507 Male 38 4 ## 4 4 2 3 3 1 58291 52051 6285 Male 35 4 ## 5 5 1 1 1 1 58354 48061 4855 Female 32 2 ## 6 6 2 2 2 3 57618 53386 4056 Male 34 4 # Print number of rows in data frame (tibble) object nrow(PerfRew) ## [1] 95 There are 11 variables and 95 cases (i.e., employees) in the PerfRew data frame: EmpID, Perf_Qual, Perf_Prod, Perf_Effort, Perf_Admin, SalesRevenue, BasePay_2018, VariablePay_2018, Sex, Age, and EducationLevel. Per the output of the str (structure) function above, all of the variables except for Sex are of type integer (continuous: interval, ratio), and Sex is of type character (nominal, dichotomous). The Perf_Qual, Perf_Prod, Perf_Effort, and Perf_Admin variables reflect four subjective performance-evaluation rating dimensions (as rated by direct supervisors), where Perf_Qual refers to perceived performance quality, Perf_Prod refers to perceived productivity, Perf_Effort refers to perceived effort, and Perf_Admin refers to perceived performance on administrative duties; each dimension was rated on a 1-5 scale in which higher values indicate higher performance (1 = poor performance, 5 = exceptional performance. The SalesRevenue variable is a measure of objective performance, as it reflects the 2018 sales-revenue contributions made by employees (in USD). The BasePay_2018 variable reflects the amount of base pay earned by employees during 2018 (in USD), and the VariablePay_2018 variable reflects the amount of variable pay (e.g., sales commission, bonuses) earned by employees during 2018 (in USD). The Sex variable has two levels: Male and Female, and the Age variable contains each employee’s age as of December 31, 2018. Finally, EducationLevel is an ordinal variable in which 1 = has high school degree or GED, 2 = completed some college courses, 3 = has Associate’s degree, 4 = has Bachelor’s degree, and 5 = complete some graduate courses or graduate degree. Even though the EducationLevel variable technically has an ordinal measurement scale. 51.2.3.1 Create Composite Variable Based on Performance Evaluation Dimensions Technically, each of these performance evaluation dimension variables has an ordinal measurement scale; however, because all four dimensions are intended to assess some aspect of the job performance domain, we will see if we might be justified in creating a composite variable that represents overall performance. In doing so, we will hopefully be able to make the argument that the overall performance composite variable has an interval measurement scale. To determine whether it is appropriate to create a composite variable from the four performance evaluation dimensions, we will follow the logic of the chapter on creating a composite variable based on a multi-item measure. Because that chapter explains the logic and process in detail, we will breeze through the steps in this tutorial. To get started, we will install and access the psych package using the install.packages and library functions, respectively (if you haven’t already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Now let’s compute Cronbach’s alpha for the four-item performance evaluation measure. # Estimate Cronbach&#39;s alpha for the four-item Engagement measure alpha(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)]) ## ## Reliability analysis ## Call: psych::alpha(x = PerfRew[, c(&quot;Perf_Qual&quot;, &quot;Perf_Prod&quot;, &quot;Perf_Effort&quot;, ## &quot;Perf_Admin&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.94 0.94 0.92 0.78 14 0.011 2.9 1.2 0.77 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.91 0.94 0.95 ## Duhachek 0.91 0.94 0.96 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Perf_Qual 0.90 0.90 0.86 0.76 9.3 0.017 0.00035 0.76 ## Perf_Prod 0.93 0.93 0.90 0.81 12.7 0.013 0.00130 0.81 ## Perf_Effort 0.92 0.92 0.89 0.78 10.9 0.015 0.00310 0.77 ## Perf_Admin 0.91 0.91 0.88 0.78 10.7 0.015 0.00067 0.77 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Perf_Qual 95 0.94 0.94 0.92 0.88 2.9 1.3 ## Perf_Prod 95 0.89 0.89 0.83 0.81 2.9 1.3 ## Perf_Effort 95 0.91 0.91 0.87 0.84 2.9 1.3 ## Perf_Admin 95 0.92 0.92 0.88 0.85 2.9 1.3 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Perf_Qual 0.17 0.25 0.22 0.20 0.16 0 ## Perf_Prod 0.17 0.27 0.24 0.17 0.15 0 ## Perf_Effort 0.19 0.18 0.27 0.23 0.13 0 ## Perf_Admin 0.16 0.26 0.22 0.20 0.16 0 For all four dimensions, Cronbach’s alpha is .94, which is great and indicates an acceptable level of internal consistency reliability for this multi-dimensional measure. The “Reliability if an item is dropped” table indicates that dropping any of the dimensions/items would decrease Cronbach’s alpha, which would be undesirable. Given this, we will feel justified in creating a composite variable based the mean score across these four dimensions for each case. Let’s name the composite variable Perf_Overall. # Create composite (overall scale score) variable based on Engagement items PerfRew$Perf_Overall &lt;- rowMeans(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)], na.rm=TRUE) Now we have a variable called Perf_Overall that represents overall job performance, based on the supervisor ratings along the four dimensions. Further, we can argue that this new composite variable has an interval measurement scale. 51.2.3.2 Recode Employee Sex Variable The dichotomous employee sex variable (Sex) currently has the following levels: Female and Male. To make this variable amenable to a point-biserial correlation, we will recode the Female as 1 and Male as 0. To do so, we will use the recode function from the dplyr package. I should note that there are other ways in which we could recode these values, and some of those approaches are covered in the chapter on data cleaning. We will need to install and access the dplyr package using the install.packages and library functions, respectively (if you haven’t already done so). # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) To use the recode function we will do the following: Specify the name of the data frame object (PerfRew) followed by the $ operator and the name of variable we wish to recode (Sex). In doing so, we will overwrite the existing variable called Sex. Follow this with the &lt;- assignment operator. Specify the name of the recode function to the right of the &lt;- assignment operator. As the first argument in the recode function, specify the name of the the name of the data frame object (PerfRew) followed by the $ operator and the name of variable we wish to recode (Sex). As the second argument, type the name of one of the values we wish to change (Female), followed by the = operator and what we’d like to change that value to (1). Note: If we wished to change the value to a non-numeric text (i.e., character) value, then we would need to surround the new value in quotation marks (\" \"). As the third argument, repeat the previous step for the second value we wish to change (Male = 0). # Recode Female as 1 and Male as 0 for Sex variable PerfRew$Sex &lt;- recode(PerfRew$Sex, Female = 1, Male = 0) We’ve successfully recoded the Sex variable! Just remember that 1 indicates Female and 0 indicates Male, as this will be important for interpreting a point-biserial correlation, should we estimate one. 51.2.4 Visualize Association Using a Bivariate Scatter Plot A bivariate scatter plot can be used to visualize the association between two continuous (interval, ratio) variables. Let’s create a bivariate scatter plot to visualize the association between the Perf_Prod (productivity) dimension of the performance evaluation tool and the SalesRevenue (sales revenue generated) variable, both of which are continuous. The ScatterPlot function from the lessR package (Gerbing, Business, and University 2021) does a nice job generating scatter plots – and it even provides an estimate of the correlation by default. If you haven’t already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Let’s start by visualizing the association between the overall performance evaluation composite variable (Perf_Overall) we created above and the criterion variable of sales revenue generated during the year (SalesRevenue). And let’s imagine data frame (PerfRew) contains cases who are all sales professionals, and thus the amount of sales revenue generated should be indicator of job performance. Thus, we can argue that both the Perf_Overall and SalesRevenue variables are conceptually similar as they “tap into” key aspects of job performance for these sales professionals. To begin, type the name of the ScatterPlot function. As the first two arguments of the function, we’ll type the names of the two variables we wish to visualize: Perf_Overall and SalesRevenue. The variable name that we type after the x= argument will set the x-axis, and the variable name that we type after the y= argument will set the y-axis. Conventionally, we place the criterion variable on the y-axis, as it is the outcome. As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (PerfRew). # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 ## In our plot window, we can see a fairly clear positive linear association between Perf_Overall and SalesRevenue. Further, the distribution is ellipse-shaped, which gives us some evidence that the underlying distribution between the two variables is likely bivariate normal – thereby satisfying a key statistical assumption for a Pearson product-moment correlation. Note that the ScatterPlot function automatically provides an estimate of the correlation coefficient in the output (r = .509), along with the associated p-value (p &lt; .001). Visually, this bivariate scatter plot provides initial evidence of convergent validity for these two variables. 51.2.4.1 Optional: Stylizing the ScatterPlot Function from lessR If you would like to optionally stylize your scatter plot, we can use the xlab= and ylab= arguments to change the default names of the x-axis and y-axis, respectively. # Optional: Styling the scatter plot ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, xlab=&quot;Overall Performance Evaluation Score&quot;, ylab=&quot;Annual Sales Revenue Generated ($)&quot;) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 ## We can also superimpose an ellipse by adding the argument ellipse=TRUE, which can visually aid our judgment on whether the distribution is bivariate normal. # Optional: Styling the scatter plot ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, xlab=&quot;Overall Performance Evaluation Score&quot;, ylab=&quot;Annual Sales Revenue Generated ($)&quot;, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 ## 51.2.5 Estimate Correlations Now we’re ready to practice estimating Pearson product-moment and point-biserial correlations. For both, we’ll work through the statistical assumptions along the way. 51.2.5.1 Estimate Pearson Product-Moment Correlation Because both Perf_Overall and SalesRevenue are measures of job performance, we will consider them to be conceptually similar – and even measures of the same overarching construct: job performance. Thus, we will investigate whether there is evidence of convergent validity between these two variables. Given that both Perf_Overall and SalesRevenue are continuous variables, let’s see if these two variables satisfy the statistical assumptions for estimating and interpreting a Pearson product-moment correlation. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Each Variable Shows a Univariate Normal Distribution: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Given that, we’ll begin by checking to see how many cases have data for both continuous variables, and we’ll use a combination of the drop_na function from the tidyr package (Wickham, Vaughan, and Girlich 2023) and the nrow function from base R. First, if you haven’t already, install and access the tidyr package. # Install package install.packages(&quot;tidyr&quot;) # Access package library(tidyr) First, using the drop_na function, as the first argument, type the name of the data frame object (PerfRew). As the second and third arguments, type the names of the two variables (Perf_Overall, SalesRevenue). Second, following the drop_na function, insert the pipe (%&gt;%) operator. Finally, after the pipe (%&gt;%) operator, type the nrow function without any arguments specified. This code will first create a new data frame (tibble) in which rows with missing data on either variable are removed, and then the number of rows in the new data frame (tibble) will be evaluated. Effectively, this code performs listwise deletion and tells us how many rows were retained after the listwise deletion. Because we aren’t assigning this data frame to an object, this is only a temporary data frame – and not one that will become part of our Global Environment. # Drop cases (rows) where either Perf_Overall or SalesRevenue # have missing data (NAs) &amp; pipe (%&gt;%) the resulting data # frame to the nrow function drop_na(PerfRew, Perf_Overall, SalesRevenue) %&gt;% nrow() ## [1] 95 After attempting to drop any rows with missing data (NAs), we find that the sample size is 95, which is the same as our original sample size. This indicates that we did not have any missing values on either of these variables. Because 95 &gt; 30, we can assume that the assumption of univariate normality has been met for both variables. To learn how to test this assumption if there are fewer than 30 cases in a sample, please see the end-of-chapter supplement. Each Variable Is Free of Univariate Outliers: To visualize whether there are any univariate outliers for either variable, we will use the BoxPlot function from the lessR package to generate a box-and-whiskers plot (box plot). Let’s start with the Perf_Overall variable, and enter it as the first argument in the BoxPlot function. As the second argument, type data= followed by the name of the data frame object (PerfRew). # Create box plot BoxPlot(Perf_Overall, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(Perf_Overall, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- Perf_Overall --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 2.911 ## Stnd Dev : 1.202 ## IQR : 1.875 ## Skew : 0.000 [medcouple, -1 to 1] ## ## Minimum : 1.000 ## Lower Whisker: 1.000 ## 1st Quartile : 2.000 ## Median : 3.000 ## 3rd Quartile : 3.875 ## Upper Whisker: 5.000 ## Maximum : 5.000 ## ## No (Box plot) outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## Perf_Overall 10 2.25 If outliers or other influential cases were present, they would appear beyond the whiskers (i.e., outer bars) in the plot. No potential outliers were detected for Perf_Overall. Now let’s create a box plot for the SalesRevenue variable. # Create box plot BoxPlot(SalesRevenue, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(SalesRevenue, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(SalesRevenue, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- SalesRevenue --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 60852.88 ## Stnd Dev : 6501.54 ## IQR : 7734.50 ## Skew : 0.04 [medcouple, -1 to 1] ## ## Minimum : 42512.00 ## Lower Whisker: 48157.00 ## 1st Quartile : 57027.00 ## Median : 60779.00 ## 3rd Quartile : 64761.50 ## Upper Whisker: 73329.00 ## Maximum : 76365.00 ## ## ## --- Outliers --- from the box plot: 2 ## ## Small Large ## ----- ----- ## 42512.0 76365.0 ## ## Number of duplicated values: 0 The box plot indicates that there may be two potential outliers, which appear in red beyond the whiskers of the plot. Generally, I recommend using caution when deciding whether to remove outlier cases, and I tend to err on the side of not removing outliers – unless they are outlandishly separated from the other scores (e.g., +/- 3 standard deviations). If we did decide to remove these cases, then I would recommend running a sensitivity analysis, which means running the focal analysis with and without the cases included. I think we can reasonably conclude that we have satisfied the assumption that the variables are (mostly) free of univariate outliers. Association Between Variables Is Linear, Variables Demonstrate a Bivariate Normal Distribution, &amp; Variables Are Free of Bivariate Outliers: To test these statistical assumptions, we will estimate a bivariate scatter plot using the same function as the previous section. We’ll add two additional arguments, though. First, let’s add the ellipse=TRUE argument, which will superimpose an ellipse; this will facilitate our interpretation of whether there is a bivariate normal distribution. Second, let’s add the out_cut=.05 argument, which will helps us visualize the top 5% of cases from the center that might be bivariate outliers. # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, ellipse=TRUE, out_cut=.05) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 ## ## ## &gt;&gt;&gt; Outlier analysis with Mahalanobis Distance ## ## MD ID ## ----- ----- ## 9.57 73 ## 7.45 50 ## 6.14 15 ## 5.18 13 ## 5.04 66 ## ## 4.85 54 ## 4.81 90 ## 4.43 48 ## ... ... Again, it appears as though the association between the two variables is linear (as opposed to nonlinear). Further, the bivariate distribution seems to be normal, as it takes on more-or-less an ellipse shape. Finally, there may be a few potential bivariate outliers, but none of them look too “out-of-bounds” or too extreme; thus, let’s conclude that none of these more extreme cases are extreme enough to warrant removal. Overall, we can reasonably conclude that the statistical assumptions for a Pearson product-moment correlation have been satisfied. Estimate Pearson Product-Moment Correlation: To estimate a Pearson product-moment correlation, we’ll use the Correlation function from the lessR package. Conveniently, this function will take the same first three arguments as the ScatterPlot function. # Estimate Pearson product-moment correlation using Correlation function from lessR Correlation(x=Perf_Overall, y=SalesRevenue, data=PerfRew) ## Correlation Analysis for Variables Perf_Overall and SalesRevenue ## ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = 3976.099 ## ## Sample Correlation: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 Sample Technical Write-Up: Based on a sample of 95 employees (N = 95), we can conclude that the correlation between overall performance evaluation scores and sales revenue generated is statistically significant, as the p-value is less than the conventional two-tailed alpha level of .05 (r = 0.509, p &lt; .001, 95% CI[.343, .644]). Further, because the correlation coefficient is positive, it indicates that there is a positive linear association between Perf_Overall and SalesRevenue. Using the commonly used thresholds in the table below, we can conclude that – in terms of practical significance – the correlation coefficient is large in magnitude, as its absolute value exceeds .50. With respect to the 95% confidence interval, it is likely that the range from .343 to .644 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. Overall, the statistically significant and large-in-magnitude correlation between overall scores on the performance evaluation and sales revenue generated provides evidence of criterion-related validity – and specifically evidence of convergent validity, as these two conceptually related variables were indeed correlated to a statistically significant extent. r Description .10 Small .30 Medium .50 Large 51.2.5.2 Estimate Point-Biserial Correlation We’ll shift gears and practice estimating the point-biserial correlation between the Perf_Overall continuous variable and the Sex dichotomous variable. Let’s assume that the Perf_Overall variable is a measure of overall job performance for the employees in this sample and that employees’ sex should not have a bearing on the performance evaluation ratings they receive from their respective supervisors. Given that, we are looking to see if there is evidence of discriminant validity between these two conceptually distinguishable variables. Before estimating their correlation, let’s see if these two variables satisfy the statistical assumptions for estimating and interpreting a point-biserial correlation. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. One of the Variables Is Continuous &amp; One of the Variables Is Dichotomous: We can describe the Perf_Overall variable has having an interval measurement scale, and thus it is a continuous variable. We can describe the Sex variable as having a nominal measurement scale and specifically as being a dichotomous variable because it is operationalized as having two levels: 1 = Female, 0 = Male. We have satisfied both of these assumptions. The Continuous Variable Variable Shows a Univariate Normal Distribution at Each Level of the Dichotomous Variable: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Let’s figure out if the number of cases at each level of the dichotomous variable exceeds 30. Using the drop_na function from the tidyr package: As the first argument, type the name of the data frame object (PerfRew). As the second and third arguments, type the names of the two variables (Perf_Overall, Sex). Following the drop_na function, insert the pipe (%&gt;%) operator. This will pipe the data frame object (tibble) created by the drop_na function to the subsequent function. After the pipe (%&gt;%) operator, type the name of the group_by function from the dplyr package, and as the sole argument, insert the name of the dichotomous variable (Sex). Following the group_by function, insert another pipe (%&gt;%) operator. After the pipe (%&gt;%) operator, type the name of the summarize function from the dplyr package, and as the sole argument, type the name of some new variable that will contain the counts (e.g., count) followed by the = operator and the n function from dplyr. Do not include any arguments in the n function. Note: If you’d like a refresher on aggregating and segmenting data, check out the chapter that covers those operations. # Drop cases (rows) where either Perf_Overall or Sex # have missing data (NAs), # pipe (%&gt;%) the resulting data # frame to the group_by function, &amp; # compute the counts for each level # of the Sex variable drop_na(PerfRew, Perf_Overall, Sex) %&gt;% group_by(Sex) %&gt;% summarize(count = n()) ## # A tibble: 2 × 2 ## Sex count ## &lt;dbl&gt; &lt;int&gt; ## 1 0 54 ## 2 1 41 As shown in the resulting table, there are more than 30 cases for each level of the dichotomous Sex variable. Thus, we can assume that the assumption of univariate normality has been met for the continuous variable at both levels of the dichotomous variable. To learn how to test this assumption if there are fewer than 30 cases in each sub-sample, please see the end-of-chapter supplement. Continuous Variable Is Free of Univariate Outliers: To visualize whether there are any univariate outliers for either variable, we will use the BoxPlot function from the lessR package to generate a box-and-whiskers plot (box plot). Type the name of the Perf_Overall variable as the first argument in the BoxPlot function, and as the second argument, type data= followed by the name of the data frame object (PerfRew). # Create box plot BoxPlot(Perf_Overall, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(Perf_Overall, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- Perf_Overall --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 2.911 ## Stnd Dev : 1.202 ## IQR : 1.875 ## Skew : 0.000 [medcouple, -1 to 1] ## ## Minimum : 1.000 ## Lower Whisker: 1.000 ## 1st Quartile : 2.000 ## Median : 3.000 ## 3rd Quartile : 3.875 ## Upper Whisker: 5.000 ## Maximum : 5.000 ## ## No (Box plot) outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## Perf_Overall 10 2.25 If outliers or other influential cases were present, they would appear beyond the whiskers (i.e., outer bars) in the plot. No potential outliers were detected for Perf_Overall, which indicates that we have satisfied the statistical assumption that the continuous variable is free of univariate outliers. The Variance of the Continuous Variable Is Approximately Equal for Each Level of the Dichotomous Variable: To test this statistical assumption, we will perform a statistical test called Levene’s test using the leveneTest function from the car package (Fox and Weisberg 2019). This function will allow us to test whether we can assume the variances to be approximately equal, which is sometimes referred to as homogeneity of variances. If you haven’t already, install and access the car package. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the continuous variable (Perf_Overall) to the left of the ~ operator and the name of the dichotomous variable (Sex) to the right of the ~ operator. This function requires that the variable to the right of the ~ operator is non-numeric, so let’s wrap the Sex variable in the as.factor function from base R to convert the dichotomous variable to a non-numeric factor within the leveneTest function; this will not permanently change the Sex variable to a factor, though. For the second argument, use data= to specify the name of the data frame (PerfRew). # Compute Levene&#39;s test for equal variances leveneTest(Perf_Overall ~ as.factor(Sex), data=PerfRew) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 2.3502 0.1287 ## 93 This function tests the null hypothesis that the variances are equal, and the output indicates that the p-value (i.e., Pr(&gt;F) = .1287) associated with Levene’s test is equal to or greater than a two-tailed alpha level of .05. Thus, we fail to reject the null hypothesis that the variances are equal and thus conclude that the variances are equal. We have satisfied this statistical assumption. Overall, we can reasonably conclude that the statistical assumptions for a point-biserial correlation have been satisfied. Estimate Point-Biserial Correlation: Fortunately, because a point-biserial correlation is equal to a Pearson product-moment correlation when the dichotomous variable has been converted to quantitative values, we can use the same Correlation function from the lessR package that we used for estimating the Pearson product-moment correlation. # Estimate point-biserial / Pearson product-moment correlation using Correlation function from lessR Correlation(x=Perf_Overall, y=Sex, data=PerfRew) ## Correlation Analysis for Variables Perf_Overall and Sex ## ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 95 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = -0.081 ## ## Sample Correlation: r = -0.135 ## ## Hypothesis Test of 0 Correlation: t = -1.312, df = 93, p-value = 0.193 ## 95% Confidence Interval for Correlation: -0.327 to 0.069 Sample Technical Write-Up: Based on a sample of 95 employees (N = 95), we can conclude that the correlation between overall performance evaluation scores and employee sex is not statistically significant, as the p-value was greater than the conventional two-tailed alpha level of .05 (r = -.135, p = .193, 95% CI[-.327, .069]). This leads us to conclude that employees’ overall performance evaluation ratings are not associated with their sex. Thus, we found evidence of discriminant validity, as these two conceptually dissimilar variables were not found to be correlated. 51.2.6 Create Correlation Matrix When we have a large number of variables for which we wish to evaluate convergent and discriminant validity, often it makes sense to create a correlation matrix as opposed to estimating the correlation for each pair of variables one by one. There are different functions we can use to create a correlation matrix for a set of numeric variables. Here, we will learn how to use thecorr.test and lowerCor functions from the psych package (Revelle 2023). Before we review the functions, let’s drop the EmpID variable from the data frame, as it wouldn’t make sense to correlate another variable with a unique identifier variable like EmpID. # Drop EmpID unique identifier variable PerfRew$EmpID &lt;- NULL Previously, we recoded the Sex variable such that Female became 1 and Male became 0. As a precautionary step, we will ensure that the Sex variable is indeed now numeric by applying the as.numeric function. We will overwrite the existing Sex variable by using the &lt;- assignment operator. Using the $ operator, we can attach this new (overwritten) variable to the existing PerfRew data frame object. To the right of the &lt;- assignment operator, type the name of the as.numeric function. As the sole argument, type the name of the data frame object (PerfRew), followed by the $ operator and the name of the variable we wish to convert to numeric (Sex). # Convert Sex variable to type numeric PerfRew$Sex &lt;- as.numeric(PerfRew$Sex) The corr.test function from the psych package has the advantage of producing both the correlation coefficients and a p-values. If you haven’t already, install and access the psych package using the install.packages and library functions, respectively. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Type the name of the corr.test function (not to be confused with the cor.test function from base R). As the first argument, type the name of your data frame object (PerfRew). As the second argument, specify method=\"pearson\" to estimate Pearson product-moment correlations. If you were estimating the associations between a set of rank-order variables, you could replace “pearson” with “spearman” to estimate Spearman correlations. # Create correlation matrix using corr.test function corr.test(PerfRew, method=&quot;pearson&quot;) ## Call:corr.test(x = PerfRew, method = &quot;pearson&quot;) ## Correlation matrix ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## Perf_Qual 1.00 0.77 0.81 0.85 0.46 0.11 0.44 -0.10 0.12 -0.02 ## Perf_Prod 0.77 1.00 0.76 0.74 0.56 0.17 0.58 -0.21 0.23 -0.06 ## Perf_Effort 0.81 0.76 1.00 0.77 0.50 0.21 0.48 -0.18 0.07 0.05 ## Perf_Admin 0.85 0.74 0.77 1.00 0.34 0.11 0.34 -0.01 0.12 -0.10 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 0.06 0.91 0.02 0.10 0.02 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 0.08 -0.18 0.40 0.10 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 -0.07 0.15 0.01 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 1.00 -0.02 -0.11 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 -0.02 1.00 0.12 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 -0.11 0.12 1.00 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 -0.13 0.15 -0.04 ## Perf_Overall ## Perf_Qual 0.94 ## Perf_Prod 0.89 ## Perf_Effort 0.91 ## Perf_Admin 0.92 ## SalesRevenue 0.51 ## BasePay_2018 0.16 ## VariablePay_2018 0.50 ## Sex -0.13 ## Age 0.15 ## EducationLevel -0.04 ## Perf_Overall 1.00 ## Sample Size ## [1] 95 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## Perf_Qual 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## Perf_Prod 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.83 1.00 ## Perf_Effort 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## Perf_Admin 0.00 0.00 0.00 0.00 0.03 1.00 0.03 1.00 1.00 1.00 ## SalesRevenue 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## BasePay_2018 0.30 0.11 0.04 0.27 0.55 0.00 1.00 1.00 0.00 1.00 ## VariablePay_2018 0.00 0.00 0.00 0.00 0.00 0.44 0.00 1.00 1.00 1.00 ## Sex 0.35 0.04 0.08 0.95 0.83 0.07 0.50 0.00 1.00 1.00 ## Age 0.25 0.03 0.48 0.24 0.34 0.00 0.15 0.86 0.00 1.00 ## EducationLevel 0.81 0.58 0.65 0.35 0.81 0.34 0.93 0.30 0.23 0.00 ## Perf_Overall 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.19 0.15 0.73 ## Perf_Overall ## Perf_Qual 0 ## Perf_Prod 0 ## Perf_Effort 0 ## Perf_Admin 0 ## SalesRevenue 0 ## BasePay_2018 1 ## VariablePay_2018 0 ## Sex 1 ## Age 1 ## EducationLevel 1 ## Perf_Overall 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option The output produced includes three sections: “Correlation matrix”, “Sample Size”, and “Probability values” (p-values). To find the corresponding p-value for a correlation displayed in the matrix, look to the “Probability values matrix”, and note that the lower diagonal includes the traditional p-values, while the upper diagonal includes the adjusted p-values based on multiple tests (i.e., correcting for family-wise error). To estimate the confidence intervals, assign the results of the function and its argument to an object named whatever you would like (matrixexample) using the &lt;- operator. After that, use the print function from base R with the name of the new object as the first argument and short=FALSE as the second argument. # Create correlation matrix objet with p-values &amp; confidence intervals matrixexample &lt;- corr.test(PerfRew, method=&quot;pearson&quot;) # Print correlation matrix with p-values &amp; confidence intervals print(matrixexample, short=FALSE) ## Call:corr.test(x = PerfRew, method = &quot;pearson&quot;) ## Correlation matrix ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## Perf_Qual 1.00 0.77 0.81 0.85 0.46 0.11 0.44 -0.10 0.12 -0.02 ## Perf_Prod 0.77 1.00 0.76 0.74 0.56 0.17 0.58 -0.21 0.23 -0.06 ## Perf_Effort 0.81 0.76 1.00 0.77 0.50 0.21 0.48 -0.18 0.07 0.05 ## Perf_Admin 0.85 0.74 0.77 1.00 0.34 0.11 0.34 -0.01 0.12 -0.10 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 0.06 0.91 0.02 0.10 0.02 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 0.08 -0.18 0.40 0.10 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 -0.07 0.15 0.01 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 1.00 -0.02 -0.11 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 -0.02 1.00 0.12 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 -0.11 0.12 1.00 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 -0.13 0.15 -0.04 ## Perf_Overall ## Perf_Qual 0.94 ## Perf_Prod 0.89 ## Perf_Effort 0.91 ## Perf_Admin 0.92 ## SalesRevenue 0.51 ## BasePay_2018 0.16 ## VariablePay_2018 0.50 ## Sex -0.13 ## Age 0.15 ## EducationLevel -0.04 ## Perf_Overall 1.00 ## Sample Size ## [1] 95 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## Perf_Qual 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## Perf_Prod 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.83 1.00 ## Perf_Effort 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## Perf_Admin 0.00 0.00 0.00 0.00 0.03 1.00 0.03 1.00 1.00 1.00 ## SalesRevenue 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 ## BasePay_2018 0.30 0.11 0.04 0.27 0.55 0.00 1.00 1.00 0.00 1.00 ## VariablePay_2018 0.00 0.00 0.00 0.00 0.00 0.44 0.00 1.00 1.00 1.00 ## Sex 0.35 0.04 0.08 0.95 0.83 0.07 0.50 0.00 1.00 1.00 ## Age 0.25 0.03 0.48 0.24 0.34 0.00 0.15 0.86 0.00 1.00 ## EducationLevel 0.81 0.58 0.65 0.35 0.81 0.34 0.93 0.30 0.23 0.00 ## Perf_Overall 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.19 0.15 0.73 ## Perf_Overall ## Perf_Qual 0 ## Perf_Prod 0 ## Perf_Effort 0 ## Perf_Admin 0 ## SalesRevenue 0 ## BasePay_2018 1 ## VariablePay_2018 0 ## Sex 1 ## Age 1 ## EducationLevel 1 ## Perf_Overall 0 ## ## Confidence intervals based upon normal theory. To get bootstrapped values, try cor.ci ## raw.lower raw.r raw.upper raw.p lower.adj upper.adj ## Prf_Q-Prf_P 0.68 0.77 0.84 0.00 0.59 0.88 ## Prf_Q-Prf_E 0.73 0.81 0.87 0.00 0.65 0.90 ## Prf_Q-Prf_A 0.78 0.85 0.89 0.00 0.71 0.92 ## Prf_Q-SlsRv 0.29 0.46 0.61 0.00 0.17 0.68 ## Prf_Q-BP_20 -0.10 0.11 0.30 0.30 -0.20 0.40 ## Prf_Q-VP_20 0.26 0.44 0.59 0.00 0.14 0.67 ## Prf_Q-Sex -0.29 -0.10 0.11 0.35 -0.38 0.20 ## Prf_Q-Age -0.08 0.12 0.31 0.25 -0.19 0.41 ## Prf_Q-EdctL -0.22 -0.02 0.18 0.81 -0.28 0.24 ## Prf_Q-Prf_O 0.91 0.94 0.96 0.00 0.88 0.97 ## Prf_P-Prf_E 0.66 0.76 0.83 0.00 0.58 0.87 ## Prf_P-Prf_A 0.63 0.74 0.82 0.00 0.54 0.86 ## Prf_P-SlsRv 0.40 0.56 0.68 0.00 0.29 0.75 ## Prf_P-BP_20 -0.04 0.17 0.36 0.11 -0.16 0.46 ## Prf_P-VP_20 0.42 0.58 0.70 0.00 0.31 0.76 ## Prf_P-Sex -0.40 -0.21 -0.01 0.04 -0.50 0.11 ## Prf_P-Age 0.03 0.23 0.41 0.03 -0.10 0.51 ## Prf_P-EdctL -0.26 -0.06 0.15 0.58 -0.33 0.23 ## Prf_P-Prf_O 0.84 0.89 0.93 0.00 0.80 0.94 ## Prf_E-Prf_A 0.68 0.77 0.84 0.00 0.60 0.88 ## Prf_E-SlsRv 0.33 0.50 0.64 0.00 0.21 0.71 ## Prf_E-BP_20 0.01 0.21 0.39 0.04 -0.12 0.49 ## Prf_E-VP_20 0.31 0.48 0.62 0.00 0.19 0.70 ## Prf_E-Sex -0.37 -0.18 0.02 0.08 -0.47 0.14 ## Prf_E-Age -0.13 0.07 0.27 0.48 -0.22 0.36 ## Prf_E-EdctL -0.16 0.05 0.25 0.65 -0.23 0.32 ## Prf_E-Prf_O 0.87 0.91 0.94 0.00 0.83 0.96 ## Prf_A-SlsRv 0.15 0.34 0.51 0.00 0.02 0.59 ## Prf_A-BP_20 -0.09 0.11 0.31 0.27 -0.20 0.40 ## Prf_A-VP_20 0.14 0.34 0.50 0.00 0.02 0.59 ## Prf_A-Sex -0.21 -0.01 0.20 0.95 -0.21 0.20 ## Prf_A-Age -0.08 0.12 0.32 0.24 -0.19 0.41 ## Prf_A-EdctL -0.29 -0.10 0.11 0.35 -0.38 0.21 ## Prf_A-Prf_O 0.88 0.92 0.94 0.00 0.84 0.96 ## SlsRv-BP_20 -0.14 0.06 0.26 0.55 -0.23 0.34 ## SlsRv-VP_20 0.86 0.91 0.94 0.00 0.82 0.95 ## SlsRv-Sex -0.18 0.02 0.22 0.83 -0.23 0.28 ## SlsRv-Age -0.10 0.10 0.30 0.34 -0.21 0.39 ## SlsRv-EdctL -0.18 0.02 0.23 0.81 -0.24 0.29 ## SlsRv-Prf_O 0.34 0.51 0.64 0.00 0.22 0.72 ## BP_20-VP_20 -0.12 0.08 0.28 0.44 -0.22 0.36 ## BP_20-Sex -0.37 -0.18 0.02 0.07 -0.47 0.14 ## BP_20-Age 0.21 0.40 0.55 0.00 0.09 0.64 ## BP_20-EdctL -0.11 0.10 0.29 0.34 -0.21 0.39 ## BP_20-Prf_O -0.04 0.16 0.35 0.12 -0.16 0.45 ## VP_20-Sex -0.27 -0.07 0.13 0.50 -0.35 0.22 ## VP_20-Age -0.06 0.15 0.34 0.15 -0.17 0.44 ## VP_20-EdctL -0.19 0.01 0.21 0.93 -0.22 0.24 ## VP_20-Prf_O 0.33 0.50 0.64 0.00 0.21 0.71 ## Sex-Age -0.22 -0.02 0.18 0.86 -0.26 0.23 ## Sex-EdctL -0.30 -0.11 0.10 0.30 -0.40 0.20 ## Sex-Prf_O -0.33 -0.13 0.07 0.19 -0.43 0.18 ## Age-EdctL -0.08 0.12 0.32 0.23 -0.19 0.42 ## Age-Prf_O -0.05 0.15 0.34 0.15 -0.17 0.44 ## EdctL-Prf_O -0.24 -0.04 0.17 0.73 -0.31 0.24 The 95% confidence intervals appear at the bottom of the output, under the section “Confidence intervals based upon normal theory”. Each row represents a unique correlation; note that the function abbreviates the variable names, so you’ll have to do your best to interpret them. We can also write the matrixexample object we created to a .csv file by using the write.csv function. As the first argument, enter the name of the correlation matrix object (matrixexample), followed by a $, followed by either r, p, or ci to write the correlation matrix, p-values, or confidence intervals, respectively. As the second argument, come up with a name for the data file that will appear in your working directory, but be sure to keep the .csv extension. The files will appear in your working directory, and you can open them in Microsoft Excel or Google Sheets. # Write correlation matrix to working directory write.csv(matrixexample$r, &quot;Correlation Matrix Example.csv&quot;) # Write p-values to working directory write.csv(matrixexample$p, &quot;p-values Example.csv&quot;) # Write confidence intervals to working directory write.csv(matrixexample$ci, &quot;Confidence Intervals Example.csv&quot;) If you just want to view the lower diagonal of the correlation matrix (with just the correlation coefficients), apply the lowerCor function from the psych package, using the same arguments as the corr.test function. # Create correlation matrix using lowerCor function lowerCor(PerfRew, method=&quot;pearson&quot;) ## Prf_Q Prf_P Prf_E Prf_A SlsRv BP_20 VP_20 Sex Age EdctL Prf_O ## Perf_Qual 1.00 ## Perf_Prod 0.77 1.00 ## Perf_Effort 0.81 0.76 1.00 ## Perf_Admin 0.85 0.74 0.77 1.00 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 1.00 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 -0.02 1.00 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 -0.11 0.12 1.00 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 -0.13 0.15 -0.04 1.00 If our goal is only to read the sign and magnitude of each correlation, then the viewing just the lower diagonal makes that task much easier. To learn how to make heatmap data visualizations for a correlation matrix and how to present the results in American Psychological Association (APA) style, please check out the chapter supplement. 51.2.7 Summary In this chapter, we learned how to create a scatter plot using the ScatterPlot function from the lessR package, how to estimate a correlation using the Correlation function from the lessR package, and how to create a correlation matrix using the corr.test and lowerCor functions from the psych package. 51.3 Chapter Supplement In this chapter, we will learn how to test the statistical assumption of a univariate normal distribution when we have fewer than 30 cases by using the Shapiro-Wilk test. In addition, we will learn how to create an American Psychological Association (APA) style table and other data visualizations for a correlation matrix. 51.3.1 Functions &amp; Packages Introduced Function Package set.seed base R slice_sample dplyr shapiro.test base R tapply base R apa.cor.table apaTables cor.plot psych corrgram corrgram 51.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object PerfRew &lt;- read_csv(&quot;PerfMgmtRewardSystemsExample.csv&quot;) ## Rows: 95 Columns: 11 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Sex ## dbl (10): EmpID, Perf_Qual, Perf_Prod, Perf_Effort, Perf_Admin, SalesRevenue, BasePay_2018, VariablePay_2018, Age, EducationLevel ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(PerfRew) ## [1] &quot;EmpID&quot; &quot;Perf_Qual&quot; &quot;Perf_Prod&quot; &quot;Perf_Effort&quot; &quot;Perf_Admin&quot; &quot;SalesRevenue&quot; &quot;BasePay_2018&quot; ## [8] &quot;VariablePay_2018&quot; &quot;Sex&quot; &quot;Age&quot; &quot;EducationLevel&quot; # Print variable type for each variable in data frame (tibble) object str(PerfRew) ## spc_tbl_ [95 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:95] 1 2 3 4 5 6 7 8 9 10 ... ## $ Perf_Qual : num [1:95] 3 1 2 2 1 2 5 2 2 2 ... ## $ Perf_Prod : num [1:95] 3 1 1 3 1 2 5 1 3 2 ... ## $ Perf_Effort : num [1:95] 3 1 1 3 1 2 5 1 3 3 ... ## $ Perf_Admin : num [1:95] 4 1 1 1 1 3 5 1 2 2 ... ## $ SalesRevenue : num [1:95] 57563 54123 56245 58291 58354 ... ## $ BasePay_2018 : num [1:95] 53791 52342 50844 52051 48061 ... ## $ VariablePay_2018: num [1:95] 6199 1919 7507 6285 4855 ... ## $ Sex : chr [1:95] &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Age : num [1:95] 39 48 38 35 32 34 57 43 35 47 ... ## $ EducationLevel : num [1:95] 2 4 4 4 2 4 3 2 4 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Perf_Qual = col_double(), ## .. Perf_Prod = col_double(), ## .. Perf_Effort = col_double(), ## .. Perf_Admin = col_double(), ## .. SalesRevenue = col_double(), ## .. BasePay_2018 = col_double(), ## .. VariablePay_2018 = col_double(), ## .. Sex = col_character(), ## .. Age = col_double(), ## .. EducationLevel = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(PerfRew) ## # A tibble: 6 × 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female 39 2 ## 2 2 1 1 1 1 54123 52342 1919 Male 48 4 ## 3 3 2 1 1 1 56245 50844 7507 Male 38 4 ## 4 4 2 3 3 1 58291 52051 6285 Male 35 4 ## 5 5 1 1 1 1 58354 48061 4855 Female 32 2 ## 6 6 2 2 2 3 57618 53386 4056 Male 34 4 # Print number of rows in data frame (tibble) object head(PerfRew) ## # A tibble: 6 × 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex Age EducationLevel ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female 39 2 ## 2 2 1 1 1 1 54123 52342 1919 Male 48 4 ## 3 3 2 1 1 1 56245 50844 7507 Male 38 4 ## 4 4 2 3 3 1 58291 52051 6285 Male 35 4 ## 5 5 1 1 1 1 58354 48061 4855 Female 32 2 ## 6 6 2 2 2 3 57618 53386 4056 Male 34 4 # Create composite (overall scale score) variable based on Engagement items PerfRew$Perf_Overall &lt;- rowMeans(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)], na.rm=TRUE) # Drop EmpID unique identifier variable PerfRew$EmpID &lt;- NULL # Recode Female as 1 and Male as 0 for Sex variable PerfRew$Sex &lt;- dplyr::recode(PerfRew$Sex, Female = 1, Male = 0) # Convert Sex variable to type numeric PerfRew$Sex &lt;- as.numeric(PerfRew$Sex) 51.3.3 shapiro.test Function from Base R Evidence of a univariate normal distribution is a common statistical assumption for parametric statistical analyses like the Pearson product-moment correlation, point-biserial correlation, independent-samples t-test, and paired-samples t-test. In accordance with the central limit theorem, when the sample size exceeds 30 (N &gt; 30), then the sampling distribution tends to approximate normality. Consequently, when we have larger sample sizes, we can generally assume that we have met the assumption of univariate normality. In contrast, when the sample size has 30 or fewer cases, then we should formally test the assumption of univariate normality using a statistical test like the Shapiro-Wilk test and/or a data visualization like a histogram. The null hypothesis for the Shapiro-Wilk test is that the distribution is normal; thus, only when the associated p-value is less than the alpha level of .05 will we conclude that the distribution violates the assumption of normality. For the sake of demonstration, let’s randomly select 25 cases from our PerfRew data frame object by using the slice_sample. function from the dplyr package (Wickham et al. 2023). If you haven’t already, be sure to install and access the dplyr package. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) Let’s make our random sampling reproducible by setting a seed value using the set.seed function from base R. # Set seed for reproducibility of random sampling set.seed(1985) To randomly sample 25 cases from the PerfRew data frame object, we will do the following: Come up with a name for a new data frame object that will house our randomly sampled cases. In this example, I’m naming the new object temp using the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the slice_sample function from the dplyr package. As the first argument in the function, we’ll type the name of the data frame object from which we wish to randomly sample (PerfRew). As the second argument, we’ll type n= followed by the number of cases we wish to randomly sample. This function will by default randomly sample without replacement. # Randomly sample 25 cases from data frame object temp &lt;- slice_sample(PerfRew, n=25) Now we have a data frame object called temp that has fewer than 30 cases, which makes it necessary to formally evaluate whether the assumption of univariate normality has been satisfied. To compute the Shapiro-Wilk normality test for all cases with scores on a single continuous variable in our sample, we can use the shapiro.test function from base R. First, type the name of the shapiro.test function. As the sole argument, type the name of the data frame (temp), followed by the $ operator and the name of the continuous variable in question (Perf_Overall). # Compute Shapiro-Wilk normality test for univariate normal # distribution statistical assumption shapiro.test(temp$Perf_Overall) ## ## Shapiro-Wilk normality test ## ## data: temp$Perf_Overall ## W = 0.96315, p-value = 0.4808 The output indicates that the p-value of .4808 is equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed. In other words, we have evidence that the outcome variable is likely normally distributed for both conditions, which suggests that we have met the this statistical assumption for this continuous variable; if another continuous variable also met this statistical assumption, then we would build more confidence that a Pearson product-moment correlation is an appropriate analysis. If our goal is to test the statistical assumption that a continuous variable is normally distributed at each level of a categorical (e.g., dichotomous) variable, then we can use a combination of the tapply and shapiro.test functions from base R. The tapply function will apply the shapiro.test function to the continuous variable for each level of the categorical variable. Type the name of the tapply function from base R. As the first argument, type the name of the data frame object (temp), followed by the $ operator and the name of the continuous variable (Perf_Overall). As the second argument, type the name of the data frame object (temp), followed by the $ operator and the name of the categorical (e.g., dichotomous) variable (Sex). As the third argument, type the name of the shapiro.test function from base R. # Compute Shapiro-Wilk normality test for univariate normal # distribution statistical assumption tapply(temp$Perf_Overall, temp$Sex, shapiro.test) ## $`0` ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.97492, p-value = 0.9106 ## ## ## $`1` ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.80628, p-value = 0.02408 The output indicates that the p-value associated with the Perf_Overall variable for cases where Sex is equal to 1 (Female) is .9106, which is equal to or greater than the conventional two-tailed alpha level of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed within this segment of the data. The p-value associated with the Perf_Overall variable for cases where Sex is equal to 0 (Male) is 02408, which is less than the conventional two-tailed alpha level of .05; therefore, we reject the null hypothesis that the values are normally distributed within this segment of the data, leading us to conclude that there is likely a violation of the assumption of univariate normality for the segment of cases where Sex is equal to 0 (Male). Taken together, the Shapiro-Wilk tests indicate that a parametric statistical analysis like a point-biserial correlation may not be appropriate. 51.3.4 APA-Style Results Table If we wish to present the results of our correlation matrix to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package (Stanley 2021). If you haven’t already, be sure to install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.cor.table function is easy to use. Just type the name of the apa.cor.table function, and in the function parentheses, include the name of the data frame (with only variables of type numeric) as the sole argument (PerfRew). The APA-style correlation matrix will appear in your console. # Create APA-style correlation matrix using apa.cor.table function apa.cor.table(PerfRew) ## ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 4 5 6 7 8 ## 1. Perf_Qual 2.93 1.33 ## ## 2. Perf_Prod 2.85 1.30 .77** ## [.68, .84] ## ## 3. Perf_Effort 2.93 1.30 .81** .76** ## [.73, .87] [.66, .83] ## ## 4. Perf_Admin 2.94 1.32 .85** .74** .77** ## [.78, .89] [.63, .82] [.68, .84] ## ## 5. SalesRevenue 60852.88 6501.54 .46** .56** .50** .34** ## [.29, .61] [.40, .68] [.33, .64] [.15, .51] ## ## 6. BasePay_2018 52090.52 2670.02 .11 .17 .21* .11 .06 ## [-.10, .30] [-.04, .36] [.01, .39] [-.09, .31] [-.14, .26] ## ## 7. VariablePay_2018 8131.65 3911.81 .44** .58** .48** .34** .91** .08 ## [.26, .59] [.42, .70] [.31, .62] [.14, .50] [.86, .94] [-.12, .28] ## ## 8. Sex 0.43 0.50 -.10 -.21* -.18 -.01 .02 -.18 -.07 ## [-.29, .11] [-.40, -.01] [-.37, .02] [-.21, .20] [-.18, .22] [-.37, .02] [-.27, .13] ## ## 9. Age 39.67 8.83 .12 .23* .07 .12 .10 .40** .15 -.02 ## [-.08, .31] [.03, .41] [-.13, .27] [-.08, .32] [-.10, .30] [.21, .55] [-.06, .34] [-.22, .18] ## ## 10. EducationLevel 3.13 1.02 -.02 -.06 .05 -.10 .02 .10 .01 -.11 ## [-.22, .18] [-.26, .15] [-.16, .25] [-.29, .11] [-.18, .23] [-.11, .29] [-.19, .21] [-.30, .10] ## ## 11. Perf_Overall 2.91 1.20 .94** .89** .91** .92** .51** .16 .50** -.13 ## [.91, .96] [.84, .93] [.87, .94] [.88, .94] [.34, .64] [-.04, .35] [.33, .64] [-.33, .07] ## ## 9 10 ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## .12 ## [-.08, .32] ## ## .15 -.04 ## [-.05, .34] [-.24, .17] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If you would like to write (export) the correlation matrix table to your working directory as a .doc or .rtf document, as an additional argument, add filename= followed by what you would like to name the new file in quotation marks (\" \"). Be sure to include either .doc or .rtf as the file extension. Once you have run the code below, open the new file in Microsoft Word. # Create APA-style correlation matrix using apa.cor.table function apa.cor.table(PerfRew, filename=&quot;APA Correlation Matrix Table.doc&quot;) The apa.reg.table function from the apaTables package can table correlation matrix results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. 51.3.5 cor.plot Function from psych package To create a correlation matrix with a heatmap, we must first create a correlation matrix object using the cor function from base R. We will assign the results of the cor function to an object named something of your choosing (e.g., matrixexample2) using the &lt;- assignment operator. We will apply then cor function to the PerfRew data frame object, and specify that we wish to estimate Pearson product-moment correlations (and by extension point biserial correlations, if applicable) by specifying method=\"pearson\" as the second argument. # Create and name correlation matrix object using cor function matrixexample2 &lt;- cor(PerfRew, method=&quot;pearson&quot;) Let’s make a correlation matrix with a heatmap based on the matrixexample2 correlation matrix object we just created, we will use the cor.plot function from the psych package. If you haven’t already, be sure to install and access the psych package. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) To begin, type the name of the cor.plot function. As the first argument, type the name of the correlation matrix object you just created (matrixexample2). As the second argument, leave the numbers=TRUE argument as is. You can save the image by selecting Export from the Plots window in RStudio. # Create a heatmap correlation matrix cor.plot(matrixexample2, numbers=TRUE) The heatmap displays stronger positive correlations in a darker hue of blue and stronger negative correlations in a darker hue of red. Correlations that are equal to zero will be white. 51.3.6 corrgram Function from corrgram package To create a correlogram heatmap, we will use the corrgram function from the corrgram package (Wright 2021). If you haven’t already, install and access the corrgram package. # Install package install.packages(&quot;corrgram&quot;) # Access package library(corrgram) Let’s begin by creating a correlogram heatmap in which the lower diagonal is a gradient-shaded heatmap and the upper diagonal is a series of pies depicting the magnitude of correlations. Type the name of the corrgram function. As the first argument, type the name of the data frame object (PerfRew). As the second argument, specify order=FALSE to keep the order of the variables as they are in the data frame. As the third argument, include the argument lower.panel=panel.shade to create a gradient-shared heatmap in the lower diagonal. As the fourth argument, include the argument upper.panel=panel.pie to convey the magnitude of each correlation as a proportion of a filled-in pie. As the fifth argument, include the argument text.panel=panel.txt to include variable names on the off diagonal. # Create a correlogram heatmap corrgram(PerfRew, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie, text.panel=panel.txt) References "],["polynomialregression.html", "Chapter 52 Investigating Nonlinear Associations Using Polynomial Regression 52.1 Conceptual Overview 52.2 Tutorial", " Chapter 52 Investigating Nonlinear Associations Using Polynomial Regression In this chapter, we will learn how to investigate nonlinear associations between two continuous (interval, ratio) variables using polynomial regression. There are other ways in which we can estimate nonlinear associations, but in this chapter, we’ll focus just on polynomial regression. 52.1 Conceptual Overview A common way to explore a nonlinear association between two variables is to estimate a polynomial regression model. If you think way back to algebra class, a polynomial is a mathematical expression with multiple terms, which might include constants, variables, or exponents. With polynomial regression, we are able to estimate curvature in the relation between two variables, such as quadratic or cubic functional forms. For example, let’s imagine that we have variables \\(X\\) and \\(Y\\), and we suspect that the association between the two variables is nonlinear. Specifically, let’s assume that the true association between \\(X\\) and \\(Y\\) can be expressed as the following quadratic equation: \\(\\hat{Y} = 8 - 9X + X^2\\) where \\(\\hat{Y}\\) is the predicted (fitted) value of the outcome variable \\(Y\\). If we plot this equation, we get the following parabolic shape. If we only estimated a linear association between \\(X\\) and \\(Y\\), we might miss the fact that these two variables are indeed associated with one another – or we might miss the true functional form of their association. In addition to quadratic terms, we can also include cubic terms, which can manifest as an association with two bends or curves: \\(\\hat{Y} = 1 - .25x + .75X^2 + .35X^3\\) If we plot this equation, we get the following functional form. As the name implies, polynomial regression is a type of regression. If you haven’t already, prior to following along with this tutorial, I recommend checking out the chapters that introduce simple linear regression, multiple linear regression, and moderated multiple linear regression. 52.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a polynomial regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; The association between the predictor and outcome variables is nonlinear; Average residual error value is zero; Residual errors are normally distributed. Note: Regarding the first statistical assumption (i.e., cases randomly sampled from population), we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 52.1.2 Statistical Significance In polynomial regression, we are often interested in the significance of the highest order term in the model (e.g., \\(X^3\\)) when controlling for the lower order terms (e.g., \\(X\\), \\(X^2\\)). Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficient’s p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficient’s p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficient’s p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 52.1.2.1 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a polynomial regression model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. In contrast, a standardized regression coefficient (\\(\\beta\\)) is estimated by standardizing the scores of the both the predictor and outcome variables. Given this, a standardized regression coefficient can be compared to other standardized regression coefficients; however, we should take caution when making such comparisons in a polynomial regression model, as we are effectively controlling for lower-order terms in the model. In a polynomial regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variable (i.e., R2). Conceptually, we can think of the overlap between the variability in the predictor and outcome variables as the variance explained (R2). I’ve found that the R2 is often more readily interpretable by non-analytics audiences. For example, an R2 of .10 in a polynomial regression model can be interpreted as: 10% of the variability in scores on the outcome variable can be explained by scores on the predictor variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect. 52.1.2.2 Sample Write-Up Based on a sample of 107 employees (N = 107), We found that the association between emotional stability scores and supervisor ratings of job performance was nonlinear. Using polynomial regression, we found that the quadratic term for emotional stability was statistically significant (b = -.475, p &lt; .001), which implies that there is a statistically significant nonlinear association with a quadratic functional form. Moreover, with the quadratic term included, the model explained 22.7% of the variability in supervisor ratings of job performance, which was an 18.9% improvement over a simple linear regression model that lacked the quadratic term. The functional form of the association can be described as an inverted “U”, such that supervisor ratings of performance are highest for those employees with more moderate levels of emotional stability. 52.2 Tutorial This chapter’s tutorial demonstrates how to estimate a polynomial regression model in R. 52.2.1 Functions &amp; Packages Introduced Function Package plot base R lm base R summary base R mean base R I base R hist base R seq base R min base R max base R lines base R poly base R predict base R 52.2.2 Initial Steps If you haven’t already, save the file called “Nonlinear.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “Nonlinear.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;Nonlinear.csv&quot;) ## Rows: 107 Columns: 2 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): ES, Perf ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;ES&quot; &quot;Perf&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spc_tbl_ [107 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ES : num [1:107] 0.2 0.9 2.1 1.9 1.6 1.8 1.4 2.3 1.7 0.2 ... ## $ Perf: num [1:107] 1.6 10.1 6.7 4.9 3.4 15.1 11.7 6.8 26.5 8.3 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ES = col_double(), ## .. Perf = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 2 ## ES Perf ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 1.6 ## 2 0.9 10.1 ## 3 2.1 6.7 ## 4 1.9 4.9 ## 5 1.6 3.4 ## 6 1.8 15.1 # Print number of rows in data frame (tibble) object nrow(df) ## [1] 107 There are 2 variables and 107 cases (i.e., employees) in the df data frame: ES and Perf. Per the output of the str (structure) function above, both variables are of type numeric (continuous: interval, ratio). The ES variable is a measure of emotional stability, with scores ranging from 1 (low emotional stability) to 10 (high emotional stability). The Perf variable includes performance evaluation data that can range from 0 (low performance) to 30 (high performance). 52.2.3 Visualize Association Using a Bivariate Scatter Plot A bivariate scatter plot can be used to visualize the association between two continuous (interval, ratio) variables. This type of plot can help us identify whether an association between variables might be nonlinear. Let’s create a bivariate scatter plot to visualize the association between the ES (emotional stability) and the Perf (supervisor-ratings of job performance) variables, both of which are continuous. The plot function from base R will help us generate a very simple scatter plot, which will be fine for our purposes. To begin, type the name of the plot function. As the first argument, we’ll type the name of the data frame object (df), followed by the $ operator and the name of the predictor variable (ES). As the second argument, we’ll type the name of the data frame object (df), followed by the $ operator and the name of the outcome variable (Perf). # Create scatter plot using plot function from base R plot(df$ES, df$Perf) In our plot window, we can see evidence of a nonlinear association between scores on ES and scores on Perf. In fact, the association seems to take the functional form of a quadratic inverted “U” shape. This suggests that a simple linear regression model would not be appropriate for exploration the nature of the association between these two variables. Instead, we should try estimating a polynomial regression model. 52.2.4 Estimate Polynomial Regression Model It’s customary to build a polynomial regression model in an iterative manner. That is, we’ll start with a simple linear regression model, with ES as the predictor variable and Perf as the outcome variable, and then work our way up to a polynomial regression model by adding additional terms. 52.2.4.1 Step One: Simple Linear Regression Model Let’s name our simple linear regression model object mod1. For a review, check out the chapter that covers simple linear regression. # Estimate simple linear regression model mod1 &lt;- lm(Perf ~ ES, data=df) # Print summary of results summary(mod1) ## ## Call: ## lm(formula = Perf ~ ES, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2413 -4.0120 0.3052 5.4448 10.6708 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.6540 1.3533 12.306 &lt;0.0000000000000002 *** ## ES 0.4931 0.2419 2.038 0.044 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.95 on 105 degrees of freedom ## Multiple R-squared: 0.03807, Adjusted R-squared: 0.02891 ## F-statistic: 4.155 on 1 and 105 DF, p-value: 0.04402 The regression coefficient associated with ES is .493 (p = .044), which is statistically significant, so that seems promising; however, let’s plot the predicted/fitted values based on our model with the residuals (errors). To do so, we will use the plot function from base R, and enter the name of our regression model object (mod1) as the first argument. As the second argument, enter the numeral 1 to request the residuals vs. fitted values plot. # Diagnostic plot: Residuals vs. Fitted Values plot(mod1, 1) The plot shows evidence of what looks like a nonlinear association. Given that, we have found additional evidence that the association between the ES and Perf variables violates key statistical assumptions that should be satisfied to interpret a simple linear regression. Thus, we should proceed forward by constructing a polynomial regression model. 52.2.4.2 Step Two: Polynomial Regression Model We’ll build upon our previous model by adding a quadratic term to our model. Prior to doing so, however, let’s grand-mean center our predictor variable (ES) to reduce the collinearity between ES and its quadratic (product) term. We’ll call the grand-mean centered variable c_ES. For more information on centering variables, check out the chapter on centering and standardizing variables. Note: While grand-mean centering may improve estimates of standard errors, it does not generally affect the overall model fit to the data; thus, when plotting model, you may choose to use the uncentered predictor variables. # Grand-mean center predictor variable and create new variable df$c_ES &lt;- df$ES - mean(df$ES, na.rm=TRUE) There are different ways we can add the quadratic term to the model. Let’s start with what I believe is the most intuitive approach. Let’s name this polynomial regression model object (mod2) using the &lt;- assignment operator. To the right of the &lt;- assignment operator, start with the simple linear regression model we specified in the previous step. In the regression model formula, type the name of your grand-mean centered predictor variable (c_ES), followed by the + operator. After the + operator, insert the I function from base R, and within its parentheses, type the name of the grand-mean centered predictor variable (c_ES) followed by ^2 to signal that you want to include a quadratic term. # Estimate polynomial regression model: Option 1 mod2 &lt;- lm(Perf ~ c_ES + I(c_ES^2), data=df) # Print summary of results summary(mod2) ## ## Call: ## lm(formula = Perf ~ c_ES + I(c_ES^2), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2757 -4.1173 0.9039 4.9100 10.9401 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.70895 0.94469 24.038 &lt; 0.0000000000000002 *** ## c_ES 0.57739 0.21850 2.642 0.0095 ** ## I(c_ES^2) -0.47454 0.09405 -5.045 0.00000193 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.259 on 104 degrees of freedom ## Multiple R-squared: 0.2272, Adjusted R-squared: 0.2124 ## F-statistic: 15.29 on 2 and 104 DF, p-value: 0.00000151 Looking at the output, note that the product (quadratic, second-order) term is statistically significant (b = -.475, p &lt; .001), which implies that there is a statistically significant nonlinear association between ES and Perf that specifically takes a quadratic functional form. Note that the unadjusted R2 value increased from .038 (3.8%) in the simple linear regression model to .227 (22.7%) in this polynomial regression model; that is a huge amount of incremental variance explained in Perf after adding the quadratic term of ES to the model. Let’s plot the fitted/predicted values in relation to the residuals, as we did with the simple linear regression model above. # Diagnostic plot: Residuals vs. Fitted Values plot(mod2, 1) That looks much better with regard to meeting the statistical assumption that the average residual error value is zero. Next, let’s check to see if we can feel reasonably confident that the data meet the statistical assumption that the residual errors are normally distributed for each level of the predictor variable. To do so, we will use the histogram (hist) function from base R. As the sole argument, enter the summary function with the regression model (mod2) in the parentheses, followed by the $ operator and residuals (which extracts the vector of residuals from your model summary). # Diagnostic plot: Distribution of residuals hist(summary(mod2)$residuals) The distribution of the residuals seems to be approximately normal and centered around zero, so we can conclude that we have pretty good evidence that we have met that statistical assumption. Given that we have good reason to believe we have met the aforementioned statistical assumptions, we can proceed forward with interpreting and exploring our polynomial regression model. If we create a sequence of predictor (x) variable values (that I call xvalues) using the seq function from base R and plug those values along with the regression equation values from the output above into the plot function from base R, we view the predicted form of the nonlinear association. # Create object xvalues w/ continuous sequence of 1000 values # to include observed range of centered predictor values xvalues &lt;- seq(min(df$c_ES), # Mininum of observed range max(df$c_ES), # Maximum of observed range length=1000) # Number of sequence values within range # Create scatterplot for predictor and outcome variables plot(df$c_ES, # Insert predictor variable df$Perf, # Insert outcome variable xlab=&quot;Emotional Stability&quot;, # Set x-axis label ylab=&quot;Supervisor-Rated Performance&quot;) # Set y-axis label # Add estimated regression equation line to scatterplot lines(xvalues, # Insert continuous sequence of values object 22.70895 + 0.57739*(xvalues^1) + -0.47454*(xvalues^2), # Insert regression equation col=&quot;red&quot;) # Set line color to red As you can see, supervisor-ratings of performance are highest when emotional stability is somewhere around its mean. Thus, it seems that there is a sweet-spot for the Big Five personality dimension of emotional stability when it comes to performance. People who are really high in emotional stability might have a tendency to be too relaxed and may not be motivated to perform, whereas those who are really low in emotional stability might be too prone to experiencing emotional fluctuations and volatility to perform well. We can also estimate the polynomial regression model without having to create a new grand-mean-centered predictor variable. The poly function from base R allows us to create orthogonal (uncorrelated) polynomial (e.g., product, quadratic) terms. We’ll name this model mod3. # Estimate polynomial regression model: Option 2 mod3 &lt;- lm(Perf ~ poly(ES, 2), # Insert 2 for quadratic data=df) # Specify data frame # Print summary of results summary(mod3) ## ## Call: ## lm(formula = Perf ~ poly(ES, 2), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2757 -4.1173 0.9039 4.9100 10.9401 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.0486 0.6051 31.481 &lt; 0.0000000000000002 *** ## poly(ES, 2)1 14.1666 6.2589 2.263 0.0257 * ## poly(ES, 2)2 -31.5788 6.2589 -5.045 0.00000193 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.259 on 104 degrees of freedom ## Multiple R-squared: 0.2272, Adjusted R-squared: 0.2124 ## F-statistic: 15.29 on 2 and 104 DF, p-value: 0.00000151 Note that the unadjusted R2 value of .227 is the same for both approaches, but due to differences in how the polynomial is created/estimated, we see differences in the coefficients between the two approaches. Nonetheless, if we plot the second approach, we see the same functional form. Here, we use the predict function from base R to create predicted values. A benefit to using this approach is that the predictor (x) variable remains in its original scale (i.e., 1-5) in the model and plot. In the script below, I also show a different way of specifying the plot function arguments. # Create object xvalues w/ continuous sequence of 1,000 values # to include observed range of uncentered predictor values xvalues &lt;- seq(min(df$ES), # Mininum of observed range max(df$ES), # Maximum of observed range length=1000) # Number of sequence values within range # Estimate predicted values predicted_values &lt;- predict(mod3, # Regression model/equation data.frame(ES=xvalues)) # Values to fit # Create scatterplot for predictor and outcome variables plot(Perf ~ ES, # Specify outcome and predictor data=df, # Specify data frame xlab=&quot;Emotional Stability&quot;, # Set x-axis label ylab=&quot;Supervisor-Rated Performance&quot;) # Set y-axis label # Add estimated regression equation line to scatterplot lines(xvalues, # sequence of predictor values predicted_values, # Insert fitted/predicted values based on model col=&quot;red&quot;) # Set line color to red As you can see, those two options for estimating a polynomial regression model reach the same end. If you’d like to add a cubic term to the model, please do the following. # Estimate polynomial regression model: Option 1 mod4 &lt;- lm(Perf ~ c_ES + I(c_ES^2) + I(c_ES^3), data=df) # Print summary of results summary(mod4) ## ## Call: ## lm(formula = Perf ~ c_ES + I(c_ES^2) + I(c_ES^3), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.4004 -4.2345 0.9338 4.9217 10.9981 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.720359 0.951473 23.879 &lt; 0.0000000000000002 *** ## c_ES 0.664814 0.556168 1.195 0.235 ## I(c_ES^2) -0.474834 0.094512 -5.024 0.00000214 *** ## I(c_ES^3) -0.006664 0.038953 -0.171 0.864 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.288 on 103 degrees of freedom ## Multiple R-squared: 0.2274, Adjusted R-squared: 0.2049 ## F-statistic: 10.11 on 3 and 103 DF, p-value: 0.000006791 Or, an equivalent approach is the following. # Estimate polynomial regression model: Option 2 mod5 &lt;- lm(Perf ~ poly(ES, 3), # Insert 3 for cubic data=df) # Print summary of results summary(mod5) ## ## Call: ## lm(formula = Perf ~ poly(ES, 3), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.4004 -4.2345 0.9338 4.9217 10.9981 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.0486 0.6079 31.334 &lt; 0.0000000000000002 *** ## poly(ES, 3)1 14.1666 6.2884 2.253 0.0264 * ## poly(ES, 3)2 -31.5788 6.2884 -5.022 0.00000216 *** ## poly(ES, 3)3 -1.0758 6.2884 -0.171 0.8645 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.288 on 103 degrees of freedom ## Multiple R-squared: 0.2274, Adjusted R-squared: 0.2049 ## F-statistic: 10.11 on 3 and 103 DF, p-value: 0.000006791 As you can see, adding the cubic term did not explain additional variability in the outcome. Thus, we would conclude in this situation that the association between ES and Perf has a quadratic functional form. Sample Technical Write-Up: Based on a sample of 107 employees (N = 107), We found that the association between emotional stability scores and supervisor ratings of job performance was nonlinear. Using polynomial regression, we found that the quadratic term for emotional stability was statistically significant (b = -.475, p &lt; .001), which implies that there is a statistically significant nonlinear association between ES and Perf that specifically takes a quadratic functional form. Moreover, with the quadratic term included, the model explained 22.7% of the variability in supervisor ratings of job performance, which was an 18.9% improvement over a simple linear regression model that lacked the quadratic term. The functional form of the association can be described as an inverted “U”, such that supervisor ratings of performance are highest for those employees with more moderate levels of emotional stability. 52.2.5 Summary In this chapter, we learned how to explore a nonlinear association by estimating a polynomial regression model. References "],["lassoregression.html", "Chapter 53 Supervised Statistical Learning Using Lasso Regression 53.1 Conceptual Overview 53.2 Tutorial", " Chapter 53 Supervised Statistical Learning Using Lasso Regression In this chapter, we will learn how to apply k-fold cross-validation to train a lasso (LASSO) regression model. 53.1 Conceptual Overview Least absolute shrinkage and selection operator (lasso, Lasso, LASSO) regression is a regularization method and a form of supervised statistical learning (i.e., machine learning) that is often applied when there are many potential predictor variables. Typically, when applying lasso regression the analyst’s primary goal is to improve model prediction, and other scientific research goals goals, such as understanding, explaining, or determining the cause of the outcome/phenomenon of interest, are of little or no interest. In other words, lasso regression can be a great exploratory tool for building a model that accurately predicts a particular outcome, especially when the model includes a large number of predictor variables. Of note, given its emphasis on prediction, there is often little if any attention paid to whether a non-zero lasso regression coefficient is statistically significant; with that said, there has been some preliminary work toward developing significance tests for lasso regression coefficients (Lockhart, Taylor, and Tibshirani 2014). 53.1.1 Shrinkage Lasso regression selects only the most important predictor variables for predicting an outcome by shrinking the regression coefficients associated with the least important predictor variables to zero. In doing so, lasso regression can reduce the amount of shrinkage that occurs when a trained (i.e., estimated) model is applied to new data, where shrinkage refers to a reduction in model performance (e.g., variance explained in the outcome) when applied to a new data sample. In general, lasso regression is most often applied for the following reasons: (a) to avoid overfitting a model to the data on which it was trained (i.e., estimated), which can be problematic with conventional regression techniques (e.g., ordinary least squares linear regression models, generalized linear models), especially when there is a large number of predictor variables, and (b) to select only the most important predictor variables (i.e., features) from a much larger number of predictor variables. 53.1.2 Regularization Lasso regression is a specific regularization method refereed to as a L1 regularization method, and it is related to other regularization methods like ridge regression and elastic net. The purpose of regularization is to reduce the variance of parameter estimates (i.e., regression coefficients), even if such a reduction comes with the cost of introducing or allowing for additional bias; ultimately, this means finding the optimal level of model complexity. 53.1.3 Tuning Lasso regression involves two tuning parameters called alpha and lambda. For lasso regression, the alpha tuning parameter responsible for what is called the mixing percentage and is set to one. When alpha is set to one (\\(\\alpha\\) = 1), we are estimating a lasso regression model, and when alpha is set to zero (\\(\\alpha\\) = 0), we are estimating a ridge regression model. Thus, given that alpha remains constant for lasso regression, all we need to remember is to set alpha to one when specifying the model. The more important turning parameter for lasso regression is lambda (\\(\\lambda\\)), which is our regularization parameter. Different values of lambda are evaluated to arrive at a final model that strikes a balance between model parsimony and accuracy. To achieve this end, the lambda tuning parameter places a constraint on the maximum absolute value of the regression coefficients in the model and adds a penalty to non-zero regression coefficients. So what are the effects of different lambda values? First, when lambda is zero, the model regression coefficients (i.e., parameter estimates) will approximate a conventional (e.g., ordinary least squares regression) model, and no regression coefficients will shrink to zero (i.e., be eliminated). Second, when lambda is large, regression coefficients with smaller absolute values will shrink toward zero. Third, when lambda becomes too large, all regression coefficients shrink to zero, resulting in a very parsimonious but (likely) inaccurate model. Moreover, the value of the lambda tuning parameter influences variance and bias. Specifically, as lambda gets smaller, variance grows larger, where variance refers to the uncertainty (or dispersion) of the regression coefficient estimates. Conversely, as lambda gets larger, bias grows larger, where bias refers to the differences between the regression coefficient estimates and the true population parameters. In lasso regression, variance is minimized at the expense of greater bias. In this way, lasso regression differs from more traditional forms of regression. For example, in ordinary least squares (OLS) regression, bias is minimized at the expense of greater variance. When training a lasso regression model, an array of possible lambda values are used to tune the model. To arrive at an optimal lambda value, model accuracy (e.g., amount of error) is noted at each level of lambda, with examples of model accuracy indices including (pseudo) R2, mean-squared error (MSE), root mean-squared error (RMSE), classification accuracy, and Cohen’s kappa (\\(\\kappa\\)) (see Figure 1 for an example); the model accuracy index used will depend, in part, on the type of LASSO regression model specified (e.g., linear, logistic). Given that larger lambda result in a larger penalty to non-zero regression coefficients, model parsimony is also affected as model lambda is adjusted; specifically, models become more parsimonious (i.e., fewer predictor variables with non-zero regression coefficients) as lambda increases in magnitude, but at some point, model predictive accuracy starts to suffer when a model becomes too parsimonious and most (or all) of regression coefficients fall to zero (see Figure 2 for an example). Consequently, the optimal lambda value will strike a balance between model accuracy and model parsimony. [Note: To facilitate interpretation, we often apply a logarithmic (log) transformation to lambda.] This plot is an example of a logarithmic tranformation of lambda in relation to mean-squared error (MSE). This plot is an example of a logarithmic tranformation of lambda in relation to regression coefficient magnitude, where x1-x10 refer to 10 predictor variables 53.1.4 Model Type Selection An important part of the lasso regression model building process is selecting the appropriate model type. Often, the characteristics of the outcome variable will inform which type of model should be used within lasso regression. For example, if the outcome variable is continuous in terms of its measurement scale (i.e., interval or ratio scale), then a linear model is likely appropriate. As another example, if the outcome variable is dichotomous (i.e., binary), then a generalized linear model like a logistic regression model (i.e., logit model) is likely appropriate. Traditionally, lasso regression has been applied to linear regression models; however, it can, however, also be applied to other families of models, such as generalized linear models. The model type selected will also influence what statistical assumptions should be met; with that said, lasso regression can help address multicollinearity that can pose problems for traditional forms of regression (e.g., OLS linear, logistic). 53.1.5 Cross-Validation It is advisable to use cross-validation when training a model, as cross-validation can improve model prediction performance and how well the model generalizes to other data from the same population. There are different approaches to carrying out cross-validation, with one of the simplest being simple empirical cross-validation, which you can learn more about in the chapter on statistical and empirical cross-validation. Another approach is called k-fold cross-validation, which is quite popular among practitioners and is introduced in the chapter on k-fold cross-validation applied to logistic regression. In this chapter, we will apply lasso regression using a k-fold cross-validation framework, as this approach is useful when tuning the lambda parameter. Regardless of the type of cross-validation used, it is advisable to apply cross-validation in a predictive analytics framework, which is described next. 53.1.6 Predictive Analytics As a reminder, true predictive analytics (predictive modeling) involves training (estimating, building) a model in one or more samples (e.g., training data) and then evaluating (testing) how well the model performs in a separate sample (i.e., test data) from the same population. The term predictive analytics is a big umbrella term, and predictive analytics can be applied to many different types of models, including lasso regression. When building a model using a predictive analytics framework, one of our goals is to minimize prediction errors (i.e., improve prediction accuracy) when the model is applied to “fresh” or “new” data (e.g., test data). Fewer prediction errors when applying the model to new data indicate that the model is more accurate. At the same time, we want to avoid overfitting our model such that it predicts with a high level of accuracy in our training data but doesn’t generalize well to our test data; applying a trained model to new data can help us evaluate with the extent to which we might have overfit a model to the original data. Fortunately, as described above, a major selling point of lasso regression is that it can help reduce model overfitting. 53.1.7 Conceptual Video For a more in-depth review of lasso regression, please check out the following conceptual video. Link to conceptual video: https://youtu.be/hZHcmLW5JO8 53.2 Tutorial This chapter’s tutorial demonstrates training a lasso regression model using k-fold cross-validation. 53.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/5GZ5BHOugBQ Additionally, in the following video tutorial, I show how to compare the results of an OLS multiple linear regression model to a lasso regression model, where both are trained using k-fold cross-validation. Link to video tutorial: https://youtu.be/NUfbl7ijZ0Q 53.2.2 Functions &amp; Packages Introduced Function Package set.seed base R nrow base R as.data.frame base R createDataPartition caret trainControl caret seq base R train caret log base R plot base R summary base R varImp caret ggplot ggplot2 predict base R print base R RMSE caret R2 caret coef base R function base R data.frame base R matrix base R colnames base R rownames base R round base R c base R list base R as.table base R resamples caret compare_models caret 53.2.3 Initial Steps If you haven’t already, save the file called “lasso.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “lasso.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;lasso.csv&quot;) ## Rows: 1000 Columns: 37 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (37): y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;y&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; &quot;x6&quot; &quot;x7&quot; &quot;x8&quot; &quot;x9&quot; &quot;x10&quot; &quot;x11&quot; &quot;x12&quot; &quot;x13&quot; &quot;x14&quot; &quot;x15&quot; &quot;x16&quot; &quot;x17&quot; &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; &quot;x21&quot; ## [23] &quot;x22&quot; &quot;x23&quot; &quot;x24&quot; &quot;x25&quot; &quot;x26&quot; &quot;x27&quot; &quot;x28&quot; &quot;x29&quot; &quot;x30&quot; &quot;x31&quot; &quot;x32&quot; &quot;x33&quot; &quot;x34&quot; &quot;x35&quot; &quot;x36&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spc_tbl_ [1,000 × 37] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ y : num [1:1000] -0.886 0.763 0.209 1.746 0.22 ... ## $ x1 : num [1:1000] -0.57 1.763 -0.016 1.476 -1.288 ... ## $ x2 : num [1:1000] -0.669 1.077 0.264 0.153 -1.091 ... ## $ x3 : num [1:1000] -0.978 1.094 -1.302 1.804 0.226 ... ## $ x4 : num [1:1000] -0.948 2.427 -0.493 -1 0.713 ... ## $ x5 : num [1:1000] -1.703 1.011 -0.049 -0.768 1.731 ... ## $ x6 : num [1:1000] -0.623 2.574 -0.61 0.008 -1.21 ... ## $ x7 : num [1:1000] 0.176 1.32 -0.565 -0.083 1.125 ... ## $ x8 : num [1:1000] -1.624 2.917 -1.626 0.478 -1.053 ... ## $ x9 : num [1:1000] -1.189 1.997 -0.95 0.204 0.622 ... ## $ x10: num [1:1000] -0.519 0.882 0.676 1.182 0.234 ... ## $ x11: num [1:1000] -0.495 1.155 0.531 0.687 0.596 ... ## $ x12: num [1:1000] 0.008 2.895 0.434 1.195 -0.004 ... ## $ x13: num [1:1000] 0.269 1.996 0.499 -0.523 -1.714 ... ## $ x14: num [1:1000] 0.017 3.665 1.998 -2.024 0.754 ... ## $ x15: num [1:1000] -0.32 0.885 -0.776 4.072 -0.335 ... ## $ x16: num [1:1000] -0.849 1.132 -0.404 0.658 -0.154 ... ## $ x17: num [1:1000] -4.33 6.26 1.66 1.11 -2.2 ... ## $ x18: num [1:1000] -2.405 6.338 1.684 6.423 0.379 ... ## $ x19: num [1:1000] -0.195 0.236 0.011 0.036 0.082 -0.419 -0.327 0.034 -0.422 -0.164 ... ## $ x20: num [1:1000] 0.634 0.356 0.422 1.693 0.593 ... ## $ x21: num [1:1000] -0.853 1.685 0.557 -0.498 1.248 ... ## $ x22: num [1:1000] -1.537 1.771 -0.882 -0.316 -0.028 ... ## $ x23: num [1:1000] -0.675 0.715 0.623 -0.885 0.246 ... ## $ x24: num [1:1000] -1.077 0.007 -0.223 0.315 1.728 ... ## $ x25: num [1:1000] -0.851 1.382 -0.1 0.384 -0.212 ... ## $ x26: num [1:1000] -1.578 0.172 1.537 1.268 0.616 ... ## $ x27: num [1:1000] -2 2.62 -2.11 1.64 2.14 ... ## $ x28: num [1:1000] -0.641 3.905 -0.914 0.357 0.266 ... ## $ x29: num [1:1000] -1.223 1.874 0.792 0.509 1.008 ... ## $ x30: num [1:1000] -0.615 1.862 0.64 0.483 1.469 ... ## $ x31: num [1:1000] -0.751 1.291 0.147 -1.321 0.268 ... ## $ x32: num [1:1000] -0.464 3.085 0.558 0.917 0.762 ... ## $ x33: num [1:1000] -1.749 0.424 0.584 1.264 1.445 ... ## $ x34: num [1:1000] -0.841 -0.189 0.58 2.599 2.538 ... ## $ x35: num [1:1000] -0.933 0.448 -1.856 -0.091 2.658 ... ## $ x36: num [1:1000] -1.296 1.706 0.707 0.454 1.118 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. y = col_double(), ## .. x1 = col_double(), ## .. x2 = col_double(), ## .. x3 = col_double(), ## .. x4 = col_double(), ## .. x5 = col_double(), ## .. x6 = col_double(), ## .. x7 = col_double(), ## .. x8 = col_double(), ## .. x9 = col_double(), ## .. x10 = col_double(), ## .. x11 = col_double(), ## .. x12 = col_double(), ## .. x13 = col_double(), ## .. x14 = col_double(), ## .. x15 = col_double(), ## .. x16 = col_double(), ## .. x17 = col_double(), ## .. x18 = col_double(), ## .. x19 = col_double(), ## .. x20 = col_double(), ## .. x21 = col_double(), ## .. x22 = col_double(), ## .. x23 = col_double(), ## .. x24 = col_double(), ## .. x25 = col_double(), ## .. x26 = col_double(), ## .. x27 = col_double(), ## .. x28 = col_double(), ## .. x29 = col_double(), ## .. x30 = col_double(), ## .. x31 = col_double(), ## .. x32 = col_double(), ## .. x33 = col_double(), ## .. x34 = col_double(), ## .. x35 = col_double(), ## .. x36 = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 37 ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.886 -0.57 -0.669 -0.978 -0.948 -1.70 -0.623 0.176 -1.62 -1.19 -0.519 -0.495 0.008 0.269 0.017 -0.32 -0.849 -4.32 -2.40 ## 2 0.763 1.76 1.08 1.09 2.43 1.01 2.57 1.32 2.92 2.00 0.882 1.16 2.90 2.00 3.66 0.885 1.13 6.26 6.34 ## 3 0.209 -0.016 0.264 -1.30 -0.493 -0.049 -0.61 -0.565 -1.63 -0.95 0.676 0.531 0.434 0.499 2.00 -0.776 -0.404 1.66 1.68 ## 4 1.75 1.48 0.153 1.80 -1 -0.768 0.008 -0.083 0.478 0.204 1.18 0.687 1.20 -0.523 -2.02 4.07 0.658 1.11 6.42 ## 5 0.22 -1.29 -1.09 0.226 0.713 1.73 -1.21 1.12 -1.05 0.622 0.234 0.596 -0.004 -1.71 0.754 -0.335 -0.154 -2.20 0.379 ## 6 -0.458 -0.619 -1.82 -1.35 -0.036 -3.79 -2.02 -0.072 -1.26 -0.999 -1.12 -2.61 -1.85 -2.16 0.311 -5.68 -0.409 -0.999 -3.67 ## # ℹ 18 more variables: x19 &lt;dbl&gt;, x20 &lt;dbl&gt;, x21 &lt;dbl&gt;, x22 &lt;dbl&gt;, x23 &lt;dbl&gt;, x24 &lt;dbl&gt;, x25 &lt;dbl&gt;, x26 &lt;dbl&gt;, x27 &lt;dbl&gt;, x28 &lt;dbl&gt;, ## # x29 &lt;dbl&gt;, x30 &lt;dbl&gt;, x31 &lt;dbl&gt;, x32 &lt;dbl&gt;, x33 &lt;dbl&gt;, x34 &lt;dbl&gt;, x35 &lt;dbl&gt;, x36 &lt;dbl&gt; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 1000 The data frame (df) has 1000 cases and the following 37 variables: y and x1 through x36. We’ll use y as our outcome variable and x1 - x36 as our predictor variables. Per the output of the str (structure) function above, all of the variables are of type numeric. Sticking with the theme of performance management for this part of the book, we’ll imagine that the y variable is some kind of measure of job performance. 53.2.4 Process Overview In this tutorial, we will implement lasso regression using k-fold cross-validation. If you wish to learn more about k-fold cross-validation, please refer to the chapter on statistical and empirical cross-validation. As a reminder, you can perform k-fold cross-validation with different types of statistical models. We will also engage in true predictive analytics (i.e., predictive modeling), such that we will partition the data into training and test data, train the model on the training data using k-fold cross-validation, and evaluate the final model’s predictive accuracy based on the test data. 53.2.5 Partition the Data Let’s begin by randomly partitioning (i.e., splitting) our data into training and test data frames. To get this process started, we will install and access the caret package (Kuhn 2021) (if you haven’t already). The caret package has a number of different functions that are useful for implementing predictive analytics and a variety of machine learning models. # Install caret package if you haven&#39;t already install.packages(&quot;caret&quot;) # Access caret package library(caret) Because we’ll also be using the glmnet package for our lasso regression, we need to install that package as well. # Install glmnet package if you haven&#39;t already install.packages(&quot;glmnet&quot;) Using the createDataPartition function from the caret package, we will partition the data such that 80% of the cases will be randomly assigned to one split, and the remaining 20% of cases will be assigned to the other split. However, immediately before doing so, I recommend using the set.seed function from base R and include a number of your choosing as the sole parenthetical argument; this will create “random” operations that are reproducible. With your seed set, create a name for the index of unique cases that will identify which cases should be included in the first split involving 80% of the original data frame cases; here, I call this matrix object index and assign values to it using the &lt;- operator. To the right of the &lt;-, enter the name of the createDataPartition function. As the first argument, type the name of the data frame (df), followed by the $ and the name of the outcome variable (y). [Note: I have noticed that the createDataPartition sometimes adds an extra case to the partitioned data index object, which seems to occur more frequently when the outcome variable is of type factor as opposed to numeric, integer, or character. An extra case added to the training data frame is not a big deal when we have this many cases, so if it happens, we won’t worry about it. If you would like to avoid this, you can try converting the outcome variable from type factor to type numeric, integer, or character, whichever makes the most sense given the variable. You can always convert the outcome back to type factor after this process, which I demonstrate in this tutorial. This “fix” works some of the time but not all of the time.] As a the second argument, set p=.8 to note that you wish to randomly select 80% (.8) of the cases from the data frame. As the third argument, type list=FALSE to indicate that we want to the values in matrix form to facilitate reference to them in a subsequent step. As the fourth argument, type times=1 to indicate that we only want to request a single partition of the data frame. # Set seed for reproducible random partitioning of data set.seed(1985) # Partition data and create index matrix of selected values index &lt;- createDataPartition(df$y, p=.8, list=FALSE, times=1) Now it’s time to use the index matrix object we created above to assign cases to the training data frame (which we will name train_df) and to the testing data frame (which we will name test_df). Beginning with the training data frame, we assign the random sample of 80% of the original data frame (df) to the train_df object by using the &lt;- operator. Specifically, we specify the name of the original data frame (df), followed by brackets ([ ]). In the brackets, include the name of the index object we created above, followed by a comma (,). The placement of the index object before the comma indicates that we are selecting rows with the selected unique row numbers (i.e., index) from the index matrix. Next, we do essentially the same thing with the test_df, except that we insert a minus (-) symbol before index to indicate that we don’t want cases associated with the unique row numbers contained in the index matrix. # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] If you received the error message shown below, then please proceed to convert the original data frame df from a tibble to a conventional data frame; after that, repeat the steps above from which you received the error message originally, which I will do below. If you did not receive this message, then you can skip this step and proceed on to the step in which you verify the number of rows in each data frame. \\(\\color{red}{\\text{Error: `i` must have one dimension, not 2.}}\\) # Convert data frame object (df) to a conventional data frame object df &lt;- as.data.frame(df) # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] To check our work and to be sure that 80% of the cases ended up in the train_df data frame and the remaining 20% ended up in the test_df data frame, let’s apply the nrow function from base R. # Verify number of rows (cases) in each data frame nrow(train_df) ## [1] 801 nrow(test_df) ## [1] 199 Indeed, 801 (~ 80%) of the original 1,000 cases ended up in the train_df data frame, and 199 (~ 20%) of the original 1,000 cases ended up in the test_df data frame, which is close enough to a perfect 80/20 partition. 53.2.6 Specify k-Fold Cross-Validation Now it’s time to specify the type of training method we want to apply, which in this tutorial is k-fold cross-validation. Let’s create and name our specifications object ctrlspecs using the &lt;- operator. To the right of the &lt;- operator, type the name of the trainControl function from the caret package. As the first argument, specify the method by setting method=\"cv\", where \"cv\" represents cross-validation. As the second argument, set number=10 to indicate that we want 10 folds (i.e., 10 resamples), which means we will specifically use 10-fold cross-validation. As the third argument, type savePredictions=\"all\" to save all of the hold-out predictions for each of our resamples from the train_df data frame, where hold-out predictions refer to those cases that weren’t used in training each iteration of the model based on each fold of data but were used for validating the model trained at each fold. # Specify 10-fold cross-validation as training method ctrlspecs &lt;- trainControl(method=&quot;cv&quot;, number=10, savePredictions=&quot;all&quot;) 53.2.7 Specify and Train Lasso Regression Model Before we specify our lasso regression model, we need to create a vector of potential lambda tuning parameter values. We’ll begin by creating an object to which we can assign the potential values; let’s make things clear and simple by naming this object lambda_vector using the &lt;- operator. Next, we will create a vector of possible lambda by setting the base to 10 and then exponentiating it by a sequence of 500 values ranging from 5 to -5. Remember, that the ^ operator is how we assign exponents. The seq (sequence) function from base R is specified with the first argument being one end of the sequence (e.g., 5) and the second argument being the other end of the sequence (e.g., -5); the upper and lower limits of the sequence probably do not need to be this wide, but to be on the safe side, I recommend using these same limits for other sequences of lambdas. The third argument of the seq function is the length of the sequence in terms of the number of values generated within the sequence; let’s set the length to 500 (length=500); I also recommend using this same length of 500 for other vectors of potential lambdas you might generate. We will reference this lambda_vector object we created when we specify our model training parameters. # Create vector of potential lambda values lambda_vector &lt;- 10^seq(5, -5, length=500) To specify (and ultimately train) our lasso regression model, we will use the train function from the caret package. I recommend using the set.seed function from base R and include a number of your choosing as the sole parenthetical argument; this will create “random” operations that are reproducible. Moving on to the train function, come up with a name for your model object; here, I name the model object model1 and use the &lt;- operator to assign the model specifications to it. Next, type the name of the train function. As the first argument, let’s specify our linear model. To the left of the tilde (~) symbol, type the name of the continuous outcome variable called y. To the right of the tilde (~) symbol, type ., which will automatically specify all remaining variables in the data frame as predictor variables; if you don’t want to specify all remaining variables as predictor variables, then you can do it the old-fashioned way by entering each variable name separated by a + symbol. As the second argument in the train function, type data= followed by the name of the training data frame object (train_df). As the third argument, use the preProcess argument to mean center (i.e., subtract the mean of the variable from each value) and standardize (i.e., divide each value of the variable by the variable’s standard deviation) the predictor variables prior to training the model, which is advisable when using lasso regression; to do so, use the c (combine) function from base R with \"center\" and \"scale\" as the two arguments. As the fourth argument, specify method=\"glmnet\", which will allow us to access regularization method models such as lasso regression or ridge regression. As the fifth argument, use tuneGrid to specify the alpha and lambda tuning parameters. Specifically, type the name of the expand.grid function from base R with alpha set to 1 (alpha=1) as the first argument (to specify that we want a lasso regression model) and lambda set to lambda_vector (which is the vector of potential lambda values we generated) as the second argument. Behind the scenes, this specification of the expand.grid function creates a data frame with all of our potential lambda values as unique rows and alpha set as a constant of 1 for each corresponding row. As the sixth argument, type trControl= followed by the name of the object that includes your k-fold cross-validation specifications, which we created above (ctrlspecs). As the final argument, specify na.action=na.omit, which will listwise delete any cases with missing data on any of the variables specified in the model; we don’t have any missing data in our data, but if we did, we would need to specify this argument. [When lasso regression models are estimated, the train function by default selects the optimal model based on root mean-squared error (RMSE), but this can be changed to R2 by including the argument weights=\"Rsquared\".] Note: Currently, available caret package models do not support maximum likelihood (ML) estimation, where ML has advantages when data are missing (completely) at random. It is beyond the scope of this tutorial to show how ML might be incorporated, but the caret package handbook has information on how you can specify your own model and presumably an outside estimator like ML. # Set seed for reproducible random selection and assignment operations set.seed(1985) # Specify lasso regression model to be estimated using training data # and k-fold cross-validation process model1 &lt;- train(y ~ ., data=train_df, preProcess=c(&quot;center&quot;,&quot;scale&quot;), method=&quot;glmnet&quot;, tuneGrid=expand.grid(alpha=1, lambda=lambda_vector), trControl=ctrlspecs, na.action=na.omit) Our 10-fold cross-validation and model training process that we implemented above did the following. First, at each fold (i.e., resample), all possible lambda values from our lambda_vector object were evaluated based on the estimated model’s predictive accuracy/performance in the kth holdout validation sample for that fold, with model accuracy/performance signified by root mean-squared error (RMSE), R2 (Rsquared), and mean absolute error (MAE); however, by default, the training algorithm selected the lambda value in which RMSE, specifically, was lowest (i.e., minimized). As noted above, this evaluation criterion can be changed to R2 (Rsquared) by including the argument weights=\"Rsquared\" in the train function. Second, after the first step was repeated for all 10 folds in the 10-fold cross-validation process, the average of the best lambda at each fold was computed to arrive at the optimal lambda. Third, a final lasso regression model was estimated based on the complete train_df data frame, and the optimal lambda value was applied to determine the final best model, including the regression coefficients; the final model accuracy/performance metrics (e.g., RMSE, Rsquared) represent an average of the metrics across the 10 folds based on the optimal lambda value applied to each fold. Now it’s time to sift through the results. Let’s begin by finding out what the optimal lambda value is based on the model training process we just carried out. To find out, we can specify the name of the model object we trained (model1) followed by the $ operator and bestTune, which will retrieve the best alpha and lambda tuning parameter values. We already know alpha is equal to 1 because we set it as a constant to specify we wanted a lasso regression model, we but will anxiously await what the optimal lambda value is by running the following command. # Best tuning parameters (alpha, lambda) model1$bestTune ## alpha lambda ## 162 1 0.0168443 As you can see in the output above, alpha is indeed 1, and the optimal lambda is .017 (after some rounding). Given that we really only wanted to know the optimal lambda value, we can request that directly by adding $lambda to the previous command. # Best lambda tuning parameter model1$bestTune$lambda ## [1] 0.0168443 Next, let’s look at the regression coefficients for the final model in which the optimal lambda value was applied. To do so, type the name of the coef function from base R, and as the first argument, specify the name of the model object we trained (model1) followed by the $ operator and finalModel, where the latter calls up the final model estimated based on the complete train_df data frame as the final phase of the 10-fold cross-validation process. As the second argument, specify the name of the model object we trained (model1) followed by the $ operator and bestTune$lambda, which when combined with the first argument, limits the retrieved regression coefficients to only those associated with the optimal lambda value. # Lasso regression model coefficients coef(model1$finalModel, model1$bestTune$lambda) ## 37 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 0.001559301 ## x1 0.105710879 ## x2 0.132145274 ## x3 0.076835793 ## x4 0.137463746 ## x5 . ## x6 0.086437871 ## x7 0.064530835 ## x8 0.107235443 ## x9 0.023979445 ## x10 . ## x11 . ## x12 0.022841341 ## x13 -0.005409866 ## x14 0.070845008 ## x15 . ## x16 0.018059851 ## x17 . ## x18 0.038648205 ## x19 0.036944920 ## x20 -0.011799344 ## x21 0.040398213 ## x22 . ## x23 . ## x24 . ## x25 . ## x26 . ## x27 . ## x28 . ## x29 0.027449222 ## x30 . ## x31 0.012105496 ## x32 0.067979921 ## x33 -0.003821760 ## x34 -0.076544682 ## x35 -0.049270620 ## x36 0.075633216 As you can see, the optimal lambda value shrunk a number of the regression coefficients to zero, where zero is represented by a period (.). In this way, you can see how lasso regression can be used for variable selection (i.e., feature selection), as only the most predictive predictor variables had non-zero regression coefficients. The next step is not required, but I find it helpful to understand visually how the lambda value influences model prediction error (i.e., accuracy), as indicated by RMSE. For interpretation purposes, however, we perform a logarithmic transformation on the lambda values using the log function from base R. To create this data visualization, we will use the plot function from base R. As the first argument, introduce the log function with model1$results$lambda as the sole parenthetical argument, where the latter calls up our vector of 500 potential lambda values. As the second argument, type model1$results$RMSE to retrieve the vector of RMSE values (averaged across the 10 folds) based on applying each of our potential lambda values at each fold. As the third argument, we’ll type xlab= followed by a meaningful label for the x-axis (in quotation marks). As the fourth argument, we’ll type ylab= followed by a meaningful label for the y-axis (in quotation marks). As the final argument, I added xlim= followed by c(-5, 0) to constrain the x-axis limits to -5 and 0, respectively; you don’t necessarily need to do this, and if you do, you’ll likely need to play around with those limits to find ones that focus in on the more meaningful areas of the plot. # Plot log(lambda) &amp; root mean-squared error (RMSE) plot(log(model1$results$lambda), model1$results$RMSE, xlab=&quot;log(lambda)&quot;, ylab=&quot;Root Mean-Squared Error (RMSE)&quot;, xlim=c(-5,0) ) Remember, our goal is to minimize model prediction error and, specifically, RMSE. As you can see in the plot above, the lowest RMSE value (~ .84) seems to occur around a log(lambda) value of approximately -4 or so. To see what that exact value is, let’s apply a logarithmic transformation to our optimal lambda using the log function. # Best tuned lambda log(model1$bestTune$lambda) ## [1] -4.083743 As you can see, the log of our lambda is -4.084 (with rounding), which aligns with our visual estimate from the plot of about -4. Interestingly, I haven’t been able to find an existing function or command associated with the caret package and the glmnet model function that makes just the average RMSE and Rsquared values across the k (i.e., 10) folds easily accessible, and specifically for the optimal lambda value. Thus, we’re going to do this the old-fashioned way (or at least more cumbersome way) by creating our own functions to carry out those operations. For space considerations, I’m not going to explain each aspect of the functions, but I encourage you to take a peek at the operations within the braces ({ }) to see if you can follow the logic. First, let’s create a function called RMSE_lasso. # Create function to identify RMSE for best lambda, # where x = RMSE vector, y = lambda vector, &amp; z = optimal lambda value RMSE_lasso &lt;- function(x, y, z){ temp &lt;- data.frame(x, y) colnames(temp) &lt;- c(&quot;RMSE&quot;, &quot;lambda_val&quot;) rownum &lt;- which(temp$lambda_val==z) print(temp[rownum,]$RMSE) } Now, let’s apply the RMSE_lasso to our data by plugging the RMSE and lambda vectors from our final model, along with the optimal lambda value. # Apply newly created Rsquared_lasso function RMSE_lasso(x=model1$results$RMSE, # x = RMSE vector y=model1$results$lambda, # y = lambda vector z=model1$bestTune$lambda) # z = optimal lambda value ## [1] 0.8400556 Our RMSE value based on the final model with the optimal lambda is .840 (with rounding). Second, let’s create a function called Rsquared_lasso # Create function to identify Rsquared for best lambda, # where x = Rsquared vector, y = lambda vector, &amp; z = optimal lambda value Rsquared_lasso &lt;- function(x, y, z){ temp &lt;- data.frame(x, y) colnames(temp) &lt;- c(&quot;Rsquared&quot;, &quot;lambda_val&quot;) rownum &lt;- which(temp$lambda_val==z) print(temp[rownum,]$Rsquared) } Now, let’s apply the Rsquared_lasso to our data by plugging the Rsquared and lambda vectors from our final model, along with the optimal lambda value. # Apply newly created Rsquared_lasso function Rsquared_lasso(x=model1$results$Rsquared, # x = Rsquared vector y=model1$results$lambda, # y = lambda vector z=model1$bestTune$lambda) # z = optimal lambda value ## [1] 0.4150716 Our Rsquared value based on the final model with the optimal lambda is .415 (with rounding). We’ll revisit these RMSE_lasso and Rsquared_lasso functions a little bit later on in this tutorial. Moving on, let’s investigate the regression coefficients from our final model based on the optimal lambda value. Specifically, we will evaluate the importance of the various predictor variables in our model. Variables with higher importance values contribute more to estimation. To do so, we’ll use the varImp function from the caret package, and as the sole parenthetical argument, we will type the name of our trained model (model1). # Estimate the importance of different predictor variables varImp(model1) ## glmnet variable importance ## ## only 20 most important variables shown (out of 36) ## ## Overall ## x4 100.000 ## x2 96.131 ## x8 78.010 ## x1 76.901 ## x6 62.880 ## x3 55.895 ## x34 55.684 ## x36 55.020 ## x14 51.537 ## x32 49.453 ## x7 46.944 ## x35 35.843 ## x21 29.388 ## x18 28.115 ## x19 26.876 ## x29 19.968 ## x9 17.444 ## x12 16.616 ## x16 13.138 ## x31 8.806 As you can see, variables x4 and x2 are two of the most important predictor variables in terms of estimating the outcome variable y. Let’s create a horizontal bar chart to view these same results. Simply call up the ggplot2 package (Wickham 2016) using the library function from base R (and if you haven’t already, be sure to install the package first: install.packages(\"ggplot2\")). Next, type the name of the ggplot function with varImp function we specified above as the sole parenthetical argument. # Visualize the importance of different predictor variables library(ggplot2) ggplot(varImp(model1)) Note that the y-axis is labeled “Feature,” which is just another way of saying “predictor variable” in this context. Remember way back when in this tutorial when we partitioned our data into the train_df and test_df? Well, now we are going to apply our lasso regression model that we trained using k-fold cross-validation and the train_df data frame to our test_df data frame. First, using our final lasso regression model, we need to estimate predicted values for individuals’ scores on y based the predictor variable values in the test_df data. Let’s call the object to which we will assign these predictions predictions by using the &lt;- operator. To the right of the &lt;- operator, type the name of the predict function from base R. As the first argument, type the name of the model object we built using the train function (model1). As the second argument, type newdata= followed by the name of the testing data frame (test_df). # Predict outcome using model from training data based on testing data predictions1 &lt;- predict(model1, newdata=test_df) Next, let’s calculate the RMSE and Rsquared values based on our predictions. At this point, you might be saying, “Wait, didn’t we just do that a few steps earlier?” Well, yes, we did previously compute RMSE and Rsquared values based on our optimal lambda value, but we did so based on the train_df data frame, and these values represent their averages across the k (i.e., 10) folds for the given lambda value. The RMSE and Rsquared values we are about to compute differ because they will be based on how well our final model predicts the outcome when applied to the holdout test_df data frame. We will use the &lt;- operator to name the data frame object containing the RMSE and Rsquared values, which we can reference later on; let’s call this object mod1perf. To the right of the &lt;- operator, type the name of the data.frame function from base R, which will allow us to create a data frame object. As the first argument, let’s create the first column of data and name this column RMSE followed by the = operator and the RMSE function from caret with our predictions1 vector as the first argument and the vector containing our outcome (y) values from our test_df data frame as the second argument (test_df$y). As the second argument of the data.frame function, create the second column of data and name this column Rsquared followed by the = operator and the R2 function from caret with our predictions1 vector as the first argument and the vector containing our outcome (y) values from our test_df data frame as the second argument (test_df$y). As a final step, use the print function from base R to print the mod1perf data frame object to our console. # Model performance/accuracy mod1perf &lt;- data.frame(RMSE=RMSE(predictions1, test_df$y), Rsquared=R2(predictions1, test_df$y)) # Print model performance/accuracy results print(mod1perf) ## RMSE Rsquared ## 1 0.8219381 0.4700103 As you can see, using predictive analytics, we found that our lasso regression model yielded an RMSE of .822 and an Rsquared of .470 (with rounding) when applied to the holdout test_df data frame. Note that our RMSE value is lower and our Rsquared value is higher when we apply the model to new data (test_df), as compared to the same values based on the old data (train_df); this illustrates one of the advantages of using k-fold cross-validation and lasso regression, as they help to reduce shrinkage in terms of model fit/performance when a model is applied to new data. Below I provide some generic benchmarks for interpreting the practical significance of the Rsquared (R2) values. R2 Description .01 Small .09 Medium .25 Large Finally, we can also estimate 95% prediction intervals. # Estimate 95% prediction intervals pred.int &lt;- predict(model1, newdata=test_df, interval=&quot;prediction&quot;) # Join fitted (predicted) values and upper and lower prediction interval values to data frame test_df &lt;- cbind(test_df, pred.int) # Print first 6 rows head(test_df) ## y x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 ## 6 -0.458 -0.619 -1.822 -1.348 -0.036 -3.787 -2.025 -0.072 -1.263 -0.999 -1.121 -2.608 -1.849 -2.163 0.311 -5.685 -0.409 -0.999 -3.667 ## 12 -0.150 0.841 0.449 0.574 0.762 0.741 -0.511 0.062 1.318 1.177 -0.842 -0.953 -0.824 0.603 -0.106 0.264 0.605 -2.714 0.646 ## 19 -0.182 0.039 -0.283 -0.512 -0.872 -0.650 0.485 -1.231 -1.971 0.936 -0.749 -1.812 -0.138 -1.548 0.646 -0.769 0.630 5.730 4.166 ## 25 2.373 1.123 1.160 0.310 1.276 0.422 1.447 0.328 0.777 0.713 0.409 0.823 1.081 2.032 3.576 -0.527 0.566 2.347 5.440 ## 33 2.304 0.535 0.626 -0.031 -0.231 1.821 -0.836 0.147 1.840 1.076 -0.835 0.040 0.143 -0.403 -2.242 0.355 0.407 3.195 1.348 ## 37 0.679 0.843 0.730 0.469 0.840 0.085 0.670 0.930 0.894 1.584 0.605 0.331 0.578 -0.761 2.498 3.071 0.546 6.986 -1.552 ## x19 x20 x21 x22 x23 x24 x25 x26 x27 x28 x29 x30 x31 x32 x33 x34 x35 x36 ## 6 -0.419 -0.516 -1.090 -0.493 -0.106 -2.004 0.371 0.561 -0.411 -2.154 0.332 -0.683 0.592 -1.762 -1.748 2.331 -1.404 -2.973 ## 12 -0.236 -1.133 -0.216 -0.014 -0.242 -0.227 0.860 -0.279 0.088 1.367 -2.066 -0.651 -0.750 0.483 -0.142 2.579 0.310 -2.223 ## 19 0.202 0.812 -1.785 -1.057 -0.014 -0.128 -2.299 0.027 -0.778 0.615 -2.827 -1.479 0.832 -0.736 -0.856 -0.351 -2.330 -1.208 ## 25 0.259 -1.100 0.179 -0.497 -1.819 -0.033 -0.031 0.596 0.835 -0.113 0.394 -1.702 0.701 1.204 0.844 -0.205 0.257 2.426 ## 33 -0.042 -0.475 0.488 -0.117 2.058 -0.186 -0.450 0.758 -0.999 1.564 -0.949 1.817 1.786 1.041 -0.176 -0.525 2.055 0.232 ## 37 0.183 1.617 1.013 1.045 -1.877 0.354 1.543 -0.803 0.082 -0.748 1.446 -0.856 -0.214 0.572 2.362 1.199 1.440 1.778 ## pred.int ## 6 -1.147386264 ## 12 -0.003962978 ## 19 -0.325320211 ## 25 1.148565410 ## 33 0.153837839 ## 37 0.690189801 53.2.8 Optional: Compare to Lasso Model to OLS Multiple Linear Regression Model This next section is optional, so proceed if you wish. In the event you are interested, we will learn how to train a conventional ordinary least squares (OLS) multiple linear regression (MLR) model based on the same variables that we used for the LASSO regression. Further, just like we did with LASSO regression, we will train the OLS MLR model using k-fold cross-validation. If you need a refresher on OLS MLR, please refer to the chapter on estimating incremental validity using multiple linear regression. Note, however, that when using k-fold cross-validation for an OLS MLR model (or other non-machine learning model), we are really just interested in getting a glimpse at how well the model will likely perform when given new data at a later time. We will use the same k-fold cross-validation specifications object that we created for our LASSO regression model training (ctrlspecs), so if you’re starting a fresh section with this section, then you will need to go to the previous section in this section in which we created the ctrlspecs object and run it. Just as we did before, be sure to set a seed using the set.seed function prior to training your model. Let’s call this OLS MLR training model object model2 using the &lt;- operator. To the right of the &lt;-, type the name of the train function from the caret package. With the exception of the method=\"lm\" argument, the rest of the arguments should look familiar. The method=\"lm\" argument simply tells the function that we want to use the lm (linear model) function from base R to estimate our OLS MLR model. After specifying the train function and creating the model2 object, use the print function to print the results of the model to your console. # Set seed for reproducible random selection and assignment operations set.seed(1985) # Specify OLS MLR model to be estimated using training data # and k-fold cross-validation process model2 &lt;- train(y ~ ., data=train_df, preProcess=c(&quot;center&quot;,&quot;scale&quot;), method=&quot;lm&quot;, trControl=ctrlspecs, na.action=na.omit) # Model fit print(model2) ## Linear Regression ## ## 801 samples ## 36 predictor ## ## Pre-processing: centered (36), scaled (36) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 721, 721, 721, 721, 720, 721, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 0.8467902 0.4069315 0.6730339 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE As you can see in the output, the average RMSE across the 10 folds is .847, and the average Rsquared is .407. Let’s take a peek at the regression coefficients for the OLS MLR model estimated based on the entire train_df data frame. To do so, type the name of the summary function from base R with the name of the model object (model2) as the sole parenthetical argument. # OLS MLR model coefficients with statistical significance tests summary(model2) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.31440 -0.54251 0.05817 0.52693 2.95915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0015593 0.0292866 0.053 0.95755 ## x1 0.1064869 0.0393948 2.703 0.00702 ** ## x2 0.1288509 0.0415154 3.104 0.00198 ** ## x3 0.0844271 0.0405929 2.080 0.03787 * ## x4 0.1374366 0.0444428 3.092 0.00206 ** ## x5 0.0037226 0.0403916 0.092 0.92659 ## x6 0.0900118 0.0422154 2.132 0.03331 * ## x7 0.0797314 0.0417962 1.908 0.05681 . ## x8 0.1052692 0.0391978 2.686 0.00740 ** ## x9 0.0231167 0.0409762 0.564 0.57282 ## x10 -0.0245487 0.0410393 -0.598 0.54990 ## x11 0.0115515 0.0406161 0.284 0.77618 ## x12 0.0429262 0.0421068 1.019 0.30831 ## x13 -0.0540795 0.0401108 -1.348 0.17798 ## x14 0.0920183 0.0368723 2.496 0.01278 * ## x15 -0.0131627 0.0394906 -0.333 0.73899 ## x16 0.0400716 0.0386785 1.036 0.30052 ## x17 -0.0157952 0.0380508 -0.415 0.67818 ## x18 0.0615636 0.0387173 1.590 0.11223 ## x19 0.0609104 0.0383000 1.590 0.11217 ## x20 -0.0390587 0.0392237 -0.996 0.31966 ## x21 0.0637443 0.0401520 1.588 0.11280 ## x22 0.0123254 0.0406316 0.303 0.76171 ## x23 0.0105869 0.0335785 0.315 0.75263 ## x24 -0.0135661 0.0406415 -0.334 0.73862 ## x25 0.0190199 0.0391689 0.486 0.62740 ## x26 0.0070111 0.0358145 0.196 0.84485 ## x27 -0.0001692 0.0408429 -0.004 0.99670 ## x28 -0.0403859 0.0392460 -1.029 0.30378 ## x29 0.0509068 0.0387079 1.315 0.18885 ## x30 0.0014375 0.0380287 0.038 0.96986 ## x31 0.0321008 0.0352222 0.911 0.36238 ## x32 0.0954990 0.0414220 2.306 0.02140 * ## x33 -0.0488635 0.0407278 -1.200 0.23061 ## x34 -0.0911234 0.0355243 -2.565 0.01050 * ## x35 -0.0815071 0.0400127 -2.037 0.04199 * ## x36 0.0975800 0.0398941 2.446 0.01467 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8289 on 764 degrees of freedom ## Multiple R-squared: 0.4548, Adjusted R-squared: 0.4291 ## F-statistic: 17.7 on 36 and 764 DF, p-value: &lt; 0.00000000000000022 The summary function provides us with the regression coefficients along with their statistical significance tests. Note that none of the regression coefficients are zeroed out, which contrasts with our final LASSO regression model. If you just want to call up the regression coefficients without their statistical significance tests, then just specify your model object (model2) followed by $finalModel$coefficients. # OLS MLR model coefficients model2$finalModel$coefficients ## (Intercept) x1 x2 x3 x4 x5 x6 x7 x8 ## 0.0015593009 0.1064868762 0.1288508857 0.0844271182 0.1374366297 0.0037225690 0.0900118226 0.0797314105 0.1052691528 ## x9 x10 x11 x12 x13 x14 x15 x16 x17 ## 0.0231166560 -0.0245487466 0.0115514738 0.0429261616 -0.0540795343 0.0920183162 -0.0131626874 0.0400715853 -0.0157952042 ## x18 x19 x20 x21 x22 x23 x24 x25 x26 ## 0.0615636356 0.0609103741 -0.0390587426 0.0637442959 0.0123254180 0.0105869070 -0.0135661097 0.0190199415 0.0070111163 ## x27 x28 x29 x30 x31 x32 x33 x34 x35 ## -0.0001692177 -0.0403859078 0.0509067573 0.0014374660 0.0321007692 0.0954989890 -0.0488634949 -0.0911234150 -0.0815070991 ## x36 ## 0.0975800099 Now it’s time to compare our lasso regression model (model1) with our OLS MLR model (model2), and we’ll start by comparing how well the models performed on the training data during the 10-fold cross-validation process. I’ll show you two ways in which you can compile this information. For the first approach, we will use the list function from base R. Enter the name of the first model (model1) as the first argument and the name of the second model (model2) as the second argument. Assign this list object to an object we’ll call model_list using the &lt;- operator. Next, we will type the name of the resamples object from the caret package with our model_list object as the sole parenthetical argument, and we will assign the results of this function to an object we’ll call resamp; the resamples function collates the model performance metrics (i.e., mean absolute error [MAE], RMSE, Rsquared) across the folds of the k-fold cross-validations for us. Finally, as the last step, enter the resamp object as the sole parenthetical argument in the summary function. # Compare model performance of k-fold cross-validation on train_df model_list &lt;- list(model1, model2) resamp &lt;- resamples(model_list) summary(resamp) ## ## Call: ## summary.resamples(object = resamp) ## ## Models: Model1, Model2 ## Number of resamples: 10 ## ## MAE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.6001450 0.6485866 0.6542165 0.6676563 0.6872213 0.7584773 0 ## Model2 0.6087479 0.6460566 0.6594453 0.6730339 0.6904705 0.7568991 0 ## ## RMSE ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.7564365 0.8136099 0.8355552 0.8400556 0.8681174 0.9250472 0 ## Model2 0.7665855 0.8211470 0.8463947 0.8467902 0.8670284 0.9312064 0 ## ## Rsquared ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## Model1 0.2692479 0.3360087 0.4404051 0.4150716 0.487333 0.5324680 0 ## Model2 0.2510815 0.3348805 0.4255335 0.4069315 0.473309 0.5384864 0 As you can see, the model performance metrics are grouped into MAE, RMSE, and Rsquared for Model1 and Model2. For Model1, the “Mean” column provides the average model performance metric value across the 10 folds (i.e., resamples) for the optimal lambda tuning parameter value. For Model2, the “Mean” column provides the average model performance metric value across the 10 folds. As you can see, Model1 (which is the lasso regression model) has a slightly lower RMSE value and a slightly higher Rsquared value, which indicates better model performance with the training data (train_df). If you’d like to create a single trimmed down table of the different RMSE and Rsquared values for model1 and model2, you can create a matrix object using the matrix function from base R. We’ll call this matrix object comp using the &lt;- operator. As the first argument in the matrix function, type the c (combine) function, with the values for the lasso regression RMSE and Rsquared followed by the same metric values for the OLS MLR, for a total of four arguments. Note that for the LASSO regression RMSE and Rsquared values, I have entered the RMSE_lasso and Rsquared_lasso functions we created previously. As the second argument in the matrix object, indicate there are two columns (i.e., one for RMSE values and one for Rsquared values). As the final argument, type byrow=TRUE argument to indicate that the matrix will be filled by rows. As a next step, name the columns and rows of the matrix object using the colnames and rownames functions from base R. After that, use the as.table function to convert the matrix to a table object. Finally, use the round function to display on three places after the decimal in the table. # Create matrix to compare model performance based on train_df comp &lt;- matrix(c( RMSE_lasso(x=model1$results$RMSE, y=model1$results$lambda, z=model1$bestTune$lambda), Rsquared_lasso(x=model1$results$Rsquared, y=model1$results$lambda, z=model1$bestTune$lambda), model2$results$RMSE, model2$results$Rsquared), ncol=2, byrow=TRUE) ## [1] 0.8400556 ## [1] 0.4150716 # Name the columns and rows of comp matrix object colnames(comp) &lt;- c(&quot;RMSE&quot;,&quot;R-square&quot;) rownames(comp) &lt;- c(&quot;LASSO Regression&quot;,&quot;OLS Linear Regression&quot;) # Convert matrix to a table comp &lt;- as.table(comp) # Round table values to three places after decimal round(comp, 3) ## RMSE R-square ## LASSO Regression 0.840 0.415 ## OLS Linear Regression 0.847 0.407 If you would like to know whether the estimated RMSE and Rsquared values from the 10 folds for the model1 10-fold cross-validation differ significantly from the estimated RMSE and Rsquared values from the 10 folds for the model2 10-fold cross-validation, we can use the compare_models function from the caret package. Simply enter the model object names as the first two arguments and metric=\"RMSE\"or metric=\"Rsquared\" as the third argument. This function applies a paired-samples (i.e., one-sample) t-test to evaluate with the mean of the differences between the two models’ performance metric values (across the 10 folds) differs significantly from zero. # Compare models with paired-samples (one-sample) t-test compare_models(model1, model2, metric=&quot;RMSE&quot;) ## ## One Sample t-test ## ## data: x ## t = -2.5958, df = 9, p-value = 0.02893 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.0126036525 -0.0008657173 ## sample estimates: ## mean of x ## -0.006734685 compare_models(model1, model2, metric=&quot;Rsquared&quot;) ## ## One Sample t-test ## ## data: x ## t = 2.7233, df = 9, p-value = 0.02348 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.001378438 0.014901850 ## sample estimates: ## mean of x ## 0.008140144 As you can see, both t-tests indicate that model1 outperformed model2 in the training data train_df with respect to RMSE and Rsquared values. As a final step, let’s repeat what we did for our lasso regression model (model1) by seeing how well our OLS MLR model (model2) predicts the outcome y when we feed fresh data from the test_df into our model. As before, we’ll create a data frame object (mod2perf) containing the RMSE and Rsquared values. # Predict outcome using model from training data based on testing data predictions2 &lt;- predict(model2, newdata=test_df) # Model performance mod2perf &lt;- data.frame(RMSE=RMSE(predictions2, test_df$y), Rsquared=R2(predictions2, test_df$y)) # Print model performance results print(mod2perf) ## RMSE Rsquared ## 1 0.8360987 0.4512161 Using this predictive analytics approach, we found that our OLS MLR regression model yielded an RMSE of .836 and an Rsquared of .451 (with rounding) when applied to the holdout test_df data frame. Let’s see how well the OLS MLR regression model compares to our lasso regression model in terms of predictive performance/accuracy. To do so, we’ll apply the matrix, colnames, rownames, as.table, and round functions, which are all from base R. These functions were explained previously in this tutorial, so if you have questions, venture back to their prior applications. # Compare model1 and model2 predictive performance based on test_df comp2 &lt;- matrix(c(mod1perf$RMSE, mod1perf$Rsquared, mod2perf$RMSE, mod2perf$Rsquared), ncol=2,byrow=TRUE) # Name the columns and rows of comp matrix object colnames(comp2) &lt;- c(&quot;RMSE&quot;,&quot;R-square&quot;) rownames(comp2) &lt;- c(&quot;LASSO Regression&quot;,&quot;OLS Multiple Linear Regression&quot;) # Convert matrix object to a table comp2 &lt;- as.table(comp2) # Round table values to three places after decimal round(comp2, 3) ## RMSE R-square ## LASSO Regression 0.822 0.470 ## OLS Multiple Linear Regression 0.836 0.451 As displayed in the table, in terms of predictive performance/accuracy, the lasso regression model outperforms the OLS MLR regression model. That is, the lasso regression model “learned” a more predictive model. 53.2.9 Summary In this chapter, we learned how to train a linear lasso regression model using k-fold cross-validation and how to evaluate the model using holdout test data (i.e., predictive analytics). Further, we compared the predictive performance/accuracy of a lasso regression model to an ordinary least squares (OLS) multiple linear regression (MLR) model. References "],["pathanalysis.html", "Chapter 54 Investigating Processes Using Path Analysis 54.1 Conceptual Overview 54.2 Tutorial", " Chapter 54 Investigating Processes Using Path Analysis In this chapter, we will learn how to apply path analysis in order to investigate processes that influence employees’ performance of a particular behavior. 54.1 Conceptual Overview In its simplest forms, path analysis can represent a single linear regression model (i.e., one outcome, one predictor), but in its more complex, path analysis can represent a system of multiple equations. Path analysis is often useful for testing theoretical or conceptual models with multiple outcome variables and/or outcome variables that also serve as predictor variables (e.g., mediators). Both conventional multiple linear regression modeling and structural equation modeling can be used for path analysis, and in this tutorial we will focus on the latter approach, as it allows for model estimation in a single step and the assessment of overall model fit to the data across the equations specified in the model. Path analysis can be used to evaluate models with presumed causal chains of variables, and this is why the approach is sometimes called causal modeling. We should be careful with how we use the term causal modeling, however, as showing that a chain of variables are linked to together in a model does not necessarily mean that the variables are causally related. In this chapter, we will focus on recursive models, which means the direct relations between two variables presumed to be causally related can only be unidirectional. A nonrecursive model allows for bidirectional associations between variables presumed to be causally related and for a predictor variable to be correlated with the residual error of its presumed outcome variable. 54.1.1 Path Diagram It is customary to depict a path analysis model visually using a path diagram, and as mentioned above, path analysis can be used to test a theoretical or conceptual model of interest. Let’s use the Theory of Planned Behavior (Ajzen 1991). For a simplified version of the theory, please refer to the Figure 1 below. Figure 1: Conceptual representation of the Theory of Planned Behavior (Ajzen, 1991) In a simplified form, the Theory of Planned Behavior posits that an intention to perform a particular behavior influences the individual’s decision to enact that behavior, and attitude toward the behavior, perception of norms pertaining to that behavior, and perception of control over performing that behavior influence the individual’s intention to perform the behavior. We can specify the Theory of Planned Behavior as a path diagram by first drawing the (manifest, observed) variables and the directional (structural) relations (paths) between the variables as implied by the theory, where rectangles represent the variables and directional arrows represent the directional relations between variables, which is depicted in the path diagram below (see Figure 2). Figure 2: Path diagram depicting Theory of Planned Behavior For exogenous variables in the model, on the one hand, we can choose to estimate their variances and the covariances between them; on the other hand, we can choose not to estimate their variances and covariances between them. For our purposes, we’ll practice specifying the variances and covariances between exogenous variables, but note that when missing data are present and full information maximum likelihood is deployed, the decision to estimate the variances of exogenous variables can have additional implications, which go beyond the scope of this tutorial. Exogenous variables serve only as predictor variables in the model, meaning that no other variables in the model to predict them and their causes exist outside of the model. As shown in the figure above, variances are typically represented as curved double-sided arrows, where both arrows point to the same variable. Covariances are also represented as double-sided arrows in which the arrows connect two distinct variables. (Residual) error terms (which are sometimes called disturbances) are added to endogenous variables, where endogenous variables refer to those variables that are predicted by another variable in the model, meaning that at least one presumed cause is modeled. Note that an endogenous variable may also be specified as the predictor of another endogenous variable, as is the case in our example path diagram representing the Theory of Planned Behavior. In addition, please note that an error term represents variability in an endogenous variable that remains unexplained even after the effects of the specified predictor variables are accounted for. Note, for example, that when we have multiple endogenous variables at the same stage of the model (e.g., first-stage mediators, terminal outcomes), we may choose to allow their error terms to covary. In sum, the conventional symbols used in a path diagram are shown Figure 3 below. With the exception of the manifest variable symbol, all of these symbols represent components of the model that can be estimated statistically using path analysis. These model components that can (and will) be estimated are commonly referred to as parameters or free parameters. Figure 3: Conventional path diagram symbols and their meanings Importantly, please note that in our path diagram because there are no direct relations specified from Attitude to Behavior, Norms to Behavior, and Control to Behavior, which implies that those direct relations are constrained to zero. If we suspected that, in fact, those direct relations are non-zero, then we would draw direction relations (i.e., single-headed arrows) in our path diagram and estimate their values during path analysis. 54.1.2 Model Identification Model identification has to do with the number of (free) parameters specified in the model relative to the number of unique (non-redundant) sources of information available, and model implication has important implications for assessing model fit and estimating parameter estimates. Just-identified: In a just-identified model (i.e., saturated model), the number of parameters (e.g., structural relations, variances) is equal to the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is equal to zero. In just-identified models, the model parameters can be estimated, but the model fit cannot be assessed in a meaningful way, aside from the R2 value. As a specific applications of path analysis, simple linear and multiple linear regression models are always just-identified. Over-identified: In an over-identified model, the number of parameters (e.g., structural relations, variances) is less than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is greater than zero. In over-identified models, the model parameters can be estimated, and the model fit can be assessed. Under-identified: In an under-identified model, the number of parameters (e.g., structural relations, variances) is greater than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is less than zero. In under-identified models, the model parameters and model fit cannot be estimated. Sometimes we might say that such models are called overparameterized because they have more parameters to be estimated than unique (non-redundant) sources of information. Most (if not all) statistical software packages that allow structural equation modeling – and, by extension, path analysis – to automatically compute the degrees of freedom for a model or provide an error message if the model is under-identified. As such, we don’t need to count the number of sources of unique (non-redundant) sources of information and free parameters by hand. With that said, to understand model identification and its various forms at a deeper level, it is often helpful to practice calculating the degrees freedom by hand when first learning. The formula for calculating the number of unique (non-redundant) sources of information available for a particular model is as follows: \\(i = \\frac{p(p+1)}{2}\\) where \\(p\\) is the number of manifest (observed) variables to be modeled. This formula calculates the number of possible unique covariances and variances for the variables specified in the model – or in other words, it calculates the lower diagonal of a covariance matrix, including the variances. In the path diagram we specified above, there are five manifest variables: Attitude, Norms, Control, Intention, and Behavior. Thus, in the following formula, \\(p\\) is equal to 5, and thus the number of unique (non-redundant) sources of information is 15. \\(i = \\frac{5(5+1)}{2} = \\frac{30}{2} = 15\\) To count the number of free parameters (\\(k\\)), simply add up the number of the specified direction relations, variances, covariances, and error terms in the path analysis model. As shown in Figure 4 below, our example path analysis model has 12 free parameters. \\(k = 12\\) To calculate the degrees of freedom (df) for the model, subtract the number of free parameters from the number unique (non-redundant) sources of information, which in this example is equal to 3, as shown below. Thus, the degrees of freedom for the model is 3, which means the model is over-identified. \\(df = i - k = 15 - 12 = 3\\) Figure 4: Counting the number of (free) parameters in specified path analysis model 54.1.3 Model Fit When a model is over-identified (df &gt; 0), the extent to which the specified model fits the data can be assessed using various model fit indices, such as the chi-square test (\\(\\chi^{2}\\)), comparative fit index (CFI), Tucker-Lewis index (TLI), root mean square error of approximation (RMSEA), and standardized root mean square residual (SRMR). For a commonly cited reference on cutoffs for fit indices, please refer to Hu and Bentler (1999). Chi-square test: The chi-square test can be used to assess whether the model fits the data, where a statistically significant chi-square value (e.g., p &lt; .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p &gt; .05) indicates that the model fits the data reasonably well. The null hypothesis for the chi-square test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. The chi-square test is sensitive to sample size and non-normal variable distributions. Comparative fit index (CFI): As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that the CFI compares the focal model to a baseline model, which is commonly referred to as the null or independence model. The CFI is generally less sensitive to sample size than the chi-square test. A CFI value greater than or equal to .90 generally indicates good model fit to the data. Tucker-Lewis index (TLI): Like the CFI, the Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. The TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes. A TLI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. Root mean square error of approximation (RMSEA): The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus ends up effectively rewarding more parsimonious models. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified). In general, an RMSEA value that is less than or equal to .08 indicates good model fit to the data, although some relax that cutoff to .10. Standardized root mean square residual (SRMR): Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .08 generally indicates good fit to the data. Summary of model fit indices: The conventional cutoffs for the aforementioned model fit indices – like any rule of thumb – should be applied with caution and with good judgment and intention. Further, these indices don’t always agree with one another, which means that we often look across multiple fit indices and come up with our best judgment of whether the model adequately fits the data. Generally, it is not advisable to interpret model parameter estimates unless the model fits the data adequately. Below is a table of the conventional cutoffs for the model fit indices. Fit Index Cutoff for Adequate Fit \\(\\chi^{2}\\) \\(p \\ge .05\\) CFI \\(\\ge .90\\) TLI \\(\\ge .95\\) RMSEA \\(\\le .08\\) SRMR \\(\\le .08\\) 54.1.4 Parameter Estimates In path analysis, parameter estimates (e.g., direct relations, covariances, variances) can be interpreted like those from a regression model, where the associated p-values or confidence intervals can be used as indicators of statistical significance. 54.1.5 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a path analysis model overlap with those associated with multiple linear regression, and thus for a review of those assumptions please see the chapter on incremental validity using multiple linear regression. 54.1.6 Conceptual Video For a more in-depth review of path analysis, please check out the following conceptual video. Link to conceptual video: https://youtu.be/UGIVPtFKwc0 54.2 Tutorial This chapter’s tutorial demonstrates estimate a path analysis model using R. 54.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/vMStRfsUTBg 54.2.2 Functions &amp; Packages Introduced Function Package sem lavaan summary base R lm base R 54.2.3 Initial Steps If you haven’t already, save the file called “PlannedBehavior.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PlannedBehavior.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;PlannedBehavior.csv&quot;) ## Rows: 199 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): attitude, norms, control, intention, behavior ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;attitude&quot; &quot;norms&quot; &quot;control&quot; &quot;intention&quot; &quot;behavior&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spc_tbl_ [199 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ attitude : num [1:199] 2.31 4.66 3.85 4.24 2.91 2.99 3.96 3.01 4.77 3.67 ... ## $ norms : num [1:199] 2.31 4.01 3.56 2.25 3.31 2.51 4.65 2.98 3.09 3.63 ... ## $ control : num [1:199] 2.03 3.63 4.2 2.84 2.4 2.95 3.77 1.9 3.83 5 ... ## $ intention: num [1:199] 2.5 3.99 4.35 1.51 1.45 2.59 4.08 2.58 4.87 3.09 ... ## $ behavior : num [1:199] 2.62 3.64 3.83 2.25 2 2.2 4.41 4.15 4.35 3.95 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. attitude = col_double(), ## .. norms = col_double(), ## .. control = col_double(), ## .. intention = col_double(), ## .. behavior = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 5 ## attitude norms control intention behavior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.31 2.31 2.03 2.5 2.62 ## 2 4.66 4.01 3.63 3.99 3.64 ## 3 3.85 3.56 4.2 4.35 3.83 ## 4 4.24 2.25 2.84 1.51 2.25 ## 5 2.91 3.31 2.4 1.45 2 ## 6 2.99 2.51 2.95 2.59 2.2 # Print number of rows in data frame (tibble) object nrow(df) ## [1] 199 There are 5 variables and 199 cases (i.e., employees) in the df data frame: attitude, norms, control, intention, and behavior. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). All of the variables are self-reports from a survey, where respondents rated their level of agreement (1 = strongly disagree, 5 = strongly agree) for the items associated with the attitude, norms, control, and intention scales, and where respondents rated the frequency with which they enact the behavior associated with the behavior variable (1 = never, 5 = all of the time). The attitude variable reflects an employee’s attitude toward the behavior in question. The norms variable reflects an employee’s perception of norms pertaining to the enactment of the behavior. The control variable reflects an employee’s feeling of control over being able to perform the behavior. The intention variable reflects an employee’s intention to enact the behavior. The behavior variable reflects an employee’s perception of the frequency with which they actually engage in the behavior. 54.2.4 Specify &amp; Estimate Path Analysis Models We will use functions from the lavaan package (Rosseel 2012) to specify and estimate our path analysis model. The lavaan package also allows structural equation modeling with latent variables, but we won’t cover that in this tutorial. If you haven’t already install and access the lavaan package using the install.packages and library functions, respectively. For background information on the lavaan package, check out the package website. # Install package install.packages(&quot;lavaan&quot;) # Access package library(lavaan) Let’s begin by showing how a multiple linear regression model can be estimated using lavaan and it’s functions. Specifically, let’s focus on the first part of our Theory of Planned Behavior model, where attitude, norms, and control are proposed as predictors intention. Using the &lt;- operator, name the specified model object something of your choosing (specmod). To the right of the &lt;- operator, enter quotation marks (\" \"), and within them, specify your model, which in this case is a multiple linear regression equation: intention ~ attitude + norms + control. Just like we would with other regression functions in R, we use the tilde (~) to separate our outcome variable from our predictor variable, where the outcome variable goes to the left of the tilde, and the predictor variables go to the right. Using the sem function from lavaan, type the name of the specified regression model (specmod) as the first argument and data= followed by the name of the data from which the variables belong as the second argument. Using the &lt;- operator, name the estimated model object something, and here I name it fitmod for fitted model. Using the summary function from base R, request a summary of the model fit and parameter estimate results. As the first argument in the summary function, type the name of the estimated model function from the previous step (fitmod). As the second argument, type fit.measures=TRUE to request the model fit indices as part of the output. As the third argument, type rsquare=TRUE to request the unadjusted R2 value for the model. # Specify path analysis model specmod &lt;- &quot; intention ~ attitude + norms + control &quot; # Estimate model fitmod &lt;- sem(specmod, data=df) # Request summary of results summary(fitmod, fit.measures=TRUE, rsquare=TRUE) ## lavaan 0.6.15 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 4 ## ## Number of observations 199 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 91.633 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -219.244 ## Loglikelihood unrestricted model (H1) -219.244 ## ## Akaike (AIC) 446.489 ## Bayesian (BIC) 459.662 ## Sample-size adjusted Bayesian (SABIC) 446.990 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## attitude 0.352 0.058 6.068 0.000 ## norms 0.153 0.059 2.577 0.010 ## control 0.275 0.058 4.740 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.530 0.053 9.975 0.000 ## ## R-Square: ## Estimate ## intention 0.369 Towards the top of the output, alongside “Number of observations”, the number 199 appears, which indicates that 199 employees’ data were included in this analysis. Further below, you will see a zero (0) next to “Degrees of freedom”, which indicates that the model is just-identified. Thus, we will ignore the model fit information and skip down to the “Regressions” section. The path coefficient (i.e., regression coefficient) between attitude and intention is statistically significant and positive (b = .352, p &lt; .001). The path coefficient between norms and intention is statistically significant and positive (b = .153, p = .010). The path coefficient between control and intention is statistically significant and positive (b = .275, p &lt; .001). Under the “R-Square” section, we see the unadjusted R2 value for the outcome variable intention, which is equal to .369; that is, collectively, attitude, norms, and control explain 36.9% of the variance in intention. We can also explicitly model the covariances/correlations between the predictor (exogenous) variables in the model by using the double tilde (~~) operator. Note that on separate lines under the regression equation script, we can specify that we want attitude to be able to freely covary with norms and control and for norms to be able to freely covary with control. Everything else in your script remains the same as in our previously specified model. As noted above, in a real-life situation, we should think carefully about what it means to add these covariances to exogenous variables, particularly in the case of estimating the model using full information maximum likelihood and in the presence of missing data. We are specifying these covariances here for demonstration purposes. # Specify path analysis model specmod &lt;- &quot; # Direct relations intention ~ attitude + norms + control # Covariances attitude ~~ norms + control norms ~~ control &quot; # Estimate model fitmod &lt;- sem(specmod, data=df) # Request summary of results summary(fitmod, fit.measures=TRUE, rsquare=TRUE) ## lavaan 0.6.15 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 199 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 136.306 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1011.828 ## Loglikelihood unrestricted model (H1) -1011.828 ## ## Akaike (AIC) 2043.656 ## Bayesian (BIC) 2076.589 ## Sample-size adjusted Bayesian (SABIC) 2044.908 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## attitude 0.352 0.058 6.068 0.000 ## norms 0.153 0.059 2.577 0.010 ## control 0.275 0.058 4.740 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitude ~~ ## norms 0.200 0.064 3.128 0.002 ## control 0.334 0.070 4.748 0.000 ## norms ~~ ## control 0.220 0.065 3.411 0.001 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.530 0.053 9.975 0.000 ## attitude 0.928 0.093 9.975 0.000 ## norms 0.830 0.083 9.975 0.000 ## control 0.939 0.094 9.975 0.000 ## ## R-Square: ## Estimate ## intention 0.369 Just as before, the degrees of freedom (df) for the model is equal to zero (0), which again indicates that this model is just-identified, which is what we would expect from a multiple linear regression model as well. In addition, note that the path coefficients remain the same (along with their associated p-values), and the R2 value remains the same. In the new, output, however, the covariances are estimated, and when we estimate the covariances between variables, the variances of the variables involved in the covariances are also estimated by default. Often the covariances and variances are not of substantive interest when interpreting a path analysis model, though. Using the lm (linear model) function from base R we can verify that our path analysis results using the sem function from lavaan are equivalent to multiple linear regression results from a standard linear regression function. For a review of the lm function, please refer to the chapter supplement for estimating incremental validity using multiple linear regression. # Estimate multiple linear regression model fitmod &lt;- lm(intention ~ attitude + norms + control, data=df) # Request summary of results summary(fitmod) ## ## Call: ## lm(formula = intention ~ attitude + norms + control, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.80282 -0.52734 -0.06018 0.51228 1.85202 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.58579 0.23963 2.445 0.0154 * ## attitude 0.35232 0.05866 6.006 0.00000000913 *** ## norms 0.15250 0.05979 2.550 0.0115 * ## control 0.27502 0.05862 4.692 0.00000509027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7356 on 195 degrees of freedom ## Multiple R-squared: 0.369, Adjusted R-squared: 0.3593 ## F-statistic: 38.01 on 3 and 195 DF, p-value: &lt; 0.00000000000000022 As you can see, aside from the number of digits reported after the decimal, the lm regression coefficients are equivalent to the sem path coefficients. In addition, the unadjusted R2 value is the same. Now it’s time to test a full model of the Theory of Planned Behavior by adding the equation in which intention predicts behavior. We will build on our previous path analysis model script by specifying that intention is a predictor of behavior (i.e., behavior ~ intention). Everything else in our script can remain the same. # Specify path analysis model specmod &lt;- &quot; # Direct relations intention ~ attitude + norms + control behavior ~ intention # Covariances attitude ~~ norms + control norms ~~ control &quot; # Estimate model fitmod &lt;- sem(specmod, data=df) # Request summary of results summary(fitmod, fit.measures=TRUE, rsquare=TRUE) ## lavaan 0.6.15 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 199 ## ## Model Test User Model: ## ## Test statistic 2.023 ## Degrees of freedom 3 ## P-value (Chi-square) 0.568 ## ## Model Test Baseline Model: ## ## Test statistic 182.295 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.019 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1258.517 ## Loglikelihood unrestricted model (H1) -1257.506 ## ## Akaike (AIC) 2541.035 ## Bayesian (BIC) 2580.555 ## Sample-size adjusted Bayesian (SABIC) 2542.538 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.103 ## P-value H_0: RMSEA &lt;= 0.050 0.735 ## P-value H_0: RMSEA &gt;= 0.080 0.120 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.019 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## intention ~ ## attitude 0.352 0.058 6.068 0.000 ## norms 0.153 0.059 2.577 0.010 ## control 0.275 0.058 4.740 0.000 ## behavior ~ ## intention 0.453 0.065 7.014 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## attitude ~~ ## norms 0.200 0.064 3.128 0.002 ## control 0.334 0.070 4.748 0.000 ## norms ~~ ## control 0.220 0.065 3.411 0.001 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .intention 0.530 0.053 9.975 0.000 ## .behavior 0.699 0.070 9.975 0.000 ## attitude 0.928 0.093 9.975 0.000 ## norms 0.830 0.083 9.975 0.000 ## control 0.939 0.094 9.975 0.000 ## ## R-Square: ## Estimate ## intention 0.369 ## behavior 0.198 Again, we have 199 observations (employees) used in this analysis, but this time, the degrees of freedom is equal to 3, which indicates that our model is over-identified. Because the model is over-identified, we can interpret the model fit indices to assess the how well the model we specified fits the data. First, the chi-square test value appears to the right of the text “Model Fit Test Statistic”, and the associated p-value appears below. The chi-square test is nonsignificant (\\(\\chi^{2}(df=3)=2.023, p=.568\\)), which indicates that the model does not fit significantly worse than a model that fits the data perfectly; thus, we have our first indicator that the model fits the data reasonably well. Second, the comparative fit index (CFI) is 1.000, which is greater than the conventional .90 cutoff, which also indicates that the model fits the data reasonably well. Third, the Tucker-Lewis index (TLI) is 1.019, which is greater than the conventional .95 cutoff, which also indicates that the model fits the data reasonably well. Fourth, the root mean square error of approximation (RMSEA) value of .000 is less than the conventional cutoff of .08, and thus we have further evidence that the model fits the data reasonably well. Fifth, the standardized root mean square residual (SRMR) value of .019 is less than the conventional cutoff of .08, and thus we have further evidence that the model fits the data reasonably well. This is one of the relatively rare occasions in which all of the model fit indices are in agreement with one another, leading us to conclude that the specified model fits the data well. We can now feel comfortable proceeding forward interpreting the parameter estimates. The path coefficient (i.e., direct relation, presumed causal relation) between attitude and intention is statistically significant and positive (b = .352, p &lt; .001); the path coefficient between norms and intention is also statistically significant and positive (b = .153, p = .010); and the path coefficient between control and intention is also statistically significant and positive (b = .275, p &lt; .001). Further, the path coefficient between intention and behavior is statistically significant and positive (b = .453, p &lt; .001). The covariances and variances are not of substantive interest, so we will ignore them. The unadjusted R2 value for the outcome variable intention is .369, which indicates that, collectively, attitude, norms, and control explain 36.9% of the variance in intention, and the unadjusted R2 value for the outcome variable behavior is .198, which indicates that intention explains 19.8% of the variance in behavior. Overall, the results lends support to the propositions of the Theory of Planned Behavior, at least based on this sample of employees. 54.2.5 Additional Information on Model Specification Notation Thus far we have focused on using the tilde (~) operator to note directional relations (i.e., path coefficients) and the double tilde (~~) to specify a covariance. The plus (+) operator allows us to add variables to one side of the directional relation or covariance equation. There are equivalent approaches for specifying directional relations. If we want to specify that attitude, norms, and control are predictors of intention we could use either approach shown below when specifying our model. # Equivalent approaches to specifying directional relations ### Approach 1 specmod &lt;- &quot; intention ~ attitude + norms + control &quot; ### Approach 2 specmod &lt;- &quot; intention ~ attitude intention ~ norms intention ~ control &quot; The same logical applies if we hypothetically wanted to specify attitude, norms, and control as predictors of both intention and behavior. It’s really up to you how you decide to specify your model when equivalent approaches are possible. # Equivalent approaches to specifying directional relations ### Approach 1 specmod &lt;- &quot; intention + behavior ~ attitude + norms + control &quot; ### Approach 2 specmod &lt;- &quot; intention ~ attitude + norms + control behavior ~ attitude + norms + control &quot; ### Approach 3 specmod &lt;- &quot; intention ~ attitude intention ~ norms intention ~ control behavior ~ attitude behavior ~ norms behavior ~ control &quot; Further, there are equivalent approaches for specifying covariances. If we want to specify that attitude, norms, and control are permitted to covary with one another, we could use either approach below. # Equivalent approaches to specifying covariances ### Approach 1 specmod &lt;- &quot; attitude ~~ norms + control norms ~~ control &quot; ### Approach 2 specmod &lt;- &quot; attitude ~~ norms attitude ~~ control norms ~~ control &quot; If you would like to explicitly specify a variance component in your model (even if the model will do this by default for variables with specified covariances), you would use the double tilde (~~) operator with the same variable’s name on either side of the double tilde. # Specifying variances specmod &lt;- &quot; attitude ~~ attitude norms ~~ norms control ~~ control &quot; Finally, if you have a single exogenous variable in your model (that does not share a covariance with any other variable), it is up to you whether you wish to specify the variance component for that exogenous variable as a free parameter (or not). The model fit and parameter estimates will remain the same. 54.2.6 Summary In this chapter, we explored the building blocks of path analysis, which allowed us to simultaneously fit a model with more than one outcome variable and with a variable that acts as both a predictor and an outcome. To do so, we used the sem function from the lavaan package and evaluated model fit indices and parameter estimates. References "],["mediationanalysis.html", "Chapter 55 Estimating a Mediation Model Using Path Analysis 55.1 Conceptual Overview 55.2 Tutorial", " Chapter 55 Estimating a Mediation Model Using Path Analysis In this chapter, we will learn how to estimate a mediation analysis using path analysis. 55.1 Conceptual Overview Mediation analysis is used to investigate whether an intervening variable transmits the effects of a predictor variable on an outcome variable. Both multiple regression and structural equation modeling (path analysis) can be used to perform mediation analysis, but structural equation modeling (path analysis) offers a single-step process that often more efficient. When implemented using path analysis, mediation analysis follows the same model specification, model identification, and model fit considerations as general applications of path analysis. For more information on path analysis, please refer to the previous chapter on path analysis. Just as in a conventional path analysis model, we often represent our mediation analysis model visually as a path diagram. For illustration purposes, let’s begin with a path diagram of the direct relation between a predictor variable called Attitude with an outcome variable called Behavior (see Figure 1). This path diagram specifies Attitude (towards the Behavior) as a predictor of the Behavior, with any error in prediction captured in the (residual) error term. By convention, we commonly refer to this path as the c path. Figure 1: Path diagram of the direct relation between Attitude (predictor variable) and Behavior (outcome variable) In the path diagram depicted in Figure 2, we introduce an intervening variable, which we commonly refer to as the mediator variable – or just as the mediator. As we saw in Figure 1, a direct relation between the predictor variable Attitude and the outcome variable Behavior is shown; however, note that we now refer to this relation as the c’ path (spoken: “c prime”) because it represents the direct effect of the predictor variable on the outcome variable in the presence of (i.e., statistically controlling) for the effect of the mediator variable. The mediator variable is called Intention, and as you can see, it intervenes between the predictor and outcome variables. The direct relation between the predictor variable and mediator variable is commonly referred to as the a path, and note that because the mediator variable is an endogenous variable (i.e., has a specified cause in the model), it as a (residual) error term. The direct relation between the mediator variable and the outcome variable is commonly referred to as the b path. The effect of the predictor variable on the outcome variable via the mediating variable is referred to as the indirect effect given that the predictor variable indirectly influences the outcome variable through another variable; in other words, the mediator variable transmits the effect of the predictor variable to the outcome variable. Figure 2: Path diagram of the mediation analysis model in which Intention (mediator) mediates the effect of Attitude (predictor variable) on Behavior (outcome variable) The mediation model shown in Figure 2 is an example of a partial mediation. We can distinguish between a partial and a complete mediation. A partial mediation is a mediation in which the direct relation (c’) between the predictor variable and the outcome variable is non-zero; in other words, a direct relation between the predictor and the outcome is estimated as a free parameter. A complete mediation (i.e., full mediation) is a mediation in which the direct relation (c’) between the predictor variable and the outcome variable is zero; in other words, the direct relation between the predictor and the outcome is fixed at zero, which means we don’t specify the direct relation at all in our model – or we specify the direct relation but fix it to zero. A complete mediation is shown in Figure 3. Figure 3: Examples of two equivalent path diagram representations of the same complete mediation 55.1.1 Estimation of Indirect Effect There are different approaches to estimating the indirect effect, such as causal steps (Baron and Kenny 1986), difference in coefficients, and product of coefficients (MacKinnon and Dwyer 1993). Because the product of coefficients approach is efficient to implement, we will focus on that approach in this tutorial. Using this approach, the indirect effect is estimated by computing the product of the coefficients for directional relations (i.e., paths), which include the relation between the predictor variable and the mediator variable (a path) and the relation between the mediator variable and the outcome variable (b). The statistical significance of the indirect effect can be computed by dividing the product of the coefficients by its standard error, and then the resulting ratio (quotient) can be compared to a standard normal distribution – or another distribution. There are different methods for implementing the product of coefficients approach, and we will focus on the (multivariate) delta method and a resampling method. First, in the (multivariate) delta method (where the Sobel test is a specific form), the standard error for the product of coefficients is compared to a standard normal distribution to determine the indirect effect’s significance level (i.e., p-value). This method does tend to show bias in smaller samples [e.g., N &lt; 50; MacKinnon, Warsi, and Dwyer (1995); MacKinnon et al. (2002)], however, and assumes the sampling distribution of the indirect effect is normal, which is not often a tenable assumption when dealing with the product of two coefficients. Second, resampling methods constitute a broad family of methods in which data are re-sampled (with replacement) a specified number of times (e.g., 5,000), and a normal sampling distribution need not be assumed. Examples of resampling methods broadly include bootstrapping, the permutation method, and the Monte Carlo method. Bootstrapping is perhaps one of the most common a popular resampling methods today, and bootstrapping can be used to estimate confidence intervals for an indirect effect estimated using the product of coefficients approach. Many resampling approaches tend to show some bias in small samples [e.g., N &lt; 20-80; Koopman et al. (2015)]; however, resampling methods like bootstrapping are often preferred over the delta method because they do not hold the strict assumption that the sampling distribution is normal. There are a multiple types of bootstrapping, such as percentile, bias-corrected, and bias-corrected and accelerated, most of which can be implemented with relative ease in R. 55.1.2 Model Identification Because we are testing for mediation using path analysis in this chapter, please refer to the Model Identification section from previous chapter on path analysis. 55.1.3 Model Fit please refer to the Model Fit section from previous chapter on path analysis. 55.1.4 Parameter Estimates In mediation analysis, parameter estimates (e.g., direct relations, covariances, variances) can be interpreted like those from a regression model, where the associated p-values or confidence intervals can be used as indicators of statistical significance. 55.1.5 Statistical Assumptions The statistical assumptions that should be met prior to estimating and/or interpreting a mediation analysis model are the same that should be met when running a path analysis model; for a review of those assumptions please review to the previous chapter path analysis. 55.1.6 Conceptual Video For a more in-depth review of path analysis, please check out the following conceptual video. Link to conceptual video: https://youtu.be/3zA4iB0aAvg 55.2 Tutorial This chapter’s tutorial demonstrates perform mediation analysis in a path analysis framework using R. 55.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/za-KTX1A5us 55.2.2 Functions &amp; Packages Introduced Function Package sem lavaan summary base R set.seed base R parameterEstimates lavaan 55.2.3 Initial Steps If you haven’t already, save the file called “PlannedBehavior.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PlannedBehavior.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;PlannedBehavior.csv&quot;) ## Rows: 199 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): attitude, norms, control, intention, behavior ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;attitude&quot; &quot;norms&quot; &quot;control&quot; &quot;intention&quot; &quot;behavior&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spc_tbl_ [199 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ attitude : num [1:199] 2.31 4.66 3.85 4.24 2.91 2.99 3.96 3.01 4.77 3.67 ... ## $ norms : num [1:199] 2.31 4.01 3.56 2.25 3.31 2.51 4.65 2.98 3.09 3.63 ... ## $ control : num [1:199] 2.03 3.63 4.2 2.84 2.4 2.95 3.77 1.9 3.83 5 ... ## $ intention: num [1:199] 2.5 3.99 4.35 1.51 1.45 2.59 4.08 2.58 4.87 3.09 ... ## $ behavior : num [1:199] 2.62 3.64 3.83 2.25 2 2.2 4.41 4.15 4.35 3.95 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. attitude = col_double(), ## .. norms = col_double(), ## .. control = col_double(), ## .. intention = col_double(), ## .. behavior = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 5 ## attitude norms control intention behavior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.31 2.31 2.03 2.5 2.62 ## 2 4.66 4.01 3.63 3.99 3.64 ## 3 3.85 3.56 4.2 4.35 3.83 ## 4 4.24 2.25 2.84 1.51 2.25 ## 5 2.91 3.31 2.4 1.45 2 ## 6 2.99 2.51 2.95 2.59 2.2 # Print number of rows in data frame (tibble) object nrow(df) ## [1] 199 There are 5 variables and 199 cases (i.e., employees) in the df data frame: attitude, norms, control, intention, and behavior. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). All of the variables are self-reports from a survey, where respondents rated their level of agreement (1 = strongly disagree, 5 = strongly agree) for the items associated with the attitude, norms, control, and intention scales, and where respondents rated the frequency with which they enact the behavior associated with the behavior variable (1 = never, 5 = all of the time). The attitude variable reflects an employee’s attitude toward the behavior in question. The norms variable reflects an employee’s perception of norms pertaining to the enactment of the behavior. The control variable reflects an employee’s feeling of control over being able to perform the behavior. The intention variable reflects an employee’s intention to enact the behavior. The behavior variable reflects an employee’s perception of the frequency with which they actually engage in the behavior. 55.2.4 Specify &amp; Estimate a Mediation Analysis Model We will use functions from the lavaan package (Rosseel 2012) to specify and estimate our mediation analysis model. The lavaan package also allows structural equation modeling with latent variables, but we won’t cover that in this tutorial. If you haven’t already install and access the lavaan package using the install.packages and library functions, respectively. For background information on the lavaan package, check out the package website. # Install package install.packages(&quot;lavaan&quot;) # Access package library(lavaan) Let’s specify a partial mediation model in which attitude is the predictor variable, intention is the mediator variable, and behavior is the outcome variable. [Note: If we wanted to specify a complete mediation model, then we would not specify the direct relation from the predictor variable to the outcome variable.] We will first apply the delta method for estimating the indirect effect and then follow it up with the percentile bootstrapping method. 55.2.4.1 Delta Method To apply the delta method, which is a product of coefficients method, we will do the following. We will specify the model and name it specmod using the &lt;- operator. If we precede a predictor variable with a letter or a series of characters without spaces between them and then follow that up with an asterisk (*), we can label a path (or covariance or variance) so that later we can reference it. In this case, we will label the direct relation between the predictor variable (attitude) and the outcome variable as c (i.e., behavior ~ c*attitude), and we will label the paths between the predictor variable and the mediator variable and the mediator variable and the outcome variable as a and b (respectively). To estimate the indirect effect using the product of coefficients method, we will type an arbitrary name for the indirect effect (such as ab) followed by the := operator, and then followed by the product of the a and b paths we previously labeled (i.e., a*b). We will create a model estimation object (which here we label fitmod), and apply the sem function from lavaan, with the model specification object (specmod) as the first argument. As the second argument, type data= followed by the name of the data from which the variables named in the model specification object come (df). Type the name of the summary function from base R. As the first argument, type the name of the model estimation object we created in the previous step (fitmod). As the second and third arguments, respectively, type fit.measures=TRUE and rsquare=TRUE to request the model fit indices and the unadjusted R2 values for the endogenous variables. # Specify mediation analysis model specmod &lt;- &quot; # Path c&#39; (direct effect) behavior ~ c*attitude # Path a intention ~ a*attitude # Path b behavior ~ b*intention # Indirect effect (a*b) ab := a*b &quot; # Estimate model fitmod &lt;- sem(specmod, data=df) # Request summary of results summary(fitmod, fit.measures=TRUE, rsquare=TRUE) ## lavaan 0.6.15 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 199 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 103.700 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -481.884 ## Loglikelihood unrestricted model (H1) -481.884 ## ## Akaike (AIC) 973.767 ## Bayesian (BIC) 990.234 ## Sample-size adjusted Bayesian (SABIC) 974.393 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## behavior ~ ## attitude (c) 0.029 0.071 0.412 0.680 ## intention ~ ## attitude (a) 0.484 0.058 8.333 0.000 ## behavior ~ ## intention (b) 0.438 0.075 5.832 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .behavior 0.698 0.070 9.975 0.000 ## .intention 0.623 0.062 9.975 0.000 ## ## R-Square: ## Estimate ## behavior 0.199 ## intention 0.259 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.212 0.044 4.778 0.000 Again, we have 199 observations (employees) used in this analysis, and the degrees of freedom is equal to 0, which indicates that our model is just-identified. Because the model is just-identified, there are no available degrees of freedom with which to estimate some of the common model fit indices (e.g., RMSEA), so we will not evaluate model fit. Instead, we will move on to interpreting model parameter estimates. The path coefficient (i.e., direct relation, presumed causal relation) between attitude and intention (i.e., path a) is statistically significant and positive (b = .484, p &lt; .001). Further, the path coefficient between intention and behavior (i.e., path b) is also statistically significant and positive (b = .484, p &lt; .001). The direct effect path between attitude and behavior, however, is nonsignificant (b = .029, p = .680). The (residual) error variances for behavior and intentions are not of substantive interest, so we will ignore them. The unadjusted R2 values for the endogenous variable intention is .259, which indicates that attitude explains 25.9% of the variance in intention, and the unadjusted R2 value for the outcome variable behavior is .199, which indicates that, collectively attitude and intention explain 19.9% of the variance in behavior. Moving on to the estimate of the indirect effect (which you can find in the Defined Parameters section), we see that the indirect effect object we specified (ab) is positive and statistically significant (IE = .212, p &lt; .001), which indicates that intention mediates the association between attitude and intention. Again, this estimate was computed using the delta method. 55.2.4.2 Percentile Bootstrapping Method The percentile bootstrapping method is generally recommended over the delta method, and thus, my recommendation is to use the percentile bootstrapping method when estimating indirect effects. Depending upon the processing speed and power of your machine, the approach make take a minute or multiple minutes to perform the number of bootstrap iterations (i.e., re-samples) requested. The initial model specification and model estimation steps the same as would be used for the delta method, as we will use these to get the model fit information and direct effect parameter estimates, which don’t require bootstrapping. However, after that, differences emerge. Specify the model in the same way as was done previously in preparation for the delta method, and name the object something (e.g., specmod). Estimate the model using the sem function, just as you would with the delta method, as this will generate the model fit indices and parameter estimates that we will interpret for direction relations; let’s name the model estimation object fitmod1 in this example. Just as you would with the delta method, type the name of the summary function from base R. As the first argument, type the name of the model estimation object we created in the previous step (fitmod1). As the second and third arguments, respectively, type fit.measures=TRUE and rsquare=TRUE to request the model fit indices and the unadjusted R2 values for the endogenous variables. Bootstrapping involves random draws (i.e., re-samples) so the results may vary each time unless we set a “random seed” to allow us to reproduce the results; using the set.seed function from base R, let’s choose 2019 as the arbitrary seed value, and enter it as the sole parenthetical argument in the set.seed function. Now it’s time to generate the bootstrap re-sampling by once again applying the sem function. This time, let’s use the &lt;- operator to name the model estimation object fitmod2 to distinguish it from the one we previously created. As the first argument in the sem function, the name of the model specification object is entered (specmod). As the second argument, data= is entered, followed by the name of the data frame object (df). As the third argument, the approach for estimating the standard errors is indicated by typing se= followed by bootstrap to request bootstrap standard errors. As the fourth argument, type bootstrap= followed by the number of bootstrap re-samples (with replacement) we wish to request. Generally, requesting at least 5,000 re-sampling iterations is advisable, so let’s type 5000 after bootstrap=. Type the name of the parameterEstimates function from the lavaan package to request that percentile bootstrapping be applied to the 5,000 re-sampling iterations requested earlier. As the first argument, type the name of the model estimation object (fitmod). As the second argument, type ci=TRUE to request confidence intervals. As the third argument, type level=0.95 to request the 95% confidence interval be estimated. As the fourth argument, type boot.ci.type=\"perc\" to apply the percentile bootstrap approach. It may take a few minutes for your requested bootstraps to run. # Specify mediation analysis model specmod &lt;- &quot; # Path c&#39; (direct effect) behavior ~ c*attitude # Path a intention ~ a*attitude # Path b behavior ~ b*intention # Indirect effect (a*b) ab := a*b &quot; # Estimate model (direct effect parameter estimates) fitmod1 &lt;- sem(specmod, data=df) # Request summary of results summary(fitmod1, fit.measures=TRUE, rsquare=TRUE) ## lavaan 0.6.15 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 199 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 103.700 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -481.884 ## Loglikelihood unrestricted model (H1) -481.884 ## ## Akaike (AIC) 973.767 ## Bayesian (BIC) 990.234 ## Sample-size adjusted Bayesian (SABIC) 974.393 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## behavior ~ ## attitude (c) 0.029 0.071 0.412 0.680 ## intention ~ ## attitude (a) 0.484 0.058 8.333 0.000 ## behavior ~ ## intention (b) 0.438 0.075 5.832 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .behavior 0.698 0.070 9.975 0.000 ## .intention 0.623 0.062 9.975 0.000 ## ## R-Square: ## Estimate ## behavior 0.199 ## intention 0.259 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.212 0.044 4.778 0.000 # Set seed set.seed(2022) # Estimate model (bootstrap) fitmod2 &lt;- sem(specmod, data=df, se=&quot;bootstrap&quot;, bootstrap=5000) # Request percentile bootstrap 95% confidence intervals parameterEstimates(fitmod2, ci=TRUE, level=0.95, boot.ci.type=&quot;perc&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 behavior ~ attitude c 0.029 0.065 0.451 0.652 -0.099 0.156 ## 2 intention ~ attitude a 0.484 0.054 8.980 0.000 0.378 0.587 ## 3 behavior ~ intention b 0.438 0.071 6.166 0.000 0.297 0.576 ## 4 behavior ~~ behavior 0.698 0.056 12.375 0.000 0.581 0.800 ## 5 intention ~~ intention 0.623 0.055 11.351 0.000 0.515 0.730 ## 6 attitude ~~ attitude 0.928 0.000 NA NA 0.928 0.928 ## 7 ab := a*b ab 0.212 NA NA NA 0.136 0.300 Note: If you receive the following error message, you can safely ignore it, as it simply means that some of the bootstrap draws (i.e., re-samples) did not work successfully but most did; we can still be confident in our findings, and this warning does not apply to the original model we estimated prior to the bootstrapping. lavaan WARNING: the optimizer warns that a solution has NOT been found!lavaan WARNING: the optimizer warns that a solution has NOT been found!lavaan WARNING: only 4997 bootstrap draws were successful. The model fit information and direct relation (non-bootstrapped) parameter estimates are the same as when we estimated the model as part of the delta method. Nonetheless, I repeat those findings here and then extend them with the percentile bootstrap findings for the indirect effect. We have 199 observations (employees) used in this analysis, and the degrees of freedom is equal to 0, which indicates that our model is just-identified. Because the model is just-identified, there are no available degrees of freedom with which to estimate some of the common model fit indices (e.g., RMSEA), so we will not evaluate model fit. Instead, we will move on to interpreting model parameter estimates. The path coefficient (i.e., direct relation, presumed causal relation) between attitude and intention (i.e., path a) is statistically significant and positive (b = .484, p &lt; .001). Further, the path coefficient between intention and behavior (i.e., path b) is also statistically significant and positive (b = .484, p &lt; .001). The direct effect path between attitude and behavior, however, is nonsignificant (b = .029, p = .680). The (residual) error variances for behavior and intentions are not of substantive interest, so we will ignore them. The unadjusted R2 values for the endogenous variable intention is .259, which indicates that attitude explains 25.9% of the variance in intention, and the unadjusted R2 value for the outcome variable behavior is .199, which indicates that, collectively attitude and intention explain 19.9% of the variance in behavior. Moving on, we get the delta method estimate of the indirect effect (which you can find in the Defined Parameters section), and we see that the indirect effect object we specified (ab) is positive and statistically significant (IE = .212, p &lt; .001), which indicates that intention mediates the association between attitude and intention. Again, however, that is the delta method estimate of the indirect effect, so let’s now take a look at the percentile bootstrap estimate of the indirect effect. At the very end of the output appears a table containing, among other things, the 95% confidence intervals for various parameters. We are only interested in the very last row which displays the 95% confidence interval for the ab object we specified using the product of coefficients approach. The point estimate for the indirect effect is .212, and 95% confidence interval does not include zero and positive, so we can conclude that the indirect effect of attitude on behavior via intention is statistically significant and positive (95% CI[.136, 300]). In other words, the percentile bootstrapping method indicates that intention mediates the association between attitude and behavior. If this confidence interval included zero in its range, then we would conclude that there is no evidence that intention is a mediator. 55.2.5 Summary In this chapter, we explored the building blocks of mediation analysis using a path analysis framework. To do so, we used the sem and parameterEstimates functions from the lavaan package and to estimate the indirect effect. References "],["compensation.html", "Chapter 56 Introduction to Employee Compensation &amp; Reward Systems 56.1 Chapters Included", " Chapter 56 Introduction to Employee Compensation &amp; Reward Systems Employee reward systems refer to the constellation of monetary and nonmonetary rewards that an organization offers, which includes direct pay (e.g., base pay, variable pay), indirect pay (e.g., benefits), and other returns (e.g., nonmonetary awards). With respect to direct pay, organizations have an opportunity to ensure that jobs are paid equitably within the organization as well as in relation to other organizations; these processes are referred to as internal equity (i.e., internal alignment and external equity (i.e., external competitiveness. Further, organizations should ensure that individuals within an organization are paid fairly, which can be described as individual equity. 56.1 Chapters Included In the following chapters, you will have opportunities to learn how to evaluate different aspects of an organization’s reward systems. Preparing Market Survey Data Estimating a Market Pay Line Using Linear &amp; Polynomial Regression Identifying Pay Determinants Using Hierarchical Linear Regression Computing Compa Ratios &amp; Investigating Pay Compression "],["marketsurvey.html", "Chapter 57 Preparing Market Survey Data 57.1 Conceptual Overview 57.2 Tutorial", " Chapter 57 Preparing Market Survey Data In this chapter, we will learn how to prepare market survey data by aging and applying weights to the data. 57.1 Conceptual Overview A market review refers to the “process of collecting pay data for benchmark jobs from other organizations” (Bauer et al. 2025). When conducting a market review, it is a common practice to acquire data from a market survey, where a market survey (salary survey) refers to a survey used to collect pay data on benchmark jobs (key jobs), which is often conducted by a third-party vendor or government agency. Examples of such third-party parties include traditional surveys (e.g., CareerOneStop.org), customized surveys, and web-based platforms or surveys (e.g., PayScale.com). The overarching goal of a market review is often to ensure what is called external equity (external competitiveness), which has to do with how competitive an organization’s pay practices are for particular job relative to other competitor organizations. For example, for a given job, an organization might lead, match, or lag the market pay rates for that job. By ensuring external equity in our organization for critical jobs, we will hopefully be better positioned to attract, motivate, and retain talented individuals. In fact, some organization’s adopt adhere to market pricing for certain jobs, which refers basing the pay levels for a job directly on competitor organizations’ pay levels. The market-review process consists of the following four steps. Step 1: Choose Two or More Market-Survey Sources. No single market survey is ever perfect, as every market survey suffers from sampling and measurement error, some market surveys are conducting using more (or less) rigorous survey methodologies, and some market survey provide more breadth whereas others provide more depth. Given these reasons, we typically opt to gather data from at least two market-survey sources so that we don’t rely to heavily on a single flawed survey. Step 2: Match the Job Descriptions. Often we won’t be able to find market survey data for every single job in our organization. Instead, we focus on what are called benchmark jobs (key jobs), which are jobs that are common within or across industries. For example, in the healthcare industry in the U.S., the job of a Registered Nurse would be considered a benchmark job, as most hospitals will employee at least one Registered Nurse and thus have pay data for that job. Once we’ve identified our benchmark jobs, we look through the market survey to see if we can find job title and/or job description matches. Even if the job title of our benchmark job differs from that of a benchmark job in the market survey, we can still potentially match the job based on the job description. This can be a tricky step, as the amount of information you have in your organization’s job description for the benchmark job in question may far exceed the amount of information in the market survey. Thus, using our subject matter expertise and knowledge of the job in question, we do our best to ensure that the market survey job is in fact equivalent or comparable to the job in our organization. As an example of how job titles and job descriptions in two market-survey sources can differ for what is essentially the same benchmark job, please refer to Table 1. Table 1: Example of how job titles and descriptions can differ between market-survey sources for the same benchmark job. Step 3: Age the Survey Data. Different market surveys will collect pay data from organizations at different points in time, which means that the raw data from these surveys will often need to be aged to some date in the future. That is, often we wish to estimate the market pay rate of a benchmark job for some time point in the future. In doing so, we can make the survey data more up-to-date, consistent, and comparable across the market-survey sources we have access to. To do so, we need to determine the annual market movement rate (e.g., 3.1%) based on a budget growth projection, and using this information we can calculate what is referred to as an aging factor. Step 4: Apply Survey Weights. Different market surveys are based on different sample sizes (e.g., number of organizations, number of employees), and as noted above, some market surveys are more rigorous from a methodological perspective. To account for sampling error, we can apply compute sample-weighted means (i.e., averages) in order to “weight” the survey data. In doing so, we can prioritize market pay data from market surveys based on larger sample sizes. Larger sample sizes are associated with lower sampling error, as they tend to me more representative of the underlying population from which the sample was drawn. In the following sections, we will learn what it means to “age” market survey data and how we can apply market survey weights to the data. Doing so, allows us to make better judgments regarding the external competitiveness of an organization’s pay practices. 57.1.1 Aging Market Survey Data Aging market survey (or pay) data refers to the process of “[identifying] when the pay data were originally collected and then [weighting] the data based on the expected change in the market pay rates resulting from merit-based increases, cost-of-living adjustments, and other factors that affect pay” (Bauer et al. 2025, 380–81). The process requires simple arithmetic and includes the following steps: Identify an appropriate annual wage/salary budget growth projection from a vendor like WorldatWork (e.g., 3.1% or .031). If the the budget growth projection from step 1 is an annual growth projection, then divide the value by 12 obtain a monthly value (e.g., .031 / 12 = .003). Determine how many months the data need to be aged (e.g., 7 months), multiply the value from step 2 by the number of months (e.g., .003 x 7 = .018), and given that growth is expected, add 1 to the product (e.g., .018 + 1 = 1.018) – the resulting value (e.g., 1.018) is the aging factor. Multiply each pay rate by the aging factor (e.g., 15.00 x 1.018 = 15.27) to age the data. Given that different market survey sources might have been collected at different times, we may need to calculate and apply a different aging factor to each market survey source. 57.1.2 Applying Market Survey Weights Applying market survey weights allows us to take into consideration the amount of sampling error (or lack thereof) in and/or level of rigor of different market survey sources. The process requires simple arithmetic and follows any aging of the data that was needed. We weight market survey data in different ways and for different reasons. For example, we could give greater weights to those surveys whose pay rate estimates were based on more employees or on more organizations. Alternatively, we could weight market survey data on the level of methodological rigor applied by the developer of the survey. In this chapter, we will calculate sample-weighted means based on the number of employees used to calculate the median hourly rate for a given job from a given survey; we could, however, have just as easily applied the same process but based on the number of organizations used to calculate the median hourly rate for a given job from a given survey. 57.1.3 Conceptual Video For a more in-depth review of market surveys, please check out the following conceptual video. Link to conceptual video: https://youtu.be/ey_QzvUIFyQ 57.2 Tutorial This chapter’s tutorial demonstrates age market survey and apply market survey weights using R. 57.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorials below. Link to video tutorial: https://youtu.be/RUWuiwV8PPE Link to video tutorial: https://youtu.be/ixi7vozspUs 57.2.2 Functions &amp; Packages Introduced Function Package round base R View base R print base R function base R matrix base R nrow base R rowSums base R c base R 57.2.3 Initial Steps If you haven’t already, save the file called “MarketSurveyData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “MarketSurveyData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object md &lt;- read_csv(&quot;MarketSurveyData.csv&quot;) ## Rows: 6 Columns: 8 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Job_ID, Job_Title ## dbl (6): Survey_1, Survey_2, Survey_3, SS_Survey_1, SS_Survey_2, SS_Survey_3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(md) ## [1] &quot;Job_ID&quot; &quot;Job_Title&quot; &quot;Survey_1&quot; &quot;Survey_2&quot; &quot;Survey_3&quot; &quot;SS_Survey_1&quot; &quot;SS_Survey_2&quot; &quot;SS_Survey_3&quot; # Print variable type for each variable in data frame (tibble) object str(md) ## spc_tbl_ [6 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Job_ID : chr [1:6] &quot;FE10&quot; &quot;FE12&quot; &quot;GE12&quot; &quot;GE13&quot; ... ## $ Job_Title : chr [1:6] &quot;Packaging Operator&quot; &quot;Machine Operator&quot; &quot;Electronics Assembler&quot; &quot;Coil Winder&quot; ... ## $ Survey_1 : num [1:6] 14.1 15.9 15.7 16 19.4 ... ## $ Survey_2 : num [1:6] 14.5 15.8 15.8 15.5 20.1 ... ## $ Survey_3 : num [1:6] 14 16 15.3 16.1 19.8 ... ## $ SS_Survey_1: num [1:6] 101 88 90 56 26 32 ## $ SS_Survey_2: num [1:6] 35 29 27 21 20 22 ## $ SS_Survey_3: num [1:6] 13 11 11 6 4 4 ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Job_ID = col_character(), ## .. Job_Title = col_character(), ## .. Survey_1 = col_double(), ## .. Survey_2 = col_double(), ## .. Survey_3 = col_double(), ## .. SS_Survey_1 = col_double(), ## .. SS_Survey_2 = col_double(), ## .. SS_Survey_3 = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(md) ## # A tibble: 6 × 8 ## Job_ID Job_Title Survey_1 Survey_2 Survey_3 SS_Survey_1 SS_Survey_2 SS_Survey_3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FE10 Packaging Operator 14.1 14.5 14.0 101 35 13 ## 2 FE12 Machine Operator 15.9 15.8 16.0 88 29 11 ## 3 GE12 Electronics Assembler 15.7 15.8 15.3 90 27 11 ## 4 GE13 Coil Winder 16.0 15.5 16.1 56 21 6 ## 5 GE14 Electronics Technician 19.4 20.1 19.8 26 20 4 ## 6 IE08 Quality Assurance Technician 18.0 17.9 18.6 32 22 4 # Print number of rows in data frame (tibble) object nrow(md) ## [1] 6 There are 6 variables and 6 cases (i.e., jobs) in the md data frame: Job_ID, Job_Title, Survey_1, Survey_2, Survey_3, SS_Survey_1, SS_Survey_2, and SS_Survey_3. Per the output of the str (structure) function above, the Job_ID and Job_Titlevariables are of type character, and the other variables are of type numeric (continuous: interval/ratio). The Job_ID variable contains the unique identifiers for jobs, and the Job_Title variable contains the job titles. The three survey variables (Survey_1, Survey_2, Survey_3) include the median hourly pay rates for these jobs, all of which are nonexempt, from each of the three market survey sources used by the organization. The variables SS_Survey_1, SS_Survey_2, and SS_Survey_3 contain the number of employees used in each market survey source to estimate the median hourly pay rate for a given job. 57.2.4 Age the Data To age the data from the first market survey (Survey_1), follow the following four steps. Step One: To identify an appropriate annual wage/salary budget growth projection, we will use a reputable source (e.g., WorldatWork). For this tutorial, let’s assume that the annual wage/salary budget growth projection is 3.1% (i.e., .031). We’ll assign the budget growth projection value to an object called ABG (Annual Budget Growth) using the &lt;- assignment operator. # Step One: Assign annual budget growth projection value to object ABG ABG &lt;- .031 Step Two: If the the budget growth projection from Step One is an annual growth projection (which it is in this case), then for Step Two, we will divide the value by 12 obtain a monthly value. Let’s assign the calculated monthly budget growth projection value to an object called MBG (Monthly Budget Growth) using the &lt;- assignment operator. # Step Two: Assign monthly budget growth projection value to object MBG MBG &lt;- ABG / 12 Step Three: For this step, we will first determine how many months the data need to be aged. Let’s assume that the data for the first market survey (Survey_1) were estimated 6 months ago and that we want to age the data 1 month into the future. Given those assumptions, we need to age the market survey data by a total of 7 months (i.e., 6 + 1 = 7). Second, multiply the value from Step Two by the number of months that the data need to be aged. Third, given that growth is expected, add 1 to the product. The resulting value is the aging factor. Let’s assign the aging factor value to the object AF (aging factor). # Step Three: Calculate and assign aging factor to object AF AF &lt;- (MBG * 7) + 1 Step Four: Let’s multiply each hourly pay rate from the first market survey (Survey_1) by the aging factor to age the data. We’ll create a new variable called Survey_1_aged and and assign the aged survey data to it using the &lt;- assignment operator. Make sure to note that the Survey_1 variable and the new Survey_1_agedvariable belong to the md data frame by using the $ operator. As an optional follow-up step, use the round function from base R to round the values from the new Survey_1_agedvariable to two digits after the decimal. # Step Four: Age the data md$Survey_1_aged &lt;- md$Survey_1 * AF # Optional: Round the new variable to two digits after the decimal md$Survey_1_aged &lt;- round(md$Survey_1_aged, 2) Now that you have aged the data, take a look at the data frame object (md) to marvel at your work. # View data frame View(md) Alternatively, you can print the vector of sample-weighted means to your console. # Print vector of aged survey data print(md$Survey_1_aged) ## [1] 14.37 16.19 15.94 16.30 19.70 18.27 Note: Given that different market survey sources might have been collected at different times, you may need to calculate and apply a different aging factor to each market survey source. For the sake of brevity, let’s assume that the three market-survey sources all require the same aging factor (which likely won’t be the case in real life). We are doing this so that we can prepare to pass the aged data along to the next, phase in which we apply market-survey weights. # Age remaining market survey data (with rounding to two digits) md$Survey_2_aged &lt;- round(md$Survey_2 * AF, 2) md$Survey_3_aged &lt;- round(md$Survey_3 * AF, 2) 57.2.4.1 Optional: Age the Data with a Function To streamline the four-step process from above, we can create our own function to age data. For example, we can create a function called age using function from base R. The script contained within the curly braces ({ }) essentially replicates what we did in the four-step process above. You don’t need to change anything in the script below – just run it as is. # Optional: Create an aging function age &lt;- function(data, ABG, age_months) { MBG &lt;- ABG / 12 AF &lt;- (MBG * age_months) + 1 round((data * AF), 2) } Using the new age function, as the first argument, type data= followed by the name of data frame (md) followed by $ and the name of the market survey variable (Survey_1). As the second argument, type ABG= followed by the projected budget growth value (.031). As the third argument, type age_months followed by the number of months for which you would like to age the data. Use the &lt;- assignment operator to assign the resulting values to a new variable (Survey_1_aged) to be added to your existing data frame (md). # Age the market survey data md$Survey_1_aged &lt;- age(data=md$Survey_1, ABG=.031, age_months=7) # Print vector of aged survey data print(md$Survey_1_aged) ## [1] 14.37 16.19 15.94 16.30 19.70 18.27 Note: Given that different market survey sources might have been collected at different times, you may need to calculate and apply a different aging factor to each market survey source. For the sake of brevity, let’s assume that the three market-survey sources all require the same aging factor (which likely won’t be the case in real life). We are doing this so that we can prepare to pass the aged data along to the next, phase in which we apply market-survey weights. # Age remaining market survey data (with rounding to two digits) md$Survey_2_aged &lt;- age(data=md$Survey_2, ABG=.031, age_months=7) md$Survey_3_aged &lt;- age(data=md$Survey_3, ABG=.031, age_months=7) 57.2.5 Compute the Sample-Weighted Means After aging the data, we are ready to compute the sample-weighted means, which allows us to apply market survey weights to the data. To compute the sample-weighted means for each job, first, we will multiply each job’s median hourly pay rate by the survey sample size on which its based, sum those products, and then divide that sum by the grand total of the sample sizes for all surveys used to estimate a job’s median hourly rate. This process gives us the sample-weighted mean for each job. Let’s use the &lt;- assignment operator to assign the sample-weighted means to a new variable called Weighted_Mean that we will apply to our data frame (md). As a final step, let’s apply the round function from base R to round the values from the new Weighted_Meanvariable to two digits after the decimal. # Compute sample-weighted means md$Weighted_Mean &lt;- ((md$Survey_1_aged * md$SS_Survey_1) + (md$Survey_2_aged * md$SS_Survey_2) + (md$Survey_3_aged * md$SS_Survey_3)) / (md$SS_Survey_1 + md$SS_Survey_2 + md$SS_Survey_3) # Optional: Round the new variable to two digits after the decimal md$Weighted_Mean &lt;- round(md$Weighted_Mean, 2) Now that we have computed sample-weighted means based on the median hourly pay rate for each job, take a look at the data frame verify that we performed this operation correctly. # View data frame View(md) Alternatively, you can print the vector of sample-weighted means to your console. # Print vector of sample-weighted means print(md$Weighted_Mean) ## [1] 14.45 16.19 15.94 16.18 20.05 18.29 57.2.5.1 Optional: Compute the Sample-Weighted Means with a Function If we wish to streamline the sample-weighted mean calculations from above, we can create our own function. For example, we can create a function called weighted.mean using function from base R. The script contained within the curly braces ({ }) replicates what we did above but uses matrix mathematics instead. You don’t need to change anything in the script below – just run it as is. # Optional: Create a sample-weighted mean function weighted.mean &lt;- function(pay_rates, sample_sizes) { a &lt;- matrix(pay_rates, nrow=nrow(md)) b &lt;- matrix(sample_sizes, nrow=nrow(md)) c &lt;- a * b d &lt;- rowSums(c) / rowSums(b) round(d, 2) } Using the new weighted.mean function, as the first argument, we will type pay_rates= followed by the vector of market survey variables that contain the pay rate data from each market survey; be sure to include the name of the data frame (md) followed by $ and the name of the market survey variable (e.g., Survey_1) for each market survey variable. As the second argument, we will type sample_sizes= followed by the vector of sample size variables that contain the sample size data for each market survey, making sure to include the name of the data frame (md) followed by $ and the name of the sample size variable (e.g., SS_Survey_1). To create the aforementioned vectors, we will use the c function from base R. Use the &lt;- assignment operator to assign the resulting values to a new variable (Weighted_Mean) to be added to your existing data frame (md). # Calculate sample-weighted means md$Weighted_Mean &lt;- weighted.mean(pay_rates=c(md$Survey_1_aged, md$Survey_2_aged, md$Survey_3_aged), sample_sizes=c(md$SS_Survey_1, md$SS_Survey_2, md$SS_Survey_3)) # Print vector of sample-weighted means print(md$Weighted_Mean) ## [1] 14.45 16.19 15.94 16.18 20.05 18.29 57.2.6 Summary In this chapter, we first learned how to age market survey data using simple arithmetic and variable assignment in R. Next, we learned how to apply market survey weights by calculating sample-weighted means. References "],["marketpayline.html", "Chapter 58 Estimating a Market Pay Line Using Linear &amp; Polynomial Regression 58.1 Conceptual Overview 58.2 Tutorial", " Chapter 58 Estimating a Market Pay Line Using Linear &amp; Polynomial Regression In this chapter, we will learn how to estimate a market pay line using simple linear regression and polynomial regression. 58.1 Conceptual Overview Estimating a market pay line allows us to determine the extent to which our internal job structure is aligned with external market pay practices for benchmark jobs. Further, a market pay line can help us estimate an appropriate pay level for a nonbenchmark job. Finally, a market pay line can be used as the basis for a pay policy line and, ultimately, creating pay bands/grades. To estimate a market pay line, we must regress market pay data on job evaluation points or numeric pay grade levels. If we assume that the association is linear, then we can apply simple linear regression. If, however, we assume that the association is nonlinear, then we can apply polynomial regression or perform a logarithmic transformation of the pay variable. In this chapter, we will practice both simple linear regression and polynomial regression to estimate linear and nonlinear associations, respectively. For a review of these forms of regression and additional background information, please refer to the chapters covering simple linear regression and polynomial regression. Please note that in this chapter our predictor variable will be job evaluation points, as the point-factor system/method of job evaluation tends to be one of the most common approaches. Using numeric pay grades/levels that were developed using some type of job evaluation approach would also be acceptable. 58.1.1 Statistical Assumptions For information regarding the statistical assumptions that should be met for simple linear regression and polynomial regression, please refer to the corresponding section from the chapters covering simple linear regression and polynomial regression. 58.1.2 Statistical Significance For information regarding statistical significance in the context of simple linear regression and polynomial regression, please refer to the corresponding section from the chapters covering simple linear regression and polynomial regression. 58.1.3 Practical Significance For information regarding practical significance in the context of simple linear regression and polynomial regression, please refer to the corresponding section from the chapters covering simple linear regression and polynomial regression. 58.1.4 Conceptual Video For a more in-depth review of market surveys, please check out the following conceptual video. Link to conceptual video: https://youtu.be/NEUmJmsIt3k 58.2 Tutorial This chapter’s tutorial demonstrates how to estimate a market pay line using simple linear regression and polynomial regression in R. 58.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/lfOccvOWMAU 58.2.2 Functions &amp; Packages Introduced Function Package filter dplyr lm base R plot base R cooks.distance base R sort base R head base R summary base R abline base R predict base R View base R lines base R poly base R seq base R min base R max base R data.frame base R 58.2.3 Initial Steps If you haven’t already, save the file called “MarketPayLine.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “MarketPayLine.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object mp &lt;- read_csv(&quot;MarketPayLine.csv&quot;) ## Rows: 25 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Job_Family, Job_ID, Job_Title ## dbl (2): Points, Pay ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(mp) ## [1] &quot;Job_Family&quot; &quot;Job_ID&quot; &quot;Job_Title&quot; &quot;Points&quot; &quot;Pay&quot; # Print variable type for each variable in data frame (tibble) object str(mp) ## spc_tbl_ [25 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Job_Family: chr [1:25] &quot;OA&quot; &quot;OA&quot; &quot;OA&quot; &quot;OA&quot; ... ## $ Job_ID : chr [1:25] &quot;OA132&quot; &quot;OA133&quot; &quot;OA134&quot; &quot;OA135&quot; ... ## $ Job_Title : chr [1:25] &quot;Front Desk Receptionist&quot; &quot;Mail Clerk&quot; &quot;Data Entry Specialist&quot; &quot;Admissions Coordinator&quot; ... ## $ Points : num [1:25] 300 401 444 456 465 469 470 475 482 484 ... ## $ Pay : num [1:25] 25000 35095 32735 29428 30101 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Job_Family = col_character(), ## .. Job_ID = col_character(), ## .. Job_Title = col_character(), ## .. Points = col_double(), ## .. Pay = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(mp) ## # A tibble: 6 × 5 ## Job_Family Job_ID Job_Title Points Pay ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 OA OA132 Front Desk Receptionist 300 25000 ## 2 OA OA133 Mail Clerk 401 35095 ## 3 OA OA134 Data Entry Specialist 444 32735 ## 4 OA OA135 Admissions Coordinator 456 29428 ## 5 OA OA136 Call Center Representative 465 30101 ## 6 OA OA137 Office Specialist 469 32613 # Print number of rows in data frame (tibble) object nrow(mp) ## [1] 25 There are 5 variables and 25 cases (i.e., jobs) in the mp data frame: Job_Family, Job_ID, Job_Title, Points, and Pay. Per the output of the str (structure) function above, the Job_Family, Job_ID, and Job_Titlevariables are of type character, and the other variables are of type numeric (continuous: interval/ratio). The Job_Family variable inludes the code for two job families: NU (nurse) and OA (office). The Job_ID variable contains the unique identifiers for the benchmark jobs jobs, and the Job_Title variable contains the job titles. The Points variable includes job evaluation points associated with each benchmark job. The Pay variable includes the market pay rates based on market survey/review data. 58.2.4 Estimate a Market Pay Line In this tutorial, we will work through three examples of estimating a market pay line. First, we will estimate a market pay line that shows strong fit to the data. Second, we will estimate a market pay line that shows weaker fit to the data. Third, we will estimate a nonlinear market pay line – well, technically a market pay curve. 58.2.4.1 Example of Stronger Linear Model Fit For this first model, we will estimate a market pay line that shows strong fit to the data. In fact, the market pay line shows that the internal job structure (as evidenced by job evaluation points) is closely in tune with the prevailing market pay rates for the benchmark jobs. Prepare Data. As an initial step, we need to subset our mp data frame object. You might not necessarily need to do this with real data, but we need to do so with this data frame object. Specifically, we will subset (i.e., filter) the mp data frame object such that we only retain those cases (i.e., jobs) from the nurse (NU) job family. When working with real data, we might choose to drill down to specific job families (like we are doing here), or we might include all job families in a single model. Sometimes this decision might be made for us, as perhaps the organization has only completed a job evaluation for certain job families. In other instances, we might subset our data by geographic location (e.g., country), for example. To subset our data, we will use the filter function from the dplyr package (Wickham et al. 2023). For more information on the filtering data, check out the chapter on filtering data. If you haven’t already, please install and access the dplyr package. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) Using an approach with pipes, first, use the &lt;- assignment operator to name the filtered data frame that we will create. For this example, I name the new filtered data frame nurse. Second, type the name of the original data frame we read in at the beginning of this tutorial, which we named mp, followed by the pipe (%&gt;%) operator. This will “pipe” our data frame into the subsequent function. Third, either on the same line or on the next line, type the filter function. Fourth, within the function parentheses, type the name of the variable we wish to filter the data frame by, which in this example is Job_Family. Fourth, type a logical operator, which for this example is ==. Fifth, type a value for the filter variable, which in this example is “NU” (for nurse job family); because the Job_Family variable is of type character, we need to put quotation marks (\" \") around the value of the variable that we wish to filter by. # Subset/filter data to retain just nurse family jobs nurse &lt;- mp %&gt;% filter(Job_Family==&quot;NU&quot;) Visualize Bivariate Association. Just as we would with any bivariate regression model involving two continuous variables, we will begin by creating a scatter plot to “eyeball” whether we think there is evidence of linearity. Using the plot function from base R, as the first argument, type the name of the data frame object (nurse), followed by the $ operator and the name of the predictor variable, which is the job evaluation points (Points) variable. As the second argument, type the name of the data frame object (nurse), followed by the $ operator and the name of the outcome variable, which is the market pay (Pay) variable. Optionally, as the third and fourth arguments, use xlab and ylab to specify the labels for the x-axis and y-axis, respectively. # Create scatter plot for predictor and outcome variables plot(nurse$Points, # Insert predictor variable nurse$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;) # Set y-axis label As you can see, there is a very clear linear trend, and you can perhaps envision a regression line superimposed on the plot with minimal residual error. It seems safe to assume that simple linear regression will be appropriate in this case. Specify Model. Next, using the lm function from base R, we will specify our simple linear regression model. Note that the lm (linear model) function is described in detail in the chapter supplement for the chapter that introduces simple linear regression. We use the lm function here instead of, say, the Regression function from lessR because the lm function is more flexible and will be easier to work with in this context. Let’s come up with a name for the regression model object and include it to the left of the &lt;- assignment operator; here, I name the model reg.nurse. To the right of the &lt;- operator, type the name of the lm function. As the first argument, specify the regression model. Remember, the outcome variable (Pay) goes to the left of the tilde (~), and the predictor variable (Points) goes to the right of the tilde (~). As the second argument, type data= followed by the name of the data frame object to which both variables in the model belong (nurse). # Specify simple linear regression model reg.nurse &lt;- lm(Pay ~ Points, data=nurse) Note that we won’t request any output at this point because first we are going to evaluate whether we have reason to believe that we have met the statistical assumptions of a simple linear regression. Test Statistical Assumptions. Recall that we generated a bivariate scatter plot between the Points and Pay variables to assess the assumption of linearity. In addition, we have good reason to believe that we have also met the assumption of bivariate normality, as the plotted values follow nearly in a perfect line. Now we will generate additional plots and other output to inform our conclusions whether we have satisfied additional statistical assumptions. We will begin by generating a scatter plot displaying the association between the fitted (predicted) values and residuals. To do so, we will use the plot function from base R. As the first argument, enter the name of the regression model object we created above (reg.nurse). As the second argument, type the numeral 1, which will request the first of four possible diagnostic plots, of which we will review three. # Diagnostics plot: fitted values &amp; residuals plot(reg.nurse, 1) The plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable, but the language “fitted” is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential bivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals do not appear to be necessarily equal across observations (potential evidence of heteroscedasticity), but the average residual error value appears to be about zero (which is good). Further, there appear to be three cases that are flagged as a potential bivariate outliers (i.e., row numbers 4, 6, and 7). Now, we should be cautious interpreting these values, however, because we have a very small sample size here (i.e., 7), and even very minor residual errors could appear to be notable. Ideally, we would want to estimate a model with at least 10 cases/observations, but for the sake of demonstration here, we are using a smaller sample. Like any statistical model, we need to use our best judgment. in determining whether to remove outliers/influential cases or not. If we dropped those three cases, we would be left with just four cases, do dropping them would not be wise. We would, however, want to verify that the predictor and outcome values for these cases are accurate. All in all, for a sample size of just seven cases, this plot doesn’t look too bad. As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the plot script from above, but this time, enter the numeral 2 (instead of 1) to request the second diagnostic plot. # Diagnostics plot: normal Q-Q plot plot(reg.nurse, 2) Normally distributed residuals will fall along the dotted diagonal line. As you can see, the residuals fall, for the most part, on or very near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers 4, 6, and 7. Even those potential outliers, however, fall very close to the line. As another diagnostic plot, let’s look at Cook’s distance (D) across cases. Once again, adapt the plot script from above, but this time, enter the numeral 4 to request the fourth diagnostic plot. We’re skipping the third plot. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.nurse, 4) Clearly, cases associated with row numbers 6 and 7 seem like they might have problematic Cook’s distances, but let’s be careful not to jump to conclusions. Instead, let’s estimate the exact values of the Cook’s distance values using the cooks.distance function from base R, along with the sort function from base R. In addition, let’s view the Cook’s distance values using the head function from R. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.nurse) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 7 6 1 5 2 4 3 ## 0.98120478471 0.42525723584 0.11795980895 0.08726355151 0.07273564234 0.06107998202 0.00006240292 There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size, which in this case would give us .57. Here, we see only the Cook’s distance value associated with row number 7 exceeds .57, flagging it as a potential outlier/influential case. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). Technically, none of our Cook’s distance values exceed 1, and given our very small sample size, let’s go with the cutoff of 1, which means that we will assume that we don’t have any outliers/influential cases. All in all, in spite of the very small sample size, we can be reasonably satisfied that we have met the statistical assumptions of a simple linear regression. Interpret the Model Results. To request the results of our simple linear regression model, we will type the name of the summary function from base R and include whatever we named your regression model (reg.nurse) as the sole parenthetical argument. The summary function simply returns a summary of your estimated regression model results. # Get summary of simple linear regression model results summary(reg.nurse) ## ## Call: ## lm(formula = Pay ~ Points, data = nurse) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 1379.34 -2125.51 99.47 3359.72 -2863.64 -5584.40 5735.03 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -99921.51 9098.08 -10.98 0.000109 *** ## Points 226.22 11.09 20.39 0.00000525 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4243 on 5 degrees of freedom ## Multiple R-squared: 0.9881, Adjusted R-squared: 0.9857 ## F-statistic: 415.8 on 1 and 5 DF, p-value: 0.000005247 The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called Coefficients contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, t-values, and p-values. Typically, the intercept value and its significance test are not of substantive interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficient for the predictor variable (Points) in relation to the outcome variable (Pay) is of substantive interest. Here, we see that the unstandardized regression coefficient for Points is 226.22, and its associated p-value is less than .001 (b = 226.22, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: For every additional job evaluation point, market pay increases by $226.22. In other words, a single job evaluation point is worth $226.22 according to this model. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Pay = -99921.51 + (226.22 * Points)\\) If we plug in, for instance, the 600 for Points, then we get $35,810.49, as shown below: \\(35810.49 = -99921.51 + (226.22 * 600)\\) Thus, we are able to estimate the pay for nonbenchmark jobs using our estimated regression model. Below the table containing the regression coefficient estimates, the (unadjusted) R-squared (R2) and adjusted R2 values appear, which are indicators of the model’s fit to the data as well as the extent to which the predictor variable explains variability in the outcome variable. Sometimes the R-squared (R2) value is referred to as the coefficient of determination. First, the R-squared (R2) value of .988 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 98.8% of the variance in Pay is explained by Points. This raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .986 (or 98.6%). Typically, it’s a good idea to report both values, and in this case, both of these values are exceptionally high, which means that the model does an outstanding job of fitting the data. In other words, the internal alignment of these nurse family of jobs within the organization closely mirrors the external market pay practices for these jobs. Below the R-squared (R2) and adjusted R2 values, the F-value and p-value is a statistical test of overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. The F-value and its associated p-value indicate whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variable(s). In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 415.8 and its associated p-value is very small (less than .001), the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). You can think of the R2 values as indicators of effect size at the model level. Below are some rules of thumb for qualitatively interpreting the magnitude of the effect size in the context of estimating a market pay line. Note that these rules of thumb depart from the typical rules of thumb we use for R2 values because estimating a market pay line is very special context. R2 Qualitative Descriptor .90-1.00 Excellent .80-.89 Great .70-.79 Good .50-.69 Acceptable .40-.49 Questionable .00-.39 Unacceptable Visualize the Model. It’s often useful to plot the regression model – specifically, the regression line – as it reflects the market pay line. To do so, let’s start with the same script we used to visualize the bivariate association between Points and Pay above; however, as additional arguments, let’s use xlim and ylim to specify the x-axis and y-axis ranges, which requires specifying a two-value vector with the c function from base R. To superimpose the regression line on the scatter plot, we will use the abline function from base R. As the first argument, type the name of the regression model (reg.nurse). As an optional second argument, type col= followed by the name of a color in quotation marks (\"red\") to specify the color of the line; the default is black. # Create scatter plot for predictor and outcome variables plot(nurse$Points, # Insert predictor variable nurse$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(550,1000), # Set x-axis lower and upper limits ylim=c(0,140000)) # Set y-axis lower and upper limits # Add regression line to plot abline(reg.nurse, col=&quot;red&quot;) As you can see, our regression line very closely fits the data, which was evidenced statistically by the very high R2 value. Predicting Market Pay Line Values. Using the regression model we estimated (reg.nurse), we can plug in the existing job evaluation points for the benchmark jobs in the data to see what the estimated market pay values would be for those jobs (and then we could compare them with the actual pay for those jobs, assuming we had access to those data, which we don’t in this example). These predicted values could feasibly be used to help determine the midpoints for pay grades/bands. Let’s create a new variable called Predicted_Pay that contains these predicted pay values for known job evaluation point values. Specify that the new Predicted_Pay variable will be attached to the existing nurse data frame by using the $ operator. Next, use the &lt;- operator to assign values to the new variable. To the right of the &lt;- operator, type the name of the predict function from base R. As the first parenthetical argument, type the name of the regression model (reg.nurse). As the second argument, type newdata= followed by the name of the existing data frame called nurse which will allow the function to pull actual values on the Points variable and plug them into the regression equation for prediction purposes. # Predict outcome values using model nurse$Predicted_Pay &lt;- predict(reg.nurse, newdata=nurse) # View your data frame View(nurse) Note that the new Predicted_Pay variable in your nurse data frame contains values that could be used as the midpoint for these select benchmark jobs from the nurse job family. Optional: Estimating Minimum &amp; Maximum Pay for Benchmark Jobs. If we wish to, we can estimate the minimum and maximum pay for benchmark jobs (or for pay grades). To do so, we need to come up with a range spread value, which represents how much greater the maximum possible pay for a job (or grade) differs from the minimum. The range spread value you choose should be based on the pay philosophy and policies in your organization. For the sake of demonstrate, let’s set the range spread at 20% (.20) for the nurse job family. Next, create new variables called Min_Pay and Max_Pay. To create the Min_Pay variable, divide the Predicted_Pay variable we created above by 1 plus the range spread divided by 2. # Create minimum pay variable nurse$Min_Pay &lt;- nurse$Predicted_Pay / (1 + .20/2) To create the Max_Pay variable, multiply the Min_Pay variable times 1 plus the range spread. # Create maximum pay variable nurse$Max_Pay &lt;- nurse$Min_Pay * (1 + .20) Let’s now add the our midpoint (which reflects Predicted_Pay), the minimum pay (Min_Pay), and the maximum pay (Max_Pay) on the same scatterplot we created a bit earlier in the tutorial. Using the lines function from base R, we will add the three lines corresponding to the midpoint, minimum, and maximum for the nurse family of benchmark jobs. # Create scatterplot for predictor and outcome variables plot(nurse$Points, # Insert predictor variable nurse$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(550,1000), # Set x-axis lower and upper limits ylim=c(0,140000)) # Set y-axis lower and upper limits # Add midpoint (predicted pay) line to plot lines(nurse$Points, nurse$Predicted_Pay, col=&quot;red&quot;) # Add minimum pay line to plot lines(nurse$Points, nurse$Min_Pay) # Add maximum pay line to plot lines(nurse$Points, nurse$Max_Pay) 58.2.4.2 Example of Weaker Linear Model Fit In this second example, we will estimate a market pay line that shows weaker fit to the data than the first example. As an initial step, we need to subset our mp data frame object once more. Just as we did above, to subset our data, we will use the filter function from the dplyr package. If you haven’t already, please install and access the dplyr package. Just as we did in the first example, filter the mp data frame, except this time retain only those cases that are associated with the “OA” (office) job family. Let’s name the new data frame office. # Subset/filter data to retain just office family jobs office &lt;- mp %&gt;% filter(Job_Family==&quot;OA&quot;) Visualize Bivariate Association. We will begin by creating a scatter plot to “eyeball” whether we think there is evidence of linearity. # Create scatter plot for predictor and outcome variables plot(office$Points, # Insert predictor variable office$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;) # Set y-axis label There appears to be more-or-less a linear trend. It seems reasonable to assume that simple linear regression will be appropriate for fitting a model to these data. Specify Model. Next, let’s specify our simple linear regression model for the office family of jobs. Let’s name the regression model object reg.office. # Specify simple linear regression model reg.office &lt;- lm(Pay ~ Points, data=office) Test Statistical Assumptions. Based on the scatter plot we created above, we can feel reasonably confident that met the assumption of bivariate normality, as the plotted values seem to somewhat adhere to an ellipse shape. Next, let’s generate a scatter plot displaying the association between the fitted (predicted) values and residuals. # Diagnostics plot: fitted values &amp; residuals plot(reg.office, 1) In this plot, we can see that the variances of the residuals do not appear to be necessarily equal across observations (potential evidence of heteroscedasticity), but the average residual error value appears to be reasonably close to zero (which is good). Further, there appear to be three cases that are flagged as a potential bivariate outliers (i.e., row numbers 13, 16, and 18). If these were real data, we would want to verify that the predictor and outcome values for these three cases are accurate. All in all, this plot doesn’t look too bad, so let’s move on to the next diagnostic plot. The Q-Q plot provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). # Diagnostics plot: normal Q-Q plot plot(reg.office, 2) As you can see, some of the residuals fall on or near the line with the exception of some cases that deviate, which shows some departure from normality in the residuals. The following plot shows the Cook’s distance (D) across cases. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.office, 4) The cases associated with row numbers 17 and 18 seem like they might have problematic Cook’s distances, but let’s be careful not to jump to conclusions. Instead, let’s estimate the exact values of the Cook’s distance values. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.office) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 18 17 2 16 13 15 8 4 10 5 12 ## 0.656762965 0.223627186 0.077425779 0.074014305 0.049588694 0.048505115 0.048235670 0.048134209 0.045766159 0.043496277 0.037231721 ## 7 14 1 6 11 9 3 ## 0.036133566 0.030636966 0.029627806 0.012312294 0.008786797 0.001666541 0.001378862 There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size, which in this case would give us .22. Here, we see only the Cook’s distance values associated with row numbers 17 and 18 exceed .2, flagging them as potential outliers/influential cases. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cook’s distance values exceed 1, and thus we will assume that we don’t have any troublesome outliers/influential cases. While by no means have we completely satisfied the statistical assumptions, we can be reasonably satisfied - although it should already be apparent that we will have a weaker-fitting model. Interpret the Model Results. To request the results of our simple linear regression model, type the name of the summary function from base R and include whatever you named your regression model (reg.office) as the sole parenthetical argument. # Get summary of simple linear regression model results summary(reg.office) ## ## Call: ## lm(formula = Pay ~ Points, data = office) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5326.1 -4263.5 113.7 4540.1 5658.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2866.05 5547.92 0.517 0.613 ## Points 69.04 10.72 6.441 0.00000815 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4534 on 16 degrees of freedom ## Multiple R-squared: 0.7217, Adjusted R-squared: 0.7043 ## F-statistic: 41.49 on 1 and 16 DF, p-value: 0.00000815 Here, we see that the unstandardized regression coefficient for Points is 69.04, and its associated p-value is less than .001 (b = 69.04, p &lt; .001), indicating that it is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: For every additional job evaluation point, market pay increases by $69.04. In other words, a single job evaluation point is worth $69.04 according to this model. Below the table containing the regression coefficient estimates, the (unadjusted) R-squared (R2) and adjusted R2 values appear. First, the R-squared (R2) value of .722 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample. In this case, we find that 72.2% of the variance in Pay is explained by Points. Next, we see that the adjusted R2 value is slightly smaller at .704 (or 70.4%). In this example, we see that the F-value is 41.49 and its associated p-value is less than .05, indicating statistical significance. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). You can think of the R2 values as indicators of effect size at the model level. Below are some rules of thumb for qualitatively interpreting the magnitude of the effect size in the context of estimating a market pay line. Note that these rules of thumb depart from the typical rules of thumb we use for R2 values because estimating a market pay line is very special context. In this example, we might describe the fit of our model as “good.” R2 Qualitative Descriptor .90-1.00 Excellent .80-.89 Great .70-.79 Good .50-.69 Acceptable .40-.49 Questionable .00-.39 Unacceptable Visualize the Model. Now it’s time to plot the regression model. # Create scatter plot for predictor and outcome variables plot(office$Points, # Insert predictor variable office$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(300,750), # Set x-axis lower and upper limits ylim=c(10000,70000)) # Set y-axis lower and upper limits # Add regression line to plot abline(reg.office, col=&quot;red&quot;) As you can see, our regression line fits the data pretty well, which was evidenced statistically by the “good” R2 value. Predict Market Pay Line Values. Let’s create a new variable called Predicted_Pay that contains these predicted pay values for known job evaluation point values. # Predict outcome values using model office$Predicted_Pay &lt;- predict(reg.office, newdata=office) # View your data frame View(office) Note that the new Predicted_Pay variable in your office data frame contains values that could be used as the midpoint for these select benchmark jobs from the nurse job family. Optional: Estimating Minimum &amp; Maximum Pay for Benchmark Jobs. If we wish to, we can estimate the minimum and maximum pay for benchmark jobs (or for pay grades). For the sake of demonstrate, let’s set the range spread at 40% (.40) for the office job family. Next, create new variables called Min_Pay and Max_Pay. To create the Min_Pay variable, divide the Predicted_Pay variable we created above by 1 plus the range spread divided by 2. # Create minimum pay variable office$Min_Pay &lt;- office$Predicted_Pay / (1 + .40/2) To create the Max_Pay variable, multiply the Min_Pay variable times 1 plus the range spread. # Create maximum pay variable office$Max_Pay &lt;- office$Min_Pay * (1 + .40) Let’s now add the our midpoint (which reflects Predicted_Pay), the minimum pay (Min_Pay), and the maximum pay (Max_Pay) on the same scatter plot we created a bit earlier in the tutorial. # Create scatter plot for predictor and outcome variables plot(office$Points, # Insert predictor variable office$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(300,750), # Set x-axis lower and upper limits ylim=c(10000,70000)) # Set y-axis lower and upper limits # Add midpoint (predicted pay) line to plot lines(office$Points, office$Predicted_Pay, col=&quot;red&quot;) # Add minimum pay line to plot lines(office$Points, office$Min_Pay) # Add maximum pay line to plot lines(office$Points, office$Max_Pay) 58.2.4.3 Example of Nonlinear Market Pay Curve In this third tutorial, we will estimate a market pay line – well, actually a curve – that is nonlinear. Specifically, we will apply first apply simple linear regression and then polynomial regression, where the latter will result in better fit to the data given the nonlinear nature of the association between Points and Pay in the sample data frame (mp). Let us assume that our goal is to estimate the association between job evaluation points (i.e., internal job structure) and market pay rates (i.e., external market) across job families, which in the mp data frame includes both the nurse and office job families. Visualize Bivariate Association. Begin by creating a scatter plot to “eyeball” whether we think there is evidence of linearity or nonlinearity. # Create scatter plot for predictor and outcome variables plot(mp$Points, # Insert predictor variable mp$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;) # Set y-axis label There appears to be somewhat of an exponential trend but definitely with some degree of linearity. Given these characteristics, let’s first estimate a model with simple linear regression in which we assume a linear association. Specify Simple Linear Regression Model. Next, we will specify our simple linear regression model. Let’s name the regression model object reg.all. # Specify simple linear regression model reg.all &lt;- lm(Pay ~ Points, data=mp) Test Statistical Assumptions for Simple Linear Regression. Based on the scatter plot we created above and the apparent nonlinearity, we cannot be very confident that we have met the assumptions of bivariate normality or linearity. Next, let’s generate a scatter plot displaying the association between the fitted (predicted) values and residuals. # Diagnostics plot: fitted values &amp; residuals plot(reg.all, 1) In this plot, we can see that the variances of the residuals do not appear to be equal across observations (potential evidence of heteroscedasticity), and the average residual error value appears to be greater than zero. The form of the association seems to imply that there might be a departure from a linear association. Let’s move on to the next diagnostic plot. The Q-Q plot provides an indication as to whether the residuals are normality distributed (another one of our statistical assumptions for simple linear regression). # Diagnostics plot: normal Q-Q plot plot(reg.all, 2) As you can see, some of the residuals fall on or near the line with the exception of some cases that deviate, which shows some departure from normality in the residuals. The following plot shows the Cook’s distance (D) across cases. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.all, 4) The case associated with row number 25 seems like it might have a problematic Cook’s distance value. Next, let’s estimate the exact values of the Cook’s distance values. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.all) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 25 1 19 2 23 20 15 24 18 16 8 ## 1.054185542 0.282264751 0.124866652 0.075576062 0.061955323 0.058036351 0.057818032 0.053965377 0.041351819 0.022738034 0.017260506 ## 7 14 22 12 3 13 10 21 11 ## 0.015861019 0.014309202 0.008216950 0.006718502 0.005249071 0.004335400 0.004304829 0.003941973 0.003246351 There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size, which in this case would give us .16. Here, we see only the Cook’s distance values associated with row numbers 1 and 25 exceed .2, flagging them as potential outliers/influential cases. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). Only the Cook’s distance value associated with row number 25 exceeds 1, and thus we should be somewhat concerned that this case might be an outlier/influential case. While by no means have we satisfied the statistical assumptions, the diagnostics did not look too bad given the relatively small sample size we are working with. Let’s proceed forward and see what the model results look like. Interpret the Simple Linear Regression Model Results. To request the results of our simple linear regression model, type the name of the summary function from base R and include whatever you named your regression model (reg.all) as the sole parenthetical argument. # Get summary of simple linear regression model results summary(reg.all) ## ## Call: ## lm(formula = Pay ~ Points, data = mp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19954.4 -5074.6 -0.9 6103.2 22107.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -35596.77 7194.05 -4.948 0.00005305291 *** ## Points 145.44 11.65 12.483 0.00000000001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10270 on 23 degrees of freedom ## Multiple R-squared: 0.8714, Adjusted R-squared: 0.8658 ## F-statistic: 155.8 on 1 and 23 DF, p-value: 0.00000000001002 Here, we see that the unstandardized regression coefficient for Points is 145.44, and its associated p-value is less than .001 (b = 145.44, p &lt; .001), indicating that it is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: For every additional job evaluation point, market pay increases by $145.44. In other words, a single job evaluation point is worth $145.44 according to this model. Next, the R-squared (R2) value of .871 indicates that 87.1% of the variance in Pay is explained by Points. Next, we see that the adjusted R2 value is slightly smaller at .866 (or 86.6%). Both of these R2 values are quite good, leading us to conclude that this model does a great job fitting the data. The F-value is 155.8 and its associated p-value is less than .05, indicating that the estimated model outperforms a model with no predictor variable(s). R2 Qualitative Descriptor .90-1.00 Excellent .80-.89 Great .70-.79 Good .50-.69 Acceptable .40-.49 Questionable .00-.39 Unacceptable Visualize the Simple Linear Regression Model. Now it’s time to plot the simple linear regression model. # Create scatter plot for predictor and outcome variables plot(mp$Points, # Insert predictor variable mp$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(300,1000), # Set x-axis lower and upper limits ylim=c(10000,140000)) # Set y-axis lower and upper limits # Add regression line to plot abline(reg.all, col=&quot;red&quot;) As you can see, our regression line fits the data pretty well, but the nonlinear nature of the association seems more apparent when we see the line superimposed on the scatter plot. Accordingly, let’s estimate a polynomial regression model to see if we can improve the fit to the data. Specify Polynomial Regression Model. Next, specify our simple linear regression model. Let’s name the regression model object reg.all_poly. Given that we see one “bend” in the association, let’s estimate a quadratic functional form by including the function poly from base R with the predictor variable (Points) as the first argument and the numeral 2 as the second argument to indicate that we want to estimated a squared (quadratic) term. If we wanted to estimate a cubic functional form (i.e., two “bends”), we would enter the numeral 3. # Specify polynomial regression model reg.all_poly &lt;- lm(Pay ~ poly(Points, 2), data=mp) Test Statistical Assumptions for Polynomial Regression. Let’s generate a scatter plot displaying the association between the fitted (predicted) values and residuals. # Diagnostics plot: fitted values &amp; residuals plot(reg.all_poly, 1) In this plot, we can see that the variances of the residuals do not appear to be equal across observations (potential evidence of heterocedasticity), and the average residual error value appears to be greater than zero; however, this plot seems to be an improve on the one that we saw for the simple linear regression on these same data. Let’s move on to the next diagnostic plot. The Q-Q plot provides an indication as to whether the residuals are normality distributed (another one of our statistical assumptions for simple linear regression). # Diagnostics plot: normal Q-Q plot plot(reg.all_poly, 2) As you can see, some of the residuals fall relatively close to the line with the exception of some cases that deviate more substantially. This plot doesn’t look much better than the one corresponding to the simple linear regression model, but it doesn’t necessarily look worse. While by no means have we completely satisfied all of the statistical assumptions, the diagnostics did not reveal anything too unreasonable, as we the polynomial regression model seems to be closer to meeting the assumption of equal variances for residuals and that the average residual is close to zero. Thus, let’s proceed forward and see what the model results look like for the polynomial regression model. Interpret the Polynomial Regression Model Results. To obtain the results of our polynomial regression model, type the name of the summary function from base R and include whatever you named your regression model (reg.all_poly) as the sole parenthetical argument. # Get summary of polynomial regression model results summary(reg.all_poly) ## ## Call: ## lm(formula = Pay ~ poly(Points, 2), data = mp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11328.8 -3158.5 -539.3 3996.0 8322.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50468 1155 43.694 &lt; 0.0000000000000002 *** ## poly(Points, 2)1 128185 5775 22.196 &lt; 0.0000000000000002 *** ## poly(Points, 2)2 41129 5775 7.122 0.000000385 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5775 on 22 degrees of freedom ## Multiple R-squared: 0.9611, Adjusted R-squared: 0.9576 ## F-statistic: 271.7 on 2 and 22 DF, p-value: 0.0000000000000003097 Here, we see that the regression coefficient for poly(Points, 2)2 (which is the squared term) is 41129, and its associated p-value is less than .001 (b = 41129, p &lt; .001), indicating that there is evidence of a statistically significant quadratic function form in terms of the association between Points and Pay; if this quadratic term were nonsignificant, then we would not conclude that there is evidence of a quadratic functional form. In a polynomial regression model, we only really care whether the coefficient associated with the highest polynomial (e.g., squared term) is statistically significant. Next, the R-squared (R2) value of .961 indicates that 96.1% of the variance in Pay is explained by Points, which is notably higher than the R2 value of .871 for the simple linear regression model. Next, we see that the adjusted R2 value is slightly smaller at .958 (or 95.8%). Both of these R2 values are excellent, leading us to conclude that this model does an excellent job fitting the data. Thus, we should probably go with the polynomial regression model with a quadratic (squared term), as it fits the data better; that said, sometimes we might opt for the linear model if it is significant, as it might offer a more parsimonious solution. The F-value is 271.7 and its associated p-value is less than .05, indicating that the estimated model outperforms a model with no predictor variable(s). R2 Qualitative Descriptor .90-1.00 Excellent .80-.89 Great .70-.79 Good .50-.69 Acceptable .40-.49 Questionable .00-.39 Unacceptable To specify the polynomial regression equation, we will need to add the raw=TRUE argument to the poly function, as shown below. Note that the p-value associated with the highest degree polynomial - which in this example is the quadratic/squared term - remains the same, but the p-values change for the intercept/constant and lower degree polynomial(s) (e.g., first degree polynomial, which is the linear coefficient in this example). If you’re interested in interpreting the significance level of the intercept/constant and lower degree polynomial(s), then interpret the model without the argument raw=TRUE in the poly function. Nevertheless, if we are interested in writing out the regression equation, then we should use the model results when raw=TRUE. # Specify polynomial regression model with raw coefficients reg.all_poly &lt;- lm(Pay ~ poly(Points, 2, raw=TRUE), data=mp) summary(reg.all_poly) ## ## Call: ## lm(formula = Pay ~ poly(Points, 2, raw = TRUE), data = mp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11328.8 -3158.5 -539.3 3996.0 8322.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66380.50514 14879.66470 4.461 0.000196 *** ## poly(Points, 2, raw = TRUE)1 -188.47826 47.34229 -3.981 0.000631 *** ## poly(Points, 2, raw = TRUE)2 0.25081 0.03522 7.122 0.000000385 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5775 on 22 degrees of freedom ## Multiple R-squared: 0.9611, Adjusted R-squared: 0.9576 ## F-statistic: 271.7 on 2 and 22 DF, p-value: 0.0000000000000003097 Here is how we would specify the polynomial regression equation: \\(Pay = 66380.51 - 188.48 * Points + .25 * Points^2\\) Let’s imagine that we have a nonbenchmark job worth 630 points. If we plug 630 points into the model, then we get an estimated pay value of $46,863.11. \\(Pay = 66380.51 - 188.48 * 630 + .25 * 630^2\\) \\(Pay = 46863.11\\) Note: When we use the lm function, sometimes our coefficients and p-values will be in scientific notation (e.g., 3.023E-4). We can of course interpret the scientific notation values as is, but if we want to use standard notation, then we can precede the running of our lm model with the the following code: options(scipen=999). # Set standard notation and specify polynomial regression model with raw coefficients options(scipen=999) reg.all_poly &lt;- lm(Pay ~ poly(Points, 2, raw=TRUE), data=mp) summary(reg.all_poly) ## ## Call: ## lm(formula = Pay ~ poly(Points, 2, raw = TRUE), data = mp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11328.8 -3158.5 -539.3 3996.0 8322.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66380.50514 14879.66470 4.461 0.000196 *** ## poly(Points, 2, raw = TRUE)1 -188.47826 47.34229 -3.981 0.000631 *** ## poly(Points, 2, raw = TRUE)2 0.25081 0.03522 7.122 0.000000385 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5775 on 22 degrees of freedom ## Multiple R-squared: 0.9611, Adjusted R-squared: 0.9576 ## F-statistic: 271.7 on 2 and 22 DF, p-value: 0.0000000000000003097 Visualize the Polynomial Regression Model. Let’s plot the polynomial regression model. Plotting a nonlinear association is a bit more involved than plotting a linear association. As the first step, create a sequence of predictor values to plug into the regression model we estimated. As I show below, this involves the seq (sequence), min (minimum), and max (maximum) functions from base R. By default, I typically estimate a sequence of values of length 1000. If you adapt this script for other data, be sure to change the name of the data frame and the name of the predictor variable to whatever they are called in the data frame. Note that in order for this script to work, it is important to name the vector object whatever the predictor variable is called, which in this example is Points. As the second step, convert the vector object that contains the sequence of predictor values to a data frame; I recommend just keeping the data frame object name the same (newdat). As the third step, use the predict function from base R to predict values for the outcome from the given model (first argument) and the given predictor values (second argument); I recommend leaving the vector object containing the predicted values the same (predicted_values). As the fourth step, use the same plot script that you used above. As the final step, use the lines function from base R to superimpose the estimated curve on the scatter plot from the previous step. The first argument is the name of the vector of predictor values from the first step (Points). The second argument is the vector of predicted values (predicted_values) based on the model. The third (optional) argument is to specify the color of the curve, which in this case is col=\"red\". # Step 1: Create object Points w/ continuous sequence of 1,000 values # to include observed range of predictor values # Change the object Points to whatever the name of your # predictor variable is Points &lt;- seq(min(mp$Points), # Minimum of observed range max(mp$Points), # Maximum of observed range length=1000) # Number of sequence values within range # Step 2: Convert vector to data frame called newdat newdat &lt;- data.frame(Points) # Change the object Points to name of predictor variable # Step 3: Estimate predicted values predicted_values &lt;- predict(reg.all_poly, # Regression model object name newdata=newdat) # Values to fit # Step 4: Create scatter plot for predictor and outcome variables plot(mp$Points, # Insert predictor variable mp$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(300,1000), # Set x-axis lower and upper limits ylim=c(10000,140000)) # Set y-axis lower and upper limits # Step 5: Add regression curve to scatter plot lines(Points, # vector of predictor values predicted_values, # Insert fitted/predicted values based on model col=&quot;red&quot;) # Set curve color to red As you can see, our estimated curve from the polynomial regression model fits the data notably better than the line from the simple linear regression model. Predict Market Pay Line Values for Polynomial Regression Model. Let’s create a new variable called Predicted_Pay that contains these predicted pay values for known job evaluation point values. # Predict outcome values using model mp$Predicted_Pay &lt;- predict(reg.all_poly, # name of regression model object newdata=mp) # name of data frame object # View the data frame View(mp) Note that the new Predicted_Pay variable in your mp data frame contains values that could be used as the midpoint for these select benchmark jobs from the nurse job family. Optional: Estimating Minimum &amp; Maximum Pay for Benchmark Jobs Using Polynomial Regression Model. If we wish to, we can estimate the minimum and maximum pay for benchmark jobs (or for pay grades). For the sake of demonstrate, let’s set the range spread at 40% (.40) for the benchmark jobs in the mp data frame. Next, create new variables called Min_Pay and Max_Pay. To create the Min_Pay variable, divide the Predicted_Pay variable we created above by 1 plus the range spread divided by 2. # Create minimum pay variable mp$Min_Pay &lt;- mp$Predicted_Pay / (1 + .40/2) To create the Max_Pay variable, multiply the Min_Pay variable times 1 plus the range spread. # Create maximum pay variable mp$Max_Pay &lt;- mp$Min_Pay * (1 + .40) Let’s now add the our midpoint (which reflects Predicted_Pay), the minimum pay (Min_Pay), and the maximum pay (Max_Pay) on the same scatterplot we created a bit earlier in the tutorial. All we are doing below is adding a sixth step to the previous plot we built, and in this sixth step, we use the lines function from base R to plot the minimum pay and maximum pay curves # Step 1: Create object Points w/ continuous sequence of 1,000 values # to include observed range of predictor values # Change the object Points to whatever the name of your # predictor variable is Points &lt;- seq(min(mp$Points), # Minimum of observed range max(mp$Points), # Maximum of observed range length=1000) # Number of sequence values within range # Step 2: Convert vector to data frame called newdat newdat &lt;- data.frame(Points) # Change the object Points to name of predictor variable # Step 3: Estimate predicted values predicted_values &lt;- predict(reg.all_poly, # Regression model object name newdata=newdat) # Values to fit # Step 4: Create scatter plot for predictor and outcome variables plot(mp$Points, # Insert predictor variable mp$Pay, # Insert outcome variable xlab=&quot;Job Evaluation Points&quot;, # Set x-axis label ylab=&quot;Market Pay ($)&quot;, # Set y-axis label xlim=c(300,1000), # Set x-axis lower and upper limits ylim=c(10000,140000)) # Set y-axis lower and upper limits # Step 5: Add regression curve to scatter plot lines(Points, # vector of predictor values predicted_values, # Insert fitted/predicted values based on model col=&quot;red&quot;) # Set curve color to red # Step 6: Add minimum and maximum pay curves to plot lines(mp$Points, mp$Min_Pay) lines(mp$Points, mp$Max_Pay) 58.2.5 Summary In this chapter, worked through three examples of estimating a market pay line. First, we estimated a market pay line that showed strong fit to the data using simple linear regression. Second, we estimated a market pay line that showed weaker fit to the data using simple linear regression. Third, we estimated a a market pay curve using polynomial regression. References "],["paydeterminants.html", "Chapter 59 Identifying Pay Determinants &amp; Evaluating Pay Equity Using Hierarchical Linear Regression 59.1 Conceptual Overview 59.2 Tutorial", " Chapter 59 Identifying Pay Determinants &amp; Evaluating Pay Equity Using Hierarchical Linear Regression In this chapter, we will learn how to identify pay determinants using hierarchical linear regression. Identifying pay determinants is an important part of pay equity analyses. 59.1 Conceptual Overview Pay determinants refer to any characteristics, behaviors, or conditions that are associated with the amount of pay employees receive. For example, in an organization that monetarily rewards those with longer lengths of service, length of service will likely be a pay determinant. Ideally, pay determinants should be legally defensible and should be related to job performance and other characteristics or behaviors that are valued by the organization. We can use hierarchical linear regression to identify pay determinants, evaluate their incremental validity, and determine whether there is evidence of pay inequity based on protected class variables. 59.1.1 Review of Hiearchical Linear Regression Hierarchical linear regression is an extension of multiple linear regression, where the latter was introduced in the chapter on incremental validity. The approach simply means that we are building up to a full model in what are called steps or blocks, wherein each step or block we add more predictor variables to our model until we eventually reach whatever we have specified as our full model. There are different reasons why we might do this. For instance, we might want to know if, collectively, two or more additional predictor variables explain incremental variance in the outcome variable after accounting for the other predictor variables already in the model. In doing so, we can compare what are referred to as nested models. A nested model model is a model that is missing one or more of the predictor variables of a full model (i.e., larger model) that contains the same predictor variables as the nested model as well as additional predictor variables. In practice, it is fairly simple to implement hierarchical linear regression as long as you are already comfortable with multiple linear regression. 59.1.1.1 Statistical Assumptions Because we will be performing hierarchical linear regression using multiple linear regression, to estimate and interpret the associated models, we need to make sure we have satisfied the statistical assumptions of multiple linear regression, which are described here. 59.1.1.2 Statistical Significance For information regarding statistical significance in the context of multiple linear regression, please refer to the corresponding section from this chapter on incremental validity. 59.1.1.3 Practical Significance For information regarding practical significance in the context of multiple linear regression, please refer to the corresponding section from this chapter on incremental validity. 59.1.2 Conceptual Videos For a more in-depth review of pay determinants, please check out the following conceptual video. Link to conceptual video: https://youtu.be/EMC2aBvjB2s For an introduction to pay equity and how hierarchical linear regression can be applied to identify potential pay inequity, please check out the following conceptual video. Link to conceptual video: https://youtu.be/L4HysNZn97U For a more in-depth review of hierarchical linear regression, please check out the following conceptual video. Link to conceptual video: https://youtu.be/Ney5uM6sWgY 59.1.2.1 Sample Write-Up To evaluate the pay determinants of 132 Customer Service Representatives (CSRs), we performed followed a two-step hierarchical linear regression process. Specifically, we were interested in determining whether the pay raise percentages that CSRs received showed evidence of pay inequity based on the protect characteristics of sex and race. In the first step, we estimated a multiple linear regression model in which the percentage pay raise variable served as the outcome variable and education level and job performance ratings served as the merit-based predictor variables; we included education level and job performance ratings as merit-based predictor variables because they are used by the organization to determine the size of CSRs’ pay raises. We found that both education level and job performance ratings were statistically significantly and positively associated with percentage pay raise received when controlling for one another (b = .217, p &lt; .001 and b = .296, p &lt; .001, respectively), such that higher education levels and higher job performance ratings were associated with larger percentage pay raises. In terms of model fit, the unadjusted R2 and adjusted R2 values were .384 and .374, respectively, where the unadjusted R2 value indicates that approximately 38% of the variance in percentage pay raise received by CSRs was explained by their education level and job performance ratings. As evidenced by the F-test, the model fit the data better than a model containing no predictor variables (F = 40.16, p &lt; .001). In the second step, we added sex and race as protected-characteristics predictor variables to our multiple linear regression model from the first step. Both education level and job performance ratings remained statistically significantly and positively associated with the percentage pay raise received when controlling for one another and the sex and race variables (b = .219, p &lt; .001 and b = .283, p &lt; .001, respectively). On the one hand, sex was not statistically significantly associated with the percentage pay raise received when controlling for the other variables in the model (b = .162, p = .167), which indicates that female CSRs were not paid differently than male CSRs when holding constant other predictor variables in the model. On the other hand, employee race was significantly associated with the percentage pay raise received. Specifically, when statistically controlling for other predictor variables, white employees received significantly higher percentage pay raises than Asian employees (b = -1.182, p &lt; .001) and Black CSRs (b = -.595, p &lt; .001), and Black employees received significantly higher percentage pay raises than Asian employees (b = .587, p &lt; .001). In terms of model fit, the unadjusted R2 and adjusted R2 values were .592 and .576, respectively, where the unadjusted R2 value indicates that approximately 59% of the variance in percentage pay raise received by CSRs was explained by their education level, job performance ratings, sex, and race. As evidenced by the F-test, the model fit the data better than a model containing no predictor variables (F = 36.61, p &lt; .001). Because the first step model is nested within the second step model, we performed a nested model comparison and found that the second step model fit the data statistically significantly better than the first step model (F = 21.49, p &lt; .001); in terms of practical significance, the difference in unadjusted R2 was .209, which indicates that the second step model explained an additional 21% of the variance in percentage pay raise when compared to the first step model, which can be considered a medium-to-large difference. In sum, when statistically controlling for the merit-based predictor variables of education level and job performance ratings, the protected characteristic variable of sex did not show evidence of incremental validity whereas the protected variable of race did; this provides evidence of pay inequity exists based on race for CSRs’ percentage pay raises. Moreover, collectively, sex and race explained an additional 21% of the variability in percentage pay raises. 59.2 Tutorial This chapter’s tutorial demonstrates how to perform hierarchical linear regression in R. 59.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/OGXX3ldUMoI 59.2.2 Functions &amp; Packages Introduced Function Package lm base R summary base R nrow base R model.frame base R anova base R tapply base R mean base R 59.2.3 Initial Steps If you haven’t already, save the file called “PayDeterminants.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “PayDeterminants.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object pd &lt;- read_csv(&quot;PayDeterminants.csv&quot;) ## Rows: 132 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): sex, race ## dbl (3): raise, educ, perf ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(pd) ## [1] &quot;raise&quot; &quot;educ&quot; &quot;perf&quot; &quot;sex&quot; &quot;race&quot; # Print variable type for each variable in data frame (tibble) object str(pd) ## spc_tbl_ [132 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ raise: num [1:132] 2.4 1.7 2.6 1.1 3.6 2.4 1 0.6 2.2 2.5 ... ## $ educ : num [1:132] 3 3 5 5 4 2 2 2 4 4 ... ## $ perf : num [1:132] 7.1 3.8 10 4 10 8.9 9.9 5.4 7.4 7.5 ... ## $ sex : chr [1:132] &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;female&quot; ... ## $ race : chr [1:132] &quot;asian&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. raise = col_double(), ## .. educ = col_double(), ## .. perf = col_double(), ## .. sex = col_character(), ## .. race = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(pd) ## # A tibble: 6 × 5 ## raise educ perf sex race ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2.4 3 7.1 female asian ## 2 1.7 3 3.8 female black ## 3 2.6 5 10 male black ## 4 1.1 5 4 female black ## 5 3.6 4 10 female black ## 6 2.4 2 8.9 male black # Print number of rows in data frame (tibble) object nrow(pd) ## [1] 132 There are 5 variables and 132 cases (i.e., employees) in the pd data frame: raise, educ, perf, sex, and race. Per the output of the str (structure) function above, the raise, educ, and perf variables are of type numeric, and the sex and race variables are of type character. The raise variable represents the percentage annual raise each employee received. The educ variable is ordinal but numerically coded and can be treated as a continuous variable in this context, where 1 = high school diploma or GED, 2 = some college, 3 = associate’s degree, 4 = bachelor’s degree, and 5 = graduate certificate or degree. The perf variable contains the supervisor-rated annual performance rating scores, which can range from 1-10, with 10 indicating very high performance. The sexvariable refers to employee sex (female, male). The race variable contains three level (asian, black, white). 59.2.4 Perform Hierarchical Linear Regression I will demonstrate one approach to running hierarchical linear regression models, which is fairly straightforward to implement. The approach requires the use of the lm and anova functions from base R. Specifically, I will demonstrate hierarchical linear regression using a two-step (i.e., two-block) example. For both steps/blocks, the percentage pay raise (raise) will be the outcome variable. In the first step/block, we will specify a model in which education level (educ) and performance level (perf) are the predictor variables, as these are expected to be acceptable determinants of pay raise percentage. In the second step/block, we will specify the full model in which sex (sex) and race/ethnicity (race) are added to the model as additional predictor variables, where sex and race are considered unacceptable determinants of pay raise percentage. Our goal will be two determine if the sex and race variables collectively explain a statistically significant amount of incremental variance in the outcome variable raise after accounting for the variance explained by educ and perf. If we find significant incremental variance explained in raise by sex and race, then we potentially have evidence of managers exhibiting implicit or explicit bias when determining pay raise percentages for employees. Note: To save space, I don’t review the statistical assumption testing that should be conducted when estimating a multiple linear regression model. When analyzing real data, be sure to carry out those diagnostic tests, and pay close attention to collinearity. To learn how to test the statistical assumptions for a multiple linear regression model, please refer to the chapter on estimating the incremental validity of a selection tool using multiple linear regression. 59.2.4.1 Step One (Block One) In the first step (first block), we will specify a multiple linear regression model with raise as the outcome variable and educ and perf as the predictor variables. We are going to use the lm (linear model) function from base R instead of the Regression or reg.brief functions from lessR because the models we create using the lm function are a bit easier to work with (in my opinion) when it comes to hierarchical linear regression. Using the &lt;- assignment operator, let’s name the model object step1 to reference the fact that it is our first step in the hierarchical linear regression process. Next, type the name of the lm function. As the first parenthetical argument, specify the following regression model: raise ~ educ + perf. Note that the tilde ~ symbol separates the outcome variable (left of the tilde) from the predictor variables (right of the tilde). As the second argument, type data= followed by the name of the data frame object (pd) in which our model variables can be found. On a subsequent line, type the name of the summary function from base R, with the name of our regression model object (step1) as the sole parenthetical argument. Finally, to determine the number of cases retained for the analysis (which becomes relevant if any data were missing on the focal variables), use the nrow function from base R with the model.frame function from base R as an argument. As the sole argument for the model.frame function, type the name of the regression model object (step1). # Step 1 Model: Regress raise on educ and perf step1 &lt;- lm(raise ~ educ + perf, data=pd) summary(step1) ## ## Call: ## lm(formula = raise ~ educ + perf, data = pd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.02060 -0.51293 -0.06175 0.51531 2.32785 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.59071 0.33450 -1.766 0.079767 . ## educ 0.21700 0.06016 3.607 0.000441 *** ## perf 0.29603 0.03607 8.207 0.000000000000201 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7999 on 129 degrees of freedom ## Multiple R-squared: 0.3837, Adjusted R-squared: 0.3742 ## F-statistic: 40.16 on 2 and 129 DF, p-value: 0.00000000000002752 # Number of cases retained for the analysis nrow(model.frame(step1)) ## [1] 132 Let’s review the findings from the first step of our hierarchical linear regression. Using a sample of 132 employees, we estimated the first step in a two-step hierarchical linear regression. Specifically, we regressed the percentage pay raise received by employees (raise) on their education level (educ) and supervisor-rated performance (perf). We found that both education level (b = .217, p &lt; .001) and performance (b = .296, p &lt; .001) were significantly and positively associated with the percentage raise received when controlling for one another. The (unadjusted multiple) R2 and adjusted R2 values are .384 and .374 (F = 40.16, p &lt; .001); focusing on the unadjusted R2 value, approximately 38% of the variance in percentage pay raise received by employees is explained by their education level and supervisor-rated performance scores. Because the F-value associated with the model is statistically significant (p &lt; .001), we can conclude that this model fits the data better than a model containing no predictor variables. 59.2.4.2 Step Two (Block Two) In the second step (second block), we will specify a multiple linear regression model with raise as the outcome variable and educ and perf as the predictor variables, but we will also add the predictor variables of sex and race. For this model, let’s name the model object step2. # Step 2 Model: Regress raise on educ, perf, sex, and race step2 &lt;- lm(raise ~ educ + perf + sex + race, data=pd) summary(step2) ## ## Call: ## lm(formula = raise ~ educ + perf + sex + race, data = pd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.65048 -0.42558 -0.00791 0.45156 2.32498 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.22478 0.29597 -4.138 0.00006355901993 *** ## educ 0.21890 0.04959 4.414 0.00002159553969 *** ## perf 0.28282 0.02988 9.464 &lt; 0.0000000000000002 *** ## sexmale 0.16160 0.11633 1.389 0.167236 ## raceblack 0.58704 0.14852 3.953 0.000128 *** ## racewhite 1.18247 0.15405 7.676 0.00000000000394 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6583 on 126 degrees of freedom ## Multiple R-squared: 0.5923, Adjusted R-squared: 0.5761 ## F-statistic: 36.61 on 5 and 126 DF, p-value: &lt; 0.00000000000000022 # Number of cases retained for the analysis nrow(model.frame(step2)) ## [1] 132 Let’s review the findings from the second step of our hierarchical linear regression. Using the same sample of 132 employees as the first step, we regressed the percentage pay raise received by employees (raise) on their education level (educ), supervisor-rated performance (perf), sex (sex), and race/ethnicity (race). Both education level (b = .219, p &lt; .001) and performance (b = .283, p &lt; .001) remained significantly and positively associated with the percentage raise received when controlling for one another and the sex and race/ethnicity variables. On the one hand, employee sex was not significantly associated with the percentage pay raise received when controlling for the other variables in the model (b = .162, p = .167). On the other hand, employee race/ethnicity was significantly associated with percentage pay raise received. Because the employee race/ethnicity variable (race) is categorical and has three or more levels (i.e., asian, black, white), the lm function uses a process called dummy coding (contrast coding) to make comparisons between the different races/ethnicities with respect to the raise outcome variable. For more information on dummy coding, see the section called Optional: More Information on Dummy Coding below. By default, the lm function orders the race categories alphabetical order, and the first category alphabetically automatically serves as the reference category, which in this case is the asian category. The results indicate that black employees received significantly higher percentage pay raises than asian employees (b = .587, p &lt; .001) when controlling for other predictor variables in the model, and likewise white employees received significantly higher percentage pay raises than asian employees (b = 1.182, p &lt; .001). As you might have noticed, the output does not give us the comparison between black and white employees. To compare those two groups, we need to re-level the race variable. To do so, we will overwrite the existing race variable in our pd data frame object using the &lt;- operator. Using the relevel function from base R, we will specify the as.factor function from base R as the first argument; within the as.factor function parentheses, we will specify the name of the data frame object pd followed by the $ operator and the name of the race variable. As the second argument in the relevel function, we will specify the ref= argument followed by the name of the category that we wish to set as the new reference group, which in this case is white. # Re-level the race variable to make white the reference group pd$race &lt;- relevel(as.factor(pd$race), ref=&quot;white&quot;) With the race variable re-leveled, we are ready to re-estimate our multiple linear regression model. # Step 2 Model: Regress raise on educ, perf, sex, and re-leveled race variable step2 &lt;- lm(raise ~ educ + perf + sex + race, data=pd) summary(step2) ## ## Call: ## lm(formula = raise ~ educ + perf + sex + race, data = pd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.65048 -0.42558 -0.00791 0.45156 2.32498 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.04231 0.29843 -0.142 0.887 ## educ 0.21890 0.04959 4.414 0.00002159553969 *** ## perf 0.28282 0.02988 9.464 &lt; 0.0000000000000002 *** ## sexmale 0.16160 0.11633 1.389 0.167 ## raceasian -1.18247 0.15405 -7.676 0.00000000000394 *** ## raceblack -0.59543 0.13203 -4.510 0.00001465492514 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6583 on 126 degrees of freedom ## Multiple R-squared: 0.5923, Adjusted R-squared: 0.5761 ## F-statistic: 36.61 on 5 and 126 DF, p-value: &lt; 0.00000000000000022 As you can see, black employees received statistically significantly lower percentage pay raises than white employees when controlling for the other predictor variables in the model (b = -.595, p &lt; .001). In sum, there is evidence that employee race/ethnicity influenced manager decisions when it came to determining the pay raise percentages for employees, which could be evidence of implicit or explicit bias. The (unadjusted multiple) R2 and adjusted R2 values are .592 and .576 (F = 36.61, p &lt; .001); focusing on the unadjusted R2 value, approximately 59% of the variance in percentage pay raise received by employees is explained by their education level, supervisor-rated performance scores, employee sex, and employee race/ethnicity. Because the F-value associated with the model is statistically significant (p &lt; .001), we can conclude that this model fits the data better than a model containing no predictor variables. 59.2.4.3 Compare Nested Models As a final step in our hierarchical linear regression, let’s use the anova function from base R to do a nested model comparison between the step1 and step2 models. This will tell us whether adding sex and race to the model significantly improved model fit (i.e., explained incrementally more variance in the outcome) over and above educ and perf. Type the name of the anova function. As the first argument, type the name of our first step model object (step1), and as the second argument, type the name of our second step model object (step2). # Nested Model Comparison: Step 1 vs. Step 2 anova(step1, step2) ## Analysis of Variance Table ## ## Model 1: raise ~ educ + perf ## Model 2: raise ~ educ + perf + sex + race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 129 82.541 ## 2 126 54.604 3 27.936 21.488 0.00000000002637 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F-value and its associated p-value (the latter of which is less than the conventional cutoff of .05) indicate that the second step model fits the data significantly better than the first step model. In other words, sex and race explain incremental variance in raise beyond the variance already explained by educ and perf. In the context of a pay raise, this is of course concerning because an employee’s sex or race/ethnicity should not factor into the percentage raise they receive. Finally, let’s reference the summary/output for each model and specifically the r.squared values to determine what the change in R2 was from the first step to the second step. The change in R2 gives as an idea of how much (in terms of practical significance) the model fit improved when adding the additional variables (i.e., sex, race). Note that we use the $ symbol to indicate that we want the r.squared value from the summary(step2) and summary(step1) output, and then we do a simple subtraction to get the change in R2. Note that we typically only interpret the change in R2 if we find that the models fit the data significantly differently from one another based on the F-value and its associated p-value in the model comparison step. # Change in R-squared summary(step2)$r.squared - summary(step1)$r.squared ## [1] 0.2085752 As you can see, the change in R2 is .209, which indicates that sex and race explain approximately 21% additional variance in raise beyond the variance already explained by educ and perf. If we consider our conventional rules of thumb for interpreting R2 as an effect size (i.e., indicator of practical significance), this is medium-to-large change, which is impressive in terms of magnitude (and deeply concerning in this specific context). R2 Description .01 Small .09 Medium .25 Large 59.2.4.4 Optional: More Information on Dummy Coding Dummy coding (contrast coding) is used in regression when we have a categorical predictor variable. To illustrate dummy coding, I reference the results from Step 2 above. In a simple two-level/category categorical variable such as sex (female, male) just one dummy variable is needed to represent sex, and by default, the lm function uses the name of the level/category that comes first alphabetically as the reference group, setting it equal to 0 (e.g., female = 0); in turn, the name of the level/category that comes next alphabetically is used as the focal group and is set equal to 1 (e.g., male = 1). Using this coding, a positive and significant regression coefficient for male (i.e., sexmale) would indicate that male employees have higher scores/values on the outcome variable. Things get a little bit trickier when we have a categorical variable with three or more categories/levels. Using the present findings as an example, behind the scenes, the lm function has created two dichotomous “dummy” variables with respect to the race variable, both of which reference the focal level/category (e.g., raceblack) as 1 and all other levels/categories (e.g., asian, white) as 0. The number of dummy variables created to analyze a categorical variable is equal to the number of categories/levels (k) minus 1 (k - 1); in this example, we have three categories/levels (asian, black, white), and thus two dummy variables were created behind the scenes. Only two dummy variables are needed for a categorical variable with three levels/categories because it can be inferred what a third dummy variable’s values would be if we have two dummy variables’ values. In this example, because the asian level/category comes first alphabetically, it is treated as the reference group, which means its mean value is reflected in the intercept/constant value of -1.22478 when controlling for other variables in the model. We can interpret each dummy variable regression coefficient as the difference between the mean of the focal level/category (e.g., raceblack) and the intercept (e.g., asian) when controlling for other variables in the model. Thus, in the second step model above, black employees received significantly higher percentage pay raises than asian employees (b = .587, p &lt; .001) when controlling for other predictor variables in the model, and likewise white employees received significantly higher percentage pay raises than asian employees (b = 1.182, p &lt; .001). Let’s dig deeper into dummy coding and illustrate its connection to analysis of variance (ANOVA). For solely explanatory purposes, let’s take a look at a model that only contains the dummy variables for the race variable in relation to the raise variable. Let’s name this model object mod. # Example model with dummy coding: Regress raise on race mod &lt;- lm(raise ~ race, data=pd) summary(mod) ## ## Call: ## lm(formula = raise ~ race, data = pd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.78478 -0.59925 0.00645 0.61522 2.31522 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.8848 0.1313 21.965 &lt; 0.0000000000000002 *** ## raceasian -1.2912 0.2070 -6.238 0.00000000586 *** ## raceblack -0.6684 0.1780 -3.756 0.000261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8908 on 129 degrees of freedom ## Multiple R-squared: 0.2358, Adjusted R-squared: 0.224 ## F-statistic: 19.9 on 2 and 129 DF, p-value: 0.00000002925 Note that in this model, the intercept/constant value of 1.594 represents the mean percentage raise for asian employees. The regression coefficient associated with raceblack (b = .623) represents the mean percentage raise for black employees minus the mean percentage raise for asian employees; because the values is positive, it means that black employees have a descriptively higher mean than asian employees. Next, the regression coefficient associated with racewhite (b = 1.291) represents the mean percentage raise for black employees minus the mean percentage raise for asian employees; because the values is positive, it means that white employees also have a descriptively higher mean than asian employees. We can confirm that the regression coefficient values mean outcome for each level of the categorical variable by using the tapply and mean functions to calculate the mean for the raise variable for each level of the race variable. # Mean values on outcome variable for each level of categorical variable tapply(pd$raise, pd$race, mean) ## white asian black ## 2.884783 1.593548 2.216364 Note that the intercept/constant value for above, which represents the mean percentage raise for the reference group (asian employees), is 1.594; this is the same as the mean we just calculated. In addition, if we add the regression coefficient for the dummy variable associated with black employees to the mean of the reference group (asian employees), we get 2.216 (1.5935 + .6228 = 2.2164), which is equal to the mean for black employees we calculated. Finally, if we add the regression coefficient for the dummy variable associated with white employees to the mean of the reference group (asian employees), we get 2.885 (1.5935 + 1.2912 = 2.8848), which is equal to the mean for white employees we calculated. Next, using the anova function from base R, we can interpret the regression model (mod) as a one-way ANOVA. # Display ANOVA results anova(mod) ## Analysis of Variance Table ## ## Response: raise ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## race 2 31.585 15.7927 19.904 0.00000002925 *** ## Residuals 129 102.353 0.7934 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The row in the table associated with race can be interpreted as an omnibus F-test for a one-way ANOVA. As you can see, the race variable explained a significant amount of the variability in raise (F = 19.904, p &lt; .001). However, like any omnibus test with three or more levels for the categorical predictor variable, we don’t know yet where the significant mean differences exist. The summary of the regression model above, provides us with information about those mean comparisons in the form of regression coefficients. In addition to illustrating how dummy coding works, this also shows that a one-way ANOVA (and other types of ANOVA) can be implemented and interpreted in a regression framework. This just goes to show that ANOVA is a specific type of regression. Please note, however, that this ANOVA model did not include the merit-based predictor variables and the protected characteristic variable of sex, so its results cannot be compared directly to the results of our Step 2 multiple linear regression model above. Finally, as a note of caution, it is not uncommon for a dummy variable (or multiple dummy variables) to be imbalanced, meaning that one category has disproportionately more representation in the data than the other. As such, it’s generally a good idea to compute the counts/frequencies for a categorical variable to determine if there are any categories that have disproportionately lower representation than others. If you do find that some categories have very few cases, then those categories should probably be filtered out prior to estimating a model wherein dummy variables will be created. 59.2.5 Summary In this chapter, we learned how to perform hierarchical linear regression in order to identify pay determinants. Hierarchical linear regression is a specific application of multiple linear regression in which we build up to a final/full model in what are often called steps or blocks. Specifically, we are comparing nested models. In this tutorial, we learned how to use the lm and anova functions from base R to conduct hierarchical linear regression. References "],["comparatio.html", "Chapter 60 Computing Compa-Ratios &amp; Investigating Pay Compression 60.1 Conceptual Overview 60.2 Tutorial", " Chapter 60 Computing Compa-Ratios &amp; Investigating Pay Compression In this chapter, we will learn how to compute compa-ratios, generate scatter plots, and evaluate whether there might be evidence of pay compression – or even pay inversion. The compa ratio is a useful tool for determining how well an organization’s actual pay practices are adhering to its pay policies. 60.1 Conceptual Overview A compa-ratio is an HR metric that indicates how well an employee is actually paid (or a group of employees are paid) relative to the midpoint of their pay grade. The compa-ratio is a useful metric for determining how well an organization is adhering to its espoused pay policy, and when coupled with other information, such as employee tenure, the compa-ratio can be used to detect whether pay compression and pay inversion exist. The compa-ratio for an employee is simple to compute, and the formula is as follows. \\(C_{employee} = \\frac{Pay}{MidPoint} \\times 100\\) where \\(Pay\\) refers to the total pay of the employee (e.g., base and variable pay combined), and \\(MidPoint\\) refers to the midpoint of the pay grade in which the employee is situated. To compute the compa-ratio for a group of employees, we can use the following formula. \\(C_{group} = \\overline{C}_{employee}\\) where \\(\\overline{C}_{employee}\\) refers to the average compa-ratio for a group of employees. 60.1.1 Conceptual Videos For a more in-depth review of compa-ratios and adherence to pay policies, please check out the following conceptual video. Link to conceptual video: https://youtu.be/zGXVjDw_vX4 60.2 Tutorial This chapter’s tutorial demonstrates how to compute compa-ratios and generate scatter plots in R. 60.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/nERabUWTubA 60.2.2 Functions &amp; Packages Introduced Function Package mean base R range base R as.Date base R class base R as.numeric base R ScatterPlot lessR Plot lessR Correlation lessR log base R 60.2.3 Initial Steps If you haven’t already, save the files called “PData.csv” and “CData.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data files for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called “PData.csv” and “CData.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects PersonalData &lt;- read_csv(&quot;PData.csv&quot;) ## Rows: 521 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): StartDate, Sex, RaceEthnicity, Disability, Veteran ## dbl (2): EmployeeID, Age_2018 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. CompensationData &lt;- read_csv(&quot;CData.csv&quot;) ## Rows: 521 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): PayGrade ## dbl (3): EmployeeID, BasePay_2018, VariablePay_2018 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Install dplyr package if you haven&#39;t already install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) # Full join (merge) without pipes MergedDF &lt;- full_join(PersonalData, CompensationData, by=&quot;EmployeeID&quot;) # Print variable names in joined (merged) data frame (tibble) object names(MergedDF) ## [1] &quot;EmployeeID&quot; &quot;StartDate&quot; &quot;Sex&quot; &quot;RaceEthnicity&quot; &quot;Age_2018&quot; &quot;Disability&quot; &quot;Veteran&quot; ## [8] &quot;PayGrade&quot; &quot;BasePay_2018&quot; &quot;VariablePay_2018&quot; # Print variable type for each variable in data frame (tibble) object str(MergedDF) ## spc_tbl_ [521 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:521] 1001 1002 1003 1004 1005 ... ## $ StartDate : chr [1:521] &quot;1/29/2013&quot; &quot;2/1/2013&quot; &quot;2/10/2013&quot; &quot;2/10/2013&quot; ... ## $ Sex : chr [1:521] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; ... ## $ RaceEthnicity : chr [1:521] &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;Black&quot; ... ## $ Age_2018 : num [1:521] 38.1 33.8 31.8 38.9 32 ... ## $ Disability : chr [1:521] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ Veteran : chr [1:521] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ PayGrade : chr [1:521] &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; ... ## $ BasePay_2018 : num [1:521] 43837 42873 42414 42398 43253 ... ## $ VariablePay_2018: num [1:521] 12777 13453 12553 12300 13217 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. StartDate = col_character(), ## .. Sex = col_character(), ## .. RaceEthnicity = col_character(), ## .. Age_2018 = col_double(), ## .. Disability = col_character(), ## .. Veteran = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Print first 6 rows of data frame (tibble) object head(MergedDF) ## # A tibble: 6 × 10 ## EmployeeID StartDate Sex RaceEthnicity Age_2018 Disability Veteran PayGrade BasePay_2018 VariablePay_2018 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1001 1/29/2013 Male White 38.1 No No C 43837 12777 ## 2 1002 2/1/2013 Male White 33.8 No No C 42873 13453 ## 3 1003 2/10/2013 Male White 31.8 No No C 42414 12553 ## 4 1004 2/10/2013 Female Black 38.8 No No C 42398 12300 ## 5 1005 2/18/2013 Female Asian 32.0 No No C 43253 13217 ## 6 1006 2/18/2013 Male White 34.4 No No C 44038 12729 # Print number of rows in data frame (tibble) object nrow(MergedDF) ## [1] 521 There are 10 variables and 56 cases (i.e., employees) in the MergedDF data frame: EmployeeID, StartDate, Sex, RaceEthnicity, Age_2018, Disability, Veteran, PayGrade, BasePay_2018, and VariablePay_2018. In this tutorial, we will focus on the StartDate, BasePay_2018, and VariablePay_2018 variables. Please note that the StartDate variable is of type character and not Date; we will need to change this later on. 60.2.4 Compute Compa-Ratio for Each Employee The compa-ratio reflects how much an employee is actually paid a given pay grade relative to the midpoint of the pay grade. For this pay grade, we will assume that the pay policy specifies the midpoint for total compensation (base pay + variable pay) as $55,000 per year. To begin, we need to create a total pay (i.e., total compensation) variable by calculating the sum of base and variable pay for each employee. Let’s name this new variable TotalPay_2018. First, type MergedDF$TotalPay_2018 followed by the &lt;- assignment operator to specify what you would like to name your new variable (TotalPay_2018). Second, add the BasePay_2018 variable to the VariablePay_2018 variable using the + operator. # Create TotalPay variable MergedDF$TotalPay_2018 &lt;- MergedDF$BasePay_2018 + MergedDF$VariablePay_2018 Next, to compute the compa-ratio for each employee, first, type MergedDF$CompaRatio followed by the &lt;- assignment operator to create and name a new variable called CompaRatio. Second, divide the TotalPay_2018 variable values by the midpoint value for the pay grade ($55,000) using the division operator (/). Third, multiply the resulting quotient by 100 to convert the ratio to a percentage (using the multiplication * operator). # Create CompaRatio variable MergedDF$CompaRatio &lt;- (MergedDF$TotalPay_2018 / 55000) * 100 # Print vector of compa-ratios print(MergedDF$CompaRatio) ## [1] 102.93455 102.41091 99.94000 99.45091 102.67273 103.21273 100.78000 101.91091 102.59818 100.54182 100.36182 101.45273 101.45818 ## [14] 101.91091 98.84545 102.44182 100.78727 101.67273 100.58545 103.08545 96.13091 99.90727 101.01273 100.31636 99.92545 100.55818 ## [27] 101.81455 97.87091 98.13091 100.82182 103.85636 103.25091 100.31091 100.82545 102.53636 99.19818 102.39455 102.43818 100.97636 ## [40] 105.27091 99.21273 99.69091 99.53455 100.79455 102.69273 101.04182 96.62182 99.81455 102.13273 101.42909 101.28000 103.68364 ## [53] 103.37818 101.49455 102.76545 99.71455 103.34182 102.51091 104.83091 100.79455 100.63455 100.65636 100.88727 97.85273 99.19818 ## [66] 99.47818 102.52364 103.95636 99.96364 99.65455 99.89636 101.16909 98.48000 101.32545 104.44364 99.52727 105.34000 100.74364 ## [79] 100.82182 99.74727 98.53273 101.25818 100.00182 100.52909 103.63455 101.42182 98.28000 101.53818 102.08727 101.67273 101.21091 ## [92] 102.84182 101.91636 100.08364 100.42545 101.98000 98.83636 97.11818 101.04727 100.34909 103.98545 100.73455 104.83818 102.08909 ## [105] 98.22364 99.24182 103.55273 101.00727 103.59818 104.69455 99.41455 99.57636 98.28182 102.50182 101.05818 100.13273 103.34545 ## [118] 100.16909 100.08364 99.96000 102.68182 98.33455 99.95818 101.23273 100.07636 99.65636 103.38909 100.06545 101.21273 102.22727 ## [131] 96.90545 103.89636 97.25273 103.99455 98.09273 101.63273 99.67818 101.21636 101.80727 100.70545 100.50909 103.63455 100.97091 ## [144] 100.88545 101.36545 99.69818 101.27818 97.04364 100.99455 102.31636 103.10909 99.72727 101.20545 103.90545 99.61455 99.10000 ## [157] 103.15091 100.82182 100.48182 103.61091 104.85636 98.27273 102.28364 102.17273 97.95455 100.96909 101.57636 99.86364 103.70909 ## [170] 100.13636 98.21818 100.75636 101.19091 94.88727 102.06545 97.74545 101.61455 96.45091 99.05091 101.37091 103.71455 102.42545 ## [183] 102.44000 99.01273 102.53091 98.81636 96.86727 100.03455 100.57636 102.18545 100.36000 101.01636 101.60182 101.52364 99.92364 ## [196] 98.84364 98.28182 101.66545 98.46545 95.66364 99.62909 98.68364 100.36909 102.20545 98.57455 100.69818 100.55636 101.51091 ## [209] 101.91636 100.72182 98.00000 101.42182 100.78727 98.79818 99.00364 99.44364 104.44727 104.08545 101.59455 103.98545 102.82545 ## [222] 101.32727 101.26364 104.41273 100.59636 103.99636 98.55455 102.20727 101.08000 103.42545 99.85455 99.48182 99.14727 98.21273 ## [235] 102.61273 103.42000 98.07455 101.98000 102.62545 100.23273 98.93091 103.51636 100.40182 103.37273 100.01273 102.51273 99.54182 ## [248] 100.88727 99.17818 100.85455 101.89273 100.16727 97.74364 105.03636 98.14727 100.66000 100.68182 101.01818 102.74545 100.94727 ## [261] 98.84545 100.26909 102.56182 99.45091 96.09455 101.67818 99.16545 101.60182 102.12727 99.23091 100.60909 102.78182 100.76909 ## [274] 101.82545 99.45818 104.44727 103.31636 100.57455 104.67636 103.63455 98.04727 102.61636 101.15818 100.59455 99.23455 102.81091 ## [287] 97.87091 101.85455 100.35091 103.24000 103.23273 99.30364 100.61818 100.40182 101.97091 100.96364 102.01091 100.39091 99.79455 ## [300] 97.88364 102.53273 99.94727 98.59818 99.61818 103.93273 101.24000 102.50545 101.36182 102.61091 99.13273 98.51091 99.14727 ## [313] 98.69818 98.77455 97.91636 98.13636 100.56545 97.22182 100.70364 101.55636 99.90909 100.32545 100.08364 99.34364 103.69636 ## [326] 98.78000 97.21273 102.41455 102.80364 102.77818 102.36182 100.95818 104.03273 100.15636 100.76364 98.53818 100.38909 97.71091 ## [339] 103.07091 97.09091 103.20909 100.58545 104.94909 99.00909 98.85455 105.56727 99.10727 99.93091 99.98182 100.25818 104.32364 ## [352] 103.45818 103.55455 105.20364 101.46727 100.25818 99.82727 101.08909 100.84182 103.13636 100.55091 96.31818 98.26364 103.61455 ## [365] 100.53091 100.64000 99.49455 97.98182 100.81818 99.67636 101.81273 103.03818 100.16545 101.36182 99.30000 102.81455 100.79273 ## [378] 99.11091 101.62545 98.10000 101.60545 100.60909 98.25273 102.60727 94.38182 101.37091 103.25455 102.39455 101.35091 102.12364 ## [391] 97.55273 103.73818 103.43455 98.92182 102.18909 100.14000 102.38545 104.12909 102.21273 100.17455 104.10182 101.56182 103.80182 ## [404] 100.64182 99.88000 101.51091 100.58727 99.96727 102.42364 98.79091 106.45273 101.48182 101.79818 100.25273 101.48182 105.98545 ## [417] 102.74727 101.72182 97.65455 99.36000 101.71273 96.34000 102.63818 102.05273 97.53636 97.82182 103.79273 100.23091 101.59818 ## [430] 97.14727 101.98000 99.67636 104.27455 99.38000 104.70364 100.30000 102.24182 101.07091 103.53273 101.63818 100.50909 98.26545 ## [443] 99.51636 99.58000 101.02909 103.68182 97.44727 99.94909 100.68909 101.01818 100.83091 103.10545 103.89636 102.14182 97.82000 ## [456] 102.07455 104.94909 97.32909 98.94545 99.34909 103.46727 100.29818 98.90000 103.40000 102.82182 103.41636 102.25636 98.99818 ## [469] 100.14182 98.37091 96.91455 101.46364 103.15455 99.12364 100.76909 101.16000 103.80364 102.32182 102.32364 101.94000 104.32727 ## [482] 102.20182 99.76182 101.62545 100.91636 99.92182 100.64909 102.45636 99.72545 102.97636 97.08727 100.64000 100.78727 98.95636 ## [495] 101.27091 102.82000 99.81091 102.24909 100.51455 102.13273 99.96364 99.02545 104.06727 103.83273 97.06182 97.63273 102.40182 ## [508] 100.16000 98.88182 99.46000 96.91273 100.87091 97.10909 103.69636 100.43455 106.18727 100.92727 101.51091 101.73091 101.50727 ## [521] 101.11273 Employees with compa-ratios that are equal to 100% are paid at the midpoint for the pay grade. Employees with compa-ratios that are greater than 100% are paid more than the midpoint for their pay grade, and employees with compa-ratios that are less than 100% are paid less than the midpoint. There are rules of thumb for what can be considered large and small compa-ratios (e.g., 80% vs. 120%), but generally speaking, interpreting compa-ratio values should be based upon the width of the specific pay grade in question, as well as other organizational factors (e.g., pay philosophy). 60.2.5 Compute Compa-Ratio for Group of Employees To compute the compa-ratio for an entire group of employees, apply the mean function from base R to the new CompaRatio variable. # Calculate mean mean(MergedDF$CompaRatio, na.rm=TRUE) ## [1] 100.9211 The value of 100.705 indicates that the organization is paying employees at 101% of the pay grade midpoint, which is a pretty good sign in terms of adherence to the pay policy midpoint for the grade. We would likely be very concerned if we instead saw a group compa-ratio of 75% or 125%. 60.2.6 Investigate Pay Compression and Pay Inversion While computing the mean compa-ratio value for a group of employees is useful, by itself, it does not provide us with direct evidence of pay compression or pay inversion. The scatter plot data visualization can be useful in this regard, particularly when we associate employee tenure with compa-ratio. Because we currently only have a start date (StartDate) variable in our data frame, we need to create a new variable that represents tenure. To create a new variable that we will call Tenure, we need to subtract employees’ start date (StartDate) values from some more recent date, which in this case is December 31, 2018 (\"2018-12-31\"). First, type MergedDF$Tenure followed by the &lt;- assignment operator to inform R that you wish to create and name new variable called Tenure and append it to the existing MergedDF data frame using the $ operator. Second, enter the as.Date function from base R with \"2018-12-31\" as the sole parenthetical argument; this is the more recent date from which we will subtract StartDate values. Third, enter the subtraction operator (-). Fourth, earlier we found that the StartDate variable is of type character and not Date; consequently, we need to convert it to type Date by applying the as.Date function, with the name of the data frame (MergedDF), $ operator, and name of the start date (StartDate) variable as the first argument, and format=\"%m/%d/%Y\" as the second argument. # Create Tenure variable MergedDF$Tenure &lt;- as.Date(&quot;2018-12-31&quot;) - as.Date(MergedDF$StartDate, format=&quot;%m/%d/%Y&quot;) Take a look at the vector of values that belong to the new Tenure variable that we just added to the MergedDF data frame. When applying arithmetic to variables of type Date, the resulting variables/output will be of the unit days. That is, when you see the value of 2142 for Tenure, it refers to 2,142 days of employment. # Print vector of Tenure variable values print(MergedDF$Tenure) ## Time differences in days ## [1] 2162 2159 2150 2150 2142 2142 2130 2130 2120 2120 2100 2100 2060 2060 2055 2055 2043 2043 2021 2021 2001 2001 1922 1922 1909 1909 ## [27] 1905 1905 1889 1889 1872 1872 1862 1862 1857 1857 1833 1833 1827 1827 1799 1799 1799 1799 1797 1797 1768 1768 1767 1767 1762 1762 ## [53] 1741 1741 1724 1724 1696 1696 1688 1688 1679 1679 1675 1675 1655 1655 1634 1634 1626 1626 1621 1621 1619 1619 1616 1616 1612 1612 ## [79] 1600 1600 1600 1600 1596 1596 1590 1590 1583 1583 1581 1581 1536 1536 1533 1533 1516 1516 1514 1514 1511 1511 1503 1503 1484 1484 ## [105] 1475 1475 1449 1449 1436 1436 1404 1404 1397 1397 1394 1394 1377 1377 1370 1370 1368 1363 1357 1350 1347 1336 1326 1311 1299 1294 ## [131] 1289 1275 1268 1253 1235 1215 1211 1208 1208 1197 1197 1197 1197 1194 1194 1189 1189 1172 1172 1168 1168 1159 1159 1159 1159 1157 ## [157] 1157 1156 1156 1152 1152 1152 1152 1151 1151 1150 1150 1149 1149 1149 1149 1149 1149 1148 1148 1147 1147 1147 1147 1147 1147 1147 ## [183] 1147 1147 1147 1146 1146 1146 1146 1144 1144 1144 1144 1143 1143 1143 1143 1142 1142 1142 1142 1142 1142 1141 1141 1141 1141 1140 ## [209] 1140 1140 1140 1140 1140 1138 1138 1138 1138 1136 1136 1134 1134 1134 1134 1133 1133 1133 1133 1131 1131 1131 1131 1131 1131 1130 ## [235] 1130 1130 1130 1130 1130 1129 1129 1128 1128 1128 1128 1128 1128 1128 1128 1128 1128 1128 1128 1127 1127 1126 1126 1126 1126 1122 ## [261] 1122 1122 1122 1122 1122 1122 1122 1122 1122 1121 1121 1120 1120 1120 1120 1119 1119 1119 1119 1119 1119 1119 1119 1118 1118 1118 ## [287] 1118 1116 1116 1116 1114 1113 1113 1113 1111 1111 1111 1111 1111 1111 1111 1111 1110 1110 1110 1110 1110 1110 1109 1109 1108 1108 ## [313] 1108 1108 1107 1107 1107 1107 1106 1106 1106 1106 1105 1105 1105 1105 1105 1103 1103 1103 1103 1102 1102 1102 1101 1101 1101 1101 ## [339] 1099 1094 1094 1094 1093 1092 1091 1090 1090 1089 1088 1088 1086 1085 1085 1085 1085 1083 1082 1081 1080 1080 1080 1080 1078 1078 ## [365] 1077 1077 1075 1075 1075 1075 1074 1074 1073 1073 1072 1072 1072 1070 1069 1069 1069 1069 1068 1068 1068 1066 1065 1065 1064 1063 ## [391] 1061 1060 1058 1057 1056 1054 1054 1053 1051 1051 1049 1049 1046 1043 1042 1041 1041 1040 1039 1038 1038 1037 1036 1034 1034 1032 ## [417] 1032 1031 1029 1029 1028 1028 1027 1026 1026 1025 1023 1022 1022 1021 1021 1021 1020 1020 1019 1017 1017 1015 1015 1013 1013 1012 ## [443] 1011 1010 1010 1008 1007 1006 1005 1005 1005 1004 1004 1002 1002 1000 1000 999 998 997 997 996 996 996 995 995 995 994 ## [469] 994 992 991 991 990 990 989 989 988 986 985 984 983 983 981 981 979 979 979 978 977 975 975 974 973 971 ## [495] 971 969 968 967 967 964 963 961 960 960 959 958 953 952 951 950 949 948 947 946 945 945 944 943 943 941 ## [521] 941 Using the class function from base R, let’s check the variable type for just the Tenure variable. # Check variable type/class of Tenure variable class(MergedDF$Tenure) ## [1] &quot;difftime&quot; As you can see, the Tenure variable is of a special numeric type called difftime. Let’s apply the as.numeric function from base R to convert the Tenure variable to a traditional variable of type numeric. Before the as.numeric function, we type MergedDF$Tenure followed by the &lt;- assignment operator to overwrite the existing Tenure variable in our MergedDF data frame. # Convert Tenure variable to numeric type MergedDF$Tenure &lt;- as.numeric(MergedDF$Tenure) To make sure that worked, let’s apply the class function once more. The conversion worked. # Check variable type/class of Tenure variable class(MergedDF$Tenure) ## [1] &quot;numeric&quot; Next, divide the Tenure variable values by 365 to get an estimate of how many years (as opposed to days) each employee has worked in their current pay grade, which is how we are defining tenure in this context. Before our subtraction equation, we type MergedDF$Tenure_Yrs &lt;- to create a new variable called Tenure_Yrs that will be appended to our MergedDF data frame. # Create Tenure_Yrs variable MergedDF$Tenure_Yrs &lt;- MergedDF$Tenure / 365 Take a look at the vector of values that belong to the new Tenure_Yrs variable that we just added to the MergedDF data frame. The values are now in the unit years. # Print vector of Tenure_Yrs variable values print(MergedDF$Tenure_Yrs) ## [1] 5.923288 5.915068 5.890411 5.890411 5.868493 5.868493 5.835616 5.835616 5.808219 5.808219 5.753425 5.753425 5.643836 5.643836 ## [15] 5.630137 5.630137 5.597260 5.597260 5.536986 5.536986 5.482192 5.482192 5.265753 5.265753 5.230137 5.230137 5.219178 5.219178 ## [29] 5.175342 5.175342 5.128767 5.128767 5.101370 5.101370 5.087671 5.087671 5.021918 5.021918 5.005479 5.005479 4.928767 4.928767 ## [43] 4.928767 4.928767 4.923288 4.923288 4.843836 4.843836 4.841096 4.841096 4.827397 4.827397 4.769863 4.769863 4.723288 4.723288 ## [57] 4.646575 4.646575 4.624658 4.624658 4.600000 4.600000 4.589041 4.589041 4.534247 4.534247 4.476712 4.476712 4.454795 4.454795 ## [71] 4.441096 4.441096 4.435616 4.435616 4.427397 4.427397 4.416438 4.416438 4.383562 4.383562 4.383562 4.383562 4.372603 4.372603 ## [85] 4.356164 4.356164 4.336986 4.336986 4.331507 4.331507 4.208219 4.208219 4.200000 4.200000 4.153425 4.153425 4.147945 4.147945 ## [99] 4.139726 4.139726 4.117808 4.117808 4.065753 4.065753 4.041096 4.041096 3.969863 3.969863 3.934247 3.934247 3.846575 3.846575 ## [113] 3.827397 3.827397 3.819178 3.819178 3.772603 3.772603 3.753425 3.753425 3.747945 3.734247 3.717808 3.698630 3.690411 3.660274 ## [127] 3.632877 3.591781 3.558904 3.545205 3.531507 3.493151 3.473973 3.432877 3.383562 3.328767 3.317808 3.309589 3.309589 3.279452 ## [141] 3.279452 3.279452 3.279452 3.271233 3.271233 3.257534 3.257534 3.210959 3.210959 3.200000 3.200000 3.175342 3.175342 3.175342 ## [155] 3.175342 3.169863 3.169863 3.167123 3.167123 3.156164 3.156164 3.156164 3.156164 3.153425 3.153425 3.150685 3.150685 3.147945 ## [169] 3.147945 3.147945 3.147945 3.147945 3.147945 3.145205 3.145205 3.142466 3.142466 3.142466 3.142466 3.142466 3.142466 3.142466 ## [183] 3.142466 3.142466 3.142466 3.139726 3.139726 3.139726 3.139726 3.134247 3.134247 3.134247 3.134247 3.131507 3.131507 3.131507 ## [197] 3.131507 3.128767 3.128767 3.128767 3.128767 3.128767 3.128767 3.126027 3.126027 3.126027 3.126027 3.123288 3.123288 3.123288 ## [211] 3.123288 3.123288 3.123288 3.117808 3.117808 3.117808 3.117808 3.112329 3.112329 3.106849 3.106849 3.106849 3.106849 3.104110 ## [225] 3.104110 3.104110 3.104110 3.098630 3.098630 3.098630 3.098630 3.098630 3.098630 3.095890 3.095890 3.095890 3.095890 3.095890 ## [239] 3.095890 3.093151 3.093151 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 3.090411 ## [253] 3.090411 3.087671 3.087671 3.084932 3.084932 3.084932 3.084932 3.073973 3.073973 3.073973 3.073973 3.073973 3.073973 3.073973 ## [267] 3.073973 3.073973 3.073973 3.071233 3.071233 3.068493 3.068493 3.068493 3.068493 3.065753 3.065753 3.065753 3.065753 3.065753 ## [281] 3.065753 3.065753 3.065753 3.063014 3.063014 3.063014 3.063014 3.057534 3.057534 3.057534 3.052055 3.049315 3.049315 3.049315 ## [295] 3.043836 3.043836 3.043836 3.043836 3.043836 3.043836 3.043836 3.043836 3.041096 3.041096 3.041096 3.041096 3.041096 3.041096 ## [309] 3.038356 3.038356 3.035616 3.035616 3.035616 3.035616 3.032877 3.032877 3.032877 3.032877 3.030137 3.030137 3.030137 3.030137 ## [323] 3.027397 3.027397 3.027397 3.027397 3.027397 3.021918 3.021918 3.021918 3.021918 3.019178 3.019178 3.019178 3.016438 3.016438 ## [337] 3.016438 3.016438 3.010959 2.997260 2.997260 2.997260 2.994521 2.991781 2.989041 2.986301 2.986301 2.983562 2.980822 2.980822 ## [351] 2.975342 2.972603 2.972603 2.972603 2.972603 2.967123 2.964384 2.961644 2.958904 2.958904 2.958904 2.958904 2.953425 2.953425 ## [365] 2.950685 2.950685 2.945205 2.945205 2.945205 2.945205 2.942466 2.942466 2.939726 2.939726 2.936986 2.936986 2.936986 2.931507 ## [379] 2.928767 2.928767 2.928767 2.928767 2.926027 2.926027 2.926027 2.920548 2.917808 2.917808 2.915068 2.912329 2.906849 2.904110 ## [393] 2.898630 2.895890 2.893151 2.887671 2.887671 2.884932 2.879452 2.879452 2.873973 2.873973 2.865753 2.857534 2.854795 2.852055 ## [407] 2.852055 2.849315 2.846575 2.843836 2.843836 2.841096 2.838356 2.832877 2.832877 2.827397 2.827397 2.824658 2.819178 2.819178 ## [421] 2.816438 2.816438 2.813699 2.810959 2.810959 2.808219 2.802740 2.800000 2.800000 2.797260 2.797260 2.797260 2.794521 2.794521 ## [435] 2.791781 2.786301 2.786301 2.780822 2.780822 2.775342 2.775342 2.772603 2.769863 2.767123 2.767123 2.761644 2.758904 2.756164 ## [449] 2.753425 2.753425 2.753425 2.750685 2.750685 2.745205 2.745205 2.739726 2.739726 2.736986 2.734247 2.731507 2.731507 2.728767 ## [463] 2.728767 2.728767 2.726027 2.726027 2.726027 2.723288 2.723288 2.717808 2.715068 2.715068 2.712329 2.712329 2.709589 2.709589 ## [477] 2.706849 2.701370 2.698630 2.695890 2.693151 2.693151 2.687671 2.687671 2.682192 2.682192 2.682192 2.679452 2.676712 2.671233 ## [491] 2.671233 2.668493 2.665753 2.660274 2.660274 2.654795 2.652055 2.649315 2.649315 2.641096 2.638356 2.632877 2.630137 2.630137 ## [505] 2.627397 2.624658 2.610959 2.608219 2.605479 2.602740 2.600000 2.597260 2.594521 2.591781 2.589041 2.589041 2.586301 2.583562 ## [519] 2.583562 2.578082 2.578082 By applying the mean and range functions from base R, we see that the average tenure (in years) for these employees is 3.46 years, the minimum tenure is 2.58 years, and the maximum tenure is 5.87 years. # Compute mean for Tenure_Yrs variable mean(MergedDF$Tenure_Yrs, na.rm=TRUE) ## [1] 3.387185 # Compute range for Tenure_Yrs variable range(MergedDF$Tenure_Yrs, na.rm=TRUE) ## [1] 2.578082 5.923288 To visualize the relationship between CompaRatio and Tenure_Yrs (to search for evidence of pay compression/inversion), we will create a scatter plot using the ScatterPlot function from the lessR package (Gerbing, Business, and University 2021). If you haven’t already, install and access the lessR package. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) To apply the ScatterPlot function, first, type the name of the function. As the first argument, type the variable name you wish to be displayed on the x-axis (CompaRatio). As the second argument, type the variable name you wish to be displayed on the y-axis (Tenure_Yrs). Finally, enter data= followed by the name of the data frame (MergedDF) to which the two variables belong. # Create scatterplot between CompaRatio &amp; Tenure_Yrs ScatterPlot(CompaRatio, Tenure_Yrs, data=MergedDF) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 521 ## Sample Correlation of CompaRatio and Tenure_Yrs: r = 0.036 ## ## Hypothesis Test of 0 Correlation: t = 0.821, df = 519, p-value = 0.412 ## 95% Confidence Interval for Correlation: -0.050 to 0.122 ## The scatter plot (visually) reveals a slight positive relationship between compa-ratios and tenure. That said, a clear pattern or trend is not easy to discern from this scatterplot. Rather, the main take-away from the scatterplot is that a lot of employees were hired more recently, which suggests that the Tenure_Yrs variable has a skewed distribution. Let’s use the Plot function from lessR to look at the distribution for Tenure_Yrs by itself. # Create violin-box-scatter (VBS) plot of Tenure_Yrs variable Plot(Tenure_Yrs, data=MergedDF) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(Tenure_Yrs, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(Tenure_Yrs, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- Tenure_Yrs --- ## Present: 521 ## Missing: 0 ## Total : 521 ## ## Mean : 3.3871848 ## Stnd Dev : 0.8141023 ## IQR : 0.6246575 ## Skew : 0.4893617 [medcouple, -1 to 1] ## ## Minimum : 2.5780822 ## Lower Whisker: 2.5780822 ## 1st Quartile : 2.9068493 ## Median : 3.0739726 ## 3rd Quartile : 3.5315068 ## Upper Whisker: 4.4547945 ## Maximum : 5.9232877 ## ## ## --- Outliers --- from the box plot: 68 ## ## Small Large ## ----- ----- ## 5.923288 ## 5.915068 ## 5.890411 ## 5.890411 ## 5.868493 ## 5.868493 ## 5.835616 ## 5.835616 ## 5.808219 ## 5.808219 ## 5.753425 ## 5.753425 ## 5.643836 ## 5.643836 ## 5.630137 ## 5.630137 ## 5.597260 ## 5.597260 ## ## + 50 more outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## Tenure_Yrs 12 3.09041095890411 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.26 size of plotted points ## out_size: 0.68 size of plotted outlier points ## jitter_y: 1.98 random vertical movement of points ## jitter_x: 0.33 random horizontal movement of points ## bw: 0.31 set bandwidth higher for smoother edges Note how the distribution is positively skewed, which shows univariate deviation from normality for the Tenure_Yrs variable. The skewness statistic in the Plot output confirms this positive skewness (.545), but that skewness statistic is not too worrisome by most standards; meaning, we could probably feel comfortable using the Tenure_Yrs variable without transforming it. Despite the not-very-concerning amount of positive skewness for the Tenure_Yrs variable, let’s take this opportunity to practice transforming a variable using a logarithmic (log) transformation. Specifically, let’s transform the Tenure_Yrs variable using a natural log transformation. To do so, we can use the log function from base R, and let’s call the new variable logTenure_Yrs. # Create logTenure variable MergedDF$logTenure_Yrs &lt;- log(MergedDF$Tenure_Yrs) Now, let’s take a look at the univariate distribution of the new logTenure_Yrs variable we just created. # Create violin-box-scatter (VBS) plot of logTenure_Yrs variable Plot(logTenure_Yrs, data=MergedDF) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(logTenure_Yrs, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(logTenure_Yrs, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- logTenure_Yrs --- ## Present: 521 ## Missing: 0 ## Total : 521 ## ## Mean : 1.19585433 ## Stnd Dev : 0.21065883 ## IQR : 0.19465486 ## Skew : 0.41943846 [medcouple, -1 to 1] ## ## Minimum : 0.94704579 ## Lower Whisker: 0.94704579 ## 1st Quartile : 1.06706979 ## Median : 1.12297073 ## 3rd Quartile : 1.26172465 ## Upper Whisker: 1.55250510 ## Maximum : 1.77889164 ## ## ## --- Outliers --- from the box plot: 54 ## ## Small Large ## ----- ----- ## 1.7788916 ## 1.7775031 ## 1.7733258 ## 1.7733258 ## 1.7695979 ## 1.7695979 ## 1.7639799 ## 1.7639799 ## 1.7592740 ## 1.7592740 ## 1.7497953 ## 1.7497953 ## 1.7305639 ## 1.7305639 ## 1.7281338 ## 1.7281338 ## 1.7222772 ## 1.7222772 ## ## + 36 more outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## logTenure_Yrs 12 1.12830407847551 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.26 size of plotted points ## out_size: 0.68 size of plotted outlier points ## jitter_y: 1.98 random vertical movement of points ## jitter_x: 0.33 random horizontal movement of points ## bw: 0.10 set bandwidth higher for smoother edges The positive skewness shows some improvement in the logTenure_Yrs variable but not much. Nevertheless, for the sake of demonstration, let’s create a scatter plot using CompaRatio the new transformed variable called (logTenure_Yrs). # Create scatterplot between CompaRatio &amp; logTenure_Yrs ScatterPlot(CompaRatio, logTenure_Yrs, data=MergedDF) ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 521 ## Sample Correlation of CompaRatio and logTenure_Yrs: r = 0.033 ## ## Hypothesis Test of 0 Correlation: t = 0.752, df = 519, p-value = 0.452 ## 95% Confidence Interval for Correlation: -0.053 to 0.119 ## The scatterplot shows a slightly positive upward association. Now, let’s use the Correlation function from lessR to see estimate whether a statistically significant association exists, and if so, how strong and in what direction. To apply the Correlation function, simply enter the name of the Correlation function, and within the parentheses, enter the same three arguments as you entered for the ScatterPlot function directly above. [Technically, we don’t need to apply the Correlation function if we already applied the ScatterPlot function because the latter provides an estimate of the correlation coefficient. Nevertheless, it will make your script clearer and more interpretable if you clearly specify the Correlation function separately from the ScatterPlot function.] # Compute correlation between CompaRatio &amp; logTenure_Yrs Correlation(CompaRatio, logTenure_Yrs, data=MergedDF) ## Correlation Analysis for Variables CompaRatio and logTenure_Yrs ## ## --- Pearson&#39;s product-moment correlation --- ## ## Number of paired values with neither missing, n = 521 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = 0.014 ## ## Sample Correlation: r = 0.033 ## ## Hypothesis Test of 0 Correlation: t = 0.752, df = 519, p-value = 0.452 ## 95% Confidence Interval for Correlation: -0.053 to 0.119 We see that the correlation is statistically significant (p = .047). Further, the association between the two variables is positive and small-to-medium in magnitude (r = .266). Thus, there seems to be evidence that employees who are paid higher relative to their pay-grade midpoint tend to have held a job in that pay grade for a longer period of time, but remember, this is based on the tenure variable to which we applied a log-odds transformation; meaning, the interpretation isn’t quite so simple. The slight-to-moderate positive association, however, is a relatively good sign as it means that the organization tends to pay employees more who have higher tenure, which is often at least partially consistent with most organizations’ pay policies/philosophies. If we had seen a negative association, that would have been perhaps suggestive of pay compression or even pay inversion within the pay grade. 60.2.7 Summary In this chapter, we learned how to compute compa ratios and generate scatter plots to evaluate if pay compression might be an issue. The compa-ratio is a useful HR metric for assessing how an individual or group is paid relative to the pay midpoint. Calculating compa-ratios comes in handy when assessing pay compression or inversion. Scatter plots and correlations can also be applied to understand whether there is evidence of pay compression (or even pay inversion). References "],["primerdata.html", "Chapter 61 Primer on Data", " Chapter 61 Primer on Data When working with an organization’s people data, we are likely to encounter data of many different forms and types. For instance, some of the data might be qualitative (e.g., responses to an open-ended survey item), and other data might be quantitative (e.g., measuring employee tenure in years). In fact, we might even decide to convert qualitative data to quantitative in certain circumstances. In video that follows, I provide a primer on data, with an emphasis on how data might be used for subsequent management, analysis, interpretation, and storytelling. Link to conceptual video: https://youtu.be/BhwbUOquMfA "],["legalethical.html", "Chapter 62 Legal &amp; Ethical Issues", " Chapter 62 Legal &amp; Ethical Issues The legal end ethical environment in which an HR analyst operates can be complex and may vary from city to city, country to country, and culture to culture. In the video that follows, I briefly introduce legal and ethical issues and considerations that an HR analyst may encounter, with an emphasis on the U.S. context. Link to conceptual video: https://youtu.be/89u-CGMyfgo "],["jdm_bias.html", "Chapter 63 Judgment, Decision Making, &amp; Bias", " Chapter 63 Judgment, Decision Making, &amp; Bias Some might assume the the processes of acquiring, managing, analyzing, interpreting, and telling a story about data are entirely objective in nature. In reality, human beings often make subjective decisions throughout these processes. Given that, we should consider how human judgment, decision making, and bias affect these data-analytic processes. In the video that follows, I provide a brief review of judgment, decision making, and bias in the context of HR analytics. Link to conceptual video: https://youtu.be/RJ9n_1XDO0Q "],["languageconsiderations.html", "Chapter 64 Language Considerations", " Chapter 64 Language Considerations How we talk about data-analytic findings and recommendations can have important implications for building in trust and confidence in the HR analytics function. While we want to demonstrate confidence in our skills as analysts, we also must recognize the uncertainty inherent in our findings and recommendations, owing to different sources of error and bias. In the video that follows, I briefly introduce and cover some language considerations that HR analysts should be aware of. Link to conceptual video: https://youtu.be/t3_QCe-0wOk "],["create_portfolio.html", "Chapter 65 Creating a Data Analytics Portfolio", " Chapter 65 Creating a Data Analytics Portfolio When searching for a job in data analytics, it can be tricky to (a) demonstrate what you know how to do or have done previously and (b) distinguish yourself from other job candidates. Creating a portfolio of your work can be one way to set yourself apart on the job market and can, in some instances, help compensate for fewer years of relevant work experience. In this document, I will provide some suggestions on how to set up a portfolio, with an emphasis on applying the R programming language. Identify a content area or context that excites you. In the Human Resource (HR) Analytics certificate, we obviously focus on the HR context and various HR content areas. Try to be more specific than that, though. For example, are you interested in employee sensing (e.g., engagement surveys), separation and retention, recruitment and selection, training and development, reward systems, or workforce planning? In your portfolio, you can present multiple projects that highlight different content areas and contexts, but I recommend focusing your first portfolio project on an area (a) that you are really passionate about and (b) in which you have some prior work experience (even if not in the application of data analytics/science to that area). Identify an area of data analytics that excites you. Begin by reflecting on what type of work you intrinsically enjoy at the moment and what work you would like to continue doing in the near future. For example, you might enjoy one of the following more than others: question formulation (e.g., problem definition), data acquisition (e.g., measure development), data management (e.g., data wrangling), data analysis (e.g., model building), or storytelling (e.g., data visualization or dashboard creation). Ideally, you will want to create a portfolio that highlights your strengths in each of the areas you wish to emphasize. With that being said, it’s also good to show at least some level of proficiency in non-emphasized areas as well. Figure out what knowledge, skills, abilities your ideal employer wants – or what it needs but doesn’t yet realize it needs. At the very least, you’ll want to figure out what knowledge, skills, or abilities the employer expects to see in ideal job candidates. You might be able to glean some of these expectations from the job posting itself or from the organization’s website. In addition, you find examples of expected knowledge, skills, and abilities in published white papers, peer-reviewed publications, or the LinkedIn profiles of other individuals who hold similar job at that organization. Even better, try to attain direct insider information from those who currently work at that organization. A portfolio also provides you with an opportunity to showcase tools, applications, knowledge, skills, or abilities that the organization likely needs to attain strategic objectives but does not yet realize or recognize. This might be especially relevant in situations in which you suspect that the organization’s data analytics capabilities are less mature or when you believe you might be overqualified. Decide whether you want your portfolio to teach, showcase, or do both. Both teaching and showcasing can be useful ways to illustrate your knowledge, skills, and abilities. If you go the teaching route, your portfolio project will likely take the form of a tutorial. A well thought out tutorial can be a good method for showing that you understand the concepts and technical applications well enough to teach another person how to do the same. If you go the showcasing route, your portfolio project will focus less on teaching and more on highlighting what you are capable of doing. If your portfolio consists of multiple projects, you might find it worthwhile to include at least one teaching project and at least one showcasing project. Find an appropriate dataset. There are many different places in which you can find toy datasets or public datasets that are free to use. Though, you’ll want to be absolutely certain that the dataset you’ve chosen is free to use and publicly available. That is, you do not want to use proprietary or private data for your portfolio. Examples of repositories for data sets include: My GitHub repository called R Tutorial Data Files, which are all datasets that I’ve simulated using R; This GitHub repository called Awesome Public Datasets; Kaggle; This Stanford University website has links to a variety of public datasets. If you can’t find an appropriate dataset, you can always simulate one using R, which is more involved and complicated. Rich Landers’ (University of Minnesota) website includes this tool, which can be used to simulate simple datasets. Create an immersive environment for the intended audience. It can be tempting to just manage, analyze, and visualize a bunch of data without providing any context or backstory. Because context matters, I recommend creating an immersive environment that orients the intended audience to the context, including variable definitions. This can be dibe via writing, audio, or video. By establishing an immersive environment, the problem you’re attempting to solve or the question you’re attempting to answer will be more meaningful – and ideally will illustrate a clear purpose (e.g., helping an organization to attain a strategic objective). Articulate a clear problem (question) that can be solved (answered) with the available data. It’s important to make sure that your portfolio project is problem- or question-focused. That is, use your portfolio to show how you can go from a problem definition (or question formulation) to a solution (or answer). In doing so, you can demonstrate your ability to conduct meaningful and purposeful data analytics. For a refresher on problem definition and question formulation, check out this chapter. Write (and annotate) your code with an emphasis on clarity. Your code provides a behind-the-scenes glimpse at your decision-making processes so make sure it’s clear and understandable. For positions that expect candidates to have less advanced programming skills, you can focus on writing code that is understandable to a broad audience and that illustrates you understand foundational concepts, operations, and techniques. It might not be the most efficient or elegant code, but it should be clear and free of errors. For positions that expect candidates to have more advanced programming skills, you’ll want to focus on writing code that is stable, reproducible, efficient, and elegant. Regarding efficiency and elegance, you’ll want to consider how long it takes your code to run and how this might be more consequential at scale, and ideally, you’ll want to write less code when possible. Be sure to include clear annotations that help explain your many decisions. If your portfolio project includes data analysis or visualization, make sure that you’ve chosen an appropriate analysis or visualization given the problem/question and available data. You can run all sorts of analyses and attain results – even when the analyses are not appropriate or meaningful given the problem/question and/or available data. If you are performing statistical analysis of the data, you’ll want to make sure that the statistical assumptions for a particular analysis have been reasonably satisfied; better yet, demonstrate in your portfolio how you tested relevant statistical assumptions. All else being equal, it’s best to choose the simplest and most easily interpretable analysis. For example, if you’re interested in comparing the means for two independent samples, then there are statistical equivalent ways to analyze the data: independent-samples t-test, one-way analysis of variance, simple linear regression, and structural equation modeling. In this example, the independent-samples t-test will likely be the simplest analysis to run and communicate given the goal of comparing the means for two independent samples. I like to think of it using this metaphor: If your objective is to get some almond milk from your corner grocery story, you could walk (independent-samples t-test) or you could drive a Ferrari (structural equation modeling); both will get you there, but one is less resource intensive. Focus on good storytelling. Make sure your portfolio project tells an accurate yet compelling story. While writing sophisticated code or running advanced analyses may impressive some, at the end of the day, your portfolio project should tell a good (and hopefully memorable) story. For a review of classic storytelling principles, check out this chapter. Solicit friendly feedback prior to sharing your portfolio with an employer. Everyone makes errors, and it’s better to have a friend or colleague catch those errors prior to sharing the portfolio with an employer. Friends or colleagues can also provide feedback on how intuitive, comprehensible, or appropriate your portfolio is. So who should you ask for feedback? Ideally, you should seek feedback from individuals who have greater expertise in the area than you to make sure you’ve done everything correctly or appropriately, but it can also be helpful to seek feedback from people who you expect will have a similar level of expertise as the intended audience, as this latter group may help you create a portfolio that is not overly complex for the given audience. Select a platform that will allow you to share your portfolio. There are many different ways in you can share a portfolio, and it’s important to select a platform that a hiring manager will be familiar with (or at least figure out how to intuitively access) and that has at least some relevance to the position to which you’re applying. One of the simplest ways to share a platform would be to share a static document (e.g., PDF). A static document may not impress some hiring managers, but if the position to which you’re applying will involve writing a lot of technical reports, then a static document might be appropriate. Further, in some instances, part or all of your portfolio might consist of published articles – although this might be most relevant for more specialized research roles. Further, in RStudio, you can create an RMarkdown file to embed code; in fact, there are packages called blogdown and bookdown that allow you to assemble multiple RMarkdown files into a coherent structure. Other platforms include: YouTube video, a playlist, or an entire channel containing multiple videos; GitHub repository; Medium article; Personal website; Tableau or PowerBI; Shiny web application. Update your portfolio regularly. You’ll want your portfolio to feel fresh and contemporary, which means that you’ll need to update your portfolio with some regularity (e.g., once or twice a year). In addition, be sure to check periodically in which some of the packages/functions you’re using have been updated, which could affect how your code works. One way to work around this is to use a dependency management package like packrat. Although using a dependency management package will help to ensure that your code works properly over time, it doesn’t guard against stale-looking code that references deprecated functions. "],["literature_search_review.html", "Chapter 66 Conducting a Literature Search &amp; Review", " Chapter 66 Conducting a Literature Search &amp; Review Prior to beginning an investigation into a particular phenomenon of interest, it is not uncommon for leading HR analytics teams to search and review the extant literature. In doing so, HR analysts can apply what has been found in other organizations and contexts to inform internal research projects. In the videos that follow, I review the scientific process, how to conduct a literature search, and how to perform a literature review. Link to video: https://youtu.be/nqv9CkEMDQY Link to video: https://youtu.be/kSVcSKpXhC4 Link to video: https://youtu.be/v5EMEnlUhac "],["statistical_practical_significance.html", "Chapter 67 Statistical &amp; Practical Significance", " Chapter 67 Statistical &amp; Practical Significance Statistical significance and practical significance provide unique types of information about an inferential statistical finding. Whereas statistical significance helps us understand uncertainty around whether a particular effect (e.g., difference, association) exists, practical significance provides us with an indication of the size of an effect. In the video that follows, I introduce the concepts of statistical and practical significance. Link to conceptual video: https://youtu.be/DNFjypdSarM "],["missingdata.html", "Chapter 68 Missing Data", " Chapter 68 Missing Data It’s no secret that real organizational data are often messy. The amount and type of missing data adds to the real-world messiness. In the video that follows, I introduce the concept of missing data, the types of missing data, and some resources and tools that we can use to account for missing data. Link to conceptual video: https://youtu.be/E4qkX2TmlXk "],["poweranalysis.html", "Chapter 69 Power Analysis", " Chapter 69 Power Analysis Both Type I Errors (e.g., false positives) and Type II Errors (e.g., false negatives) should be of concern when designing an organizational study and when interpreting the results of statistical findings. A power analysis can be conducted to design studies (e.g., appropriate sample size) that will be able to detect an effect (e.g., difference, association) of a particular magnitude while also attempting to mitigate false positives. Somewhat controversially, power analyses can also be conducted after the data have been collected in order to determine whether the design yielded sufficient power to detect an effect that truly exists in the underlying population; for more information on this controversy check out the following article: Hoenig, J. M., &amp; Heisey, D. M. (2001). The abuse of power: The pervasive fallacy of power calculations for data analysis. The American Statistician, 55(1), 19-24. In the video that follows, I provide a brief overview of power analysis. Link to conceptual video: https://youtu.be/OkbXBWuzikM "],["cfa.html", "Chapter 70 Evaluating Measurement Models Using Confirmatory Factor Analysis 70.1 Conceptual Overview 70.2 Tutorial", " Chapter 70 Evaluating Measurement Models Using Confirmatory Factor Analysis In this chapter, we will learn how to evaluate measurement models using confirmatory factor analysis (CFA), where CFA is part of the structural equation modeling (SEM) family of analyses. Specifically, we will learn how to evaluate the measurement structure and construct validity of a theoretical construct operationalized as a multi-item measure (i.e., scale, inventory, test, questionnaire). 70.1 Conceptual Overview Link to conceptual video: https://youtu.be/lhyDp2HtDiA Confirmatory factor analysis (CFA) is a latent variable modeling approach and is part of the structural equation modeling (SEM) family of analyses, which are also referred to as covariance structure analyses. CFA is a useful statistical tool for evaluating the internal structure of a measure designed to assess a theoretical construct (i.e., concept); in other words, we can apply CFA to evaluate the construct validity of a construct. CFA allows us to directly specify and estimate a measurement model, which ultimately can be incorporated into structural regression models. In CFA models, constructs are represented as latent variables (i.e., latent factors), which by nature are not directly measured. Instead, observed (manifest) variables serve as indicators of the latent construct. I should note that in this chapter, we will focus exclusively on reflective measurement models, which are models in which the latent factor is specified as the direct cause of its indicators. Not covered in this chapter are formative measurement models, which are models in which the observed variables are specified as the direct causes of the latent factor. 70.1.1 Path Diagrams It is often helpful to visualize a CFA model using a path diagram. A path diagram displays the model parameter specifications and can also include parameter estimates. Conventional path diagram symbols are shown in Figure 1. Figure 1: Conventional path diagram symbols and their meanings. For an example of how the path diagram symbols can be used to construct a visual depiction of a CFA model, please reference Figure 2. The path diagram depicts a one-factor CFA model for a multi-item role clarity measure, which means that the model has a single latent factor representing the psychological construct called role clarity. Further, four observed variables (i.e., Items 1-4) serve as indicators of the latent factor, such that the indicators are reflective of the latent factor. Putting it all together, the one-factor CFA model serves as a measurement model and represents the measurement structure of a four-item measure designed to assess the construct of role clarity. Figure 2: Example of a one-factor confirmatory factor analysis (CFA) model path diagram. By convention, the latent factor for role clarity is represented by an oval or circle. Please note that the latent factor is not directly measured; rather, we infer information about the latent factor from its four indicators, which in this example correspond to Items 1-4. The latent factor has a variance term associated with it, which represents the latent factor’s variability; in CFA models, we often to don’t spend much time interpreting latent factors’ variance terms, though. Each of the four observed variables (indicators) is represented with a rectangle. The one-directional, single-sided arrows represent the factor loadings, and point from the latent factor to the observed variables (indicators). Each indicator has a (residual) error variance term, which represents the amount of variance left unexplained by the latent factor in relation to each indicator. To illustrate the covariance path diagram symbol, let’s refer to Figure 3. When standardized, a covariance can be interpreted as a correlation. The covariance symbol is a double-sided arrow in which the arrows connect two distinct latent or observed variables. In Figure 3, the path diagram depicts a multi-factor CFA model and, more specifically, a two-factor CFA model. The first latent factor is associated with a four-item role clarity measure, and the second latent factor is associated with a four-item task mastery measure. If freely estimated, the covariance term allows the two latent factors to covary with each other. Figure 3: Example of a multi-factor confirmatory factor analysis (CFA) model path diagram. 70.1.2 Model Identification Model identification has to do with the number of (free or freely estimated) parameters specified in the model relative to the number of unique (non-redundant) sources of information available, and model implication has important implications for assessing model fit and estimating parameter estimates. Just-identified: In a just-identified model (i.e., saturated model), the number of freely estimated parameters (e.g., factor loadings, covariances, variances) is equal to the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is equal to zero. In just-identified models, the model parameter standard errors can be estimated, but the model fit cannot be assessed in a meaningful way using traditional model fit indices. Over-identified: In an over-identified model, the number of freely estimated parameters is less than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is greater than zero. In over-identified models, traditional model fit indices and parameter standard errors can be estimated. Under-identified: In an under-identified model, the number of freely estimated parameters is greater than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is less than zero. In under-identified models, the model parameter standard errors and model fit cannot be estimated. Some might say under-identified models are overparameterized because they have more parameters to be estimated than unique sources of information. Most (if not all) statistical software packages that allow structural equation modeling (and by extension, confirmatory factor analysis) automatically compute the degrees of freedom for a model or, if the model is under-identified, provide an error message. As such, we don’t need to count the number of sources of unique (non-redundant) sources of information and free parameters by hand. With that said, to understand model identification and its various forms at a deeper level, it is often helpful to practice calculating the degrees freedom by hand when first learning. The formula for calculating the number of unique (non-redundant) sources of information available for a particular model is as follows: \\(i = \\frac{p(p+1)}{2}\\) where \\(p\\) is the number of observed variables to be modeled. This formula calculates the number of possible unique covariances and variances for the variables specified in the model – or in other words, it calculates the lower diagonal of a covariance matrix, including the variances. In the single-factor CFA model path diagram specified above, there are four observed variables: Item 1, Item 2, Item 3, and Item 4. Accordingly, in the following formula, \\(p\\) is equal to 4, and the number of unique (non-redundant) sources of information is 10. \\(i = \\frac{4(4+1)}{2} = \\frac{20}{2} = 10\\) To count the number of free parameters (\\(k\\)), simply add up the number of the specified unconstrained factor loadings, variances, covariances, and (residual) error variance terms in the one-factor CFA model. Please note that for latent variable scaling and model identification purposes, we typically constrain one of the factor loadings to 1.0, which means that it is not freely estimated and thus doesn’t count as one of the free parameters. As shown in Figure 4 below, the example one-factor CFA model has 8 free parameters. \\(k = 8\\) To calculate the degrees of freedom (df) for the model, we need to subtract the number of free parameters from the number unique (non-redundant) sources of information, which in this example equates to 10 minus 8. Thus, the degrees of freedom for the model is 2, which means the model is over-identified. \\(df = i - k = 10 - 8 = 2\\) Figure 4: Counting the number of free parameters in the CFA model path diagram. 70.1.3 Model Fit When a model is over-identified (df &gt; 0), the extent to which the specified model fits the data can be assessed using a variety of model fit indices, such as the chi-square (\\(\\chi^{2}\\)) test, comparative fit index (CFI), Tucker-Lewis index (TLI), root mean square error of approximation (RMSEA), and standardized root mean square residual (SRMR). For a commonly cited reference on cutoffs for fit indices, please refer to Hu and Bentler (1999). And for a concise description of common guidelines regarding interpreting model fit indices, including differences between stringent and relaxed interpretations of common fit indices, I recommend checking out Nye (2023). Regardless of which cutoffs we apply when interpreting fit indices, we must remember that such cutoffs are merely guidelines, and it’s possible to estimate an adequate model that meets some but not all of the cutoffs given the limitations of some fit indices. Further, in light of the limitations of conventional model fit index cutoffs, McNeish and Wolf (2023) developed model- and data-specific dynamic fit index cutoffs, which we will cover later in the chapter tutorial. Chi-square test. The chi-square (\\(\\chi^{2}\\)) test can be used to assess whether the model fits the data adequately, where a statistically significant \\(\\chi^{2}\\) value (e.g., p \\(&lt;\\) .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p \\(\\ge\\) .05) indicates that the model fits the data reasonably well (Bagozzi and Yi 1988). The null hypothesis for the \\(\\chi^{2}\\) test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. Of note, the \\(\\chi^{2}\\) test is sensitive to sample size and non-normal variable distributions. Comparative fit index (CFI). As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that CFI compares the focal model to a baseline model, which is commonly referred to as the null or independence model. CFI is generally less sensitive to sample size than the chi-square test. A CFI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. Tucker-Lewis index (TLI). Like CFI, Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes; however, as Hu and Bentler (1999) noted, TLI may be not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). A TLI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. Root mean square error of approximation (RMSEA). The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus ends up effectively rewarding more parsimonious models. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified); further, Hu and Bentler (1999) noted that RMSEA may not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). In general, an RMSEA value that is less than or equal to .06 indicates good model fit to the data, although some relax that cutoff to .08 or even .10. Standardized root mean square residual (SRMR). Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .06 generally indicates good fit to the data, although some relax that cutoff to .08. Summary of model fit indices. The conventional cutoffs for the aforementioned model fit indices – like any rule of thumb – should be applied with caution and with good judgment and intention. Further, these indices don’t always agree with one another, which means that we often look across multiple fit indices and come up with our best judgment of whether the model adequately fits the data. Generally, it is not advisable to interpret model parameter estimates unless the model fits the data reasonably adequately, as a poorly fitting model may be due to model misspecification, an inappropriate model estimator, or other factors that need to be addressed. With that being said, we should also be careful to not toss out a model entirely if one or more of the model fit indices suggest less than acceptable levels of fit to the data. The table below contains the conventional stringent and more relaxed cutoffs for the model fit indices. Fit Index Stringent Cutoffs for Acceptable Fit Relaxed Cutoffs for Acceptable Fit \\(\\chi^{2}\\) \\(p \\ge .05\\) \\(p \\ge .01\\) CFI \\(\\ge .95\\) \\(\\ge .90\\) TLI \\(\\ge .95\\) \\(\\ge .90\\) RMSEA \\(\\le .06\\) \\(\\le .08\\) SRMR \\(\\le .06\\) \\(\\le .08\\) 70.1.4 Parameter Estimates In CFA models, there are various types of parameter estimates, which correspond to the path diagram symbols covered earlier (e.g., covariance, variance, factor loading). When a model is just-identified or over-identified, we can estimate the standard errors for freely estimated parameters, which allows us to evaluate statistical significance. With most software applications, we can request standardized parameter estimates, which facilitate interpretation. Factor loadings. When we standardize factor loadings, we obtain estimates for each directional relation between the latent factor and an indicator, including for the factor loading that we likely constrained to 1.0 for latent factor scaling and model identification purposes (see above). When standardized, factor loadings can be interpreted like correlations, and generally we want to see standardized estimate values between .50 and .95 (Bagozzi and Yi 1988). If a standardized factor loading falls outside of that range, we typically investigate whether there is a theoretical or empirical reason for the out-of-range estimate, and we may consider removing the associated indicator if warranted. (Residual) error variance terms. The (residual) error variance terms, which are also known as disturbance terms or uniquenesses, indicate how much variance is left unexplained by the latent factor in relation to the indicators. When standardized, error variance terms represent the proportion (percentage) of variance that remains unexplained by the latent factor. Ideally, we want to see standardized error variance terms that are less than or equal to .50. Variances. The variance estimate of the latent factor is generally not a focus when evaluating parameter estimates in a CFA model, as the variance of a latent factor depends on the factor loadings and scaling. Covariances. In a CFA model, covariances between latent factors help us understand the extent to which they are related (or unrelated). When standardized, a covariance can be interpreted as a correlation. Average variance extracted (AVE). Although not a parameter estimate, per se, average variance extracted (AVE) is a useful statistic for understanding the extent to which indicator variations can be attributed to the latent factor (Fornell and Larcker 1981). The formula for AVE takes into account the factor loadings and (residual) error variance terms associated with a latent factor. In general, we consider AVE values that are greater than or equal to .50 to be acceptable. Composite reliability (CR). Like AVE, composite reliability (CR) is not a parameter estimate; instead, it is another useful statistic that helps us understand our CFA model. CR is also known as coefficient omega (\\(\\omega\\)), and it provides an estimate of internal consistency reliability. In general, we consider CR values that are greater than or equal to .70 to be acceptable; however, if the estimate falls between .60 and .70, we might refer to the reliability as questionable but not necessarily unacceptable. 70.1.5 Model Comparisons When evaluating CFA models, especially multi-factor models, we often wish to evaluate whether a focal model performs better (or worse) than an alternative model. Comparing models can help us arrive at a more parsimonious model that still fits the data well, as well as evaluate the potential multidimensionality of a construct. As an example, imagine we have a focal model with two latent factors, and unique sets of indicators load onto their respective latent factors. Now imagine that we specify an alternative model that has one latent factor, and all the indicators from our focal model load onto that single latent factor. We can compare those two models to determine whether the alternative model fits the data about the same as our focal model or worse. When two models are nested, we can perform nested model comparisons. As a reminder, a nested model has all the same parameter estimates of a full model but has additional parameter constraints in place. If two models are nested, we can compare them using model fit indices like CFI, TLI, RMSEA, and SRMR. We can also use the chi-square difference (\\(\\Delta \\chi^{2}\\)) test (likelihood ratio test) to compare nested models, which provides a statistical test for nested-model comparisons. When two models are not nested, we can use other model fit indices like Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). With respect to these indices, the best fitting model will have lower AIC and BIC values. 70.1.6 Statistical Assumptions The statistical assumptions that should be met prior to estimating and/or interpreting a CFA model will depend on the type of estimation method. Common estimation methods for CFA models include (but are not limited to) maximum likelihood (ML), maximum likelihood with robust standard errors (MLM or MLR), weighted least squares (WLS), and diagonally weighted least squares (DWLS). WLS and DWLS estimation methods are used when there are observed variables with nominal or ordinal (categorical) measurement scales. In this chapter, we will focus on ML estimation, which is a common method when observed variables have interval or ratio (continuous) measurement scales. As Kline (2011) notes, ML estimation carries with it the following assumptions: “The statistical assumptions of ML estimation include independence of the scores, multivariate normality of the endogenous variables, and independence of the exogenous variables and error terms” (p. 159). When multivariate non-normality is a concern, the MLM or MLR estimator is a better choice than ML estimator, where the MLR estimator allows for missing data and the MLM estimator does not. 70.1.6.1 Sample Write-Up As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees on three multi-item measures targeting feelings of acceptance, role clarity, and task mastery. Using confirmatory factor analysis (CFA), we evaluated the measurement structure of the three multi-item measures, where each item served as an indicator for its respective latent factor; we did not allow indicator error variances to covary (i.e., the associations were constrained to zero) and, by default, the first indicator for each latent factor was constrained to 1 for estimation purposes and the latent-factor covariances were estimated freely. The three-factor model was estimated using the maximum likelihood (ML) estimator and a sample size of 654 new employees. Missing data were not a concern. We evaluated the model’s fit to the data using the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices. The \\(\\chi^{2}\\) test indicated that the model fit the data worse than a perfectly fitting model (\\(\\chi^{2}\\) = 295.932, df = 51, p &lt; .001). Further, the CFI and TLI estimates were .925 and .902, respectively, which did not exceed the more stringent threshold of .95 but did exceed the more relaxed threshold of .90, thereby indicating that model showed marginal fit to the data. Similarly, the RMSEA estimate was .086, which was not below the more stringent threshold of .06 but was below the more relaxed threshold of .10, thereby indicating that model showed marginal fit to the data. The SRMR estimate was .094, which was below both the stringent threshold of .06 and the relaxed threshold of .08, thereby indicating unacceptable model fit to the data. Collectively, the model fit information indicated that model showed mostly marginal fit to the data, which indicated the model may have been misspecified. Excluding the third feelings of acceptance item, the standardized factor loadings ranged from .706 to .846, which means that those item’s standardize factor loadings fell well within the acceptable range of .50-.95; however, the standardized factor loading for the third feelings of acceptance item was .186, which was a great deal outside the acceptable range. Regarding the standardized covariance estimates, the correlation between feelings of acceptance and role clarity latent factors was .248, statistically significant (p &lt; .001), and small-to-medium in terms of practical significance; the correlation between feelings of acceptance and task mastery latent factors was .263, statistically significant (p &lt; .001), and small-to-medium in terms of practical significance; and the correlation between role clarity and task mastery latent factors was .180, statistically significant (p &lt; .001), and small in terms of practical significance. The standardized error variances ranged from .284 to .966, which can be interpreted as proportions of the variance not explained by the latent factor. With the exceptions of the third feelings of acceptance item’s error variance (.966) and the fourth feelings of acceptance item’s error variance (.502), the indicator error variances were less than the recommended .50 threshold, which means that unmodeled constructs did not likely have a notable impact on the vast majority of the indicators. The standardized error variance associated with the third feelings of acceptance item was well above the .50 threshold and its value indicates that the AC latent factor fails to explain 96.6% of the variance in that item, which is unacceptable. Given the third feelings of acceptance item’s unacceptably low standardized factor loading and unacceptably high standardized error variance, we reviewed the item’s content (“My colleagues and I feel confident in our ability to complete work.”) and the feelings of acceptance construct’s conceptual definition (“the extent to which an individual feels welcomed and socially accepted at work”). Because the item’s content does not align with the conceptual definition and because of the unacceptable standardized factor loading and error variance, we decided to drop the third feelings of acceptance prior to re-estimating the model. In contrast, the standardized error variance for the fourth feelings of acceptance item was just above the .50 recommended cutoff, and after reviewing the item’s content (“My colleagues listen thoughtfully to my ideas.”) and the construct’s aforementioned conceptual definition, we determined that the item fits within the conceptual definition boundaries; thus, we decided to retain the fourth feelings of acceptance item. The average variance extracted (AVE) estimates for feelings of acceptance, role clarity, and task mastery were .440, .683, and .578, respectively. The AVE estimates associated with the the role clarity and task mastery latent factors exceeded the conventional threshold (\\(\\ge\\) .50), and thus, we can conclude that those factors showed acceptable levels of AVE. In contrast, the AVE estimate associated with the feelings of acceptance latent factor fell below the .50 cutoff; this unacceptable AVE estimate may be the result of the problematic parameter estimates associated with the third feelings of acceptance item that we noted above. The composite reliability (CR) estimates for feelings of acceptance, role clarity, and task mastery were .788, .866, and .845, respectively, which exceeded the conventional threshold of .70 and thus were deemed acceptable. In sum, the three-factor measurement model showed marginal fit to the data, and CR estimates were acceptable; however, when evaluating the parameter estimates, the standardized factor loading and standardized error variance for the third feelings of acceptance item were both unacceptable; further, while the AVE estimates associated with the role clarity and task mastery latent factors were acceptable, the AVE associated with the feelings of acceptance latent factor was unacceptable, which may be attributable to the low standardized factor loading associated with the third feelings of acceptance. Subsequently, we re-specified and re-estimated the CFA model by removing the third feelings of acceptance item. In doing so, we found the following. The updated model showed acceptable fit to the data according to CFI (.976), TLI (.968), RMSEA (.052), and SRMR (.032). The chi-square test (\\(\\chi^{2}\\) = 113.309, df = 41, p &lt; .001), however, indicated that the model did not fit the data well; that said, the chi-square test is sensitive to sample size. We concluded that in general the model showed acceptable fit to the data. Standardized factor loadings ranged from .708 to .846, which all fell well within the recommended .50-.95 acceptability range. The standardized error variances for items ranged from .284 to .499, and thus all fell below the target threshold of .50, thereby indicating that it was unlikely that an unmodeled construct had an outsized influence on any of those items. The average variance extracted (AVE) for feelings of acceptance, role clarity, and task mastery were .559, .683, and .578, respectively, which all exceeded the .50 cutoff; thus, all three latent factors showed acceptable AVE levels. Finally, the composite reliability (CR) estimates were .835 for feelings of acceptance, .866 for role clarity, and .845 for task mastery, and all indicated acceptable levels of internal consistency reliability. In sum, the updated three-factor measurement model in which the ac_3 item was removed showed acceptable fit to the data, acceptable parameter estimates, acceptable AVE estimates, and acceptable CR estimates. Thus, this specification of the three-factor CFA model will be retained moving forward. Finally, we compared the updated three-factor CFA model to models with alternative, more parsimonious measurement structures. As described above, the updated three-factor model showed acceptable fit to the data (\\(\\chi^{2}\\) = 113.309, df = 41, p &lt; .001; CFI = .976; TLI = .968; RMSEA = .052; SRMR = .032). We subsequently compared the three-factor model to more parsimonious two- and one-factor models to determine whether any of the alternative models fit the data the data approximately the same with a simpler measurement structure. For the first two-factor model, we collapsed the feelings of acceptance and role clarity latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the task mastery latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 994.577, df = 43, p &lt; .001; CFI = .689; TLI = .602; RMSEA = .184; SRMR = .137), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 881.27, \\(\\Delta df\\) = 2, p &lt; .001). For the second two-factor model, we collapsed the role clarity and task mastery latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the feelings of acceptance latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1114.237, df = 43, p &lt; .001; CFI = .650; TLI = .552; RMSEA = .195; SRMR = .173), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 1000.90, \\(\\Delta df\\) = 2, p &lt; .001). For the third two-factor model, we collapsed the feelings of acceptance and task mastery latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the role clarity latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1059.402, df = 43, p &lt; .001; CFI = .667; TLI = .575; RMSEA = .190; SRMR = .157), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 946.09, \\(\\Delta df\\) = 2, p &lt; .001). For the one-factor model, we collapsed the feelings of acceptance, role clarity, and task mastery latent factors into a single factor and all three corresponding measures’ items loaded onto the single latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1921.400, df = 43, p &lt; .001; CFI = .386; TLI = .232; RMSEA = .255; SRMR = .200), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 1808.10, \\(\\Delta df\\) = 3, p &lt; .001). In conclusion, we opted to retain the three-factor model because it fit the data significantly better than the alternative models, even though the three-factor model is more complex and thus sacrifices some degree of parsimony. 70.2 Tutorial This chapter’s tutorial demonstrates how to estimate measurement models using confirmatory factor analysis (CFA) in R. 70.2.1 Video Tutorial The video tutorial for this chapter is planned but has not yet been recorded. 70.2.2 Functions &amp; Packages Introduced Function Package cfa lavaan summary base R semPaths semPlot AVE semTools compRelSEM semTools anova base R options base R inspect lavaan cbind base R rbind base R t base R ampute mice cfaOne dynamic cfaHB dynamic 70.2.3 Initial Steps If you haven’t already, save the file called “cfa.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “cfa.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;cfa.csv&quot;) ## Rows: 654 Columns: 13 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (12): ac_1, ac_2, ac_3, ac_4, ac_5, rc_1, rc_2, rc_3, tm_1, tm_2, tm_3, tm_4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;EmployeeID&quot; &quot;ac_1&quot; &quot;ac_2&quot; &quot;ac_3&quot; &quot;ac_4&quot; &quot;ac_5&quot; &quot;rc_1&quot; &quot;rc_2&quot; &quot;rc_3&quot; &quot;tm_1&quot; ## [11] &quot;tm_2&quot; &quot;tm_3&quot; &quot;tm_4&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 654 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 13 ## EmployeeID ac_1 ac_2 ac_3 ac_4 ac_5 rc_1 rc_2 rc_3 tm_1 tm_2 tm_3 tm_4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE1001 3 4 3 2 3 5 5 5 3 3 1 2 ## 2 EE1002 3 4 4 5 5 5 4 4 1 3 3 2 ## 3 EE1003 3 3 6 5 3 5 5 4 4 5 3 4 ## 4 EE1004 4 4 5 3 4 3 4 5 4 4 3 4 ## 5 EE1005 4 2 2 2 2 5 5 4 2 2 3 4 ## 6 EE1006 3 3 2 3 3 4 4 3 4 2 3 1 The data frame includes data from a new-employee onboarding survey administered 1-month after employees’ respective start dates. The sample includes 654 employees. As part of the survey, employees responded to three multi-item measures intended to assess their level of adjustment into the organization and provided their age measured in years (age) and their gender identity (gender). Employees responded to items from the three multi-item measures using a 7-point agreement Likert-type response format, ranging from Strongly Disagree (1) to Strongly Agree (7). For all items, higher scores indicate higher levels of the construct. The first multi-item measure is designed to measure feelings of acceptance, which is conceptually defined as “the extent to which an individual feels welcomed and socially accepted at work.” The measure includes the following four items. ac_1 (“My colleagues make me feel welcome.”) ac_2 (“My colleagues seem to enjoy working with me.”) ac_3 (“My colleagues and I feel confident in our ability to complete work.”) ac_4 (“My colleagues listen thoughtfully to my ideas.”) ac_5 (“My colleagues respect my work-related opinions.”) The second multi-item measure is designed to measure role clarity, which is conceptually defined as “the extent to which an individual understands what is expected of them in their job or role.” The measure includes the following three items. rc_1 (“I understand what my job-related responsibilities are.”) rc_2 (“I understand what the organization expects of me in my job.”) rc_3 (“My job responsibilities have been clearly communicated to me.”) The third multi-item measure is designed to measure task mastery, which is conceptually defined as “the extent to which an individual feels self-efficacious in their role and feels confident in performing their job responsibilities.” The measure includes the following four items. tm_1 (“I am confident I can perform my job responsibilities effectively.”) tm_2 (“I am able to address unforeseen job-related challenges.”) tm_3 (“When I apply effort at work, I perform well.”) tm_4 (“I am proficient in the skills needed to perform my job.”) 70.2.4 Estimate One-Factor CFA Models We will begin by estimating what is referred to as a one-factor confirmatory factor analysis (CFA) model. A one-factor model has a single latent factor (i.e., latent variable), which for our purposes will represent a psychological construct targeted by one of the multi-item survey measures. Each of the measure’s items will serve as a indicator of the latent factor. Because confirmatory factor analysis (CFA) is a specific application of structural equation modeling (SEM), we will use functions from an R package developed for SEM called lavaan (latent variable analysis) to estimate our CFA models. Let’s begin by installing and access the lavaan package (if you haven’t already). # Install package install.packages(&quot;lavaan&quot;) # Access package library(lavaan) In the following sections, we will learn how to estimate an over-identified one-factor model, followed by a just-identified model. 70.2.4.1 Estimate Over-Identified One-Factor Model If you recall from the introduction to this chapter, in an over-identified model, the number of parameters (e.g., structural relations, variances) is less than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is greater than zero. In over-identified models, the model parameters can be estimated, and the model fit can be assessed. The feelings of acceptance multi-item measure contains five items, which will serve as indicators for the latent factor associated with feelings of acceptance. A conventionally specified CFA model will be over-identified if the latent factor has at least four indicators, so given that our measure has five items, this model will be over-identified. First, we must specify the one-factor model and assign it to an object that we can subsequently reference. To do so, we will do the following. Specify a name for the model object (e.g., cfa_mod), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator and within quotation marks (\" \"): Specify a name for the latent factor (e.g., AC), followed by the =~ operator, which is used to indicate how a latent factor is measured. Anything that comes to the right of the =~ operator is an indicator (e.g., item) of the latent factor. Please note that the latent factor is not something that we directly observe, so it will not have a corresponding variable in our data frame object. After the =~ operator, specify each indicator (i.e., item) associated with the latent factor, and to separate the indicators, insert the + operator. In this example, the five indicators of the feelings of acceptance latent factor (AC) are: ac_1 + ac_2 + ac_3 + ac_4 + ac_5. These are our observed variables, which conceptually are influenced by the underlying latent factor. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_3 + ac_4 + ac_5 &quot; Second, now that we have specified the model object (cfa_mod), we are ready to estimate the model using the cfa function from the lavaan package. To do so, we will do the following. Specify a name for the fitted model object (e.g., cfa_fit), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the cfa function, and within the function parentheses include the following arguments. As the first argument, insert the name of the model object that we specified above (cfa_mod). As the second argument, insert the name of the data frame object to which the indicator variables in our model belong. That is, after data=, insert the name of the data frame object (df). Note: The cfa function includes model estimation defaults, which explains why we had relatively few model specifications. For example, the function defaults to constraining the first indicator’s unstandardized factor loading to 1.0 for model fitting purposes, and constrains covariances between indicator error terms (i.e., uniquenesses) to zero (or in other words, specifies the error terms as uncorrelated). # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df) # name of data frame object Third, we will use the summary function from base R to to print the model results. To do so, we will apply the following arguments in the summary function parentheses. As the first argument, specify the name of the fitted model object that we created above (cfa_fit). As the second argument, set fit.measures=TRUE to obtain the model fit indices (e.g., CFI, TLI, RMSEA, SRMR). As the third argument, set standardized=TRUE to request the standardized parameter estimates for the model. # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 23 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 4.151 ## Degrees of freedom 5 ## P-value (Chi-square) 0.528 ## ## Model Test Baseline Model: ## ## Test statistic 976.389 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5040.319 ## Loglikelihood unrestricted model (H1) -5038.244 ## ## Akaike (AIC) 10100.638 ## Bayesian (BIC) 10145.469 ## Sample-size adjusted Bayesian (SABIC) 10113.719 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.049 ## P-value H_0: RMSEA &lt;= 0.050 0.953 ## P-value H_0: RMSEA &gt;= 0.080 0.001 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.012 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.946 0.735 ## ac_2 1.037 0.060 17.388 0.000 0.981 0.767 ## ac_3 0.231 0.063 3.677 0.000 0.218 0.157 ## ac_4 0.940 0.058 16.270 0.000 0.889 0.708 ## ac_5 1.107 0.063 17.496 0.000 1.047 0.774 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.763 0.055 13.758 0.000 0.763 0.460 ## .ac_2 0.672 0.053 12.775 0.000 0.672 0.411 ## .ac_3 1.876 0.104 17.994 0.000 1.876 0.975 ## .ac_4 0.786 0.055 14.397 0.000 0.786 0.498 ## .ac_5 0.734 0.059 12.542 0.000 0.734 0.401 ## AC 0.895 0.089 10.057 0.000 1.000 1.000 Evaluating model fit. Now that we have the summary of our model results, we will begin by evaluating key pieces of the model fit information provided in the output. Estimator. The function defaulted to using the maximum likelihood (ML) model estimator. When there are deviations from multivariate normality or categorical variables, the function may switch to another estimator. Number of parameters. Eight parameters were estimated, which as we will see later correspond to factor loadings and (error) variance components. Number of observations. Our effective sample size is 654. Had there been missing data on the observed variables, this portion of the output would have indicated how many of the observations were retained for the analysis given the missing data. How missing data are handled during estimation will depend on the type of missing data approach we apply, which is covered in more default in the section called Estimating Models with Missing Data. By default, the cfa function applies listwise deletion in the presence of missing data. Chi-square test. The chi-square (\\(\\chi^{2}\\)) test assesses whether the model fits the data adequately, where a statistically significant \\(\\chi^{2}\\) value (e.g., p \\(&lt;\\) .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p \\(\\ge\\) .05) indicates that the model fits the data reasonably well (Bagozzi and Yi 1988). The null hypothesis for the \\(\\chi^{2}\\) test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. Of note, the \\(\\chi^{2}\\) test is sensitive to sample size and non-normal variable distributions. For this model, we find the \\(\\chi^{2}\\) test in the output section labeled Model Test User Model. Because the p-value is equal to or greater than .05, we fail to reject the null hypothesis that the mode fits the data perfectly and thus conclude that the model fits the data acceptably (\\(\\chi^{2}\\) = 4.151, df = 5, p = .528). Finally, note that because the model’s degrees of freedom (i.e., 5) is greater than zero, we can conclude that the model is over-identified. Comparative fit index (CFI). As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that CFI compares our estimated model to a baseline model, which is commonly referred to as the null or independence model. CFI is generally less sensitive to sample size than the chi-square (\\(\\chi^{2}\\)) test. A CFI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. For this model, CFI is equal to 1.000, which indicates that the model fits the data acceptably. Tucker-Lewis index (TLI). Like CFI, Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes; however, as Hu and Bentler (1999) noted, TLI may be not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). A TLI value greater than or equal to .95 generally indicates good model fit to the data, although like CFI, some might relax that cutoff to .90. For this model, TLI is equal to 1.002, which indicates that the model fits the data acceptably. Loglikelihood and Information Criteria. The section labeled Loglikelihood and Information Criteria contains model fit indices that are not directly interpretable on their own (e.g., loglikelihood, AIC, BIC). Rather, they become more relevant when we wish to compare the fit of two or more non-nested models. Given that, we will will ignore this section in this tutorial. Root mean square error of approximation (RMSEA). The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus effectively rewards models that are more parsimonious. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified); further, Hu and Bentler (1999) noted that RMSEA may not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). In general, an RMSEA value that is less than or equal to .06 indicates good model fit to the data, although some relax that cutoff to .08 or even .10. For this model, RMSEA is .000, which indicates that the model fits the data acceptably. Standardized root mean square residual. Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .06 generally indicates good fit to the data, although some relax that cutoff to .08. For this model, SRMR is equal to .012, which indicates that the model fits the data acceptably. In sum, the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices all indicate that our model fit the data acceptably based on conventional rules and thresholds. This level of agreement, however, is not always going to occur. For instance, it is relatively common for the \\(\\chi^{2}\\) test to indicate a lack of acceptable fit while one or more of the relative or absolute fit indices indicates that fit is acceptable given the limitations of the \\(\\chi^{2}\\) test. Further, there may be instances where only two or three out of five of these model fit indices indicate acceptable model fit. In such instances, we should not necessarily toss out the model entirely, but we should consider whether there are model misspecifications. Of course, if all five model indices are well beyond the conventional thresholds (in a bad way), then our model likely has major issues, and we should not proceed with interpreting the parameter estimates. Fortunately, for our model, all five model fit indices signal that the model fit the data acceptably, and thus we should feel confident proceeding forward with interpreting and evaluating the parameter estimates. Evaluating parameter estimates. As noted above, our model showed acceptable fit to the data, so we can feel comfortable interpreting the parameter estimates. By default, the cfa function provides unstandardized parameter estimates, but if you recall, we also requested standardized parameter estimates. In the output, the unstandardized parameter estimates fall under the column titled Estimates, whereas the standardized factor loadings we’re interested in fall under the column titled Std.all. Factor loadings. The output section labeled Latent Variables contains our factor loadings. For this model, the loadings represent the effect of the latent factor for feelings of acceptance on the four items from the associated measure. Factor loading for ac_1. By default, the cfa function constrains the factor loading associated with the first indicator (which in this example is the observed variable ac_1) to 1.000 for model estimation purposes. Using the * operator, we can override that default in our model specification by preceding another indicator variable with 1*; for example, we could have specified our model like this: AC =~ ac_1 + 1*ac_2 + ac_3 + ac_4 + ac_4, which would have constrained the ac_2 indicator to 1.000 instead. Note, however, that there is a substantive standardized factor loading for ac_1 (\\(\\lambda\\) = .735), but it lacks standard error (SE), z-value, and p-value estimates. We can still evaluate this standardized factor loading, though, and we can conclude that it falls within Bagozzi and Yi’s (1988) recommended range for factor loadings: .50 to .95. Thus, we can conclude that the factor loading for ac_1 looks acceptable. Factor loading for ac_2. The standardized factor loading for ac_2 (\\(\\lambda\\) = .767, p &lt; .001) falls within Bagozzi and Yi’s (1988) recommended range of .50 to .95; however, this is not necessarily an issue. It could mean that this is just a very strong indicator of the construct feelings of acceptance, so we’ll consider this to be another acceptable indicator of our focal latent factor. Factor loading for ac_3. The standardized factor loading for ac_3 (\\(\\lambda\\) = .157, p &lt; .001) falls well outside of Bagozzi and Yi’s (1988) recommended range of .50 to .95, so we’ll consider this to be an unacceptable indicator of our focal latent factor. Let’s review the item’s content and the conceptual definition for feelings of acceptance, which appears in the Initial Steps section. The item content is: “My colleagues and I feel confident in our ability to complete work.” And the conceptual definition is: “the extent to which an individual feels welcomed and socially accepted at work.” Clearly, this item’s content does not fit within the bounds of the conceptual definition; in fact, it looks as though it may be more closely related to the conceptual definition for the task mastery construct. Given the very low standardized factor loading the and item content lack of alignment with the conceptual definition, we will drop this item whenever we re-estimate the model. Note: Had this standardized factor loading been just below .50 or just above .95, we would have looked at the item content to determine whether it fit with the conceptual definition, and if it had aligned with the conceptual definition, we would have likely retained the item. Factor loading for ac_4. The standardized factor loading for ac_4 (\\(\\lambda\\) = .708, p &lt; .001) falls within Bagozzi and Yi’s (1988) recommended range of .50 to .95, so we’ll consider this to be another acceptable indicator of our focal latent factor. Factor loading for ac_5. The standardized factor loading for ac_4 (\\(\\lambda\\) = .774, p &lt; .001) falls within Bagozzi and Yi’s (1988) recommended range of .50 to .95, so we’ll consider this to be another acceptable indicator of our focal latent factor. Variance components. The output section labeled Variances contains the (error) variance estimates for each observed indicator (i.e., item) of the latent factor and for the latent factor itself. As was the case with the factor loadings, we can view the standardized and unstandardized parameter estimates. Error variances for indicators. The estimates associated with the four indicator variables represent the error variances. Sometimes these are referred to as residual variances, disturbance terms, or uniquenesses. With the exception of indicator ac_3, the standardized estimates show that the error variances ranged from .411 to .498, which can be interpreted as proportions of the variance not explained by the latent factor. For example, the latent factor AC did not explain 46.0% of the variance in the indicator ac_1; this suggests that 54.0% (100% - 46.0%) of the variance in indicator ac_1 was explained by the latent factor AC. In general, error variances for indicators that are less than .50 are considered acceptable. The standardized error variance for ac_3, however, falls well above the .50 threshold (.975), which means that the latent factor AC does not explain 97.5% of the variance in indicator ac_3, which is unacceptable. Given the low standardized factor loading above, the item content’s misalignment with the conceptual definition for the construct, and this very high standardized error variance, we should feel confident that it is appropriate to remove indicator ac_3 prior to re-estimating the model. Variance of the latent factor. The variance estimate for the latent factor provides can provide an indication of the latent factors’ level variability; however, its value depends on the scaling of factor loadings, and generally it is not a point of interest when evaluating CFA models. By default, the standardized variance for the latent factor will be equal to 1.000, and thus if we wished to evaluate the latent factor variance, we would interpret the unstandardized variance in this instance. Within the semTools package, there are two additional diagnostic tools that we can apply to our model. Specifically, the AVE and compRelSEM functions allow us to estimate the average variance extracted (AVE) (Fornell and Larcker 1981) and the composite (construct) reliability (CR) (Bentler 1968). If you haven’t already, please install and access the semTools package. # Install package install.packages(&quot;semTools&quot;) # Access package library(semTools) To estimate AVE, we simply specify the name of the AVE function, and within the function parentheses, we insert the name of our fitted CFA model estimate. # Estimate average variance extracted (AVE) AVE(cfa_fit) ## AC ## 0.44 Average variance extracted (AVE). The AVE estimate was .44, which falls below the conventional threshold (\\(\\ge\\) .50). We can conclude that AVE for the five-item measurement model is in the unacceptable range, and this low AVE may have been due, in part, to the problematic ac_3 item/indicator that we flagged above due to its unacceptably low standardized factor loading and unacceptably high standardized error variance term. # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit) ## AC ## 0.776 Composite reliability (CR). The CR estimate was .776, which exceeded the conventional threshold for acceptable reliability (\\(\\ge\\) .70) as well as the more relaxed “questionable” threshold (\\(\\ge\\) .60). We can conclude that the five-item measurement model showed acceptably high reliability and specifically acceptably high internal consistency reliability. Visualize the path diagram. To visualize our CFA measurement model as a path diagram, we can use the semPaths function from the semPlot package. If you haven’t already, please install and access the semPlot package. # Install package install.packages(&quot;semPlot&quot;) # Access package library(semPlot) While there are many arguments that can be used to refine the path diagram visualization, we will focus on just four to illustrate how the semPaths function works. As the first argument, insert the name of the fitted CFA model object (cfa_fit). As the second argument, specify what=\"std\" to display just the standardized parameter estimates. As the third argument, specify weighted=FALSE to request that the visualization not weight the edges (e.g., lines) and other plot features. As the fourth argument, specify nCharNodes=0 in order to use the full names of latent and observed indicator variables instead of abbreviating them. # Visualize the measurement model semPaths(cfa_fit, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names The resulting CFA path diagram can be useful for interpreting the model specifications and the parameter estimates. Results write-up for the five-item measure of feelings of acceptance. As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees on a multi-item measure of feelings of acceptance. Using confirmatory factor analysis (CFA), we evaluated the measurement structure for a five-item measure of feelings of acceptance, where each item served as an indicator for the feelings of acceptance latent factor; we did not allow indicator error variances to covary (i.e., the associations were constrained to zero) and, by default, the first indicator of the latent factor (i.e., ac_1) was constrained to 1 for estimation purposes. The one-factor model was estimated using the maximum likelihood (ML) estimator and a sample size of 654 new employees. Missing data were not a concern. We evaluated the model’s fit to the data using the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices. The \\(\\chi^{2}\\) test indicated that the model did not fit the data worse than a perfectly fitting model (\\(\\chi^{2}\\) = 4.151, df = 5, p = .528), which provided an initial indication that the model fit was acceptable. Further, the CFI and TLI estimates were 1.000 and 1.002, respectively, which exceeded the more stringent threshold of .95, thereby indicating acceptable model fit. Similarly, the RMSEA and SRMR estimates were .000 and .006, respectively, and both fell below the more stringent threshold of .06, thereby indicating acceptable model fit. The freely estimated factor loadings associated with the ac_2, ac_3, ac_4, and ac_5 items were all statistically significantly different from zero (p &lt; .001), and the standardized factor loadings for the ac_1, ac_2, ac_4, and ac_5 items (.735, .767, .708, and .774, respectively) fell within the target .50-.95 range; however, the standardized factor loading for the ac_3 item (.157) fell well outside of the target range. The standardized error variances for items ac_1, ac_2, ac_4, and ac_5 ranged from .401 to .498, which all fell below the target threshold of .50, thereby indicating that it was unlikely that an unmodeled construct had an outsized influence on any of those four items. With that said, the standardized error variance for item ac_3 was well above the .50 threshold (.975). Given the ac_3 item’s unacceptably low standardized factor loading and unacceptably high standardized error variance, we reviewed the item’s content (“My colleagues and I feel confident in our ability to complete work.”) and the feelings of acceptance construct’s conceptual definition (“the extent to which an individual feels welcomed and socially accepted at work”). Because the item’s content does not align with the conceptual definition and because of the unacceptable standardized factor loading and error variance, we decided to drop item ac_3 prior to re-estimating the model. The average variance extracted (AVE) for the five items was .44, which fell below the conventional threshold of .50 and thus was deemed acceptable. Finally, the composite reliability (CR) reliability was .776, which exceeded the conventional threshold of .70 and thus was deemed acceptable. In sum, the measurement model for the five-item measure of feelings of acceptance showed acceptable fit to the data and CR fell in the acceptable range; however, when evaluating the parameter estimates, the standardized factor loading and standardized error variance for item ac_3 were both unacceptable; further, AVE was unacceptable, which may be attributable to the low standardized factor loading associated with item ac_3. Removing the problematic item and re-estimating the CFA model. Given the problematic nature of the ac_3 item shown above, we will re-estimate the CFA model without the ac_3 item. # Re-specify one-factor CFA model by removing AC_3 &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 &quot; # Re-estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 21 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 1.297 ## Degrees of freedom 2 ## P-value (Chi-square) 0.523 ## ## Model Test Baseline Model: ## ## Test statistic 959.900 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3905.196 ## Loglikelihood unrestricted model (H1) -3904.547 ## ## Akaike (AIC) 7826.391 ## Bayesian (BIC) 7862.256 ## Sample-size adjusted Bayesian (SABIC) 7836.856 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value H_0: RMSEA &lt;= 0.050 0.855 ## P-value H_0: RMSEA &gt;= 0.080 0.021 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.006 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.945 0.734 ## ac_2 1.034 0.060 17.317 0.000 0.978 0.765 ## ac_4 0.942 0.058 16.263 0.000 0.891 0.709 ## ac_5 1.111 0.063 17.491 0.000 1.050 0.776 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.763 0.056 13.742 0.000 0.763 0.461 ## .ac_2 0.678 0.053 12.824 0.000 0.678 0.415 ## .ac_4 0.784 0.055 14.357 0.000 0.784 0.497 ## .ac_5 0.729 0.059 12.440 0.000 0.729 0.398 ## AC 0.894 0.089 10.044 0.000 1.000 1.000 # Estimate average variance extracted (AVE) AVE(cfa_fit) ## AC ## 0.559 # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit) ## AC ## 0.835 # Visualize the measurement model semPaths(cfa_fit, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Let’s quickly review the model fit information, parameter estimates, average variance extracted (AVE), and composite reliability (CR) to see if they improved after removing item ac_3. Model fit indices. The model fit indices either remained about the same or improved after dropping item ac_3: chi-square test (\\(\\chi^{2}\\) = 1.297, df = 2, p = .523), CFI (1.000), TLI (1.002), RMSEA (.000), and SRMR (.006). Parameter estimates. The parameter estimates also showed improvements after dropping item ac_3. Factor loadings. After dropping item ac_3, all standardized factor loadings fell within the recommended range of .50 to .95, which indicated that they were acceptable. Specifically, the standardized factor loadings ranged from .709 to .776. Error variances. The standardized error variances for the remaining four items (i.e., indicators) were all below the recommended cutoff of .50, which indicated that they were acceptable. Specifically, the standardized error variances ranged from .398 to .497. Average variance extracted (AVE). After removing item ac_3, AVE increased from below the acceptable threshold of .50 to above the acceptable threshold. That is, the AVE estimate of .559 for the updated four-item feelings of acceptance measure was deemed acceptable. Composite reliability (CR) After removing item ac_3, CR remained above the acceptable .70 threshold and improved relative to the previous estimate from when item ac_3 was still included. Thus, the CR estimate of .835 indicated acceptable reliability. Results write-up for the updated four-item measure of feelings of acceptance. As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees on a five-item measure of feelings of acceptance. As noted above, we estimated a confirmatory factor analysis (CFA) model using the maximum likelihood (ML) estimator to evaluate the measurement structure for the five-item measure; however, item ac_3 showed a problematic standardized factor loading, standardized error variance, and misalignment between the item’s content and the conceptual definition for the associated measure. As a result, we re-specified and re-estimated the CFA model by removing item ac_3. In doing so, we found the following. The updated model showed acceptable fit to the data, as the chi-square test (\\(\\chi^{2}\\) = 1.297, df = 2, p = .523), CFI (1.000), TLI (1.002), RMSEA (.000), and SRMR (.006) all met their respective cutoffs. The standardized factor loadings for the ac_1, ac_2, ac_4, and ac_5 items (.734, .765, .709, and .776, respectively) fell within the target .50-.95 range, thereby indicating they were all acceptable. The standardized error variances for items ac_1, ac_2, ac_4, and ac_5 (.461, .451, .497, and .398, respectively) all fell below the target threshold of .50, thereby indicating that it was unlikely that an unmodeled construct had an outsized influence on any of those four items. The average variance extracted (AVE) for the updated four-item measure was .559, which was above the conventional threshold of .50 and thus was deemed acceptable. Finally, the composite reliability (CR) reliability was .835, which exceeded the conventional threshold of .70 and thus was deemed acceptable. In sum, the measurement model for the updated four-item measure of feelings of acceptance showed acceptable fit to the data, and the parameter estimates, AVE, and CR were acceptable; however, when evaluating the parameter estimates, the standardized factor loading and standardized error variance for item ac_3 were both unacceptable. Thus, this four-item specification of the one-factor CFA model will be retained moving forward. 70.2.4.2 Estimate Just-Identified One-Factor Model In the previous section, we evaluated the measurement structure for a four-item measure of feelings of acceptance, which resulted in an over-identified model (df &gt; 0). In this section, we will review what happens when we specify a just-identified measurement model (df = 0). For this example, we will evaluate the measurement model for the three-item measure of role clarity. As you can see below, we specified the three role clarity items as loading onto a latent factor for role clarity: RC =~ rc_1 + rc_2 + rc_3. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; RC =~ rc_1 + rc_2 + rc_3 &quot; # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 940.146 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2902.122 ## Loglikelihood unrestricted model (H1) -2902.122 ## ## Akaike (AIC) 5816.243 ## Bayesian (BIC) 5843.142 ## Sample-size adjusted Bayesian (SABIC) 5824.092 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## RC =~ ## rc_1 1.000 1.157 0.847 ## rc_2 0.967 0.044 21.977 0.000 1.119 0.812 ## rc_3 0.924 0.042 22.107 0.000 1.070 0.819 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .rc_1 0.527 0.051 10.279 0.000 0.527 0.282 ## .rc_2 0.646 0.053 12.142 0.000 0.646 0.340 ## .rc_3 0.560 0.048 11.793 0.000 0.560 0.329 ## RC 1.340 0.108 12.448 0.000 1.000 1.000 # Estimate average variance extracted (AVE) AVE(cfa_fit) ## RC ## 0.683 # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit) ## RC ## 0.866 When reviewing the model output, note that the degrees of freedom (df) is equal to zero, which indicates that the model is just-identified. When a model is just-identified, our go-to model fit indices (chi-square test, CFI, TLI, RMSEA, SRMR) become irrelevant because the model fits the data perfectly from the viewpoint of those indices. The parameter estimates, however, can be estimated as usual. Similarly, the average variance extracted (AVE) and composite reliability (CR) can also be interpreted meaningfully. Please refer to the previous section for guidance on how to interpret the parameter, AVE, and CR estimates. We can also visualize the CFA model as a path diagram for a just-identified model, just like we did with an over-identified model. # Visualize the measurement model semPaths(cfa_fit, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names 70.2.5 Estimate Multi-Factor CFA Models In the previous sections, we explored how to specify and estimate one-factor CFA models, or in other words, models with a single latent factor and all indicators loading onto that factor. In this section, we will learn how to specify multi-factor CFA models, which are models with two or more latent factors; specifically, we will focus on over-identified multi-factor CFA models. Multi-factor models are useful for determining whether theoretically distinguishable constructs are empirically distinguishable. The new-employee onboarding survey data includes responses to three multi-item measures of new-employee adjustment into the organization: feelings of acceptance, role clarity, and task mastery. We modeled feelings of acceptance and role clarity as one-factor models in the previous sections, and in this section we’re going to specify a three-factor model with three latent factors corresponding to feelings of acceptance, role clarity, and task mastery and each measure’s items loading on their respective latent factor. In doing so, we can determine whether a three-factor model fits the data acceptably. For more in-depth guidance on how to specify and evaluate a CFA model, please refer back to this section. When we specify a multi-factor model, we simply repeat the process we used for a one-factor model. That is, in this three-factor model example, we will specify three latent factors. By default, the cfa function will freely estimate the covariance parameters between the three latent factors, constrain the first indicator for each latent factor to 1, and constrain the covariance parameters between the indicator error variance components to zero. Because we will learn how to compare nested models in the following section, let’s name the specified model object cfa_mod3 and the fitted model object cfa_fit_3 to communicate that we are evaluating a three-factor model. Everything else is specified in the same manner as the one-factor models from the previous sections. # Specify three-factor CFA model &amp; assign to object cfa_mod_3 &lt;- &quot; AC =~ ac_1 + ac_2 + ac_3 + ac_4 + ac_5 RC =~ rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_3 &lt;- cfa(cfa_mod_3, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_3, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 33 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 27 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 295.932 ## Degrees of freedom 51 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3312.955 ## Degrees of freedom 66 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.925 ## Tucker-Lewis Index (TLI) 0.902 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -12142.193 ## Loglikelihood unrestricted model (H1) -11994.227 ## ## Akaike (AIC) 24338.386 ## Bayesian (BIC) 24459.430 ## Sample-size adjusted Bayesian (SABIC) 24373.705 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.086 ## 90 Percent confidence interval - lower 0.076 ## 90 Percent confidence interval - upper 0.095 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 0.846 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.094 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.945 0.734 ## ac_2 1.044 0.060 17.529 0.000 0.986 0.772 ## ac_3 0.272 0.063 4.345 0.000 0.257 0.186 ## ac_4 0.938 0.058 16.261 0.000 0.886 0.706 ## ac_5 1.100 0.063 17.473 0.000 1.040 0.768 ## RC =~ ## rc_1 1.000 1.156 0.846 ## rc_2 0.970 0.044 22.121 0.000 1.122 0.814 ## rc_3 0.925 0.042 22.217 0.000 1.069 0.819 ## TM =~ ## tm_1 1.000 1.160 0.757 ## tm_2 0.951 0.053 17.811 0.000 1.103 0.746 ## tm_3 0.963 0.053 18.112 0.000 1.117 0.760 ## tm_4 0.988 0.054 18.458 0.000 1.145 0.777 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC ~~ ## RC 0.271 0.053 5.151 0.000 0.248 0.248 ## TM 0.288 0.054 5.325 0.000 0.263 0.263 ## RC ~~ ## TM 0.241 0.063 3.840 0.000 0.180 0.180 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.764 0.055 13.861 0.000 0.764 0.461 ## .ac_2 0.661 0.052 12.733 0.000 0.661 0.404 ## .ac_3 1.858 0.103 17.959 0.000 1.858 0.966 ## .ac_4 0.792 0.055 14.522 0.000 0.792 0.502 ## .ac_5 0.750 0.058 12.847 0.000 0.750 0.410 ## .rc_1 0.530 0.051 10.449 0.000 0.530 0.284 ## .rc_2 0.641 0.053 12.155 0.000 0.641 0.338 ## .rc_3 0.562 0.047 11.918 0.000 0.562 0.330 ## .tm_1 1.001 0.074 13.528 0.000 1.001 0.427 ## .tm_2 0.970 0.070 13.839 0.000 0.970 0.444 ## .tm_3 0.913 0.068 13.446 0.000 0.913 0.423 ## .tm_4 0.861 0.067 12.901 0.000 0.861 0.396 ## AC 0.893 0.089 10.068 0.000 1.000 1.000 ## RC 1.337 0.107 12.461 0.000 1.000 1.000 ## TM 1.345 0.127 10.572 0.000 1.000 1.000 # Estimate average variance extracted (AVE) AVE(cfa_fit_3) ## AC RC TM ## 0.440 0.683 0.578 # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit_3) ## AC RC TM ## 0.788 0.866 0.845 # Visualize the measurement model semPaths(cfa_fit_3, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Evaluating model fit. Now that we have the summary of our model results, we will begin by evaluating key pieces of the model fit information provided in the output. Estimator. The function defaulted to using the maximum likelihood (ML) model estimator. When there are deviations from multivariate normality or categorical variables, the function may switch to another estimator. Number of parameters. Twenty-five parameters were estimated, which as we will see later correspond to factor loadings and (error) variance components. Number of observations. Our effective sample size is 654. Had there been missing data on the observed variables, this portion of the output would have indicated how many of the observations were retained for the analysis given the missing data. How missing data are handled during estimation will depend on the type of missing data approach we apply, which is covered in more default in the section called Estimating Models with Missing Data. By default, the cfa function applies listwise deletion in the presence of missing data. Chi-square test. The chi-square (\\(\\chi^{2}\\)) test assesses whether the model fits the data adequately, where a statistically significant \\(\\chi^{2}\\) value (e.g., p \\(&lt;\\) .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p \\(\\ge\\) .05) indicates that the model fits the data reasonably well (Bagozzi and Yi 1988). The null hypothesis for the \\(\\chi^{2}\\) test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. Of note, the \\(\\chi^{2}\\) test is sensitive to sample size and non-normal variable distributions. For this model, we find the \\(\\chi^{2}\\) test in the output section labeled Model Test User Model. Because the p-value is less than .05, we reject the null hypothesis that the mode fits the data perfectly and thus conclude that the model does not fit the data acceptably (\\(\\chi^{2}\\) = 295.932, df = 51, p &lt; .001), at least according to this test. Finally, note that because the model’s degrees of freedom (i.e., 51) is greater than zero, we can conclude that the model is over-identified. Comparative fit index (CFI). As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that CFI compares our estimated model to a baseline model, which is commonly referred to as the null or independence model. CFI is generally less sensitive to sample size than the chi-square (\\(\\chi^{2}\\)) test. A CFI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. For this model, CFI is equal to .925, which is above the .90 relaxed cutoff but below the .95 stringent cutoff. Based on this index, we can include that the model shows marginal fit to the data. Tucker-Lewis index (TLI). Like CFI, Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes; however, as Hu and Bentler (1999) noted, TLI may be not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). A TLI value greater than or equal to .95 generally indicates good model fit to the data, although like CFI, some might relax that cutoff to .90. For this model, TLI is equal to .902, which is above the .90 relaxed cutoff but below the .95 stringent cutoff. Based on this index, we can include that the model shows marginal fit to the data. Loglikelihood and Information Criteria. The section labeled Loglikelihood and Information Criteria contains model fit indices that are not directly interpretable on their own (e.g., loglikelihood, AIC, BIC). Rather, they become more relevant when we wish to compare the fit of two or more non-nested models. Given that, we will will ignore this section in this tutorial. Root mean square error of approximation (RMSEA). The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus effectively rewards models that are more parsimonious. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified); further, Hu and Bentler (1999) noted that RMSEA may not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). In general, an RMSEA value that is less than or equal to .06 indicates good model fit to the data, although some relax that cutoff to .08 or even .10. For this model, RMSEA is equal to .086, which is above the .06 stringent cutoff but below the more relaxed .10 cutoff. This indicates that the model fits the data marginally well. Standardized root mean square residual. Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .06 generally indicates good fit to the data, although some relax that cutoff to .08. For this model, SRMR is equal to .094, which indicates that the model does not fit the data acceptably. In sum, the chi-square (\\(\\chi^{2}\\)) test indicated that the model did not fit the data acceptably, but as noted above, this test is sensitive to sample size and non-normality. While the CFI, TLI, and RMSEA estimates did not meet their respective stringent cutoffs, they did meet their more relaxed cutoffs, indicating that the model fit to the data was marginal but showed room for improvement. The SRMR estimate, however, was well above the .06 stringent cutoff and the .08 relaxed cutoff, indicating that the model did not fit the data acceptably. Although not terrible, these less than optimal model fit index estimates suggest that perhaps our model was misspecified in some way. We will evaluate the parameter estimates next to see if we can identify any problematic parameter estimates that may explain the lower-than-desired model fit index estimates that we observed. Evaluating parameter estimates. As noted above, our model fit information indicated that the model marginally fit the data for the most part, so we can feel reasonably comfortable interpreting and evaluating the parameter estimates. By default, the cfa function provides unstandardized parameter estimates, but if you recall, we also requested standardized parameter estimates. In the output, the unstandardized parameter estimates fall under the column titled Estimates, whereas the standardized factor loadings we’re interested in fall under the column titled Std.all. Factor loadings. The output section labeled Latent Variables contains our factor loadings. For this model, the loadings represent the effects of the latent factors for feelings of acceptance, role clarity, and task mastery on the their respective items. By default, the cfa function constrains the factor loading associated with the first indicator of each latent factor to 1 for model estimation purposes. Note, however, that there are still substantive standardized factor loadings for those first indicators, but they lack standard error (SE), z-value, and p-value estimates. We can still evaluate those standardized factor loadings, though. First, regarding the feelings of acceptance (AC) latent factor, with the exception of the ac_3 item, all standardized factor loadings (.706-.772) fell within Bagozzi and Yi’s (1988) recommended range of .50-.95; however, the standardized factor loading for the ac_3 item was .186, which was far below the lower limit of the recommended range. Thus, the AC latent factor showed a weak association with the ac_3 item, and we’ll consider this to be an unacceptable indicator of the AC latent factor. Let’s review the item’s content and the conceptual definition for feelings of acceptance, which appears in the Initial Steps section. The item content is: “My colleagues and I feel confident in our ability to complete work.” And the conceptual definition is: “the extent to which an individual feels welcomed and socially accepted at work.” Clearly, this item’s content does not fit within the bounds of the conceptual definition; in fact, it looks as though it may be more closely related to the conceptual definition for the task mastery construct. Given the very low standardized factor loading the and item content lack of alignment with the conceptual definition, we will drop this item whenever we re-estimate the model. Note: Had this standardized factor loading been just below .50 or just above .95, we would have looked at the item content to determine whether it fit with the conceptual definition, and if it had aligned with the conceptual definition, we would have likely retained the item. Second, regarding the role clarity (RC) latent factor, all standardized factor loadings (.814-.846) fell within recommended range of .50-.95. Third, regarding the task mastery (TM) latent factor, all standardized factor loadings (.746-.777) fell within recommended range of .50-.95. Covariances. The output section labeled Covariances contains the pairwise covariance estimates for the three latent factors. As was the case with the factor loadings, we can view the standardized and unstandardized parameter estimates, where the standardized covariances can be interpreted as correlations. First, the correlation between AC and RC was .248 and statistically significant (p &lt; .001), and can be considered small-to-medium-sized in terms of practical significance. Second, the correlation between AC and TM was .263 and statistically significant (p &lt; .001), and can also be considered small-to-medium-sized in terms of practical significance. Finally, the correlation between RC and TM was .180 and statistically significant (p &lt; .001), and can be considered small-sized in terms of practical significance. Variances The output section labeled Variances contains the (error) variance estimates for each observed indicator (i.e., item) of the latent factor and for the latent factor itself. As was the case with the factor loadings, we can view the standardized and unstandardized parameter estimates. Error variances for indicators. The estimates associated with the four indicator variables represent the error variances. Sometimes these are referred to as residual variances, disturbance terms, or uniquenesses. The standardized error variances ranged from .284 to .966, which can be interpreted as proportions of the variance not explained by the latent factor. For example, the latent factor AC failed to explain 46.1% of the variance in the indicator ac_1, which is acceptable; this suggests that 53.9% (100% - 46.1%) of the variance in indicator ac_1 was explained by the latent factor AC. With the exceptions of the ac_3 item’s error variance (.996) and the ac_4 item’s error variance (.502), the indicator error variances were less than the recommended .50 threshold, which means that unmodeled constructs did not likely have a notable impact on the vast majority of the indicators. The standardized error variance associated with the ac_3 item was well above the .50 threshold and its value indicates that the AC latent factor fails to explain 96.6% of the variance in that item, which is quite bad. Given the low standardized factor loading above, the item content’s misalignment with the conceptual definition for the construct, and this very high standardized error variance, we should feel confident that it is appropriate to remove indicator ac_3 prior to re-estimating the model. In contrast, the standardized error variance for the ac_4 item was just above the .50 recommended cutoff, and if we check the item’s content (“My colleagues listen thoughtfully to my ideas.”) and the construct’s conceptual definition (“the extent to which an individual feels welcomed and socially accepted at work”), we see that the item fits within the conceptual definition boundaries; thus, we should retain the ac_4 item when re-estimating the model. Variance of the latent factors. The variance estimate for the latent factor provides can provide an indication of the latent factors’ level variability; however, its value depends on the scaling of factor loadings, and generally it is not a point of interest when evaluating CFA models. By default, the standardized variance for the latent factor will be equal to 1.000, and thus if we wished to evaluate the latent factor variance, we would interpret the unstandardized variance in this instance. Average variance extracted (AVE). The AVE estimates for feelings of acceptance (AC), role clarity (RC), and task mastery (TM) were .440, .683, and .845, respectively. The AVE estimates associated with the the RC and TM latent factors exceeded the conventional threshold (\\(\\ge\\) .50), and thus, we can conclude that those AVE estimates were acceptable. In contrast, the AVE estimate associated with the AC latent factor fell below the .50 cutoff. This lower-than-desired AVE estimate may be the result of the problematic parameter estimates associated with the ac_3 item, as noted above. Composite reliability (CR). The CR estimates for feelings of acceptance (AC), role clarity (RC), and task mastery (TM) were .788, .866, and .845, respectively, which exceeded the conventional threshold for acceptable reliability (\\(\\ge\\) .70) as well as the more relaxed “questionable” threshold (\\(\\ge\\) .60). We can conclude that three latent factors demonstrated acceptable internal consistency reliability; with that being said, we may be able to improve the CR for the AC latent factor by dropping the problematic ac_3 item. Results write-up for the initial three-factor model. As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees on three multi-item measures targeting feelings of acceptance, role clarity, and task mastery. Using confirmatory factor analysis (CFA), we evaluated the measurement structure of the three multi-item measures, where each item served as an indicator for its respective latent factor; we did not allow indicator error variances to covary (i.e., the associations were constrained to zero) and, by default, the first indicator for each latent factor (i.e., ac_1) was constrained to 1 for estimation purposes and the latent-factor covariances were estimated freely. The three-factor model was estimated using the maximum likelihood (ML) estimator and a sample size of 654 new employees. Missing data were not a concern. We evaluated the model’s fit to the data using the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices. The \\(\\chi^{2}\\) test indicated that the model fit the data worse than a perfectly fitting model (\\(\\chi^{2}\\) = 295.932, df = 51, p &lt; .000). Further, the CFI and TLI estimates were .925 and .902, respectively, which did not exceed the more stringent threshold of .95 but did exceed the more relaxed threshold of .90, thereby indicating that model showed marginal fit to the data. Similarly, the RMSEA estimate was .086, which was not below the more stringent threshold of .06 but was below the more relaxed threshold of .10, thereby indicating that model showed marginal fit to the data. The SRMR estimate was .094, which was about both the stringent threshold of .06 and the relaxed threshold of .08, thereby indicating unacceptable model fit to the data. Collectively, the model fit information indicated that model showed mostly marginal fit to the data, which indicated the model may have been misspecified. Excluding the ac_3 item, the standardized factor loadings ranged from .706 to .846, which means that those item’s standardize factor loadings fell well within the acceptable range of .50-.95; however, the standardized factor loading for the ac_3 item was .186, which was a great deal outside the acceptable range. Regarding the standardized covariance estimates, the correlation between feelings of acceptance and role clarity latent factors was .248, statistically significant (p &lt; .001), and small-to-medium in terms of practical significance; the correlation between feelings of acceptance and task mastery latent factors was .263, statistically significant (p &lt; .001), and small-to-medium in terms of practical significance; and the correlation between role clarity and task mastery latent factors was .180, statistically significant (p &lt; .001), and small in terms of practical significance. The standardized error variances ranged from .284 to .966, which can be interpreted as proportions of the variance not explained by the latent factor. With the exceptions of the ac_3 item’s error variance (.966) and the ac_4 item’s error variance (.502), the indicator error variances were less than the recommended .50 threshold, which means that unmodeled constructs did not likely have a notable impact on the vast majority of the indicators. The standardized error variance associated with the ac_3 item was well above the .50 threshold and its value indicates that the AC latent factor fails to explain 96.6% of the variance in that item, which is unacceptable. Given the ac_3 item’s unacceptably low standardized factor loading and unacceptably high standardized error variance, we reviewed the item’s content (“My colleagues and I feel confident in our ability to complete work.”) and the feelings of acceptance construct’s conceptual definition (“the extent to which an individual feels welcomed and socially accepted at work”). Because the item’s content does not align with the conceptual definition and because of the unacceptable standardized factor loading and error variance, we decided to drop item ac_3 prior to re-estimating the model. In contrast, the standardized error variance for the ac_4 item was just above the .50 recommended cutoff, and after reviewing the item’s content (“My colleagues listen thoughtfully to my ideas.”) and the construct’s conceptual definition, we determined that the item fits within the conceptual definition boundaries; thus, we decided to retain the ac_4 item when re-estimating the model. The average variance extracted (AVE) estimates for feelings of acceptance, role clarity, and task mastery were .440, .683, and .578, respectively. The AVE estimates associated with the the RC and TM latent factors exceeded the conventional threshold (\\(\\ge\\) .50), and thus, we can conclude that those factors showed acceptable levels of AVE. In contrast, the AVE estimate associated with the AC latent factor fell below the .50 cutoff; this unacceptable AVE estimate may be the result of the problematic parameter estimates associated with the ac_3 item that we noted above. The composite reliability (CR) estimates for feelings of acceptance, role clarity, and task mastery were .788, .866, and .845, respectively, which exceeded the conventional threshold of .70 and thus were deemed acceptable. In sum, the three-factor measurement model showed marginal fit to the data, and CR estimates were acceptable; however, when evaluating the parameter estimates, the standardized factor loading and standardized error variance for item ac_3 were both unacceptable; further, while the AVE estimates associated with the role clarity (RC) and task mastery (TM) latent factors were acceptable, the AVE associated with the feelings of acceptance (AC) latent factor was unacceptable, which may be attributable to the low standardized factor loading associated with item ac_3. Removing the problematic item and re-estimating the three-factor CFA model. Given the problematic nature of the ac_3 item shown above, we will re-estimate the three-factor CFA model without the ac_3 item. # Re-specify three-factor CFA model without ac_3 item &amp; assign to object cfa_mod_3 &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 RC =~ rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 &quot; # Re-estimate three-factor CFA model without ac_3 item &amp; assign to fitted model object cfa_fit_3 &lt;- cfa(cfa_mod_3, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_3, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 25 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 113.309 ## Degrees of freedom 41 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.976 ## Tucker-Lewis Index (TLI) 0.968 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11009.696 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 22069.392 ## Bayesian (BIC) 22181.470 ## Sample-size adjusted Bayesian (SABIC) 22102.095 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.052 ## 90 Percent confidence interval - lower 0.041 ## 90 Percent confidence interval - upper 0.063 ## P-value H_0: RMSEA &lt;= 0.050 0.372 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.032 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.946 0.735 ## ac_2 1.041 0.059 17.496 0.000 0.984 0.770 ## ac_4 0.940 0.058 16.301 0.000 0.889 0.708 ## ac_5 1.105 0.063 17.530 0.000 1.045 0.772 ## RC =~ ## rc_1 1.000 1.156 0.846 ## rc_2 0.970 0.044 22.118 0.000 1.122 0.814 ## rc_3 0.925 0.042 22.213 0.000 1.069 0.819 ## TM =~ ## tm_1 1.000 1.159 0.757 ## tm_2 0.951 0.053 17.801 0.000 1.103 0.746 ## tm_3 0.964 0.053 18.103 0.000 1.117 0.760 ## tm_4 0.988 0.054 18.445 0.000 1.145 0.777 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC ~~ ## RC 0.264 0.053 5.022 0.000 0.241 0.241 ## TM 0.267 0.054 4.975 0.000 0.244 0.244 ## RC ~~ ## TM 0.241 0.063 3.838 0.000 0.180 0.180 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.763 0.055 13.832 0.000 0.763 0.460 ## .ac_2 0.665 0.052 12.770 0.000 0.665 0.407 ## .ac_4 0.788 0.054 14.466 0.000 0.788 0.499 ## .ac_5 0.740 0.058 12.699 0.000 0.740 0.404 ## .rc_1 0.530 0.051 10.446 0.000 0.530 0.284 ## .rc_2 0.641 0.053 12.152 0.000 0.641 0.338 ## .rc_3 0.562 0.047 11.916 0.000 0.562 0.330 ## .tm_1 1.002 0.074 13.528 0.000 1.002 0.427 ## .tm_2 0.969 0.070 13.830 0.000 0.969 0.444 ## .tm_3 0.913 0.068 13.433 0.000 0.913 0.422 ## .tm_4 0.861 0.067 12.892 0.000 0.861 0.396 ## AC 0.894 0.089 10.072 0.000 1.000 1.000 ## RC 1.337 0.107 12.461 0.000 1.000 1.000 ## TM 1.344 0.127 10.565 0.000 1.000 1.000 # Estimate average variance extracted (AVE) AVE(cfa_fit_3) ## AC RC TM ## 0.559 0.683 0.578 # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit_3) ## AC RC TM ## 0.835 0.866 0.845 # Visualize the measurement model semPaths(cfa_fit_3, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Let’s quickly review the model fit information, parameter estimates, average variance extracted (AVE), and composite reliability (CR) to see if they improved after removing item ac_3. Model fit indices. With the exception of the chi-square test, model fit indices improved after dropping item ac_3: chi-square test (\\(\\chi^{2}\\) = 113.309, df = 41, p &lt; .001), CFI (.976), TLI (.968), RMSEA (.052), and SRMR (.032). Specifically, CFI, TLI, RMSEA, and SRMR estimates all met their more stringent cutoffs and thus indicated the model fit the data acceptably. Parameter estimates. The parameter estimates also showed improvements after dropping item ac_3. Factor loadings. After dropping item ac_3, all standardized factor loadings fell within the recommended range of .50 to .95, which indicated that they were acceptable. Specifically, the standardized factor loadings ranged from .708 to .846. Covariances. After dropping item ac_3, all standardized covariances remained statistically significant, and their practical significance levels remained similar to those observed in the initial model that included item ac_3. Specifically, the correlations were .241 (p &lt; .001), .244 (p &lt; .001), and .180 (p &lt; .001) for AC in relation to RC, AC in relation to TM, and RC in relation to TM, respectively. Error variances. After dropping item ac_3, the standardized error variances for the items (i.e., indicators) were all below the recommended cutoff of .50, which indicated that they were acceptable. Specifically, the standardized error variances ranged from .284 to .499. Average variance extracted (AVE). After removing item ac_3, AVE estimate associated with the AC latent factor increased from below the acceptable threshold of .50 to above the acceptable threshold. That is, the AVE estimate of .559 for the updated four-item feelings of acceptance measure was deemed acceptable. Further, the AVE estimates associated with the RC and TM latent factors remained above the .50 threshold. Specifically, the AVE estimates associated with the RC and TM latent factors were .683 and .578, respectively. Composite reliability (CR) After removing item ac_3, CR estimates remained above the acceptable .70 threshold and, for the AC latent factor, improved relative to the previous estimate from when item ac_3 was still included. The CR estimates of .835 for AC, .866 for RC, and .845 for TM all indicated acceptable levels of internal consistency reliability. Results write-up for the updated three-factor CFA model. As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees using three multi-item measures designed to measure feelings of acceptance, role clarity, and task mastery. As noted above, we estimated a three-factor confirmatory factor analysis (CFA) model using the maximum likelihood (ML) estimator to evaluate the measurement structure for the three multi-item measures; however, item ac_3 showed a problematic standardized factor loading, standardized error variance, and misalignment between the item’s content and the conceptual definition for the associated measure. As a result, we re-specified and re-estimated the CFA model by removing item ac_3. In doing so, we found the following. The updated model showed acceptable fit to the data according to CFI (.976), TLI (.968), RMSEA (.052), and SRMR (.032). The chi-square test (\\(\\chi^{2}\\) = 113.309, df = 41, p &lt; .001), however, indicated that the model did not fit the data well; that said, the chi-square test is sensitive to sample size. We concluded that in general the model showed acceptable fit to the data. Standardized factor loadings ranged from .708 to .846, which all fell well within the recommended .50-.95 acceptability range. The standardized error variances for items ranged from .284 to .499, and thus all fell below the target threshold of .50, thereby indicating that it was unlikely that an unmodeled construct had an outsized influence on any of those items. The average variance extracted (AVE) for feelings of acceptance, role clarity, and task mastery were .559, .683, and .578, respectively, which all exceeded the .50 cutoff; thus, all three latent factors showed acceptable AVE levels. Finally, the composite reliability (CR) estimates were .835 for feelings of acceptance, .866 for role clarity, and .845 for task mastery, and all indicated acceptable levels of internal consistency reliability. In sum, the updated three-factor measurement model in which the ac_3 item was removed showed acceptable fit to the data, acceptable parameter estimates, acceptable AVE estimates, and acceptable CR estimates. Thus, this specification of the three-factor CFA model will be retained moving forward. 70.2.6 Nested Model Comparisons When evaluating measurement structures, there are variety of circumstances in which we might wish to compare nested models. A nested model has all the same parameter estimates of a full model but has additional parameter constraints in place; a nested model will have more degrees of freedom (df) than the full model, thereby indicating that the nested model is more parsimonious. In this section, we will evaluate whether the updated and final three-factor model we estimated in the previous section, where the problematic ac_3 item was removed, fits significantly better than models with additional constraints. In general, our goal will be to retain the most parsimonious model that fits the data acceptably. 70.2.6.1 Two-Factor Model (Version a) We’ll begin by specifying an alternative model where we load the items associated with the feelings of acceptance and role clarity measures onto a single factor that we’ll label AC_RC. We’ll keep the task mastery items loaded on a latent factor labeled TM. In this way, we’ve created a two-factor model, and we’ll name this model specification object (cfa_mod_2a). # Specify two-factor CFA model &amp; assign to object (version a) cfa_mod_2a &lt;- &quot; AC_RC =~ ac_1 + ac_2 + ac_4 + ac_5 + rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_2a &lt;- cfa(cfa_mod_2a, # name of specified model object data=df) # name of data frame object At this point, you may be wondering: “How is the two-factor model (cfa_mod_2a) nested within the original three factor model (cfa_mod_3)?” That is, it may not look as though we applied any direct constraints to the three-factor model to result in the two-factor model. Perhaps the following alternative approach to specifying the two-factor model will clear up any confusion. Specifically, instead of collapsing the feelings of acceptance and role clarity items onto a single factor labeled AC_RC, we will retain the original three-factor model specification but add constraints to the model. First, we will set the covariance between the AC and RC latent factors to 1, which we can achieve by specifying: AC ~~ 1*RC. If you recall, the ~~ operator is used to specify covariances. Second, we will add the std.lv=TRUE argument to our cfa function to set all of the latent factor variances to 1 (i.e., standardized). Finally, because the AC and RC latent factors will be set to act as a single factor, we need to make sure that we constrain their respective covariances with TM to be equal, which we can achieve by specifying: AC ~~ cov*TM and RC ~~ cov*TM; note that the cov is an arbitrary constraint label that I’m applying, and you could name the constraint whatever you’d like so long as it is the same name across the two covariances and we follow the constraint name with the * operator. # Alternative approach: Specify two-factor CFA model &amp; assign to object (version a) cfa_mod_2a &lt;- &quot; AC =~ ac_1 + ac_2 + ac_3 + ac_4 + ac_5 RC =~ rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 # Constrain AC &amp; RC covariance to 1 AC ~~ 1*RC # Constrain covariances to be equal AC ~~ cov*TM RC ~~ cov*TM &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_2a &lt;- cfa(cfa_mod_2a, # name of specified model object data=df, # name of data frame object std.lv=TRUE) # constrain factor variances to 1 Because specifying the alternative approach to the two-factor model is more time intensive, let’s revert back to the initial specification and then estimate the model. # Specify two-factor CFA model &amp; assign to object (version a) cfa_mod_2a &lt;- &quot; AC_RC =~ ac_1 + ac_2 + ac_4 + ac_5 + rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_2a &lt;- cfa(cfa_mod_2a, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_2a, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 30 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 23 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 994.577 ## Degrees of freedom 43 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.689 ## Tucker-Lewis Index (TLI) 0.602 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11450.330 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 22946.660 ## Bayesian (BIC) 23049.772 ## Sample-size adjusted Bayesian (SABIC) 22976.747 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.184 ## 90 Percent confidence interval - lower 0.174 ## 90 Percent confidence interval - upper 0.194 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.137 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC_RC =~ ## ac_1 1.000 0.928 0.721 ## ac_2 1.059 0.062 17.181 0.000 0.983 0.769 ## ac_4 0.948 0.060 15.916 0.000 0.880 0.701 ## ac_5 1.110 0.065 17.056 0.000 1.030 0.761 ## rc_1 0.417 0.063 6.604 0.000 0.387 0.283 ## rc_2 0.429 0.064 6.742 0.000 0.398 0.289 ## rc_3 0.385 0.060 6.385 0.000 0.357 0.274 ## TM =~ ## tm_1 1.000 1.156 0.755 ## tm_2 0.952 0.054 17.737 0.000 1.101 0.745 ## tm_3 0.969 0.054 18.103 0.000 1.120 0.762 ## tm_4 0.991 0.054 18.411 0.000 1.146 0.778 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC_RC ~~ ## TM 0.280 0.053 5.270 0.000 0.261 0.261 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.796 0.056 14.176 0.000 0.796 0.480 ## .ac_2 0.667 0.052 12.829 0.000 0.667 0.409 ## .ac_4 0.803 0.055 14.614 0.000 0.803 0.509 ## .ac_5 0.769 0.059 13.075 0.000 0.769 0.420 ## .rc_1 1.717 0.097 17.779 0.000 1.717 0.920 ## .rc_2 1.741 0.098 17.765 0.000 1.741 0.916 ## .rc_3 1.577 0.089 17.801 0.000 1.577 0.925 ## .tm_1 1.009 0.074 13.572 0.000 1.009 0.430 ## .tm_2 0.973 0.070 13.850 0.000 0.973 0.445 ## .tm_3 0.906 0.068 13.362 0.000 0.906 0.419 ## .tm_4 0.859 0.067 12.861 0.000 0.859 0.395 ## AC_RC 0.862 0.088 9.834 0.000 1.000 1.000 ## TM 1.337 0.127 10.530 0.000 1.000 1.000 # Visualize the measurement model semPaths(cfa_fit_2a, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Just as we did with the three-factor model in the previous section, we would first evaluate the model fit, and if the model fit appears acceptable, then we would evaluate the parameter estimates. To save space, however, we will skip directly to comparing this version of the two-factor model to our original three-factor model, which I summarize in the table below. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR 3-Factor Model 113.309 41 &lt; .001 .976 .968 .052 .032 2a-Factor Model 994.577 43 &lt; .001 .689 .602 .184 .137 As you can see above, the first version (version a) of the two-factor model fits the data notably worse than the three-factor model, which suggests that the three-factor model is probably a better representation of the measurement structure. As an additional test, we can perform a nested model comparison using the chi-square (\\(\\chi^{2}\\)) difference test, which is also known as the log-likelihood (LL) test. To perform this test, we’ll apply the anova function from base R. As the first argument, we’ll insert the name of our three-factor model object (cfa_fit_3), and as the second argument, we’ll insert the name of our two-factor model object (cfa_fit_2a). # Nested model comparison using chi-square difference test anova(cfa_fit_3, cfa_fit_2a) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## cfa_fit_3 41 22069 22182 113.31 ## cfa_fit_2a 43 22947 23050 994.58 881.27 0.81989 2 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the first version of the two-factor model fits the data statistically significantly worse than the original three-factor model (\\(\\Delta \\chi^{2}\\) = 881.27, \\(\\Delta df\\) = 2, \\(p\\) &lt; .001). This corroborates what we saw with the direct comparison of model fit indices above. Note: If your anova function output defaulted to scientific notation, you can “turn off” scientific notation using the following function. After running the options function below, you can re-run the anova function to get the output in traditional notation. # Turn off scientific notation options(scipen=9999) 70.2.6.2 Two-Factor Model (Version b) We’ll now evaluate a second version of a two-factor model (version b). In this version, we’ll collapse latent factors RC and TM into a single latent factor and load their respective items on the single factor labeled RC_TM. We’ll specify the AC latent factor such that only the corresponding feelings of acceptance items load on it. # Specify two-factor CFA model &amp; assign to object (version b) cfa_mod_2b &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 RC_TM =~ rc_1 + rc_2 + rc_3 + tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_2b &lt;- cfa(cfa_mod_2b, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_2b, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 30 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 23 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 1114.237 ## Degrees of freedom 43 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.650 ## Tucker-Lewis Index (TLI) 0.552 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11510.160 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 23066.321 ## Bayesian (BIC) 23169.432 ## Sample-size adjusted Bayesian (SABIC) 23096.407 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.195 ## 90 Percent confidence interval - lower 0.185 ## 90 Percent confidence interval - upper 0.205 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.173 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.942 0.732 ## ac_2 1.043 0.060 17.383 0.000 0.982 0.769 ## ac_4 0.945 0.058 16.253 0.000 0.890 0.709 ## ac_5 1.114 0.064 17.488 0.000 1.049 0.775 ## RC_TM =~ ## rc_1 1.000 1.146 0.839 ## rc_2 0.978 0.045 21.886 0.000 1.120 0.813 ## rc_3 0.926 0.042 21.887 0.000 1.061 0.813 ## tm_1 0.345 0.055 6.227 0.000 0.395 0.258 ## tm_2 0.280 0.054 5.222 0.000 0.321 0.217 ## tm_3 0.159 0.054 2.966 0.003 0.183 0.124 ## tm_4 0.240 0.054 4.480 0.000 0.275 0.187 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC ~~ ## RC_TM 0.282 0.052 5.384 0.000 0.261 0.261 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.770 0.055 13.876 0.000 0.770 0.465 ## .ac_2 0.669 0.052 12.780 0.000 0.669 0.409 ## .ac_4 0.785 0.054 14.418 0.000 0.785 0.498 ## .ac_5 0.731 0.058 12.553 0.000 0.731 0.399 ## .rc_1 0.554 0.051 10.884 0.000 0.554 0.297 ## .rc_2 0.645 0.053 12.172 0.000 0.645 0.340 ## .rc_3 0.579 0.048 12.170 0.000 0.579 0.339 ## .tm_1 2.190 0.122 17.884 0.000 2.190 0.934 ## .tm_2 2.082 0.116 17.945 0.000 2.082 0.953 ## .tm_3 2.128 0.118 18.039 0.000 2.128 0.985 ## .tm_4 2.097 0.117 17.982 0.000 2.097 0.965 ## AC 0.887 0.089 10.013 0.000 1.000 1.000 ## RC_TM 1.312 0.107 12.310 0.000 1.000 1.000 # Visualize the measurement model semPaths(cfa_fit_2b, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the second version of the two-factor model. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR 3-Factor Model 113.309 41 &lt; .001 .976 .968 .052 .032 2a-Factor Model 994.577 43 &lt; .001 .689 .602 .184 .137 2b-Factor Model 1114.237 43 &lt; .001 .650 .552 .195 .173 As you can see above, the second version (version b) of the two-factor model also fits the data notably worse than the three-factor model, which suggests that the three-factor model is still probably a better representation of the measurement structure. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the three-factor model (cfa_fit_3) to the second version of the two-factor model (cfa_fit_2b). # Nested model comparison anova(cfa_fit_3, cfa_fit_2b) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## cfa_fit_3 41 22069 22182 113.31 ## cfa_fit_2b 43 23066 23169 1114.24 1000.9 0.8739 2 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the second version of the two-factor model fits the data statistically significantly worse than the original three-factor model (\\(\\Delta \\chi^{2}\\) = 1000.90, \\(\\Delta df\\) = 2, \\(p\\) &lt; .001). This corroborates what we saw with the direct comparison of model fit indices above. 70.2.6.3 Two-Factor Model (Version c) We’ll now evaluate a third version of a two-factor model (version c). In this version, we’ll collapse latent factors AC and TM into a single latent factor and load their respective items on the single factor labeled AC_TM. We’ll specify the RC latent factor such that only the corresponding role clarity items load on it. # Specify two-factor CFA model &amp; assign to object (version c) cfa_mod_2c &lt;- &quot; AC_TM =~ ac_1 + ac_2 + ac_4 + ac_5 + tm_1 + tm_2 + tm_3 + tm_4 RC =~ rc_1 + rc_2 + rc_3 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_2c &lt;- cfa(cfa_mod_2c, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_2c, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 32 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 23 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 1059.402 ## Degrees of freedom 43 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.667 ## Tucker-Lewis Index (TLI) 0.575 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11482.743 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 23011.486 ## Bayesian (BIC) 23114.597 ## Sample-size adjusted Bayesian (SABIC) 23041.572 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.190 ## 90 Percent confidence interval - lower 0.180 ## 90 Percent confidence interval - upper 0.200 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.157 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC_TM =~ ## ac_1 1.000 0.940 0.730 ## ac_2 1.035 0.060 17.176 0.000 0.973 0.761 ## ac_4 0.922 0.058 15.797 0.000 0.866 0.690 ## ac_5 1.070 0.063 16.858 0.000 1.006 0.743 ## tm_1 0.548 0.070 7.847 0.000 0.515 0.337 ## tm_2 0.450 0.067 6.684 0.000 0.423 0.286 ## tm_3 0.404 0.067 6.035 0.000 0.380 0.258 ## tm_4 0.475 0.067 7.073 0.000 0.447 0.303 ## RC =~ ## rc_1 1.000 1.157 0.847 ## rc_2 0.970 0.044 22.106 0.000 1.121 0.814 ## rc_3 0.924 0.042 22.204 0.000 1.069 0.819 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC_TM ~~ ## RC 0.287 0.053 5.436 0.000 0.264 0.264 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.774 0.056 13.906 0.000 0.774 0.467 ## .ac_2 0.687 0.053 13.030 0.000 0.687 0.421 ## .ac_4 0.827 0.056 14.784 0.000 0.827 0.524 ## .ac_5 0.820 0.060 13.559 0.000 0.820 0.448 ## .tm_1 2.080 0.118 17.630 0.000 2.080 0.887 ## .tm_2 2.006 0.113 17.767 0.000 2.006 0.918 ## .tm_3 2.017 0.113 17.830 0.000 2.017 0.933 ## .tm_4 1.973 0.111 17.724 0.000 1.973 0.908 ## .rc_1 0.529 0.051 10.420 0.000 0.529 0.283 ## .rc_2 0.642 0.053 12.153 0.000 0.642 0.338 ## .rc_3 0.562 0.047 11.911 0.000 0.562 0.330 ## AC_TM 0.883 0.088 9.981 0.000 1.000 1.000 ## RC 1.338 0.107 12.463 0.000 1.000 1.000 # Visualize the measurement model semPaths(cfa_fit_2c, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the third version of the two-factor model. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR 3-Factor Model 113.309 41 &lt; .001 .976 .968 .052 .032 2a-Factor Model 994.577 43 &lt; .001 .689 .602 .184 .137 2b-Factor Model 1114.237 43 &lt; .001 .650 .552 .195 .173 2c-Factor Model 1059.402 43 &lt; .001 .667 .575 .190 .157 As you can see above, the third version (version c) of the two-factor model also fits the data notably worse than the three-factor model, which suggests that the three-factor model is still probably a better representation of the measurement structure. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the three-factor model (cfa_fit_3) to the third version of the two-factor model (cfa_fit_2c). # Nested model comparison anova(cfa_fit_3, cfa_fit_2c) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## cfa_fit_3 41 22069 22182 113.31 ## cfa_fit_2c 43 23012 23115 1059.40 946.09 0.84958 2 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the third version of the two-factor model fits the data statistically significantly worse than the original three-factor model (\\(\\Delta \\chi^{2}\\) = 946.09, \\(\\Delta df\\) = 2, \\(p\\) &lt; .001). This corroborates what we saw with the direct comparison of model fit indices above. 70.2.6.4 One-Factor Model We’ll now evaluate a one-factor model. In this version, we’ll collapse all three latent factors (AC, RC, and TM) into a single latent factor and load their respective items on the single factor labeled AC_RC_TM. # Specify one-factor CFA model &amp; assign to object cfa_mod_1 &lt;- &quot; AC_RC_TM =~ ac_1 + ac_2 + ac_4 + ac_5 + rc_1 + rc_2 + rc_3 + tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_1 &lt;- cfa(cfa_mod_1, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_1, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 22 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 1921.400 ## Degrees of freedom 44 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.386 ## Tucker-Lewis Index (TLI) 0.232 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11913.742 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 23871.483 ## Bayesian (BIC) 23970.112 ## Sample-size adjusted Bayesian (SABIC) 23900.262 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.255 ## 90 Percent confidence interval - lower 0.246 ## 90 Percent confidence interval - upper 0.265 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.200 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC_RC_TM =~ ## ac_1 1.000 0.911 0.708 ## ac_2 1.053 0.064 16.450 0.000 0.959 0.751 ## ac_4 0.931 0.062 15.080 0.000 0.848 0.675 ## ac_5 1.076 0.067 16.006 0.000 0.981 0.725 ## rc_1 0.466 0.065 7.197 0.000 0.424 0.311 ## rc_2 0.482 0.065 7.384 0.000 0.440 0.319 ## rc_3 0.431 0.062 6.973 0.000 0.393 0.301 ## tm_1 0.612 0.073 8.407 0.000 0.558 0.364 ## tm_2 0.505 0.070 7.211 0.000 0.460 0.311 ## tm_3 0.441 0.070 6.339 0.000 0.402 0.273 ## tm_4 0.524 0.070 7.498 0.000 0.478 0.324 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.827 0.058 14.366 0.000 0.827 0.499 ## .ac_2 0.713 0.054 13.295 0.000 0.713 0.436 ## .ac_4 0.858 0.057 14.992 0.000 0.858 0.544 ## .ac_5 0.870 0.062 13.985 0.000 0.870 0.475 ## .rc_1 1.686 0.095 17.697 0.000 1.686 0.903 ## .rc_2 1.706 0.097 17.674 0.000 1.706 0.898 ## .rc_3 1.550 0.087 17.723 0.000 1.550 0.909 ## .tm_1 2.035 0.116 17.530 0.000 2.035 0.867 ## .tm_2 1.973 0.112 17.695 0.000 1.973 0.903 ## .tm_3 2.000 0.112 17.792 0.000 2.000 0.925 ## .tm_4 1.945 0.110 17.659 0.000 1.945 0.895 ## AC_RC_TM 0.830 0.087 9.571 0.000 1.000 1.000 # Visualize the measurement model semPaths(cfa_fit_1, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the one-factor model. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR 3-Factor Model 113.309 41 &lt; .001 .976 .968 .052 .032 2a-Factor Model 994.577 43 &lt; .001 .689 .602 .184 .137 2b-Factor Model 1114.237 43 &lt; .001 .650 .552 .195 .173 2c-Factor Model 1059.402 43 &lt; .001 .667 .575 .190 .157 1-Factor Model 1921.400 44 &lt; .001 .386 .232 .255 .200 As you can see above, the one-factor model also fits the data notably worse than the three-factor model, which suggests that the three-factor model remains the best representation of the measurement structure out of the models tested. This gives us more confidence that the three-factor model in which we distinguish between the constructs of feelings of acceptance, role clarity, and task mastery is a solid measurement structure, even though it is the most complex model (in terms of the number of freely estimated parameters) that we evaluated and compared. As before, though, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the three-factor model (cfa_fit_3) to the one-factor model (cfa_fit_1). # Nested model comparison anova(cfa_fit_3, cfa_fit_1) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## cfa_fit_3 41 22069 22182 113.31 ## cfa_fit_1 44 23872 23970 1921.40 1808.1 0.95918 3 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the one-factor model fits the data statistically significantly worse than the original three-factor model (\\(\\Delta \\chi^{2}\\) = 1808.10, \\(\\Delta df\\) = 3, \\(p\\) &lt; .001). This corroborates what we saw with the direct comparison of model fit indices above. Results write-up for nested model comparisons. As part of a new-employee onboarding survey administered 1-month after employees’ respective start dates, we assessed new employees using three multi-item measures designed to measure feelings of acceptance, role clarity, and task mastery. Guided by theory, we began by estimating a three-factor confirmatory factor analysis (CFA) model, where each construct had an associated latent factor and where each set of items loaded on its corresponding latent factor. We used the the maximum likelihood (ML) estimator to estimate the model, and missing data were not a concern. The three-factor model showed acceptable fit to the data (\\(\\chi^{2}\\) = 113.309, df = 41, p &lt; .001; CFI = .976; TLI = .968; RMSEA = .052; SRMR = .032). We subsequently compared the three-factor model to more parsimonious two- and one-factor models to determine whether any of the alternative models fit the data the data approximately the same with a simpler measurement structure. For the first two-factor model, we collapsed the feelings of acceptance and role clarity latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the task mastery latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 994.577, df = 43, p &lt; .001; CFI = .689; TLI = .602; RMSEA = .184; SRMR = .137), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 881.27, \\(\\Delta df\\) = 2, p &lt; .001). For the second two-factor model, we collapsed the role clarity and task mastery latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the feelings of acceptance latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1114.237, df = 43, p &lt; .001; CFI = .650; TLI = .552; RMSEA = .195; SRMR = .173), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 1000.90, \\(\\Delta df\\) = 2, p &lt; .001). For the third two-factor model, we collapsed the feelings of acceptance and task mastery latent factors into a single factor and both corresponding measures’ items loaded onto the single latent factor; the role clarity latent factor and its associated items remained a separate latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1059.402, df = 43, p &lt; .001; CFI = .667; TLI = .575; RMSEA = .190; SRMR = .157), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 946.09, \\(\\Delta df\\) = 2, p &lt; .001). For the one-factor model, we collapsed the feelings of acceptance, role clarity, and task mastery latent factors into a single factor and all three corresponding measures’ items loaded onto the single latent factor. The first two-factor model showed unacceptable fit to the data (\\(\\chi^{2}\\) = 1921.400, df = 43, p &lt; .001; CFI = .386; TLI = .232; RMSEA = .255; SRMR = .200), and a chi-square difference test indicated that this two-factor model fit the data significantly worse than the three-factor model (\\(\\Delta \\chi^{2}\\) = 1808.10, \\(\\Delta df\\) = 3, p &lt; .001). In conclusion, we opted to retain the three-factor model because it fit the data significantly better than the alternative models, even though the three-factor model is more complex and thus sacrifices some degree of parsimony. 70.2.6.5 Create a Matrix Comparing Model Fit Indices If our goals are to create a matrix containing only those model fit indices that we covered in this chapter and to add in the chi-square difference tests, we can do the following, which incorporates the inspect function from the lavaan package and the cbind, rbind, and t functions from base R. # Create object containing selected fit indices select_fit_indices &lt;- c(&quot;chisq&quot;,&quot;df&quot;,&quot;pvalue&quot;,&quot;cfi&quot;,&quot;tli&quot;,&quot;rmsea&quot;,&quot;srmr&quot;) # Create matrix comparing model fit indices compare_mods &lt;- cbind( inspect(cfa_fit_3, &quot;fit.indices&quot;)[select_fit_indices], inspect(cfa_fit_2a, &quot;fit.indices&quot;)[select_fit_indices], inspect(cfa_fit_2b, &quot;fit.indices&quot;)[select_fit_indices], inspect(cfa_fit_2c, &quot;fit.indices&quot;)[select_fit_indices], inspect(cfa_fit_1, &quot;fit.indices&quot;)[select_fit_indices] ) # Add more informative model names to matrix columns colnames(compare_mods) &lt;- c(&quot;3 Factor Model&quot;, &quot;2a Factor Model&quot;, &quot;2b Factor Model&quot;, &quot;2c Factor Model&quot;, &quot;1 Factor Model&quot;) # Create vector of chi-square difference tests (nested model comparisons) `chisq diff (p-value)` &lt;- c(NA, anova(cfa_fit_3, cfa_fit_2a)$`Pr(&gt;Chisq)`[2], anova(cfa_fit_3, cfa_fit_2b)$`Pr(&gt;Chisq)`[2], anova(cfa_fit_3, cfa_fit_2c)$`Pr(&gt;Chisq)`[2], anova(cfa_fit_3, cfa_fit_1)$`Pr(&gt;Chisq)`[2]) # Add chi-square difference tests to matrix object compare_mods &lt;- rbind(compare_mods, `chisq diff (p-value)`) # Round object values to 3 places after decimal compare_mods &lt;- round(compare_mods, 3) # Rotate matrix compare_mods &lt;- t(compare_mods) # Print object print(compare_mods) ## chisq df pvalue cfi tli rmsea srmr chisq diff (p-value) ## 3 Factor Model 113.309 41 0 0.976 0.968 0.052 0.032 NA ## 2a Factor Model 994.577 43 0 0.689 0.602 0.184 0.137 0 ## 2b Factor Model 1114.237 43 0 0.650 0.552 0.195 0.173 0 ## 2c Factor Model 1059.402 43 0 0.667 0.575 0.190 0.157 0 ## 1 Factor Model 1921.400 44 0 0.386 0.232 0.255 0.200 0 70.2.7 Estimate Second-Order Model In some instances, we have theoretical justification to specify and estimate a second-order model. A second-order is as CFA model in which latent factors serves as indicators for one or more superordinate latent factors. Let’s suppose that we have theoretical justification to specify a second-order model, where the feelings of acceptance (AC), role clarity (RC), and task mastery (TM) latent factors serve as indicators for a higher-order adjustment latent factor (ADJ). That is, conceptually, an individual’s level of adjustment is indicated by their feelings of acceptance, role clarity, and task mastery. Such a model might prove advantageous if a later goal is to estimate structural regression paths with other criteria, such that just the associations with the second-order adjustment latent factor (ADJ) are of interest. Specifying a second-order factor is relatively straightforward. To our updated three-factor model, where the ac_3 item was removed, we will specify a second-order factor called ADJ on which the first-order AC, RC, and TM latent factors load: ADJ =~ AC + RC + TM. # Specify second-order CFA model &amp; assign to object cfa_mod_2ord &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 RC =~ rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 ADJ =~ AC + RC + TM &quot; # Estimate second-order CFA model &amp; assign to fitted model object cfa_fit_2ord &lt;- cfa(cfa_mod_2ord, # name of specified model object data=df) # name of data frame object # Print summary of model results summary(cfa_fit_2ord, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 40 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 25 ## ## Number of observations 654 ## ## Model Test User Model: ## ## Test statistic 113.309 ## Degrees of freedom 41 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 3111.443 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.976 ## Tucker-Lewis Index (TLI) 0.968 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11009.696 ## Loglikelihood unrestricted model (H1) -10953.042 ## ## Akaike (AIC) 22069.392 ## Bayesian (BIC) 22181.470 ## Sample-size adjusted Bayesian (SABIC) 22102.095 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.052 ## 90 Percent confidence interval - lower 0.041 ## 90 Percent confidence interval - upper 0.063 ## P-value H_0: RMSEA &lt;= 0.050 0.372 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.032 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.946 0.735 ## ac_2 1.041 0.059 17.496 0.000 0.984 0.770 ## ac_4 0.940 0.058 16.301 0.000 0.889 0.708 ## ac_5 1.105 0.063 17.530 0.000 1.045 0.772 ## RC =~ ## rc_1 1.000 1.156 0.846 ## rc_2 0.970 0.044 22.118 0.000 1.122 0.814 ## rc_3 0.925 0.042 22.213 0.000 1.069 0.819 ## TM =~ ## tm_1 1.000 1.159 0.757 ## tm_2 0.951 0.053 17.801 0.000 1.103 0.746 ## tm_3 0.964 0.053 18.103 0.000 1.117 0.760 ## tm_4 0.988 0.054 18.445 0.000 1.145 0.777 ## ADJ =~ ## AC 1.000 0.573 0.573 ## RC 0.901 0.259 3.472 0.001 0.422 0.422 ## TM 0.912 0.264 3.461 0.001 0.426 0.426 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.763 0.055 13.832 0.000 0.763 0.460 ## .ac_2 0.665 0.052 12.770 0.000 0.665 0.407 ## .ac_4 0.788 0.054 14.466 0.000 0.788 0.499 ## .ac_5 0.740 0.058 12.699 0.000 0.740 0.404 ## .rc_1 0.530 0.051 10.446 0.000 0.530 0.284 ## .rc_2 0.641 0.053 12.152 0.000 0.641 0.338 ## .rc_3 0.562 0.047 11.916 0.000 0.562 0.330 ## .tm_1 1.002 0.074 13.528 0.000 1.002 0.427 ## .tm_2 0.969 0.070 13.830 0.000 0.969 0.444 ## .tm_3 0.913 0.068 13.433 0.000 0.913 0.422 ## .tm_4 0.861 0.067 12.892 0.000 0.861 0.396 ## .AC 0.601 0.108 5.572 0.000 0.672 0.672 ## .RC 1.099 0.115 9.553 0.000 0.822 0.822 ## .TM 1.100 0.129 8.532 0.000 0.819 0.819 ## ADJ 0.293 0.100 2.942 0.003 1.000 1.000 # Estimate average variance extracted (AVE) AVE(cfa_fit_2ord) ## AC RC TM ## 0.559 0.683 0.578 # Estimate composite/construct reliability (CR) compRelSEM(cfa_fit_2ord) ## AC RC TM ## 0.835 0.866 0.845 # Visualize the measurement model semPaths(cfa_fit_2ord, # name of fitted model object what=&quot;std&quot;, # display standardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names The second-order model fits the data the same as our original first-order three-factor model. A notable difference in the parameter estimates is that instead of covariances between the first-order latent factors (AC, RC, TM), we now see the three latent factors loading onto the new second-order factor (ADJ). We would precede with evaluating and interpreting the model as we did earlier in the chapter with multi-factor models. 70.2.8 Estimating Models with Missing Data When missing data are present, we must carefully consider how we handle the missing data before or during the estimations of a model. In the chapter on Missing Data, I provide an overview of relevant concepts, particularly if the data are missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR); I suggest reviewing that chapter prior to handling missing data. As a potential method for addressing missing data, the lavaan model functions, such as the cfa function, allow for full-information maximum likelihood (FIML). Further, the functions allow us to specify specific estimators given the type of data we wish to use for model estimation (e.g., ML, MLR). To demonstrate how missing data are handled using FIML, we will need to first introduce some missing data into our data. To do so, we will use a multiple imputation package called mice and a function called ampute that “amputates” existing data by creating missing data patterns. For our purposes, we’ll replace 10% (.1) of data frame cells with NA (which is signifies a missing value) such that the missing data are missing completely at random (MCAR). # Install package install.packages(&quot;mice&quot;) # Access package library(mice) # Create a new data frame object df_missing &lt;- df # Remove non-numeric variable(s) from data frame object df_missing$EmployeeID &lt;- NULL # Set a seed set.seed(2024) # Remove 10% of cells so missing data are MCAR df_missing &lt;- ampute(df_missing, prop=.1, mech=&quot;MCAR&quot;) # Extract the new missing data frame object and overwrite existing object df_missing &lt;- df_missing$amp Implementing FIML when missing data are present is relatively straightforward. For example, for the updated one-factor CFA model from a previous section, where we removed the ac_3 item, we can apply FIML in the presence of missing data by adding the missing=\"fiml\" argument to the cfa function. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 &quot; # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df_missing, # name of data frame object missing=&quot;fiml&quot;) # specify FIML # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 654 ## Number of missing patterns 5 ## ## Model Test User Model: ## ## Test statistic 1.520 ## Degrees of freedom 2 ## P-value (Chi-square) 0.468 ## ## Model Test Baseline Model: ## ## Test statistic 960.360 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3878.824 ## Loglikelihood unrestricted model (H1) -3878.064 ## ## Akaike (AIC) 7781.648 ## Bayesian (BIC) 7835.445 ## Sample-size adjusted Bayesian (SABIC) 7797.345 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.071 ## P-value H_0: RMSEA &lt;= 0.050 0.828 ## P-value H_0: RMSEA &gt;= 0.080 0.027 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.072 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.825 ## P-value H_0: Robust RMSEA &gt;= 0.080 0.028 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.006 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.949 0.737 ## ac_2 1.041 0.059 17.608 0.000 0.987 0.772 ## ac_4 0.941 0.058 16.144 0.000 0.893 0.710 ## ac_5 1.101 0.063 17.454 0.000 1.044 0.772 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 3.900 0.050 77.247 0.000 3.900 3.027 ## .ac_2 3.914 0.050 78.212 0.000 3.914 3.062 ## .ac_4 3.909 0.049 79.345 0.000 3.909 3.107 ## .ac_5 4.269 0.053 80.476 0.000 4.269 3.155 ## AC 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.759 0.056 13.670 0.000 0.759 0.457 ## .ac_2 0.659 0.053 12.529 0.000 0.659 0.403 ## .ac_4 0.785 0.055 14.313 0.000 0.785 0.496 ## .ac_5 0.740 0.059 12.520 0.000 0.740 0.404 ## AC 0.900 0.089 10.071 0.000 1.000 1.000 The FIML approach uses all observations in which data are missing on one or more endogenous variables in the model. As you can see in the output, all 654 observations were retained for estimating the model. Now watch what happens when we remove the missing=\"fiml\" argument in the presence of missing data. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 &quot; # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df_missing) # name of data frame object # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 21 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Used Total ## Number of observations 637 654 ## ## Model Test User Model: ## ## Test statistic 1.246 ## Degrees of freedom 2 ## P-value (Chi-square) 0.536 ## ## Model Test Baseline Model: ## ## Test statistic 945.071 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3805.421 ## Loglikelihood unrestricted model (H1) -3804.798 ## ## Akaike (AIC) 7626.841 ## Bayesian (BIC) 7662.496 ## Sample-size adjusted Bayesian (SABIC) 7637.096 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value H_0: RMSEA &lt;= 0.050 0.857 ## P-value H_0: RMSEA &gt;= 0.080 0.021 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.006 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.945 0.734 ## ac_2 1.042 0.061 17.221 0.000 0.985 0.770 ## ac_4 0.953 0.059 16.167 0.000 0.901 0.713 ## ac_5 1.114 0.064 17.282 0.000 1.053 0.774 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.766 0.056 13.627 0.000 0.766 0.462 ## .ac_2 0.664 0.053 12.530 0.000 0.664 0.406 ## .ac_4 0.783 0.055 14.115 0.000 0.783 0.491 ## .ac_5 0.740 0.060 12.395 0.000 0.740 0.400 ## AC 0.893 0.090 9.915 0.000 1.000 1.000 As you can see in the output, the cfa function defaults to listwise deletion when we do not specify that FIML be applied. This results in the number of observations dropping from 654 to 637 for model estimation purposes. Within the cfa function, we can also specify a specific estimator if we choose to override the default. For example, we could specify the MLR (maximum likelihood with robust standard errors) estimator if we had good reason to. To do so, we would add this argument: estimator=\"MLR\". For a list of other available estimators, you can check out the lavaan package website. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_3 + ac_4 &quot; # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df_missing, # name of data frame object estimator=&quot;MLR&quot;, # specify type of estimator missing=&quot;fiml&quot;) # specify FIML # Print summary of model results summary(cfa_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 30 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 654 ## Number of missing patterns 5 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 1.383 1.543 ## Degrees of freedom 2 2 ## P-value (Chi-square) 0.501 0.462 ## Scaling correction factor 0.896 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 566.170 587.012 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 0.964 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.003 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -4093.056 -4093.056 ## Scaling correction factor 0.961 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -4092.365 -4092.365 ## Scaling correction factor 0.952 ## for the MLR correction ## ## Akaike (AIC) 8210.113 8210.113 ## Bayesian (BIC) 8263.910 8263.910 ## Sample-size adjusted Bayesian (SABIC) 8225.810 8225.810 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.070 0.075 ## P-value H_0: RMSEA &lt;= 0.050 0.845 0.803 ## P-value H_0: RMSEA &gt;= 0.080 0.023 0.037 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.068 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.846 ## P-value H_0: Robust RMSEA &gt;= 0.080 0.020 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.008 0.008 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## AC =~ ## ac_1 1.000 0.941 0.730 ## ac_2 1.079 0.073 14.762 0.000 1.015 0.794 ## ac_3 0.261 0.067 3.874 0.000 0.246 0.177 ## ac_4 0.928 0.063 14.799 0.000 0.873 0.694 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 3.900 0.050 77.278 0.000 3.900 3.028 ## .ac_2 3.914 0.050 78.171 0.000 3.914 3.061 ## .ac_3 3.795 0.055 69.452 0.000 3.795 2.726 ## .ac_4 3.908 0.049 79.276 0.000 3.908 3.104 ## AC 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .ac_1 0.775 0.063 12.380 0.000 0.775 0.467 ## .ac_2 0.604 0.068 8.926 0.000 0.604 0.370 ## .ac_3 1.877 0.096 19.644 0.000 1.877 0.969 ## .ac_4 0.822 0.061 13.373 0.000 0.822 0.519 ## AC 0.885 0.095 9.314 0.000 1.000 1.000 70.2.9 Simulate Dynamic Fit Index Cutoffs Dynamic fit index cutoffs represent a more recent advance in evaluating model fit. For years, fixed cutoffs for common fit indices have been criticized because the appropriateness of a particular cutoff depends on a number of data- and model-specific factors. For years, the field has referenced cutoffs recommended by influential studies on model fit like Hu and Bentler (1999); however, such recommended cutoffs were based on a single CFA model and thus may not generalize as well as we’d like them to. To address the limitations of general model fit cutoffs, McNeish and Wolf (2023) developed dynamic fit index cutoffs, which are based on a simulation methodology. Further, Wolf and McNeish (2023) developed a package called dynamic to estimate dynamic fit index cutoffs for specific data sets and specific models. To explore dynamic fit index cutoffs, we need to install and access the dynamic package (if you haven’t already). # Install package install.packages(&quot;dynamic&quot;) # Access package library(dynamic) As an initial step, we need to specify and estimate a CFA model. Let’s start with a one-factor model. Specifically, we’ll use the the same model we specified for the updated over-identified one-factor model earlier in this chapter in which we dropped the ac_3 item. # Specify one-factor CFA model &amp; assign to object cfa_mod &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 &quot; # Estimate one-factor CFA model &amp; assign to fitted model object cfa_fit &lt;- cfa(cfa_mod, # name of specified model object data=df) # name of data frame object As the sole parenthetical argument in the cfaOne function from the dynamic package (which is intended for one-factor models), we will enter the name of our fitted model object (cfa_fit). Please note that because this methodology involves Monte Carlo simulations, it will take a minute (or more) to produce the desired output. # Set seed for reproducible simulation results set.seed(2024) # Compute one-factor model dynamic fit index cutoffs cfaOne(cfa_fit) ## Your DFI cutoffs: ## SRMR RMSEA CFI ## Level 1: 95/5 .016 .078 .993 ## Level 1: 90/10 -- -- -- ## ## Empirical fit indices: ## Chi-Square df p-value SRMR RMSEA CFI ## 1.297 2 0.523 0.006 0 1 The output produces the prescribed dynamic fit index cutoffs under the section labeled Your DFI cutoffs. In this output, only a single “level” of cutoffs are produced, specifically Level 1. Had there been additional levels (i.e., Level 2, Level3), they would have corresponded to progressively more relaxed cutoffs indicating worse fitting models; Level 1 corresponds to cutoffs, that if met, correspond to a better fitting model than, say, Level 2 or Level 3. With our model, the dynamic fit index cutoffs for SRMR, RMSEA, and CFI are .016, .078, and .993, respectively, which means that we would love for our actual model fit indices to be less than the first two cutoffs and greater than the last. In the output section labeled Empirical fit indices, we find our actual model fit indices. As you can see, all three of our model fit indices meet the dynamic fit index cutoffs associated with a good-fitting model. Thus, based on these cutoffs, we can conclude that our model fits the data acceptably, which is the same conclusion we arrived at with the traditional cutoffs we applied earlier in the tutorial. Let’s switch gears and simulate dynamic fit index cutoffs for a multi-factor model. Specifically, we will use our updated three-factor model from a previous section, where the ac_3 item was dropped, as an example. # Specify three-factor CFA model &amp; assign to object cfa_mod_3 &lt;- &quot; AC =~ ac_1 + ac_2 + ac_4 + ac_5 RC =~ rc_1 + rc_2 + rc_3 TM =~ tm_1 + tm_2 + tm_3 + tm_4 &quot; # Estimate three-factor CFA model &amp; assign to fitted model object cfa_fit_3 &lt;- cfa(cfa_mod_3, # name of specified model object data=df) # name of data frame object Because we’re dealing with a multi-factor model, we need to switch over to the cfaHB function from the dynamic package. Again, please note that because this involves a Monte Carlo simulation, it will take a minute or two to generate the output. # Set seed for reproducible simulation results set.seed(2024) # Compute three-factor model dynamic fit indices cfaHB(cfa_fit_3) ## Your DFI cutoffs: ## SRMR RMSEA CFI Magnitude ## Level 1: 95/5 .082 .105 .92 .528 ## Level 1: 90/10 -- -- -- ## Level 2: 95/5 .124 .156 .857 .518 ## Level 2: 90/10 -- -- -- ## ## Empirical fit indices: ## Chi-Square df p-value SRMR RMSEA CFI ## 113.309 41 0 0.032 0.052 0.976 In this example, the cfaHB function generates two levels (Level 1, Level 2) of dynamic fit index cutoffs, where Level 1 corresponds to cutoffs corresponding to better fitting models. Again, note that the actual (empirical) model fit indices for our three-factor model meet the Level 1 cutoffs for SRMR, RMSEA, and CFI, which suggests that our three-factor model fits the data well. Because dynamic fit index cutoffs are relatively new, we do not yet know whether they will gain broader traction. Still, their use makes conceptual sense, and they may be ushering in a notable shift in how we evaluate model fit. 70.2.10 Summary In this chapter, we learned how to estimate measurement models using confirmatory factor analysis (CFA). More specifically, we learned how to estimate and interpret one-factor models, multi-factor models, and second-order models, and how to compare the fit of nested models. Further, we learned how to estimate models when missing data are present and how to estimate dynamic fit index cutoffs based on Monte Carlo simulations. References "],["lgm.html", "Chapter 71 Estimating Change Using Latent Growth Modeling 71.1 Conceptual Overview 71.2 Tutorial", " Chapter 71 Estimating Change Using Latent Growth Modeling In this chapter, we will learn how to estimate change using latent growth modeling (LGM), which is part of the structural equation modeling (SEM) family of analyses. Specifically, we will learn how to estimate change trajectories to understand new employees’ adjustment into an organization. 71.1 Conceptual Overview Latent growth modeling (LGM) (or latent growth model (LGM)) is a latent variable modeling approach and is part of the structural equation modeling (SEM) family of analyses (Meredith and Tisak 1990). LGM also goes by other names such as latent curve analysis, latent growth curve modeling, or latent curve modeling. LGM is a useful statistical tool for estimating the extent to which employees’ levels of a particular construct change over time. Specifically, LGM helps us understand the functional form of change (e.g., linear, quadratic, cubic) and variation in change between employees. Although not the focus of this chapter, measurement models can be integrated into LGM through the implementation of confirmatory factor analysis (CFA); CFA was introduced in a previous chapter. In LGM, the intercept (or initial value) and slope (or functional form of change) are represented as latent variables (i.e., latent factors), which by nature are not directly measured. Instead, observed (manifest) variables serve as indicators of the latent factors. Further, the intercept and slopes latent factors provide information about the average intercept and slope for the population, as well as the between-person variation for the intercept and slope. 71.1.1 Path Diagrams It is often helpful to visualize an LGM using a path diagram. A path diagram displays the model parameter specifications and can also include parameter estimates. Conventional path diagram symbols are shown in Figure 1. Figure 1: Conventional path diagram symbols and their meanings. For an example of how the path diagram symbols can be used to construct a visual depiction of an LGM, please reference Figure 2. The path diagram depicts an unconditional unconstrained LGM with composite role clarity (RC) variables as observed measures (i.e., indicators), which means that the following parameters are freely estimated: means and variances for the intercept and slope latent factors, covariance between the intercept and slope latent factors, and variances for the observed variables. The model is considered unconditional because it is not conditioned on another variable, such as a time-invariant predictor variable. Further, role clarity (RC) measured at 1-month, 3-months, 6-months, and 9-months post-start-date serve as observed variables (i.e., RC 1m, RC 3m, RC 6m, RC 9m, respectively). That is, the RC observed measures serve as indicators of the intercept and slope latent factors, such that the indicators are reflective of the latent factors. Putting it all together, the unconditional unconstrained LGM can offer a glimpse into (a) whether employees’ levels of role clarity change on average in a particular direction (i.e., increase, decrease, no change) and (b) whether employees’ vary in terms of how their role clarity changes (if at all). Figure 2: Example of an unconditional unconstrained LGM path diagram. By convention, the latent factors for the role clarity intercept and slope are represented by an oval or circle. Please note that the latent factor is not directly measured; rather, we infer information about the latent factor from its four indicators, which in this example correspond to the four measurement occasions for role clarity. The intercept and slope latent factors have variance terms associated with them, which represent the latent factors’ variabilities; these variances terms, which are represented by double-headed arrows, reflect the extent to which the intercept and slope estimates vary between individuals. The intercept and slope latent factors also have mean (i.e., intercept, constant) terms associated with them, which are represented by triangles; these means reflect the average intercept and slope for the population. The association between the intercept and slope latent factors is depicted using the double-headed arrow, which is referred to as a covariance – or if standardized, a correlation. Each of the four observed variables (indicators) is represented with a rectangle. The one-directional, single-sided arrows extending from the latent factors to the observed variables represent the factor loadings. Each indicator has a (residual) error variance term, which represents the amount of variance left unexplained by the latent factor in relation to each indicator. 71.1.2 Model Identification Model identification has to do with the number of (free or freely estimated) parameters specified in the model relative to the number of unique (non-redundant) sources of information available, and model implication has important implications for assessing model fit and estimating parameter estimates. Just-identified: In a just-identified model (i.e., saturated model), the number of freely estimated parameters (e.g., factor loadings, covariances, variances) is equal to the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is equal to zero. In just-identified models, the model parameter standard errors can be estimated, but the model fit cannot be assessed in a meaningful way using traditional model fit indices. Over-identified: In an over-identified model, the number of freely estimated parameters is less than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is greater than zero. In over-identified models, traditional model fit indices and parameter standard errors can be estimated. Under-identified: In an under-identified model, the number of freely estimated parameters is greater than the number of unique (non-redundant) sources of information, which means that the degrees of freedom (df) is less than zero. In under-identified models, the model parameter standard errors and model fit cannot be estimated. Some might say under-identified models are overparameterized because they have more parameters to be estimated than unique sources of information. Most (if not all) statistical software packages that allow structural equation modeling (and by extension, latent growth modeling) automatically compute the degrees of freedom for a model or, if the model is under-identified, provide an error message. As such, we don’t need to count the number of sources of unique (non-redundant) sources of information and free parameters by hand. With that said, to understand model identification and its various forms at a deeper level, it is often helpful to practice calculating the degrees freedom by hand when first learning. The formula for calculating the number of unique (non-redundant) sources of information available for a particular model is as follows, which differs from the formula for confirmatory factor analysis and other structural equation models because it incorporates repeated measurement occasions over time: \\(i = \\frac{p(p+1)}{2} + t\\) where \\(p\\) is the number of observed variables to be modeled, and \\(t\\) is the number of measurement occasions (i.e., time points). In unconditional unconstrained path diagram specified above, there are four observed variables: RC 1m, RC 3m, RC 6m, and RC 9m. Accordingly, in the following formula, \\(p\\) is equal to 4. There are four measurement occasions (i.e., 1 month, 3 months, 6 months, 9 months), so \\(t\\) is equal to 4. Thus, the number of unique (non-redundant) sources of information is 14. \\(i = \\frac{4(4+1)}{2} + t = \\frac{20}{2} + 4 = 14\\) To count the number of free parameters (\\(k\\)), simply add up the number of the specified unconstrained factor loadings, variances, means, intercepts, covariances, and (residual) error variance terms in the unconditional unconstrained LGM. Please note that for latent factor scaling and time coding purposes, we typically (a) constrain all of the factor loadings associated with the intercept latent factor to 1.0 and (b) constrain the factor loadings associated with the slope latent factor to values that represent the time intervals (e.g., 0, 2, 5, 8), where zero represents the value of role clarity where the slope crosses the intercept. Had there been equal intervals for the measurement occasions for role clarity, we could have scaled the slope latent factor with factor loadings equal to 0, 1, 2, and 3; however, because the interval between RC 1m (1 month) and RC 3m (3 months) is not equal to the intervals between the other adjacent measurement occasions, we need to adjust the factor loadings by, in this instance, subtracting 1 from each measurement occasion month, so that RC 1m has a factor loading of 0, RC 3m has a factor loading of 2, RC 6m has a factor loading of 5, and RC 9m has a factor loading of 8. As shown in Figure 3 below, the example unconditional unconstrained LGM has 9 free parameters. \\(k = 9\\) To calculate the degrees of freedom (df) for the model, we need to subtract the number of free parameters from the number unique (non-redundant) sources of information, which in this example equates to 14 minus 9. Thus, the degrees of freedom for the model is 5, which means the model is over-identified. \\(df = i - k = 14 - 9 = 5\\) Figure 3: Counting the number of free parameters in the CFA model path diagram. 71.1.3 Model Fit When a model is over-identified (df &gt; 0), the extent to which the specified model fits the data can be assessed using a variety of model fit indices, such as the chi-square (\\(\\chi^{2}\\)) test, comparative fit index (CFI), Tucker-Lewis index (TLI), root mean square error of approximation (RMSEA), and standardized root mean square residual (SRMR). As noted by Newsom, in the context of an LGM, these model fit indices reflect the amount of error around estimated slopes, and a poorly fitted model does not necessarily imply the absence of change over time or the accuracy of the functional form of change. For a commonly cited reference on cutoffs for fit indices, please refer to Hu and Bentler (1999), and for a concise description of common guidelines regarding interpreting model fit indices, including differences between stringent and relaxed interpretations of common fit indices, I recommend checking out Nye (2023). With that being said, those papers focus on confirmatory factor analyses as opposed to LGM. Regardless of which cutoffs we apply when interpreting fit indices, we must remember that such cutoffs are merely guidelines, and it’s possible to estimate an acceptable model that meets some but not all of the cutoffs given the limitations of some fit indices. Chi-square test. The chi-square (\\(\\chi^{2}\\)) test can be used to assess whether the model fits the data adequately, where a statistically significant \\(\\chi^{2}\\) value (e.g., p \\(&lt;\\) .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p \\(\\ge\\) .05) indicates that the model fits the data reasonably well (Bagozzi and Yi 1988). The null hypothesis for the \\(\\chi^{2}\\) test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. Of note, the \\(\\chi^{2}\\) test is sensitive to sample size and non-normal variable distributions. Comparative fit index (CFI). As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that CFI compares the focal model to a baseline model, which is commonly referred to as the null or independence model. CFI is generally less sensitive to sample size than the chi-square test. A CFI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. Tucker-Lewis index (TLI). Like CFI, Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes; however, as Hu and Bentler (1999) noted, TLI may be not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). A TLI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. Root mean square error of approximation (RMSEA). The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus ends up effectively rewarding more parsimonious models. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified); further, Hu and Bentler (1999) noted that RMSEA may not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). In general, an RMSEA value that is less than or equal to .06 indicates good model fit to the data, although some relax that cutoff to .08 or even .10. Standardized root mean square residual (SRMR). Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .06 generally indicates good fit to the data, although some relax that cutoff to .08. Summary of model fit indices. The conventional cutoffs for the aforementioned model fit indices – like any rule of thumb – should be applied with caution and with good judgment and intention. Further, these indices don’t always agree with one another, which means that we often look across multiple fit indices and come up with our best judgment of whether the model adequately fits the data. A poorly fitting model may be due to model misspecification, an inappropriate model estimator, a large amount of error around slope estimates, or other factors that need to be addressed. With that being said, we should also be careful to not toss out a model entirely if one or more of the model fit indices suggest less than acceptable levels of fit to the data. The table below contains the conventional stringent and more relaxed cutoffs for the model fit indices. Fit Index Stringent Cutoffs for Acceptable Fit Relaxed Cutoffs for Acceptable Fit \\(\\chi^{2}\\) \\(p \\ge .05\\) \\(p \\ge .01\\) CFI \\(\\ge .95\\) \\(\\ge .90\\) TLI \\(\\ge .95\\) \\(\\ge .90\\) RMSEA \\(\\le .06\\) \\(\\le .08\\) SRMR \\(\\le .06\\) \\(\\le .08\\) 71.1.4 Parameter Estimates In LGM, there are various types of parameter estimates, which correspond to the path diagram symbols covered earlier (e.g., covariance, variance, mean, factor loading). When a model is just-identified or over-identified, we can estimate the standard errors for freely estimated parameters, which allows us to evaluate statistical significance. With most software applications, we can request standardized parameter estimates, which can facilitate the interpretation of some parameters. Factor loadings. In LGM, it is common to apply constraints to all factor loadings extending from the intercept and slope latent factors to the observed variables; with that said, we can freely estimate some factor loadings associated with the slope factor, but a discussion of that topic is beyond the scope of this introductory chapter. For our purposes, we will constrain all factor loadings associated with the intercept latent factor to 1, whereas the factor loadings associated with the slope latent factor will reflect the intervals between measurement occasions (i.e., time coding). If we were to have four measurement occasions with equally spaced time intervals between adjacent occasions, then we could apply the following factor loadings to the slope factor: 0, 1, 2, and 3. Note, however, that we have a great deal of freedom when it comes to constraining the slope factor loadings, and the factor loading that we constrain to zero represents the value of our construct (e.g., role clarity) where the slope crosses the intercept. Alternatively, we could constrain our slope factor loadings as -3, -2, -1, and 0 if our goal were to estimate the intercept as the last measurement occasion. With respect to the LGM path diagram that has been our focus thus far (see Figure 2), we constrained our slope factor loadings to 0, 2, 5, and 8 because the intervals is not equal between adjacent measurement occasions. Namely, the interval between the 1-month and 3-months role clarity measurement occasions is 2 months (3 - 1 = 2), whereas the intervals between all other adjacent measurement occasions is 3 months (6 - 3 = 3 and 9 - 6 = 3). To account for the unequal intervals, I simply subtracted one from each measurement occasion month to arrive at the 0, 2, 5, and 8 factor loadings. Variances. The variances of the latent factors represent the amount of between-person variability associated with intercept and slope estimates. A significant and large variance associated with the intercept latent factor suggests that individuals’ intercept estimates vary considerably from one another, whereas a significant or large variance associated with the slope latent factor suggests that individuals’ slope estimates vary considerably from one another. Means. The means of the latent factors represent the average intercept and slope estimates. For example, with respect to the slope factor, a significant and positive mean indicates that, on average, individuals’ slopes were positive (e.g., increase in the measured construct over time), a nonsignificant mean indicates that, on average, individuals’ slopes were approximately zero (e.g., no change in the measured construct over time), and a significant and negative mean indicates that, on average, individuals’ slopes were negative (e.g., decrease in the measured construct over time). Covariances. The covariance between the latent factors help us understand the extent to which intercept estimates are associated with slope estimates. For example, a negative covariance between the intercept and slope latent factors indicates that individuals who have higher intercept estimates tend to less positive slope estimates, whereas a positive covariance indicates that individuals who have higher intercept estimates tend to have more positive slope estimates. When standardized, a covariance can be interpreted as a correlation. (Residual) error variance terms. The (residual) error variance terms, which are also known as disturbance terms or uniquenesses, indicate how much variance is left unexplained by the latent factors in relation to the observed variables (indicators). When standardized, error variance terms represent the proportion (percentage) of variance that remains unexplained by the latent factors. 71.1.5 Model Comparisons When evaluating an LGM, we may wish to evaluate whether a focal LGM performs better (or worse) than an alternative LGM. Comparing models can help us arrive at a more parsimonious model that still fits the data well, as well as help us understand the extent to which construct demonstrates measurement invariance (or measurement equivalence) over time. As an example, imagine we are focusing on our unconditional unconstrained LGM for role clarity (see Figure 2). Now imagine that we specify an alternative model in which we constrain the (residual) error variances to be equal, which could be used as a test for homogeneity of error variances. We can compare those two models to determine whether the alternative model fits the data about the same as our focal model or worse. When two models are nested, we can perform nested model comparisons. As a reminder, a nested model has all the same parameter estimates of a full model but has additional parameter constraints in place. If two models are nested, we can compare them using model fit indices like CFI, TLI, RMSEA, and SRMR. We can also use the chi-square difference (\\(\\Delta \\chi^{2}\\)) test (likelihood ratio test) to compare nested models, which provides a statistical test for nested-model comparisons. When two models are not nested, we can use other model fit indices like Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). With respect to these indices, the best fitting model will have lower AIC and BIC values. 71.1.6 Statistical Assumptions The statistical assumptions that should be met prior to estimating and/or interpreting an LGM will depend on the type of estimation method. Common estimation methods for an LGM include (but are not limited to) maximum likelihood (ML), maximum likelihood with robust standard errors (MLM or MLR), weighted least squares (WLS), and diagonally weighted least squares (DWLS). WLS and DWLS estimation methods are used when there are observed variables with nominal or ordinal (categorical) measurement scales. In this chapter, we will focus on ML estimation, which is a common method when observed variables have interval or ratio (continuous) measurement scales. As Kline (2011) notes, ML estimation carries with it the following assumptions: “The statistical assumptions of ML estimation include independence of the scores, multivariate normality of the endogenous variables, and independence of the exogenous variables and error terms” (p. 159). When multivariate non-normality is a concern, the MLM or MLR estimator is a better choice than ML estimator, where the MLR estimator allows for missing data and the MLM estimator does not. 71.1.6.1 Sample Write-Up As part of a new-employee onboarding longitudinal study, surveys were administered 1 month, 3 months, 6 months, and 9 months after employees’ respective start dates. Each survey included a 15-item measure of role clarity, and a composite variable based on the employees’ average responses was created at each survey measurement occasion. Using an unconditional unconstrained latent growth model (LGM), we investigated whether new employees’ levels of role clarity showed linear change over time and whether role-clarity change varied between new employees. When specifying the model, we constrained the intercept factor loadings to 1, and constrained the slope factor loadings to 0, 2, 5, and 8 to account for the unequal time intervals between adjacent measurement occasions. We estimated the model using the maximum likelihood (ML) estimator and a sample size of 650 new employees. Missing data were not a concern. We evaluated the model’s fit to the data using the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices. The \\(\\chi^{2}\\) test indicated that the model fit the data as well as a perfectly fitting model (\\(\\chi^{2}\\) = 3.760 df = 5, p = .584). Further, the CFI and TLI estimates were 1.000 and 1.001, respectively, which exceeded the more stringent threshold of .95, thereby indicating that model showed acceptable fit to the data. Similarly, the RMSEA estimate was .00, which was below the more stringent threshold of .06, thereby indicating that model showed acceptable fit to the data. The SRMR estimate was .019, which was below both the stringent threshold of .06, thereby indicating unacceptable model fit to the data. Collectively, the model fit information indicated that model fit the data acceptably. Regarding the unstandardized parameter estimates, the intercept latent factor mean of 35.143 (p &lt; .001) indicated that, on average, new employees’ level of role clarity was 35.143 (out of a possible 100) 1 month after their respective start dates. The slope latent factor mean of 4.975 (p &lt; .001) indicated that, on average, new employees’ level of role clarity increased by 4.975 each month. The variances associated with the intercept and slope latent factors, however, indicated that there was a statistically significant amount of between-employee intercept and slope variation (\\(\\sigma_{intercept}\\) = 155.324, p &lt; .001; \\(\\sigma_{slope}\\) = .744, p &lt; .001). The statistically significant and negative covariance between the intercept and slope latent factors indicated that employees with higher levels of role clarity 1 month after their respective start dates tended to have less positive role clarity change trajectories (\\(\\psi\\) = -3.483, p = .002). Finally, the standardized (residual) error terms associated with the four observed role clarity measurement occasions ranged from .307 to .368, indicating that a proportionally small amount of variance was left unexplained by the intercept and slope latent factors. In sum, the estimated unconditional constrained LGM showed that new employees’ role clarity, on average, increased in a linear manner between 1 month and 9 months after employees’ respective start dates. Further, new employees varied with respect to their level of role clarity 1 month after their start dates, and their role clarity change trajectories varied in terms of slope. A conditional model with a time-invariant predictor or time-variant predictors may help explain between-employee differences in intercepts and slopes. 71.2 Tutorial This chapter’s tutorial demonstrates how to estimate change using latent growth modeling (LGM) in R. 71.2.1 Video Tutorial The video tutorial for this chapter is planned but has not yet been recorded. 71.2.2 Functions &amp; Packages Introduced Function Package pivot_longer tidyr mutate dplyr case_match dplyr ggplot ggplot2 aes ggplot2 geom_line ggplot2 labs ggplot2 scale_x_continuous ggplot2 theme_classic ggplot2 slice dplyr growth lavaan summary base R semPaths semPlot anova base R options base R inspect lavaan cbind base R rbind base R t base R ampute mice 71.2.3 Initial Steps If you haven’t already, save the file called “lgm.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “lgm.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;lgm.csv&quot;) ## Rows: 650 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): EmployeeID ## dbl (6): RC_1m, RC_3m, RC_6m, RC_9m, ProPers_d1, JobPerf_12m ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;EmployeeID&quot; &quot;RC_1m&quot; &quot;RC_3m&quot; &quot;RC_6m&quot; &quot;RC_9m&quot; &quot;ProPers_d1&quot; &quot;JobPerf_12m&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 650 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 7 ## EmployeeID RC_1m RC_3m RC_6m RC_9m ProPers_d1 JobPerf_12m ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE1001 13.0 18.1 31.2 42.4 50.6 47.8 ## 2 EE1002 39.5 58.6 68.4 71.8 62.8 55.0 ## 3 EE1003 41.3 45.6 66.4 79.3 43.8 61.7 ## 4 EE1004 38.5 65.2 47.8 64.4 47.8 33.3 ## 5 EE1005 34.2 37.7 57.1 72.7 64.8 46.9 ## 6 EE1006 42.3 54.9 74.3 96.0 100 71.0 The data frame includes data from identical new employee onboarding surveys administered 1 month, 3 months, 6 months, and 9 months after employees’ respective start dates. The sample includes 650 employees. The data frame includes a unique identifier variable called EmployeeID, such that each employee has their own unique ID. For each survey, new employees responded to a 15-item role clarity measure using a 100-point (1 = strongly disagree and 100 = strongly agree) response format, where higher scores indicate higher role clarity. A composite variable based on employees’ average scores across the 15 items has already been created for each measurement occasion. The data frame also includes employees responses to a 10-item proactive personality measure, which was administered on their respective start dates. The proactive personality measure was also assessed using a 100-point (1 = strongly disagree and 100 = strongly agree) response format, where higher scores indicate higher proactive personality. A composite variable has already been created based on new employees’ average responses to the 10 items. Employees’ direct supervisors rated their job performance along eight dimensions at 12 months after their respective start dates using a 100-point (1 = does not meet expectations and 100 = exceeds expectations) response format. A composite variable has already been created based on new supervisors’ ratings on the eight performance dimensions. The three data sources have been joined/merged for you. 71.2.4 Visualizing Change Prior to estimating a latent growth model (LGM), we will visualize the role clarity trajectories of our sample of 650 new employees. To do so, we need to begin by creating a second data frame in which the data are restructured from wide-to-long format. For more information on manipulating data from wide-to-long format, please see the chapter on that topic. To restructure the data and to ultimately create our data visualization, we will use the functions from the tidyr, dplyr, and ggplot2 packages, so let’s begin by installing and accessing those packages (if you haven’t already). # Install packages install.packages(&quot;tidyr&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;ggplot2&quot;) # Access package library(tidyr) library(dplyr) library(ggplot2) We will begin by restructuring the data from wide-to-long format by doing the following. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object df_long. Use the &lt;- operator to assign the new long-form data frame object to the object named df_long in the step above. Type the name of the original data frame object (df), followed by the pipe (%&gt;%) operator. Type the name of the pivot_longer function from the tidyr package. As the first argument in the pivot_longer function, type cols= followed by the c (combine) function. As the arguments within the c function, list the names of the variables that you wish to pivot from separate variables (wide) to levels or categories of a new variable, effectively stacking them vertically. In this example, let’s list the names of role clarity variables from the four measurement occasions: RC_1m, RC_3m, RC_6m, and RC_9m. As the second argument in the pivot_longer function, type names_to= followed by what you would like to name the new stacked variable (see previous) created from the four survey measure variables. Let’s call the new variable containing the names of the role clarity variables from the four measurement occasions as follows: “Time”. As the third argument in the pivot_longer function, type values_to= followed by what you would like to name the new variable that contains the scores for the four role clarity variables that are now stacked vertically for each case. Let’s call the new variable containing the scores from the four variables the following: “RoleClarity”. # Apply pivot_longer function to restructure data to long format (using pipe) df_long &lt;- df %&gt;% pivot_longer(cols=c(RC_1m, RC_3m, RC_6m, RC_9m), names_to=&quot;Time&quot;, values_to=&quot;RoleClarity&quot;) # Print first 8 rows of new data frame head(df_long, n=8) ## # A tibble: 8 × 5 ## EmployeeID ProPers_d1 JobPerf_12m Time RoleClarity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EE1001 50.6 47.8 RC_1m 13.0 ## 2 EE1001 50.6 47.8 RC_3m 18.1 ## 3 EE1001 50.6 47.8 RC_6m 31.2 ## 4 EE1001 50.6 47.8 RC_9m 42.4 ## 5 EE1002 62.8 55.0 RC_1m 39.5 ## 6 EE1002 62.8 55.0 RC_3m 58.6 ## 7 EE1002 62.8 55.0 RC_6m 68.4 ## 8 EE1002 62.8 55.0 RC_9m 71.8 Using the long-format df_long data frame object as input, we will create a line chart, where each line represents an individual employee’s change in role clarity from 1 months to 9 months. We will create the line chart by doing the following operations. Type the name of the long-format data frame object (df_long), followed by the pipe (%&gt;%) operator. To recode the Time variable from character to numeric, type the name of the mutate function from the dplyr package. As the sole parenthetical argument, begin by typing the name of the existing Time variable followed by = so that we can overwrite the existing variable. Type the name of the case_match function from the dplyr package. As the first argument, type the name of the Time variable we wish to recode. As the second argument, type the name of the first character level for the Time variable in quotation marks followed by the ~ operator and the numeral 1, which will signify one month (\"RC_1m\" ~ 1). As the third argument, type the name of the second character level for the Time variable in quotation marks followed by the ~ operator and the numeral 3, which will signify one month (\"RC_3m\" ~ 3). As the fourth argument, type the name of the third character level for the Time variable in quotation marks followed by the ~ operator and the numeral 6, which will signify one month (\"RC_6m\" ~ 6). As the fifth argument, type the name of the fourth character level for the Time variable in quotation marks followed by the ~ operator and the numeral 9, which will signify one month (\"RC_9m\" ~ 9). After the last parenthesis ()) of the case_match function, insert the pipe (%&gt;%) operator. To declare the aesthetics for the plot, type the name of the ggplot function from the ggplot2 package. As the sole argument in the ggplot function, type the name of the aes function. As the first argument within the aes function, type x= followed by the name of the x-axis variable, which is Time in this example. As the second argument, type y= followed by the name of the y-axis variable, which is RoleClarity in this example. As the third and final argument, type group= followed by the name of the grouping variable, which is the unique identifier variable for employees called EmployeeID (given that each employee now has four rows of data in the long-format data frame object). After the ending parenthesis ()) of the ggplot function, type the + operator. To request a line chart, type the name of the geom_line function. We will leave the geom_line function parentheses empty. After the ending parenthesis ()) of the geom_line function, type the + operator. To add/change axis labels, type the name of the labs function. As the first argument in the labs function, type x= followed by what we would like to label the x-axis. Let’s label the x-axis “Time”. As the second argument in the labs function, type y= followed by what we would like to label the y-axis. Let’s label the y-axis “Role Clarity”. After the ending parenthesis ()) of the labs function, type the + operator. To change the scaling of the x-axis to reflect the unequal interval for 1 month (1) and 3 months (3) as compared to the adjacent intervals for 3 months (3) and 6 months (6) and for 6 months (6) and 9 months (9), type the name of the scale_x_continuous function. As the first argument within the scale_x_continuous function, type breaks= followed by where the x-axis labels should appear along the axis, which for this example will correspond to months 1, 3, 6, and 9. We’ll use the c function to create a vector of those breaks: breaks=c(1, 3, 6, 9). As the second argument, type labels= followed by the c function. Within the c function parentheses, create a vector of four descriptive names for the measurement occasions. In this example, I chose: “1 month”, “3 months”, “6 months”, and “9 months”: labels=c(\"1 month\", \"3 months\", \"6 months\", \"9 months\"). After the ending parenthesis ()) of the scale_x_discrete function, type the + operator. Type the name of the theme_classic function, and leave the function parentheses empty. This will apply a simple theme to the line chart, which includes removing background gridlines. # Create a line chart representing employees&#39; change trajectories df_long %&gt;% # Recode the Time character levels to numeric values mutate(Time = case_match(Time, &quot;RC_1m&quot; ~ 1, &quot;RC_3m&quot; ~ 3, &quot;RC_6m&quot; ~ 6, &quot;RC_9m&quot; ~ 9)) %&gt;% # Declare aesthetics for plot ggplot(aes(x=Time, y=RoleClarity, group=EmployeeID)) + # Create line chart geom_line() + # Create x- and y-axis labels labs(x=&quot;Time&quot;, y=&quot;Role Clarity&quot;) + # Specify the breaks and labels for the x-axis scale_x_continuous(breaks=c(1, 3, 6, 9), labels=c(&quot;1 month&quot;, &quot;3 months&quot;, &quot;6 months&quot;, &quot;9 months&quot;)) + # Request classic, minimal look without gridlines theme_classic() In the line chart, take note of the general upward linear trend across the 650 employees; visually, it seems clear that there is likely a linear functional form and that, on average, employees’ levels of role clarity tended to increase from 1 month to 9 months. To view a reduced set of employees, which may give us clearer depiction of between-employee variation, we will use the slice function from the dplyr package. We will adapt the previous data visualization code by inserting the slice function after piping in the df_long data frame object. Because each employee has four rows of data in the long-format df_long data frame object, retaining the first 400 rows will represent 100 employees’ role clarity data. As the sole parenthetical argument in the slice function, let’s reference the first 400 rows of data by typing 1:400. # Create a line chart representing a *subset* of employees&#39; change trajectories df_long %&gt;% # Recode the Time character levels to numeric values mutate(Time = case_match(Time, &quot;RC_1m&quot; ~ 1, &quot;RC_3m&quot; ~ 3, &quot;RC_6m&quot; ~ 6, &quot;RC_9m&quot; ~ 9)) %&gt;% # Retain just the first 400 rows of data slice(1:400) %&gt;% # Declare aesthetics for plot ggplot(aes(x=Time, y=RoleClarity, group=EmployeeID)) + # Create line chart geom_line() + # Create x- and y-axis labels labs(x=&quot;Time&quot;, y=&quot;Role Clarity&quot;) + # Specify the breaks and labels for the x-axis scale_x_continuous(breaks=c(1, 3, 6, 9), labels=c(&quot;1 month&quot;, &quot;3 months&quot;, &quot;6 months&quot;, &quot;9 months&quot;)) + # Request classic, minimal look without gridlines theme_classic() The “thinned out” line chart better captures the between-employee variation in role clarity trajectories but still suggests a positive linear function form. 71.2.5 Estimate Unconditional Unconstrained Latent Growth Model We will begin by estimating what is referred to as an unconditional unconstrained latent growth model (LGM). An unconditional model is not conditional on an exogenous variable or variables (e.g., predictor variables, covariates). In other words, an unconditional model just includes the intercept and slope latent factors along with their respective indicators (i.e., observed repeated-measures variables). An unconstrained model lacks any constraints placed on intercept and slope means and variances and on (residual) error variances. With so many freely estimated parameters, unconstrained models tend to be the best fitting models. Finally, please note that we will be working with our data frame object that is in the original wide format: df. Because LGM is a specific application of structural equation modeling (SEM), we will use functions from an R package developed for SEM called lavaan (latent variable analysis) to estimate our CFA models. Let’s begin by installing and accessing the lavaan package (if you haven’t already). # Install package install.packages(&quot;lavaan&quot;) # Access package library(lavaan) First, we must specify the LGM and assign it to an object that we can subsequently reference. To do so, we will do the following. Specify a name for the model object (e.g., lgm_mod), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator and within quotation marks (\" \"): Specify a name for the intercept latent factor (e.g., i_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. Anything that comes to the right of the =~ operator is an observed variable (i.e., indicator) of the latent factor. Please note that the latent factor is not something that we directly observe, so it will not have a corresponding variable in our data frame object. After the =~ operator, specify each observed variable associated with the intercept latent factor, and to separate the observed variables, insert the + operator. In this example, the four observed variables are: RC_1m, RC_3m, RC_6m, and RC_9m. These are our observed variables, which conceptually are influenced by the underlying latent factor. To set the i_RC latent factor as the intercept, we also need to constrain each observed variable’s factor loading to 1 by preceding each observed variable by 1*. As a result, the end result should look like this: i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m. Specify a name for the slope latent factor (e.g., s_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. After the =~ operator, specify each observed variable associated with the slope latent factor, which will be the same observed variables that serve as indicators for the intercept latent factor. To separate the observed variables, insert the + operator. To set the s_RC latent factor as the slope, we also need to constrain each observed variable’s factor loading to account for the time intervals between measurement occasions, which is also known as time coding. Because role clarity was measured at 1 month, 3 months, 6 months, and 9 months, we will subtract 1 from each month, which will result in the 1-month measurement occasion becoming 0 and thus setting a measurement occasion to serve as the intercept value; in other words, conceptually, the measurement occasion coded as 0 represents the value of role clarity when its slope crosses time equal to 0. Subtracting 1 from each measurement occasion in this example also allows for us to account for the unequal time intervals associated with the measurement occasions. As a result, the end result should look like this: s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m. Note: We could have specified a different measurement occasion slope factor loading as 0 by simply changing the time coding; for example, if we were to set the last measurement occasion slope factor loading as 0, our time coding would be: s_RC =~ -8*RC_1m + -6*RC_3m + -3*RC_6m + 0*RC_9m. Finally, if the time intervals between measurement occasions had been equal, we would have been able to apply a 0, 1, 2, and 3 time coding for the slope factor loadings to set the intercept at the first measurement occasion, or -3, -2, -1, and 0 to set the intercept at the last measurement occasion. # Specify unconditional unconstrained LGM &amp; assign to object lgm_mod &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m &quot; Second, now that we have specified the model object (lgm_mod), we are ready to estimate the model using the growth function from the lavaan package. To do so, we will do the following. Specify a name for the fitted model object (e.g., lgm_fit), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the growth function, and within the function parentheses include the following arguments. As the first argument, insert the name of the model object that we specified above (lgm_mod). As the second argument, insert the name of the data frame object to which the indicator variables in our model belong. That is, after data=, insert the name of the original wide-format data frame object (df). Note: The growth function includes model estimation defaults, which explains why we had relatively few model specifications. # Estimate unconditional unconstrained LGM &amp; assign to fitted model object lgm_fit &lt;- growth(lgm_mod, # name of specified model object data=df) # name of wide-format data frame object Third, we will use the summary function from base R to to print the model results. To do so, we will apply the following arguments in the summary function parentheses. As the first argument, specify the name of the fitted model object that we created above (lgm_fit). As the second argument, set fit.measures=TRUE to obtain the model fit indices (e.g., CFI, TLI, RMSEA, SRMR). As the third argument, set standardized=TRUE to request the standardized parameter estimates for the model. # Print summary of model results summary(lgm_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 96 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 3.760 ## Degrees of freedom 5 ## P-value (Chi-square) 0.584 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10100.582 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20219.165 ## Bayesian (BIC) 20259.458 ## Sample-size adjusted Bayesian (SABIC) 20230.883 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.047 ## P-value H_0: RMSEA &lt;= 0.050 0.963 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.019 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.463 0.820 ## RC_3m 1.000 12.463 0.834 ## RC_6m 1.000 12.463 0.840 ## RC_9m 1.000 12.463 0.855 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.725 0.116 ## RC_6m 5.000 4.314 0.291 ## RC_9m 8.000 6.902 0.474 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.483 1.147 -3.036 0.002 -0.324 -0.324 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.143 0.559 62.824 0.000 2.820 2.820 ## s_RC 4.975 0.064 77.667 0.000 5.767 5.767 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 75.459 7.345 10.273 0.000 75.459 0.327 ## .RC_3m 78.779 5.811 13.557 0.000 78.779 0.353 ## .RC_6m 81.083 5.791 14.001 0.000 81.083 0.368 ## .RC_9m 65.214 7.998 8.154 0.000 65.214 0.307 ## i_RC 155.324 11.703 13.273 0.000 1.000 1.000 ## s_RC 0.744 0.211 3.520 0.000 1.000 1.000 Evaluating model fit. Now that we have the summary of our model results, we will begin by evaluating key pieces of the model fit information provided in the output. Estimator. The function defaulted to using the maximum likelihood (ML) model estimator. When there are deviations from multivariate normality or categorical variables, the function may switch to another estimator; alternatively, we can manually switch to another estimator using the estimator= argument in the growth function. Number of parameters. Nine parameters were estimated, which, as we will see later, correspond to the covariance between latent factors, latent factor means, latent factor variances, and (residual) error term variances of the observed variables (i.e., indicators). Number of observations. Our effective sample size is 650. Had there been missing data on the observed variables, this portion of the output would have indicated how many of the observations were retained for the analysis given the missing data. How missing data are handled during estimation will depend on the type of missing data approach we apply, which is covered in more default in the section called Estimating Models with Missing Data. By default, the growth function applies listwise deletion in the presence of missing data. Chi-square test. The chi-square (\\(\\chi^{2}\\)) test assesses whether the model fits the data adequately, where a statistically significant \\(\\chi^{2}\\) value (e.g., p \\(&lt;\\) .05) indicates that the model does not fit the data well and a nonsignificant chi-square value (e.g., p \\(\\ge\\) .05) indicates that the model fits the data reasonably well. The null hypothesis for the \\(\\chi^{2}\\) test is that the model fits the data perfectly, and thus failing to reject the null model provides some confidence that the model fits the data reasonably close to perfectly. Of note, the \\(\\chi^{2}\\) test is sensitive to sample size and non-normal variable distributions. For this model, we find the \\(\\chi^{2}\\) test in the output section labeled Model Test User Model. Because the p-value is equal to or greater than .05, we fail to reject the null hypothesis that the mode fits the data perfectly and thus conclude that the model fits the data acceptably (\\(\\chi^{2}\\) = 3.760, df = 5, p = .584). Finally, note that because the model’s degrees of freedom (i.e., 5) is greater than zero, we can conclude that the model is over-identified. Comparative fit index (CFI). As the name implies, the comparative fit index (CFI) is a type of comparative (or incremental) fit index, which means that CFI compares our estimated model to a baseline model, which is commonly referred to as the null or independence model. CFI is generally less sensitive to sample size than the chi-square (\\(\\chi^{2}\\)) test. A CFI value greater than or equal to .95 generally indicates good model fit to the data, although some might relax that cutoff to .90. For this model, CFI is equal to 1.000, which indicates that the model fits the data acceptably. Tucker-Lewis index (TLI). Like CFI, Tucker-Lewis index (TLI) is another type of comparative (or incremental) fit index. TLI is generally less sensitive to sample size than the chi-square test and tends to work well with smaller sample sizes; however, TLI may be not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). A TLI value greater than or equal to .95 generally indicates good model fit to the data, although like CFI, some might relax that cutoff to .90. For this model, TLI is equal to 1.001, which indicates that the model fits the data acceptably. Loglikelihood and Information Criteria. The section labeled Loglikelihood and Information Criteria contains model fit indices that are not directly interpretable on their own (e.g., loglikelihood, AIC, BIC). Rather, they become more relevant when we wish to compare the fit of two or more non-nested models. Given that, we will will ignore this section in this tutorial. Root mean square error of approximation (RMSEA). The root mean square error of approximation (RMSEA) is an absolute fit index that penalizes model complexity (e.g., models with a larger number of estimated parameters) and thus effectively rewards models that are more parsimonious. RMSEA values tend to upwardly biased when the model degrees of freedom are fewer (i.e., when the model is closer to being just-identified); further, RMSEA may not be the best choice for smaller sample sizes (e.g., N \\(&lt;\\) 250). In general, an RMSEA value that is less than or equal to .06 indicates good model fit to the data, although some relax that cutoff to .08 or even .10. For this model, RMSEA is .000, which indicates that the model fits the data acceptably. Standardized root mean square residual. Like the RMSEA, the standardized root mean square residual (SRMR) is an example of an absolute fit index. An SRMR value that is less than or equal to .06 generally indicates good fit to the data, although some relax that cutoff to .08. For this model, SRMR is equal to .019, which indicates that the model fits the data acceptably. In sum, the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices all indicate that our model fit the data acceptably based on conventional rules of thumb and thresholds. This level of agreement, however, is not always going to occur. For instance, it is relatively common for the \\(\\chi^{2}\\) test to indicate a lack of acceptable fit while one or more of the relative or absolute fit indices indicates that fit is acceptable given the limitations of the \\(\\chi^{2}\\) test. Further, there may be instances where only two or three out of five of these model fit indices indicate acceptable model fit. In such instances, we should not necessarily toss out the model entirely, but we should consider whether there are model misspecifications. Of course, if all five model indices are well beyond the conventional thresholds (in a bad way), then our model may have some major misspecification issues, and we should proceed thoughtfully when interpreting the parameter estimates. With all that said, with an LGM, the lack of model fit indicates that there is a large amount of residual error around estimated slopes, and importantly, a lack of model fit does not necessarily indicate that there is a lack of linear (or nonlinear) change. For our model, all five model fit indices signal that the model fit the data acceptably, and thus we should feel confident proceeding forward with interpreting and evaluating the parameter estimates. Evaluating parameter estimates. As noted above, our model showed acceptable fit to the data, so we can feel comfortable interpreting the parameter estimates. By default, the growth function provides unstandardized parameter estimates, but if you recall, we also requested standardized parameter estimates. In the output, the unstandardized parameter estimates fall under the column titled Estimates, whereas the standardized factor loadings we’re interested in fall under the column titled Std.all. Factor loadings. The output section labeled Latent Variables contains our factor loadings for the intercept and slope latent factors. With respect to the unstandardized factor loadings for the intercept latent factor (i_RC; see Estimates column), the constraints we applied are reported. Specifically, we constrained each of the intercept factor loadings to 1. With respect to the unstandardized factor loadings for the slope latent factor (s_RC; see Estimates column), the constraints we applied are reported. Specifically, we constrained slope factor loadings to 0, 2, 5, and 8 to correspond with measurement occasions 1 month, 3 months, 6 months, and 9 months. Covariances. The output section labeled Covariances contains the covariance between the intercept and slope latent factors. The unstandardized estimate is -3.483 (p = .002), and the standardized estimate, which can be interpreted as a correlation, is -.324. Means &amp; Intercepts. The output section labeled Intercepts contains the means for the latent factors and the intercepts for the observed variables. By default, the intercepts for the observed variables are constrained to zero, so those are not of substantive interest. The means for the intercept and slope latent factors are, however, of substantive interest. The unstandardized intercept latent factor mean of 35.143 (p &lt; .001) indicates that, on average, new employees’ level of role clarity was 35.143 (out of a possible 100) 1 month after their respective start dates. The unstandardized slope latent factor mean of 4.975 (p &lt; .001) indicates that, on average, new employees’ level of role clarity increased by 4.975 each month. Variances. The output section labeled Variances contains the (residual) error variances for each observed variable (i.e., indicator) of the latent factors and the variances of the latent factors. The unstandardized variance associated with the intercept latent factor indicates that there is a statistically significant amount of between-employee intercept variation (155.324, p &lt; .001), and the unstandardized variance associated with the slope latent factor indicates that there is a statistically significant amount of between-employee slope variation (.744, p &lt; .001). The standardized (residual) error variances associated with the four observed role clarity measurement occasions ranged from .307 to .368, indicating that a proportionally small amount of variance was left unexplained by the intercept and slope latent factors and that the error variances are similar in magnitude, potentially suggesting homogeneity of residual error variances. Visualize the path diagram. To visualize our unconditional unconstrained LGM as a path diagram, we can use the semPaths function from the semPlot package. If you haven’t already, please install and access the semPlot package. # Install package install.packages(&quot;semPlot&quot;) # Access package library(semPlot) While there are many arguments that can be used to refine the path diagram visualization, we will focus on just four to illustrate how the semPaths function works. As the first argument, insert the name of the fitted LGM object (lgm_fit). As the second argument, specify what=\"est\" to display just the unstandardized parameter estimates. As the third argument, specify weighted=FALSE to request that the visualization not weight the edges (e.g., lines) and other plot features. As the fourth argument, specify nCharNodes=0 in order to use the full names of latent and observed indicator variables instead of abbreviating them. # Visualize the unconditional unconstrained LGM semPaths(lgm_fit, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names The resulting LGM path diagram can be useful for interpreting the model specifications and the parameter estimates. Results write-up for the unconditional unconstrained LGM. As part of a new-employee onboarding longitudinal study, surveys were administered 1 month, 3 months, 6 months, and 9 months after employees’ respective start dates. Each survey included a 15-item measure of role clarity, and a composite variable based on the employees’ average responses was created at each survey measurement occasion. Using an unconditional unconstrained latent growth model (LGM), we investigated whether new employees’ levels of role clarity showed linear change over time and whether role-clarity change varied between new employees. When specifying the model, we constrained the intercept factor loadings to 1, and constrained the slope factor loadings to 0, 2, 5, and 8 to account for the unequal time intervals between adjacent measurement occasions. We estimated the model using the maximum likelihood (ML) estimator and a sample size of 650 new employees. Missing data were not a concern. We evaluated the model’s fit to the data using the chi-square (\\(\\chi^{2}\\)) test, CFI, TLI, RMSEA, and SRMR model fit indices. The \\(\\chi^{2}\\) test indicated that the model fit the data as well as a perfectly fitting model (\\(\\chi^{2}\\) = 3.760 df = 5, p = .584). Further, the CFI and TLI estimates were 1.000 and 1.001, respectively, which exceeded the more stringent threshold of .95, thereby indicating that model showed acceptable fit to the data. Similarly, the RMSEA estimate was .00, which was below the more stringent threshold of .06, thereby indicating that model showed acceptable fit to the data. The SRMR estimate was .019, which was below both the stringent threshold of .06, thereby indicating unacceptable model fit to the data. Collectively, the model fit information indicated that model fit the data acceptably. Regarding the unstandardized parameter estimates, the intercept latent factor mean of 35.143 (p &lt; .001) indicated that, on average, new employees’ level of role clarity was 35.143 (out of a possible 100) 1 month after their respective start dates. The slope latent factor mean of 4.975 (p &lt; .001) indicated that, on average, new employees’ level of role clarity increased by 4.975 each month. The variances associated with the intercept and slope latent factors, however, indicated that there was a statistically significant amount of between-employee intercept and slope variation (\\(\\sigma_{intercept}\\) = 155.324, p &lt; .001; \\(\\sigma_{slope}\\) = .744, p &lt; .001). The statistically significant and negative covariance between the intercept and slope latent factors indicated that employees with higher levels of role clarity 1 month after their respective start dates tended to have less positive role clarity change trajectories (\\(\\psi\\) = -3.483, p = .002). Finally, the standardized (residual) error terms associated with the four observed role clarity measurement occasions ranged from .307 to .368, indicating that a proportionally small amount of variance was left unexplained by the intercept and slope latent factors. In sum, the estimated unconditional constrained LGM showed that new employees’ role clarity, on average, increased in a linear manner between 1 month and 9 months after employees’ respective start dates. Further, new employees varied with respect to their level of role clarity 1 month after their start dates, and their role clarity change trajectories varied in terms of slope. A conditional model with a time-invariant predictor or time-variant predictors may help explain between-employee differences in intercepts and slopes. 71.2.6 Nested Model Comparisons When possible, we typically prefer to arrive at a model that not only fits the data but also is parsimonious in nature (i.e., less complex). With respect to LGM, there are variety of circumstances in which we might wish to compare nested models to arrive at a well-fitting yet parsimonious model. A nested model has all the same parameter estimates of a full model but has additional parameter constraints in place; a nested model will have more degrees of freedom (df) than the full model, thereby indicating that the nested model is less complex. In this section, we will build up to the unconditional unconstrained LGM from the previous section by gradually relaxing constraints. In general, our goal will be to retain the most parsimonious model that fits the data acceptably. 71.2.6.1 Estimate Baseline Model We will begin with evaluating a very simple model that we’ll refer to as the baseline model. For this model, we will freely estimate the mean of the intercept latent factor, constrain the (residual) error variances to be zero, and constrain all remaining parameters from our unconditional unconstrained model (from the previous section) to be zero. In other words, we will evaluate an intercept-only model in which intercepts aren’t allowed to vary between employees and in which the residual error variances constrained (i.e., homogeneity of variances). We will directly specify all necessary model parameters (even those that would be estimated by default), as in subsequent models we will be relaxing some of the constraints we put in place in this baseline model. Specify a name for the model object (e.g., lgm_mod1), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator and within quotation marks (\" \"): Specify a name for the intercept latent factor (e.g., i_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. After the =~ operator, specify each observed variable associated with the intercept latent factor, and to separate the observed variables, insert the + operator. In this example, the four observed variables are: RC_1m, RC_3m, RC_6m, and RC_9m. These are our observed variables, which conceptually are influenced by the underlying latent factor. To set the i_RC latent factor as the intercept, we also need to constrain each observed variable’s factor loading to 1 by preceding each observed variable by 1*. As a result, the end result should look like this: i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m. Specify a name for the slope latent factor (e.g., s_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. After the =~ operator, specify each observed variable associated with the slope latent factor, which will be the same observed variables that serve as indicators for the intercept latent factor. Because we will not be freely estimating the slope in this baseline model, we will constrain all s_RC latent factor loadings to zero, which will look like this: s_RC =~ 0*RC_1m + 0*RC_3m + 0*RC_6m + 0*RC_9m. Freely estimate the mean of the intercept factor by specifying the name of the latent factor (i_RC) followed by the ~ operator and 1. Specifying ~ 1 after a latent factor freely estimates the mean (or intercept) of that factor. Because we are setting our slope to be zero in this model, we need to constrain the mean of the slope factor to zero by specifying the name of the latent factor (s_RC) followed by the ~ operator and 0*1. Specifying s_RC ~ 0*1 constrains the mean (or intercept) of the latent factor to zero. Constrain the intercept latent factor variance to zero, which means we won’t allow intercepts to vary between employees. To do so, specify the variance of the latent factor (e.g, i_RC ~~ i_RC), except insert 0* to the right of the ~~ (variance) operator to constrain the variance to zero. The end result should look like this: i_RC ~~ 0*i_RC. Constrain the slope latent factor variance to zero as well. To do so, specify the following: s_RC ~~ 0*s_RC. Constrain the covariance between the intercept and slope latent factors to zero by specifying the following: i_RC ~~ 0*s_RC. Constrain the (residual) error variances to be equal by first specifying the variance of each observed variable (e.g., RC_1m ~~ RC_1m); however, we will apply equality constraints by inserting the same letter or word immediately after the ~~ (covariance) operator. I’ve chosen to use the letter e as the equality constraint. For example, applying the equality constraint to the first observed variable will look like this: RC_1m ~~ e*RC_1m. Specify a name for the fitted model object (e.g., lgm_fit1), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the growth function, and within the function parentheses include the following arguments. As the first argument, insert the name of the model object that we specified above (lgm_mod1). As the second argument, insert the name of the data frame object to which the indicator variables in our model belong. That is, after data=, insert the name of the original wide-format data frame object (df). Note: The growth function includes model estimation defaults, which explains why we had relatively few model specifications. Specify the name of the summary function from base R. As the first argument, specify the name of the fitted model object that we created above (lgm_fit1). As the second argument, set fit.measures=TRUE to obtain the model fit indices (e.g., CFI, TLI, RMSEA, SRMR). As the third argument, set standardized=TRUE to request the standardized parameter estimates for the model. Specify the name of the semPaths function. As the first argument, insert the name of the fitted LGM object (lgm_fit1). As the second argument, specify what=\"est\" to display just the unstandardized parameter estimates. As the third argument, specify weighted=FALSE to request that the visualization not weight the edges (e.g., lines) and other plot features. As the fourth argument, specify nCharNodes=0 in order to use the full names of latent and observed indicator variables instead of abbreviating them. # Specify baseline model &amp; assign to object lgm_mod1 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings to zero (no growth) s_RC =~ 0*RC_1m + 0*RC_3m + 0*RC_6m + 0*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 # Constrain mean of slope latent factor to zero (average slope set to zero) s_RC ~ 0*1 # Constrain intercept latent factor variance to zero (don&#39;t allow to vary) i_RC ~~ 0*i_RC # Constrain slope latent factor variance to zero (don&#39;t allow to vary) s_RC ~~ 0*s_RC # Constrain covariance between intercept &amp; slope latent factors to zero i_RC ~~ 0*s_RC # Constrain (residual) error variances to be equal (homogeneity of variances) RC_1m ~~ e*RC_1m RC_3m ~~ e*RC_3m RC_6m ~~ e*RC_6m RC_9m ~~ e*RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit1 &lt;- growth(lgm_mod1, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit1, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## Number of equality constraints 3 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 3062.245 ## Degrees of freedom 12 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.000 ## Tucker-Lewis Index (TLI) -0.250 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11629.825 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 23263.649 ## Bayesian (BIC) 23272.603 ## Sample-size adjusted Bayesian (SABIC) 23266.253 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.625 ## 90 Percent confidence interval - lower 0.607 ## 90 Percent confidence interval - upper 0.644 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.877 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 0.000 0.000 ## RC_3m 1.000 0.000 0.000 ## RC_6m 1.000 0.000 0.000 ## RC_9m 1.000 0.000 0.000 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 0.000 0.000 0.000 ## RC_6m 0.000 0.000 0.000 ## RC_9m 0.000 0.000 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC 0.000 NaN NaN ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 53.805 0.416 129.403 0.000 Inf Inf ## s_RC 0.000 NaN NaN ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 0.000 NaN NaN ## s_RC 0.000 NaN NaN ## .RC_1m (e) 449.503 12.467 36.056 0.000 449.503 1.000 ## .RC_3m (e) 449.503 12.467 36.056 0.000 449.503 1.000 ## .RC_6m (e) 449.503 12.467 36.056 0.000 449.503 1.000 ## .RC_9m (e) 449.503 12.467 36.056 0.000 449.503 1.000 # Visualize the LGM as a path diagram semPaths(lgm_fit1, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Like we did with the unconditional unconstrained model in the previous section, we typically begin by evaluating the model fit, and if the model fit appears acceptable, then we go on to evaluate the parameter estimates. To save space, however, we will just summarize the baseline model fit indices in the table below. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 As you can see above, the baseline model fits the data very poorly, which means that relaxing some of the constraints should improve the model’s fit to the data. 71.2.6.2 Freely Estimate Variance of Intercept Latent Factor We will treat the baseline model as our nested model, and in this section, we will compare the baseline model to a more complex model (i.e., more freely estimated parameters). Specifically, we will specify a model in which we freely estimate the variance of the intercept latent factor. We’ll begin with the same model specifications as the baseline model, except we will relax the constraint on the variance of the intercept latent factor (i_RC) in order to freely estimate it. As indicated by the model specification annotation with three hashtags (###) instead of the typical one hashtag, we will freely estimate the variance of the intercept latent factor by removing 0*, such that the resulting line is now: i_RC ~~ i_RC. Beyond that, we will change the model specification object name to lgm_mod2 and the fitted model object to lgm_fit2 for subsequent model comparison purposes. # Specify freely estimated intercept factor variance &amp; assign to object lgm_mod2 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings to zero (no growth) s_RC =~ 0*RC_1m + 0*RC_3m + 0*RC_6m + 0*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 # Constrain mean of slope latent factor to zero (average slope set to zero) s_RC ~ 0*1 ### Freely estimate intercept latent factor variance (allow to vary) i_RC ~~ i_RC # Constrain slope latent factor variance to zero (don&#39;t allow to vary) s_RC ~~ 0*s_RC # Constrain covariance between intercept &amp; slope latent factors to zero i_RC ~~ 0*s_RC # Constrain (residual) error variances to be equal (homogeneity of variances) RC_1m ~~ e*RC_1m RC_3m ~~ e*RC_3m RC_6m ~~ e*RC_6m RC_9m ~~ e*RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit2 &lt;- growth(lgm_mod2, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit2, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 21 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## Number of equality constraints 3 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 2997.931 ## Degrees of freedom 11 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.000 ## Tucker-Lewis Index (TLI) -0.335 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -11597.668 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 23201.336 ## Bayesian (BIC) 23214.767 ## Sample-size adjusted Bayesian (SABIC) 23205.242 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.646 ## 90 Percent confidence interval - lower 0.627 ## 90 Percent confidence interval - upper 0.666 ## P-value H_0: RMSEA &lt;= 0.050 0.000 ## P-value H_0: RMSEA &gt;= 0.080 1.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.809 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 7.873 0.371 ## RC_3m 1.000 7.873 0.371 ## RC_6m 1.000 7.873 0.371 ## RC_9m 1.000 7.873 0.371 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 0.000 0.000 0.000 ## RC_6m 0.000 0.000 0.000 ## RC_9m 0.000 0.000 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC 0.000 NaN NaN ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 53.805 0.494 108.834 0.000 6.834 6.834 ## s_RC 0.000 NaN NaN ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 61.987 9.343 6.635 0.000 1.000 1.000 ## s_RC 0.000 NaN NaN ## .RC_1m (e) 387.516 12.410 31.225 0.000 387.516 0.862 ## .RC_3m (e) 387.516 12.410 31.225 0.000 387.516 0.862 ## .RC_6m (e) 387.516 12.410 31.225 0.000 387.516 0.862 ## .RC_9m (e) 387.516 12.410 31.225 0.000 387.516 0.862 # Visualize the LGM as a path diagram semPaths(lgm_fit2, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous table by adding in the model fit information for the freely estimated intercept latent factor variance. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 Free Intercept Variance Model 2997.931 11 &lt; .001 .000 -0.335 .646 .809 Both the baseline model and the new model fit the data pretty poorly. As an additional test, we can perform a nested model comparison using the chi-square (\\(\\chi^{2}\\)) difference test, which is also known as the log-likelihood (LL) test. To perform this test, we’ll apply the anova function from base R. As the first argument, we’ll insert the name of our model containing the freely estimated intercept latent factor variance fitted model object (lgm_fit2), and as the second argument, we’ll insert the name of our baseline fitted model object (lgm_fit1). # Nested model comparison using chi-square difference test anova(lgm_fit2, lgm_fit1) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit2 11 23201 23215 2997.9 ## lgm_fit1 12 23264 23273 3062.2 64.314 0.3121 1 0.000000000000001061 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the baseline model fits the data statistically significantly worse than the current less parsimonious model in which we freely estimated the variance of the intercept latent factor (\\(\\Delta \\chi^{2}\\) = 64.314, \\(\\Delta df\\) = 1, \\(p\\) &lt; .001). Note: If your anova function output defaulted to scientific notation, you can “turn off” scientific notation using the following function. After running the options function below, you can re-run the anova function to get the output in traditional notation. # Turn off scientific notation options(scipen=9999) 71.2.6.3 Specify Linear Functional Form for Slope Latent Factor &amp; Estimate Its Mean For this model, we will update our previous model by specifying the function form of the slope latent factor and freely estimating its mean. Updates to the previous model specifications are preceded with ### annotations. Specifically, we will apply meaningful time coding to the s_RC latent factor. Because role clarity was measured at 1 month, 3 months, 6 months, and 9 months, we will subtract 1 from each month, which will result in the 1-month measurement occasion becoming 0 and thus setting a measurement occasion to serve as the intercept value; in other words, conceptually, the measurement occasion coded as 0 represents the value of role clarity when its slope crosses time equal to 0. Subtracting 1 from each measurement occasion in this example also allows for us to account for the unequal time intervals associated with the measurement occasions. As a result, the end result should look like this: s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m. In addition, we will relax the constraint on the mean of the slope latent factor (s_RC) in order to freely estimate it, which requires removing 0*, such that the resulting line is now: s_RC ~ 1. Beyond that, we will change the model specification object name to lgm_mod3 and the fitted model object to lgm_fit3 for subsequent model comparison purposes. # Specify linear functional form for slope latent factor &amp; assign to object lgm_mod3 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m ### Specify and constrain slope factor loadings to linear growth s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 ### Freely estimate mean of slope latent factor s_RC ~ 1 # Freely estimate intercept latent factor variance (allow to vary) i_RC ~~ i_RC # Constrain slope latent factor variance to zero (don&#39;t allow to vary) s_RC ~~ 0*s_RC # Constrain covariance between intercept &amp; slope latent factors to zero i_RC ~~ 0*s_RC # Constrain (residual) error variances to be equal (homogeneity of variances) RC_1m ~~ e*RC_1m RC_3m ~~ e*RC_3m RC_6m ~~ e*RC_6m RC_9m ~~ e*RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit3 &lt;- growth(lgm_mod3, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit3, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 25 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## Number of equality constraints 3 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 20.422 ## Degrees of freedom 10 ## P-value (Chi-square) 0.026 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.991 ## Tucker-Lewis Index (TLI) 0.995 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10108.913 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20225.826 ## Bayesian (BIC) 20243.734 ## Sample-size adjusted Bayesian (SABIC) 20231.034 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.040 ## 90 Percent confidence interval - lower 0.014 ## 90 Percent confidence interval - upper 0.065 ## P-value H_0: RMSEA &lt;= 0.050 0.716 ## P-value H_0: RMSEA &gt;= 0.080 0.003 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.034 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 11.740 0.788 ## RC_3m 1.000 11.740 0.788 ## RC_6m 1.000 11.740 0.788 ## RC_9m 1.000 11.740 0.788 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 0.000 0.000 ## RC_6m 5.000 0.000 0.000 ## RC_9m 8.000 0.000 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC 0.000 NaN NaN ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 35.144 0.542 64.820 0.000 2.994 2.994 ## s_RC 4.976 0.059 83.832 0.000 Inf Inf ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 137.824 8.838 15.594 0.000 1.000 1.000 ## s_RC 0.000 NaN NaN ## .RC_1m (e) 84.169 2.696 31.225 0.000 84.169 0.379 ## .RC_3m (e) 84.169 2.696 31.225 0.000 84.169 0.379 ## .RC_6m (e) 84.169 2.696 31.225 0.000 84.169 0.379 ## .RC_9m (e) 84.169 2.696 31.225 0.000 84.169 0.379 # Visualize the LGM as a path diagram semPaths(lgm_fit3, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the model in which we specified a linear functional form for the slope latent factor and freely estimated the mean of the slope latent factor. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 Free Intercept Variance Model 2997.931 11 &lt; .001 .000 -0.335 .646 .809 Linear Slope Latent Factor Model 20.422 10 .026 .991 .995 .040 .034 As you can see above, our model fit indices improve considerably when we specify a linear function form for the latent slope factor and allow its mean to be freely estimated. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the current model (lgm_fit3) to the previous model (lgm_fit2). # Nested model comparison using chi-square difference test anova(lgm_fit3, lgm_fit2) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit3 10 20226 20244 20.422 ## lgm_fit2 11 23201 23215 2997.931 2977.5 2.1399 1 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the current model fits the data statistically significantly better than the previous model (\\(\\Delta \\chi^{2}\\) = 2977.5, \\(\\Delta df\\) = 1, \\(p\\) &lt; .001). This corroborates what we saw with the direct comparison of model fit indices above. 71.2.6.4 Freely Estimate Variance of Slope Latent Factor For this model, we will update our previous model by allowing slopes to freely vary between employees; in other words, we will freely estimate the variance of the slope latent factor. The update to the previous model specifications is preceded with a ### annotation. Specifically, we will relax the constraint on the variance of the slope latent factor (s_RC) in order to freely estimate it, which requires removing 0*, such that the resulting line is now: s_RC ~~ s_RC. Beyond that, we will change the model specification object name to lgm_mod4 and the fitted model object to lgm_fit4 for subsequent model comparison purposes. # Specify freely estimated slope factor variance &amp; assign to object lgm_mod4 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings to linear growth s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 # Freely estimate mean of slope latent factor s_RC ~ 1 # Freely estimate intercept latent factor variance (allow to vary) i_RC ~~ i_RC ### Freely estimate slope latent factor variance (allow to vary) s_RC ~~ s_RC # Constrain covariance between intercept &amp; slope latent factors to zero i_RC ~~ 0*s_RC # Constrain (residual) error variances to be equal (homogeneity of variances) RC_1m ~~ e*RC_1m RC_3m ~~ e*RC_3m RC_6m ~~ e*RC_6m RC_9m ~~ e*RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit4 &lt;- growth(lgm_mod4, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit4, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## Number of equality constraints 3 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 15.917 ## Degrees of freedom 9 ## P-value (Chi-square) 0.069 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.994 ## Tucker-Lewis Index (TLI) 0.996 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10106.661 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20223.322 ## Bayesian (BIC) 20245.707 ## Sample-size adjusted Bayesian (SABIC) 20229.832 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.034 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.062 ## P-value H_0: RMSEA &lt;= 0.050 0.807 ## P-value H_0: RMSEA &gt;= 0.080 0.002 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.047 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 11.686 0.793 ## RC_3m 1.000 11.686 0.791 ## RC_6m 1.000 11.686 0.781 ## RC_9m 1.000 11.686 0.763 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.040 0.070 ## RC_6m 5.000 2.600 0.174 ## RC_9m 8.000 4.159 0.272 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC 0.000 0.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 35.144 0.537 65.429 0.000 3.007 3.007 ## s_RC 4.976 0.062 80.842 0.000 9.571 9.571 ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 136.556 8.977 15.211 0.000 1.000 1.000 ## s_RC 0.270 0.131 2.059 0.039 1.000 1.000 ## .RC_1m (e) 80.578 2.976 27.074 0.000 80.578 0.371 ## .RC_3m (e) 80.578 2.976 27.074 0.000 80.578 0.369 ## .RC_6m (e) 80.578 2.976 27.074 0.000 80.578 0.360 ## .RC_9m (e) 80.578 2.976 27.074 0.000 80.578 0.344 # Visualize the LGM as a path diagram semPaths(lgm_fit4, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the model in which we freely estimated the variance of the slope latent factor. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 Free Intercept Variance Model 2997.931 11 &lt; .001 .000 -0.335 .646 .809 Linear Slope Latent Factor Model 20.422 10 .026 .991 .995 .040 .034 Free Slope Variance Model 15.917 9 .069 .994 .996 .034 .047 As you can see above, our model fit mostly improves when we freely estimate the variance of the slope latent factor. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the current model (lgm_fit4) to the previous model (lgm_fit3). # Nested model comparison using chi-square difference test anova(lgm_fit4, lgm_fit3) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit4 9 20223 20246 15.917 ## lgm_fit3 10 20226 20244 20.422 4.5046 0.073428 1 0.0338 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the current model fits the data statistically significantly better than previous model (\\(\\Delta \\chi^{2}\\) = 4.5046, \\(\\Delta df\\) = 1, \\(p\\) = .0338). This corroborates what we saw with the direct comparison of model fit indices above. 71.2.6.5 Freely Estimate Covariance Between Intercept &amp; Slope Latent Factors For this model, we will update our previous model by allowing the intercept and slope latent factors to covary; in other words, we will freely estimate the covariance between the intercept and slope latent factors. The update to the previous model specifications is preceded with a ### annotation. Specifically, we will relax the constraint on the covariance in order to freely estimate it, which requires removing 0*, such that the resulting line is now: i_RC ~~ s_RC. Beyond that, we will change the model specification object name to lgm_mod5 and the fitted model object to lgm_fit5 for subsequent model comparison purposes. # Freely estimate covariance between intercept &amp; slope latent factors &amp; assign to object lgm_mod5 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings to linear growth s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 # Freely estimate mean of slope latent factor s_RC ~ 1 # Freely estimate intercept latent factor variance (allow to vary) i_RC ~~ i_RC # Freely estimate slope latent factor variance (allow to vary) s_RC ~~ s_RC ### Freely estimate covariance between intercept &amp; slope latent factors i_RC ~~ s_RC # Constrain (residual) error variances to be equal (homogeneity of variances) RC_1m ~~ e*RC_1m RC_3m ~~ e*RC_3m RC_6m ~~ e*RC_6m RC_9m ~~ e*RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit5 &lt;- growth(lgm_mod5, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit5, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 38 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## Number of equality constraints 3 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 6.220 ## Degrees of freedom 8 ## P-value (Chi-square) 0.623 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10101.812 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20215.625 ## Bayesian (BIC) 20242.487 ## Sample-size adjusted Bayesian (SABIC) 20223.437 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.039 ## P-value H_0: RMSEA &lt;= 0.050 0.989 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.028 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.433 0.817 ## RC_3m 1.000 12.433 0.835 ## RC_6m 1.000 12.433 0.847 ## RC_9m 1.000 12.433 0.840 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.512 0.102 ## RC_6m 5.000 3.780 0.258 ## RC_9m 8.000 6.048 0.409 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.074 1.056 -2.910 0.004 -0.327 -0.327 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 35.144 0.559 62.824 0.000 2.827 2.827 ## s_RC 4.976 0.064 77.623 0.000 6.582 6.582 ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 154.590 11.444 13.508 0.000 1.000 1.000 ## s_RC 0.572 0.170 3.372 0.001 1.000 1.000 ## .RC_1m (e) 77.167 3.027 25.495 0.000 77.167 0.333 ## .RC_3m (e) 77.167 3.027 25.495 0.000 77.167 0.348 ## .RC_6m (e) 77.167 3.027 25.495 0.000 77.167 0.358 ## .RC_9m (e) 77.167 3.027 25.495 0.000 77.167 0.352 # Visualize the LGM as a path diagram semPaths(lgm_fit5, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for the model in which we freely estimated the covariance between the intercept and slope latent factors. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 Free Intercept Variance Model 2997.931 11 &lt; .001 .000 -0.335 .646 .809 Linear Slope Latent Factor Model 20.422 10 .026 .991 .995 .040 .034 Free Slope Variance Model 15.917 9 .069 .994 .996 .034 .047 Free Covariance Model 6.220 8 .623 1.000 1.001 .000 .028 As you can see above, our model fit improves when we freely estimate the covariance between the intercept and slope latent factors. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the current model (lgm_fit5) to the previous model (lgm_fit4). # Nested model comparison using chi-square difference test anova(lgm_fit5, lgm_fit4) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit5 8 20216 20243 6.2203 ## lgm_fit4 9 20223 20246 15.9171 9.6968 0.11567 1 0.001846 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The chi-square difference test indicates that the current model fits the data statistically significantly better than previous model (\\(\\Delta \\chi^{2}\\) = 9.6968, \\(\\Delta df\\) = 1, \\(p\\) = .001846). This corroborates what we saw with the direct comparison of model fit indices above. 71.2.6.6 Freely Estimate Residual Error Variances For this final model, we will update our previous model by removing the quality constraints from the (residual) error variances. In other words, we will allow each observed variable’s error variance to be freely estimated. The update to the previous model specifications is preceded with a ### annotation. Specifically, we will relax the quality constraints by removing e* within each error variance term specification. For example, the first observed variable’s error variance should now be specified as: RC_1m ~~ RC_1m. Beyond that, we will change the model specification object name to lgm_mod6 and the fitted model object to lgm_fit6 for subsequent model comparison purposes. # Freely estimate residual error variances &amp; assign to object lgm_mod6 &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain slope factor loadings to linear growth s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Freely estimate mean of intercept latent factor i_RC ~ 1 # Freely estimate mean of slope latent factor s_RC ~ 1 # Freely estimate intercept latent factor variance (allow to vary) i_RC ~~ i_RC # Freely estimate slope latent factor variance (allow to vary) s_RC ~~ s_RC # Freely estimate covariance between intercept &amp; slope latent factors i_RC ~~ s_RC ### Freely estimate error variances (heterogeneity of variances) RC_1m ~~ RC_1m RC_3m ~~ RC_3m RC_6m ~~ RC_6m RC_9m ~~ RC_9m &quot; # Estimate LGM &amp; assign to fitted model object lgm_fit6 &lt;- growth(lgm_mod6, # name of specified model object data=df) # name of wide-format data frame object # Print summary of model results summary(lgm_fit6, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 96 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 3.760 ## Degrees of freedom 5 ## P-value (Chi-square) 0.584 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10100.582 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20219.165 ## Bayesian (BIC) 20259.458 ## Sample-size adjusted Bayesian (SABIC) 20230.883 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.047 ## P-value H_0: RMSEA &lt;= 0.050 0.963 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.019 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.463 0.820 ## RC_3m 1.000 12.463 0.834 ## RC_6m 1.000 12.463 0.840 ## RC_9m 1.000 12.463 0.855 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.725 0.116 ## RC_6m 5.000 4.314 0.291 ## RC_9m 8.000 6.902 0.474 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.483 1.147 -3.036 0.002 -0.324 -0.324 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 35.143 0.559 62.824 0.000 2.820 2.820 ## s_RC 4.975 0.064 77.667 0.000 5.767 5.767 ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC 155.324 11.703 13.273 0.000 1.000 1.000 ## s_RC 0.744 0.211 3.520 0.000 1.000 1.000 ## .RC_1m 75.459 7.345 10.273 0.000 75.459 0.327 ## .RC_3m 78.779 5.811 13.557 0.000 78.779 0.353 ## .RC_6m 81.083 5.791 14.001 0.000 81.083 0.368 ## .RC_9m 65.214 7.998 8.154 0.000 65.214 0.307 # Visualize the LGM as a path diagram semPaths(lgm_fit6, # name of fitted model object what=&quot;est&quot;, # display unstandardized parameter estimates weighted=FALSE, # do not weight plot features nCharNodes=0) # do not abbreviate names Below, I’ve expanded upon the previous model comparison table by adding in the model fit information for model in which we relaxed the homogeneity of variances constraint. Model \\(\\chi^{2}\\) df p CFI TLI RMSEA SRMR Baseline Model 3062.245 12 &lt; .001 .000 -2.500 .625 .877 Free Intercept Variance Model 2997.931 11 &lt; .001 .000 -0.335 .646 .809 Linear Slope Latent Factor Model 20.422 10 .026 .991 .995 .040 .034 Free Slope Variance Model 15.917 9 .069 .994 .996 .034 .047 Free Covariance Model 6.220 8 .623 1.000 1.001 .000 .028 Free Residual Error Variances Model 3.760 5 .584 1.000 1.001 .000 .019 As you can see above, some model fit indices improved or stayed about the same when we freely estimate each of the residual error variances associated with the observed variables. As before, we’ll also estimate the chi-square (\\(\\chi^{2}\\)) difference test, except this time we’ll compare the current model (lgm_fit6) to the previous model (lgm_fit5). # Nested model comparison using chi-square difference test anova(lgm_fit6, lgm_fit5) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit6 5 20219 20260 3.7603 ## lgm_fit5 8 20216 20243 6.2203 2.46 0 3 0.4826 The chi-square difference test indicates that the current model fits the data about the same as than previous model (\\(\\Delta \\chi^{2}\\) = 2.46, \\(\\Delta df\\) = 3, \\(p\\) = .4826). This suggests that we should go with the simpler model (lgm_fit5) in which residual error variances were constrained to be equal. 71.2.6.7 Create a Matrix Comparing Model Fit Indices If our goals are to create a matrix containing only those model fit indices that we covered in this chapter and to add in the chi-square difference tests, we can do the following, which incorporates the inspect function from the lavaan package and the cbind, rbind, and t functions from base R. # Create object containing selected fit indices select_fit_indices &lt;- c(&quot;chisq&quot;,&quot;df&quot;,&quot;pvalue&quot;,&quot;cfi&quot;,&quot;tli&quot;,&quot;rmsea&quot;,&quot;srmr&quot;) # Create matrix comparing model fit indices compare_mods &lt;- cbind( inspect(lgm_fit1, &quot;fit.indices&quot;)[select_fit_indices], inspect(lgm_fit2, &quot;fit.indices&quot;)[select_fit_indices], inspect(lgm_fit3, &quot;fit.indices&quot;)[select_fit_indices], inspect(lgm_fit4, &quot;fit.indices&quot;)[select_fit_indices], inspect(lgm_fit5, &quot;fit.indices&quot;)[select_fit_indices], inspect(lgm_fit6, &quot;fit.indices&quot;)[select_fit_indices] ) # Add more informative model names to matrix columns colnames(compare_mods) &lt;- c(&quot;Baseline Model&quot;, &quot;Free Intercept Variance Model&quot;, &quot;Linear Slope Latent Factor Model&quot;, &quot;Free Slope Variance Model&quot;, &quot;Free Covariance Model&quot;, &quot;Free Residual Error Variances Model&quot;) # Create vector of chi-square difference tests (nested model comparisons) `chisq diff (p-value)` &lt;- c(NA, anova(lgm_fit2, lgm_fit1)$`Pr(&gt;Chisq)`[2], anova(lgm_fit3, lgm_fit2)$`Pr(&gt;Chisq)`[2], anova(lgm_fit4, lgm_fit3)$`Pr(&gt;Chisq)`[2], anova(lgm_fit5, lgm_fit4)$`Pr(&gt;Chisq)`[2], anova(lgm_fit6, lgm_fit5)$`Pr(&gt;Chisq)`[2]) # Add chi-square difference tests to matrix object compare_mods &lt;- rbind(compare_mods, `chisq diff (p-value)`) # Round object values to 3 places after decimal compare_mods &lt;- round(compare_mods, 3) # Rotate matrix compare_mods &lt;- t(compare_mods) # Print object print(compare_mods) ## chisq df pvalue cfi tli rmsea srmr chisq diff (p-value) ## Baseline Model 3062.245 12 0.000 0.000 -0.250 0.625 0.877 NA ## Free Intercept Variance Model 2997.931 11 0.000 0.000 -0.335 0.646 0.809 0.000 ## Linear Slope Latent Factor Model 20.422 10 0.026 0.991 0.995 0.040 0.034 0.000 ## Free Slope Variance Model 15.917 9 0.069 0.994 0.996 0.034 0.047 0.034 ## Free Covariance Model 6.220 8 0.623 1.000 1.001 0.000 0.028 0.002 ## Free Residual Error Variances Model 3.760 5 0.584 1.000 1.001 0.000 0.019 0.483 71.2.7 Estimate Nonlinear Latent Growth Models Thus far in the chapter we have focused on estimating models in which change is constrained to be linear. In this section, we will learn how to specify quadratic and cubic models, which are examples of nonlinear functional forms. 71.2.7.1 Estimate Quadratic Latent Growth Model Just as we did with the conditional unconstrained LGM with a linear functional form in a previous section, first, we must specify the LGM and assign it to an object that we can subsequently reference. To do so, we will do the following. Specify a name for the model object (e.g., lgm_mod_quad), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator and within quotation marks (\" \"): Specify a name for the intercept latent factor (e.g., i_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. Anything that comes to the right of the =~ operator is an observed variable (i.e., indicator) of the latent factor. Please note that the latent factor is not something that we directly observe, so it will not have a corresponding variable in our data frame object. After the =~ operator, specify each observed variable associated with the intercept latent factor, and to separate the observed variables, insert the + operator. In this example, the four observed variables are: RC_1m, RC_3m, RC_6m, and RC_9m. These are our observed variables, which conceptually are influenced by the underlying latent factor. To set the i_RC latent factor as the intercept, we also need to constrain each observed variable’s factor loading to 1 by preceding each observed variable by 1*. As a result, the end result should look like this: i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m. Specify a name for the linear slope latent factor (e.g., s_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. After the =~ operator, specify each observed variable associated with the slope latent factor, which will be the same observed variables that serve as indicators for the intercept latent factor. To separate the observed variables, insert the + operator. To set the s_RC latent factor as the linear slope, we also need to constrain each observed variable’s factor loading to account for the time intervals between measurement occasions, which is also known as time coding. Because role clarity was measured at 1 month, 3 months, 6 months, and 9 months, we will subtract 1 from each month, which will result in the 1-month measurement occasion becoming 0 and thus setting a measurement occasion to serve as the intercept value; in other words, conceptually, the measurement occasion coded as 0 represents the value of role clarity when its slope crosses time equal to 0. Subtracting 1 from each measurement occasion in this example also allows for us to account for the unequal time intervals associated with the measurement occasions. As a result, the end result should look like this: s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m. Note: We could have specified a different measurement occasion slope factor loading as 0 by simply changing the time coding; for example, if we were to set the last measurement occasion slope factor loading as 0, our time coding would be: s_RC =~ -8*RC_1m + -6*RC_3m + -3*RC_6m + 0*RC_9m. Finally, if the time intervals between measurement occasions had been equal, we would have been able to apply a 0, 1, 2, and 3 time coding for the slope factor loadings to set the intercept at the first measurement occasion, or -3, -2, -1, and 0 to set the intercept at the last measurement occasion. Specify a name for the quadratic slope latent factor (e.g., q_RC), followed by the =~ operator, which is used to indicate how a latent factor is measured. After the =~ operator, specify each observed variable associated with the slope latent factor, which will be the same observed variables that serve as indicators for the intercept and linear slope latent factors. To separate the observed variables, insert the + operator. To set the q_RC latent factor as the quadratic slope, we also need to constrain each observed variable’s factor loading to account for the time intervals between measurement occasion. Specifically, we need to square each of the factor loading constrains from our linear slope factor. As a result, the end result should look like this: q_RC =~ 0*RC_1m + 4*RC_3m + 25*RC_6m + 64*RC_9m. # Specify unconditional unconstrained quadratic LGM &amp; assign to object lgm_mod_quad &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain linear slope factor loadings s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Specify and constrain quadratic slope factor loadings q_RC =~ 0*RC_1m + 4*RC_3m + 25*RC_6m + 64*RC_9m &quot; Second, now that we have specified the model object (lgm_mod_quad), we are ready to estimate the model using the growth function from the lavaan package. To do so, we will do the following. Specify a name for the fitted model object (e.g., lgm_fit_quad), followed by the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the growth function, and within the function parentheses include the following arguments. As the first argument, insert the name of the model object that we specified above (lgm_mod_quad). As the second argument, insert the name of the data frame object to which the indicator variables in our model belong. That is, after data=, insert the name of the original wide-format data frame object (df). Note: The growth function includes model estimation defaults, which explains why we had relatively few model specifications. # Estimate unconditional unconstrained quadratic LGM &amp; assign to fitted model object lgm_fit_quad &lt;- growth(lgm_mod_quad, # name of specified model object data=df) # name of wide-format data frame object Third, we will use the summary function from base R to to print the model results. To do so, we will apply the following arguments in the summary function parentheses. As the first argument, specify the name of the fitted model object that we created above (lgm_fit_quad). As the second argument, set fit.measures=TRUE to obtain the model fit indices (e.g., CFI, TLI, RMSEA, SRMR). As the third argument, set standardized=TRUE to request the standardized parameter estimates for the model. # Print summary of model results summary(lgm_fit_quad, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 146 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic 0.026 ## Degrees of freedom 1 ## P-value (Chi-square) 0.872 ## ## Model Test Baseline Model: ## ## Test statistic 1226.041 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.005 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10098.715 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20223.431 ## Bayesian (BIC) 20281.631 ## Sample-size adjusted Bayesian (SABIC) 20240.357 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.054 ## P-value H_0: RMSEA &lt;= 0.050 0.943 ## P-value H_0: RMSEA &gt;= 0.080 0.016 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.200 0.814 ## RC_3m 1.000 12.200 0.809 ## RC_6m 1.000 12.200 0.809 ## RC_9m 1.000 12.200 0.846 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 3.143 0.208 ## RC_6m 5.000 7.858 0.521 ## RC_9m 8.000 12.572 0.872 ## q_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 4.000 0.707 0.047 ## RC_6m 25.000 4.421 0.293 ## RC_9m 64.000 11.319 0.785 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -0.194 7.101 -0.027 0.978 -0.010 -0.010 ## q_RC -0.391 0.696 -0.562 0.574 -0.181 -0.181 ## s_RC ~~ ## q_RC -0.230 0.347 -0.662 0.508 -0.827 -0.827 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.055 0.577 60.704 0.000 2.873 2.873 ## s_RC 5.079 0.217 23.378 0.000 3.232 3.232 ## q_RC -0.013 0.025 -0.501 0.617 -0.072 -0.072 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 76.049 15.967 4.763 0.000 76.049 0.338 ## .RC_3m 75.815 7.696 9.851 0.000 75.815 0.333 ## .RC_6m 76.478 9.370 8.162 0.000 76.478 0.336 ## .RC_9m 61.428 27.256 2.254 0.024 61.428 0.295 ## i_RC 148.840 17.473 8.518 0.000 1.000 1.000 ## s_RC 2.470 3.613 0.683 0.494 1.000 1.000 ## q_RC 0.031 0.039 0.799 0.424 1.000 1.000 With respect to model fit, the model fit indices suggest acceptable fit to the data. With respect to the parameter estimates, we’ll focus on just the means of the latent factors. Notably, the unstandardized means of the intercept and linear slope latent factors are statistically significant (35.055, p &lt; .001 and 5.079, p &lt; .001); however, the newly introduced quadratic slope latent factor is not statistically significant (-0.013, p = .617), which suggests that the added quadratic term does not explain any additional information beyond what the the means of the intercept and linear slope latent factors already explain. Thus, we can conclude that there does not appear to be a quadratic functional form associated with the average growth trajectory for role clarity. Had the mean of the quadratic slope latent factor been statistically significant, it would have indicated that the functional form is quadratic, but that’s not the case here. Because our original unconditional constrained linear LGM from an earlier section is nested with the this quadratic LGM, we can perform a nested model comparison using the chi-square difference test. To perform this test, we’ll apply the anova function from base R to our two fitted model objects. As the first argument, we’ll insert the name of our model containing the current quadratic fitted model object (lgm_fit_quad), and as the second argument, we’ll insert the name of our linear fitted model object (lgm_fit). # Nested model comparison using chi-square difference test anova(lgm_fit_quad, lgm_fit) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## lgm_fit_quad 1 20223 20282 0.0261 ## lgm_fit 5 20219 20260 3.7603 3.7342 0 4 0.4432 The chi-square difference test indicates that the more parsimonious linear LGM does not fit the data statistically significantly worse than the current more complex quadratic LGM (\\(\\Delta \\chi^{2}\\) = 3.7342, \\(\\Delta df\\) = 4, \\(p\\) = .4432). Thus, we should retain the linear LGM instead of the quadratic LGM. 71.2.7.2 Estimate Cubic Latent Growth Model Estimating a cubic function form for an LGM requires adding a cubic slope latent factor to the quadratic LGM we specified in the previous section (e.g., c_RC). To do so, we cube the linear slope factor loading constraints, which will result in the following: c_RC =~ 0*RC_1m + 8*RC_3m + 125*RC_6m + 512*RC_9m. # Specify unconditional unconstrained cubic LGM &amp; assign to object lgm_mod_cubic &lt;- &quot; # Specify and constrain intercept factor loadings i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m # Specify and constrain linear slope factor loadings s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m # Specify and constrain quadratic slope factor loadings q_RC =~ 0*RC_1m + 4*RC_3m + 25*RC_6m + 64*RC_9m # Specify and constrain cubic slope factor loadings c_RC =~ 0*RC_1m + 8*RC_3m + 125*RC_6m + 512*RC_9m &quot; To estimate the fitted model, we follow the same process as the linear LGM and quadratic LGM, except we insert the name of the new lgm_mod_cubic specified model object as the first argument in the growth function, and we assign the resulting fitted model to an object that we’ll call lgm_fit_cubic. # Estimate unconditional unconstrained cubic LGM &amp; assign to fitted model object lgm_fit_cubic &lt;- growth(lgm_mod_cubic, # name of specified model object data=df) # name of wide-format data frame object As we did previously, we use the summary function to print a summary of the model fit and parameter estimates results, except we insert the cubic LGM fitted model object (lgm_fit_cubic) as the first argument in the function. # Print summary of model results summary(lgm_fit_cubic, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 177 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 18 ## ## Number of observations 650 ## ## Model Test User Model: ## ## Test statistic NA ## Degrees of freedom -4 ## P-value (Unknown) NA ## ## Model Test Baseline Model: ## ## Test statistic NA ## Degrees of freedom NA ## P-value NA ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) NA ## Tucker-Lewis Index (TLI) NA ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -10098.702 ## Loglikelihood unrestricted model (H1) -10098.702 ## ## Akaike (AIC) 20233.405 ## Bayesian (BIC) 20313.990 ## Sample-size adjusted Bayesian (SABIC) 20256.841 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 7.881 0.526 ## RC_3m 1.000 7.881 0.523 ## RC_6m 1.000 7.881 0.522 ## RC_9m 1.000 7.881 0.546 ## s_RC =~ ## RC_1m 0.000 NA NA ## RC_3m 2.000 NA NA ## RC_6m 5.000 NA NA ## RC_9m 8.000 NA NA ## q_RC =~ ## RC_1m 0.000 NA NA ## RC_3m 4.000 NA NA ## RC_6m 25.000 NA NA ## RC_9m 64.000 NA NA ## c_RC =~ ## RC_1m 0.000 NA NA ## RC_3m 8.000 NA NA ## RC_6m 125.000 NA NA ## RC_9m 512.000 NA NA ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC 71.363 NA 19.933 19.933 ## q_RC -16.655 NA -2.214 -2.214 ## c_RC 1.084 NA 0.895 0.895 ## s_RC ~~ ## q_RC -1.087 NA -2.507 -2.507 ## c_RC 0.003 NA 0.047 0.047 ## q_RC ~~ ## c_RC 0.167 NA 1.138 1.138 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.037 NA 4.446 4.446 ## s_RC 5.151 NA NA NA ## q_RC -0.038 NA NA NA ## c_RC 0.002 NA NA NA ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 162.783 NA 162.783 0.724 ## .RC_3m 19.290 NA 19.290 0.085 ## .RC_6m 180.973 NA 180.973 0.795 ## .RC_9m 105.095 NA 105.095 0.505 ## i_RC 62.105 NA 1.000 1.000 ## s_RC -0.206 NA NA NA ## q_RC -0.911 NA NA NA ## c_RC -0.024 NA NA NA In the output, note how the degrees of freedom for the cubic LGM is -4. This indicates that the cubic model is under-identified, and thus we can’t estimate the model’s fit to the data or the standard errors of its freely estimated parameters. To estimate an over-identified cubic LGM, we would need at least five measurement occasions. Had our cubic LGM been over-identified, we would have looked at the statistical significance of the mean of the cubic slope factor to determine whether the model showed evidence of a cubic functional form; further, we would have then been able to perform a nested model comparison with the quadratic LGM. 71.2.8 Estimating Models with Missing Data When missing data are present, we must carefully consider how we handle the missing data before or during the estimations of a model. In the chapter on Missing Data, I provide an overview of relevant concepts, particularly if the data are missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR); I suggest reviewing that chapter prior to handling missing data. As a potential method for addressing missing data, the lavaan model functions, such as the growth function, allow for full-information maximum likelihood (FIML). Further, the functions allow us to specify specific estimators given the type of data we wish to use for model estimation (e.g., ML, MLR). To demonstrate how missing data are handled using FIML, we will need to first introduce some missing data into our data. To do so, we will use a multiple imputation package called mice and a function called ampute that “amputates” existing data by creating missing data patterns. For our purposes, we’ll replace 10% (.1) of data frame cells with NA (which is signifies a missing value) such that the missing data are missing completely at random (MCAR). # Install package install.packages(&quot;mice&quot;) # Access package library(mice) # Create a new data frame object df_missing &lt;- df # Remove non-numeric variable(s) from data frame object df_missing$EmployeeID &lt;- NULL # Set a seed set.seed(2024) # Remove 10% of cells so missing data are MCAR df_missing &lt;- ampute(df_missing, prop=.1, mech=&quot;MCAR&quot;) # Extract the new missing data frame object and overwrite existing object df_missing &lt;- df_missing$amp Implementing FIML when missing data are present is relatively straightforward. For example, for the unconditional unconstrained LGM from a previous section, we can apply FIML in the presence of missing data by adding the missing=\"fiml\" argument to the growth function. # Specify unconditional unconstrained LGM &amp; assign to object lgm_mod &lt;- &quot; i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m &quot; # Estimate unconditional unconstrained LGM &amp; assign to fitted model object lgm_fit &lt;- growth(lgm_mod, # name of specified model object data=df_missing, # name of wide-format data frame object missing=&quot;fiml&quot;) # specify FIML # Print summary of model results summary(lgm_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 95 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 650 ## Number of missing patterns 5 ## ## Model Test User Model: ## ## Test statistic 2.944 ## Degrees of freedom 5 ## P-value (Chi-square) 0.709 ## ## Model Test Baseline Model: ## ## Test statistic 1197.353 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9975.174 ## Loglikelihood unrestricted model (H1) -9973.702 ## ## Akaike (AIC) 19968.347 ## Bayesian (BIC) 20008.640 ## Sample-size adjusted Bayesian (SABIC) 19980.065 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.041 ## P-value H_0: RMSEA &lt;= 0.050 0.980 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.041 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.979 ## P-value H_0: Robust RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.017 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.563 0.823 ## RC_3m 1.000 12.563 0.839 ## RC_6m 1.000 12.563 0.848 ## RC_9m 1.000 12.563 0.862 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.738 0.116 ## RC_6m 5.000 4.344 0.293 ## RC_9m 8.000 6.951 0.477 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.755 1.175 -3.196 0.001 -0.344 -0.344 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.112 0.564 62.222 0.000 2.795 2.795 ## s_RC 4.977 0.065 76.712 0.000 5.728 5.728 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 75.372 7.497 10.054 0.000 75.372 0.323 ## .RC_3m 78.474 5.890 13.324 0.000 78.474 0.350 ## .RC_6m 80.340 5.810 13.827 0.000 80.340 0.366 ## .RC_9m 66.406 8.148 8.150 0.000 66.406 0.313 ## i_RC 157.819 11.855 13.312 0.000 1.000 1.000 ## s_RC 0.755 0.218 3.459 0.001 1.000 1.000 The FIML approach uses all observations in which data are missing on one or more endogenous variables in the model. As you can see in the output, all 650 observations were retained for estimating the model. Now watch what happens when we remove the missing=\"fiml\" argument in the presence of missing data. # Specify unconditional unconstrained LGM &amp; assign to object lgm_mod &lt;- &quot; i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m &quot; # Estimate unconditional unconstrained LGM &amp; assign to fitted model object lgm_fit &lt;- growth(lgm_mod, # name of specified model object data=df_missing) # name of wide-format data frame object # Print summary of model results summary(lgm_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 93 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Used Total ## Number of observations 616 650 ## ## Model Test User Model: ## ## Test statistic 2.389 ## Degrees of freedom 5 ## P-value (Chi-square) 0.793 ## ## Model Test Baseline Model: ## ## Test statistic 1153.818 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.003 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9564.632 ## Loglikelihood unrestricted model (H1) -9563.438 ## ## Akaike (AIC) 19147.264 ## Bayesian (BIC) 19187.074 ## Sample-size adjusted Bayesian (SABIC) 19158.500 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.036 ## P-value H_0: RMSEA &lt;= 0.050 0.987 ## P-value H_0: RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.016 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.523 0.829 ## RC_3m 1.000 12.523 0.839 ## RC_6m 1.000 12.523 0.850 ## RC_9m 1.000 12.523 0.863 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.740 0.117 ## RC_6m 5.000 4.350 0.295 ## RC_9m 8.000 6.960 0.480 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.916 1.185 -3.304 0.001 -0.359 -0.359 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.324 0.575 61.481 0.000 2.821 2.821 ## s_RC 4.963 0.066 75.250 0.000 5.704 5.704 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 71.546 7.371 9.706 0.000 71.546 0.313 ## .RC_3m 78.644 5.953 13.211 0.000 78.644 0.353 ## .RC_6m 80.307 5.890 13.634 0.000 80.307 0.370 ## .RC_9m 68.051 8.281 8.218 0.000 68.051 0.323 ## i_RC 156.837 12.022 13.046 0.000 1.000 1.000 ## s_RC 0.757 0.217 3.495 0.000 1.000 1.000 As you can see in the output, the growth function defaults to listwise deletion when we do not specify that FIML be applied. This results in the number of observations dropping from 650 to 616 for model estimation purposes. Within the growth function, we can also specify a specific estimator if we choose to override the default. For example, we could specify the MLR (maximum likelihood with robust standard errors) estimator if we had good reason to. To do so, we would add this argument: estimator=\"MLR\". For a list of other available estimators, you can check out the lavaan package website. # Specify unconditional unconstrained LGM &amp; assign to object lgm_mod &lt;- &quot; i_RC =~ 1*RC_1m + 1*RC_3m + 1*RC_6m + 1*RC_9m s_RC =~ 0*RC_1m + 2*RC_3m + 5*RC_6m + 8*RC_9m &quot; # Estimate unconditional unconstrained LGM &amp; assign to fitted model object lgm_fit &lt;- growth(lgm_mod, # name of specified model object data=df_missing, # name of wide-format data frame object estimator=&quot;MLR&quot;, # specify MLR estimator missing=&quot;fiml&quot;) # specify FIML # Print summary of model results summary(lgm_fit, # name of fitted model object fit.measures=TRUE, # request model fit indices standardized=TRUE) # request standardized estimates ## lavaan 0.6.15 ended normally after 95 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 650 ## Number of missing patterns 5 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 2.944 3.078 ## Degrees of freedom 5 5 ## P-value (Chi-square) 0.709 0.688 ## Scaling correction factor 0.957 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 1197.353 1188.078 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.008 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9975.174 -9975.174 ## Scaling correction factor 0.953 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -9973.702 -9973.702 ## Scaling correction factor 0.954 ## for the MLR correction ## ## Akaike (AIC) 19968.347 19968.347 ## Bayesian (BIC) 20008.640 20008.640 ## Sample-size adjusted Bayesian (SABIC) 19980.065 19980.065 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.041 0.043 ## P-value H_0: RMSEA &lt;= 0.050 0.980 0.975 ## P-value H_0: RMSEA &gt;= 0.080 0.000 0.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.041 ## P-value H_0: Robust RMSEA &lt;= 0.050 0.979 ## P-value H_0: Robust RMSEA &gt;= 0.080 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.017 0.017 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC =~ ## RC_1m 1.000 12.563 0.823 ## RC_3m 1.000 12.563 0.839 ## RC_6m 1.000 12.563 0.848 ## RC_9m 1.000 12.563 0.862 ## s_RC =~ ## RC_1m 0.000 0.000 0.000 ## RC_3m 2.000 1.738 0.116 ## RC_6m 5.000 4.344 0.293 ## RC_9m 8.000 6.951 0.477 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## i_RC ~~ ## s_RC -3.755 1.123 -3.344 0.001 -0.344 -0.344 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 0.000 0.000 0.000 ## .RC_3m 0.000 0.000 0.000 ## .RC_6m 0.000 0.000 0.000 ## .RC_9m 0.000 0.000 0.000 ## i_RC 35.112 0.564 62.269 0.000 2.795 2.795 ## s_RC 4.977 0.065 76.677 0.000 5.728 5.728 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RC_1m 75.372 7.199 10.470 0.000 75.372 0.323 ## .RC_3m 78.474 6.076 12.914 0.000 78.474 0.350 ## .RC_6m 80.340 5.632 14.264 0.000 80.340 0.366 ## .RC_9m 66.406 7.892 8.414 0.000 66.406 0.313 ## i_RC 157.819 10.862 14.529 0.000 1.000 1.000 ## s_RC 0.755 0.219 3.443 0.001 1.000 1.000 71.2.9 Summary In this chapter, we learned how to estimate latent growth models to understand the extent to which individuals’ from a population experience change in a construct over time, compare the fit of nested models, estimate nonlinear models, and estimate models when missing data are present. References "],["mixedfactorial.html", "Chapter 72 Evaluating a Pre-Test/Post-Test with Control Group Design Using an Independent-Samples t-test 72.1 Conceptual Overview 72.2 Tutorial 72.3 Chapter Supplement", " Chapter 72 Evaluating a Pre-Test/Post-Test with Control Group Design Using an Independent-Samples t-test In this chapter, we will learn how to evaluate a pre-test/post-test with control group design using an independent-samples t-test with a difference score outcome variable. A pre-test/post-test with control group design is also referred to as a 2x2 mixed-factorial design. Using an independent-samples t-test with a difference score outcome variable is appropriate when there are the same sub-sample sizes in the training and control conditions. In the chapter supplement, we will learn how this analysis, under these conditions, is statistically equivalent to a mixed-factorial analysis of variance (ANOVA). We will begin this chapter by introducing the concepts related to mixed-factorial ANOVA before discussing the independent-samples t-test as a viable alternative. 72.1 Conceptual Overview Factorial analysis of variance (ANOVA) is part of a larger family of analyses aimed at comparing means, and it used to compare the means for two or more categorical (nominal, ordinal) predictor (independent) variables (e.g., conditions, populations). The word “factorial” in ANOVA implies that there are two or more factors (i.e., categorical predictor variables). Just like a one-way ANOVA, a factorial ANOVA has a single continuous (interval, ratio) outcome (dependent) variable. A factorial ANOVA with just two factors, is often referred to as a two-way ANOVA, where the “two” refers to two factors. A factorial ANOVA can include within-subjects factors, between-subjects factors, or a combination of the two. When we have at least one within-subjects (e.g., repeated measurements for same cases) factor and at least one between-subjects (e.g., each case assigned to just one level of factor), we commonly refer to this as a mixed-factorial ANOVA. A common example of a 2x2 mixed-factorial design in the training evaluation context is as follows. Each trainee is randomly assigned to one (and only one) level of a between-subjects training condition factor (condition levels: training, control), but every trainee completes both levels of a within-subjects assessment factor (test time levels: pre-test, post-test). A 2x2 mixed-factorial ANOVA would be an appropriate statistical analysis for this design, and in the training context, we often refer to this design as a pre-test/post-test with control group design. Pre-Test Post-Test Training Training / Pre-Test Training / Post-Test Control Control / Pre-Test Control / Post-Test When we have a mixed-factorial design, we are typically interested in looking at the interaction between the two or more factors (i.e., predictor variables). That is, we want to know if differences in means between levels of one factor is conditional on levels of another factor. For example, in a 2x2 mixed-factorial ANOVA used in training evaluation, we might wish to know whether average change in assessment scores from pre-test to post-test is conditional on the training condition (training vs. control). Presumably, we would hope that the average change in assessment scores from pre-test to post-test is larger in the training condition than the control condition. For this chapter, we will assume that there is a balanced design, which means that the same number of employees participated in the training and control conditions and that each person completed the pre-test and post-test; in other words, there is an equal number of observations in the four cells associated with this 2x2 mixed factorial design. When there is a balanced design, a 2x2 mixed-factorial ANOVA with Type I sum of squares (see below) will be statistically equivalent to an independent-samples t-test in which the continuous (interval, ratio) outcome (dependent) variable represents the difference between the two levels of the within-subjects factor (e.g., post-test minus pre-test) and the predictor (independent) variable is the between-subjects factor (e.g., condition: old training program, new training program). I argue that not only is the independent-samples t-test easier to implement in this context, but its results are often easier to interpret. As such, in the main portion of this chapter, we will learn how to apply an independent-samples t-test with a difference scores as an outcome variable in order to evaluate a 2x2 mixed-factorial balanced design; for a more in depth introduction to the independent-samples t-test, please check this previous chapter. Finally, as noted previously, in the chapter supplement, you will have an opportunity to learn how to estimate a mixed-factorial ANOVA as well as random-coefficients model, both of which are statistically equivalent to the independent-samples t-test under these conditions. Note on Type I, II, and III Sum of Squares. The default approach to calculating sum of squares in most R ANOVA functions is to use what is often referred to as Type I sum of squares. Type II and Type III sum of squares refer to the other approaches to calculating sum of squares for an ANOVA. Results are typically similar between Type I, Type II, and Type III approaches when the data are balanced across groups designated by the factors (i.e., predictor variables). To use Type II and Type III sum of squares, I recommend that you use the Anova function from the car package. If you wish to learn more about Type I, II, and III sum of squares (SS), I recommend checking out this link. 72.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a conventional independent-samples t-test include: (a) The outcome (dependent, response) variable has a univariate normal distribution in each of the two underlying populations (e.g., groups, conditions), which correspond to the two levels/categories of the independent variable; (b) The variances of the outcome (dependent, response) variable are equal across the two populations (e.g., groups, conditions). 72.1.1.1 Sample Write-Up Coming soon… 72.2 Tutorial This chapter’s tutorial demonstrates how to evaluate a pre-test/post-test with control group design using an independent-samples t-test with a difference score outcome variable in R. 72.2.1 Video Tutorial Link to the video tutorial: https://youtu.be/7xaDlTeNvYk 72.2.2 Functions &amp; Packages Introduced Function Package Plot lessR ttest lessR BarChart lessR 72.2.3 Initial Steps {#initsteps_mixedfactorial}} If you haven’t already, save the file called “TrainingEvaluation_PrePostControl.csv” into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory…. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called “TrainingEvaluation_PrePostControl.csv” using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham, Hester, and Bryan 2024). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;TrainingEvaluation_PrePostControl.csv&quot;) ## Rows: 50 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (3): EmpID, PreTest, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PreTest&quot; &quot;PostTest&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 50 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 × 4 ## EmpID Condition PreTest PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 New 41 66 ## 2 27 New 56 74 ## 3 28 New 50 62 ## 4 29 New 45 84 ## 5 30 New 41 78 ## 6 31 New 48 73 There are 50 cases (i.e., employees) and 4 variables in the df data frame: EmpID (unique identifier for employees), Condition (training condition: New = new training program, Old = old training program), PreTest (pre-training scores on training assessment, ranging from 1-100), and PostTest (post-training scores on training assessment, ranging from 1-100). Regarding participation in the training conditions, 25 employees participated in the old training program, and 25 employees participated in the new training program. Per the output of the str (structure) function above, all of the variables except for Condition are of type numeric (continuous: interval, ratio), and Condition is of type character (categorical: nominal). 72.2.4 Evaluate a Pre-Test/Post-Test with Control Group Design In this example, we will evaluate a balanced 2x2 mixed-factorial training evaluation design (i.e., pre-test/post-test with control group design), meaning there are equal numbers of cases in each of the four cells. Specifically, our within-subjects factor includes the different times of assessment (PreTest, PostTest), and our between-subjects factor is the Condition variable that contains two levels (New, Old). Our goal is to investigate whether those trainees who participated in the new training condition showed significantly greater increases in assessment scores from before to after training, as compared to those in the old training condition. 72.2.4.1 Create a Difference Score Variable As an initial step, we must create a difference score variable. To do so, we must first specify the name of the existing data frame we wish to add this new variable to (df), followed by the $ symbol and what we would like to name this new difference variable; in this example, I name the new variable diff. Second, type the &lt;- symbol to indicate that you are creating and naming a new object. Third, write simple arithmetic equation wherein PreTest scores are subtracted from PostTest scores, which means that a positive difference will indicate an increase in assessment scores from PreTest to PostTest. # Create difference score variable &amp; add to data frame object df$diff &lt;- df$PostTest - df$PreTest We will use this new difference score variable diff as our outcome variable in an independent-samples t-test. In a sense, we have reduced our within-subjects (repeated-measures) factor into a single vector of values. 72.2.4.2 Visualize the Distribution of the Difference Score Variable Before estimating an independent-samples t-test with the diff variable, let’s visually inspect the distribution and variance of this variable at each level of the between-subjects variable (Condition). We will use the Plot function from lessR. If you haven’t already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) We will use the Plot function from lessR to visually inspect the distribution of the diff (difference score) variable in each level of the Condition variable. To do so, type the name of the Plot function. As the first argument within the function, type the name of the continuous variable of interest (diff). As the second argument, type data= followed by the name of the data frame (df). As the third argument, type by1= followed by the name of the grouping variable (Condition), as this will create the trellis/lattice structure wherein two VBS plots will be created (one for each condition/group). # VBS plots of the diff variable distributions by Condition Plot(diff, # outcome variable data=df, # data frame object by1=Condition) # grouping variable ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(diff, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(diff, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## ttest(diff ~ Condition) # Add the data parameter if not the d data frame ## diff ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 22.04 11.24 -4.00 21.00 40.00 ## Old 25 0 13.28 12.79 -14.00 13.00 35.00 ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## New 3 21 25 ## Old 3 3 28 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.58 size of plotted points ## out_size: 0.81 size of plotted outlier points ## jitter_y: 1.00 random vertical movement of points ## jitter_x: 0.24 random horizontal movement of points ## bw: 5.23 set bandwidth higher for smoother edges Based on the violin-box-scatter (VBS) plot depicted in the output from the Plot function, note that (at least visually) the distributions both seem to be roughly normal, and the the variances seem to be roughly equal. These are by no means stringent tests of the statistical assumptions, but they provide us with a cursory understanding of the shape of the distributions and the variances for each of the conditions. 72.2.4.3 Estimate an Independent-Samples t-test with Difference Score Outcome Variable Now we’re ready to plug the difference score variable (diff) into an independent-samples t-test model as the outcome variable. The between-subjects factor that categorizes trainees based on which condition (Condition) they participated in (New, Old) serves as the predictor variable. As we did above, we will use the ttest function from lessR. To begin, type the name of the ttest function. As the first argument in the parentheses, specify the statistical model. To do so, type the name of the outcome variable (diff) to the left of the ~ operator and the name of the predictor variable (Condition) to the right of the ~ operator. For the second argument, use data= to specify the name of the data frame where the outcome and predictor variables are located (df). For the third argument, type paired=FALSE to inform the function that the data are not paired (i.e., you are not requesting a paired-samples t-test). # Test of the implied interaction between condition and test time # Independent-samples t-test with difference score as outcome variable ttest(diff ~ Condition, # model data=df, # data frame object paired=FALSE) # request independent-samples t-test ## ## Compare diff across Condition with levels New and Old ## Response Variable: diff, diff ## Grouping Variable: Condition, ## ## ## ------ Describe ------ ## ## diff for Condition New: n.miss = 0, n = 25, mean = 22.040, sd = 11.238 ## diff for Condition Old: n.miss = 0, n = 25, mean = 13.280, sd = 12.785 ## ## Mean Difference of diff: 8.760 ## ## Weighted Average Standard Deviation: 12.036 ## ## ## ------ Assumptions ------ ## ## Note: These hypothesis tests can perform poorly, and the ## t-test is typically robust to violations of assumptions. ## Use as heuristic guides instead of interpreting literally. ## ## Null hypothesis, for each group, is a normal distribution of diff. ## Group New Shapiro-Wilk normality test: W = 0.964, p-value = 0.502 ## Group Old Shapiro-Wilk normality test: W = 0.954, p-value = 0.309 ## ## Null hypothesis is equal variances of diff, homogeneous. ## Variance Ratio test: F = 163.460/126.290 = 1.294, df = 24;24, p-value = 0.532 ## Levene&#39;s test, Brown-Forsythe: t = -0.983, df = 48, p-value = 0.331 ## ## ## ------ Infer ------ ## ## --- Assume equal population variances of diff for each Condition ## ## t-cutoff for 95% range of variation: tcut = 2.011 ## Standard Error of Mean Difference: SE = 3.404 ## ## Hypothesis Test of 0 Mean Diff: t-value = 2.573, df = 48, p-value = 0.013 ## ## Margin of Error for 95% Confidence Level: 6.845 ## 95% Confidence Interval for Mean Difference: 1.915 to 15.605 ## ## ## --- Do not assume equal population variances of diff for each Condition ## ## t-cutoff: tcut = 2.011 ## Standard Error of Mean Difference: SE = 3.404 ## ## Hypothesis Test of 0 Mean Diff: t = 2.573, df = 47.223, p-value = 0.013 ## ## Margin of Error for 95% Confidence Level: 6.848 ## 95% Confidence Interval for Mean Difference: 1.912 to 15.608 ## ## ## ------ Effect Size ------ ## ## --- Assume equal population variances of diff for each Condition ## ## Standardized Mean Difference of diff, Cohen&#39;s d: 0.728 ## ## ## ------ Practical Importance ------ ## ## Minimum Mean Difference of practical importance: mmd ## Minimum Standardized Mean Difference of practical importance: msmd ## Neither value specified, so no analysis ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for Condition New: 6.726 ## Density bandwidth for Condition Old: 7.655 As you can see in the output, the tests of normality (Shapiro-Wilk normality test) and equal variances (Levene’s test) that appear in the Assumptions section indicate that, statistically, there are not significant departures from normality, and there is not evidence of unequal variances. For more information on these tests, please see the earlier chapter on independent-samples t-tests. In the Inference section of the output, the results of the independent-samples t-test (t = 2.573, df = 48, p = .013) indicate that there is evidence that the mean difference scores differ between conditions, as evidence by a p-value that is less than our conventional two-tailed alpha cutoff of .05. Thus, we reject the null hypothesis and conclude that the two means are different from one another. A look at the Description section tells us that the average difference (PostTest minus PreTest) for those who participated in the New training condition is 22.04, and the average difference for those who participated in the Old training condition is 13.28. When considered in tandem with the significant t-test, this indicates that trainees who participated in the new training program showed significantly greater increases in their assessment scores from before to after training than those trainees who participated in the old training program. Essentially, we are interpreting what would be a significant interaction from a statistically equivalent 2x2 mixed-factorial ANOVA model. Because we found a statistically significant difference in means, let’s interpret the effect size (Cohen’s d) as an indicator of practical significance. In the output, d is equal to .73, which is considered to be a medium-large effect, according to conventional rules-of-thumb (see table below). Please note that typically we only interpret practical significance when a difference between means is statistically significant. Cohen’s d Description \\(\\ge .20\\) Small \\(\\ge .50\\) Medium \\(\\ge .80\\) Large When we find a statistically significant difference between two means based on an independent-samples t-test, it is customary to present the two means in a bar chart. We will use the BarChart function from lessR to do so. To begin, type the name of the BarChart function. As the first argument, specify x= followed by the name of the categorical (nominal, ordinal) predictor variable, which in this example is Condition. As the second argument, specify y= followed by the name of the continuous (interval, ratio) outcome variable, which in this example is diff. As the third, argument specify stat=\"mean\" to request that the mean of the y= variable be computed by levels of the x= variable. As the fourth argument, specify data= followed by the name of the data frame object (df) to which the x= and y= variables belong. As the fifth argument, use the xlab= to provide a new x-axis label, which for our example will be “Training Condition”. As the sixth argument, use the ylab= to provide a new y-axis label, which for our example will be “Average Difference Score”. # Create bar chart BarChart(x=Condition, # categorical predictor variable y=diff, # continuous outcome variable stat=&quot;mean&quot;, # request computation of means data=df, # data frame object xlab=&quot;Training Condition&quot;, # x-axis label ylab=&quot;Average Difference Score&quot;) # y-axis label ## diff ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 22.04 11.24 -4.00 21.00 40.00 ## Old 25 0 13.28 12.79 -14.00 13.00 35.00 ## &gt;&gt;&gt; Suggestions ## Plot(diff, Condition) # lollipop plot ## ## Plotted Values ## -------------- ## New Old ## 22.040 13.280 As noted above, our statistically significant finding from the independent-samples t-test with difference score outcome variable is the equivalent of finding a statistically significant interaction term as part of a 2x2 mixed-factorial ANOVA model. Specifically, our finding implies a statistically significant interaction term between the between-subjects Condition variable and within-subjects Test variable. When a significant interaction is found, it is customary to investigate the simple effects. Given our 2x2 design, our job examining the simple effects is made easier. Next up, we’ll begin with the evaluating the between-subjects simple effects. 72.2.4.4 Evaluate the Between-Subjects Simple Effects Given that our data are from 2x2 mixed-factorial design, we have two between-subjects simple effects to evaluate: (a) difference between the New and Old conditions’ PreTest means, and (b) difference between the New and Old conditions’ PostTest means. We’ll begin with the evaluating whether there is a difference between the training and control conditions’ PreTest means. In general, when pre-test data are available, it’s a good idea to determine whether the training and control conditions have different means on the pre-test (PreTest) variable, which were collected prior to individuals’ participation in their respective training conditions. Ideally, our hope is that the average pre-test scores can be treated as equivalent, as this would indicate that the trainees started at about the same place prior to training, suggesting that they are perhaps equivalent groups. Before applying the ttest function lessR, we will use the Plot function from lessR to visually inspect the distribution of the PreTest variable in each level of the Condition variable. To do so, type the name of the Plot function. As the first argument within the function, type the name of the continuous variable of interest (PreTest). As the second argument, type data= followed by the name of the data frame (df). As the third argument, type by1= followed by the name of the grouping variable (Condition), as this will create the trellis/lattice structure wherein two VBS plots will be created (one for each condition/group). # VBS plots of the PreTest distributions by Condition Plot(PreTest, # outcome variable data=df, # data frame object by1=Condition) # grouping variable ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(PreTest, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(PreTest, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## ttest(PreTest ~ Condition) # Add the data parameter if not the d data frame ## PreTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 50.32 6.72 41.00 49.00 64.00 ## Old 25 0 48.04 6.60 36.00 49.00 60.00 ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## New 3 43 ## Old 4 51 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.55 size of plotted points ## out_size: 0.80 size of plotted outlier points ## jitter_y: 1.00 random vertical movement of points ## jitter_x: 0.28 random horizontal movement of points ## bw: 2.69 set bandwidth higher for smoother edges Based on the violin-box-scatter (VBS) plot depicted in the output from the Plot function, note that (at least visually) the distributions both seem to be roughly normally, and the the variances seem to be roughly equal. Like before, we will estimate this independent-samples t-test using the ttest function from the lessR package. In this model, however, we include the PreTest variable as the outcome variable in our model, but other than that, the arguments will remain the same as our arguments for the previous independent-samples t-test involving the difference variable (diff). # Between-subjects simple effect: # Independent-samples t-test with PreTest as outcome variable ttest(PreTest ~ Condition, # model data=df, # data frame object paired=FALSE) # request independent-samples t-test ## ## Compare PreTest across Condition with levels New and Old ## Response Variable: PreTest, PreTest ## Grouping Variable: Condition, ## ## ## ------ Describe ------ ## ## PreTest for Condition New: n.miss = 0, n = 25, mean = 50.320, sd = 6.719 ## PreTest for Condition Old: n.miss = 0, n = 25, mean = 48.040, sd = 6.598 ## ## Mean Difference of PreTest: 2.280 ## ## Weighted Average Standard Deviation: 6.659 ## ## ## ------ Assumptions ------ ## ## Note: These hypothesis tests can perform poorly, and the ## t-test is typically robust to violations of assumptions. ## Use as heuristic guides instead of interpreting literally. ## ## Null hypothesis, for each group, is a normal distribution of PreTest. ## Group New Shapiro-Wilk normality test: W = 0.947, p-value = 0.220 ## Group Old Shapiro-Wilk normality test: W = 0.965, p-value = 0.521 ## ## Null hypothesis is equal variances of PreTest, homogeneous. ## Variance Ratio test: F = 45.143/43.540 = 1.037, df = 24;24, p-value = 0.930 ## Levene&#39;s test, Brown-Forsythe: t = 0.241, df = 48, p-value = 0.811 ## ## ## ------ Infer ------ ## ## --- Assume equal population variances of PreTest for each Condition ## ## t-cutoff for 95% range of variation: tcut = 2.011 ## Standard Error of Mean Difference: SE = 1.883 ## ## Hypothesis Test of 0 Mean Diff: t-value = 1.211, df = 48, p-value = 0.232 ## ## Margin of Error for 95% Confidence Level: 3.787 ## 95% Confidence Interval for Mean Difference: -1.507 to 6.067 ## ## ## --- Do not assume equal population variances of PreTest for each Condition ## ## t-cutoff: tcut = 2.011 ## Standard Error of Mean Difference: SE = 1.883 ## ## Hypothesis Test of 0 Mean Diff: t = 1.211, df = 47.984, p-value = 0.232 ## ## Margin of Error for 95% Confidence Level: 3.787 ## 95% Confidence Interval for Mean Difference: -1.507 to 6.067 ## ## ## ------ Effect Size ------ ## ## --- Assume equal population variances of PreTest for each Condition ## ## Standardized Mean Difference of PreTest, Cohen&#39;s d: 0.342 ## ## ## ------ Practical Importance ------ ## ## Minimum Mean Difference of practical importance: mmd ## Minimum Standardized Mean Difference of practical importance: msmd ## Neither value specified, so no analysis ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for Condition New: 4.012 ## Density bandwidth for Condition Old: 3.950 As you can see in the output, the tests of normality (Shapiro-Wilk normality test) and equal variances (Levene’s test) indicate that, statistically, there are neither significant departures from normality nor evidence of unequal variances. The results of the independent-samples t-test itself (t = 1.211, df = 48, p = .232) indicate that there is no evidence that the mean pre-test scores differ between conditions, as evidenced by a p-value that is equal to or greater than our conventional two-tailed alpha cutoff of .05. Thus, we fail to reject the null hypothesis and conclude that the two means equal. This finding suggests that the New and Old conditions were roughly equivalent in terms of PreTest scores prior to engaging in training. Next, we will evaluate whether the means on the PostTest variable differ significantly between the New and Old conditions. Before doing so, we will once again use the Plot function from lessR to visually inspect the distribution of the PostTest variable in each level of the Condition variable. To do so, type the name of the Plot function. As the first argument within the function, type the name of the continuous variable of interest (PostTest). As the second argument, type data= followed by the name of the data frame (df). As the third argument, type by1= followed by the name of the grouping variable (Condition), as this will create the trellis/lattice structure wherein two VBS plots will be created (one for each condition/group). # VBS plots of the PostTest distributions by Condition Plot(PostTest, # outcome variable data=df, # data frame object by1=Condition) # grouping variable ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(PostTest, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## ttest(PostTest ~ Condition) # Add the data parameter if not the d data frame ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## Old 25 0 61.32 9.15 42.00 61.00 79.00 ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## New 4 73 ## Old 4 60 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.55 size of plotted points ## out_size: 0.80 size of plotted outlier points ## jitter_y: 1.00 random vertical movement of points ## jitter_x: 0.28 random horizontal movement of points ## bw: 4.43 set bandwidth higher for smoother edges The two violin-box-scatter (VBS) plots visually suggest that the distributions are roughly normal and the varainces are roughly equal. Now we are ready to apply an independent-samples t-test, except this time, we will specify the PostTest variable as the outcome variable. # Between-subjects simple effect: # Independent-samples t-test with PostTest as outcome variable ttest(PostTest ~ Condition, # model data=df, # data frame object paired=FALSE) # request independent-samples t-test ## ## Compare PostTest across Condition with levels New and Old ## Response Variable: PostTest, PostTest ## Grouping Variable: Condition, ## ## ## ------ Describe ------ ## ## PostTest for Condition New: n.miss = 0, n = 25, mean = 72.360, sd = 6.975 ## PostTest for Condition Old: n.miss = 0, n = 25, mean = 61.320, sd = 9.150 ## ## Mean Difference of PostTest: 11.040 ## ## Weighted Average Standard Deviation: 8.136 ## ## ## ------ Assumptions ------ ## ## Note: These hypothesis tests can perform poorly, and the ## t-test is typically robust to violations of assumptions. ## Use as heuristic guides instead of interpreting literally. ## ## Null hypothesis, for each group, is a normal distribution of PostTest. ## Group New Shapiro-Wilk normality test: W = 0.950, p-value = 0.253 ## Group Old Shapiro-Wilk normality test: W = 0.969, p-value = 0.621 ## ## Null hypothesis is equal variances of PostTest, homogeneous. ## Variance Ratio test: F = 83.727/48.657 = 1.721, df = 24;24, p-value = 0.191 ## Levene&#39;s test, Brown-Forsythe: t = -0.955, df = 48, p-value = 0.344 ## ## ## ------ Infer ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## t-cutoff for 95% range of variation: tcut = 2.011 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t-value = 4.798, df = 48, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.627 ## 95% Confidence Interval for Mean Difference: 6.413 to 15.667 ## ## ## --- Do not assume equal population variances of PostTest for each Condition ## ## t-cutoff: tcut = 2.014 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t = 4.798, df = 44.852, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.635 ## 95% Confidence Interval for Mean Difference: 6.405 to 15.675 ## ## ## ------ Effect Size ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## Standardized Mean Difference of PostTest, Cohen&#39;s d: 1.357 ## ## ## ------ Practical Importance ------ ## ## Minimum Mean Difference of practical importance: mmd ## Minimum Standardized Mean Difference of practical importance: msmd ## Neither value specified, so no analysis ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for Condition New: 4.175 ## Density bandwidth for Condition Old: 5.464 We find that the scores on the PostTest are statistically significant higher for those in the New training condition (72.360) than those in the Old training condition (64.320) (t = 4.798, df = 48, p &lt; .001), and this effect is very large (Cohen’s d = 1.357). In sum, the between-subjects simple effects help us understand the nature of the implied interaction we observed in our focal independent-samples t-test with difference score outcome variable. Putting everything together to this point, we have found that individuals started in approximately the same place in terms of pre-test scores prior to participating in their respective training programs, those who participated in the New training program showed, on average, a greater increase in scores from pre-test to post-test than those who participated in the Old training program, and those who participated in the New training program showed, on average, higher post-test scores than those who participated in the Old training program. 72.2.4.5 Evaluate the Within-Subjects Simple Effects To compute the within-subjects simple effects, we will apply paired-samples t-tests to each level of the between-subjects factor (Condition). To do so, we will use the ttest function from lessR once more. For more information about a paired-samples t-test, please check the earlier chapter. Let’s begin by evaluating whether the mean of the differences resulting from the PreTest and PostTest variables differs significantly from zero for those individuals who participated in the New training condition. In this context, we can think of our PreTest variable as representing the first level of within-subjects variable representing test time, and the PostTest variable as representing the second level of within-subjects variable representing test time. As the first argument in the ttest function, specify the PreTest variable. As the second argument, specify the PostTest variable. As the third argument, specify data= followed by the name of data frame object (df). As the fourth argument, specify paired=TRUE to request an paired-samples t-test. As the fifth argument, specify rows=(Condition==\"New\") to filter the data frame down to just those individuals who participated in the New training condition. # Within-subjects simple effect: # Paired-samples t-test for just New training condition ttest(PreTest, # first level of within-subjects variable PostTest, # second level of within-subjects variable data=df, # data frame object paired=TRUE, # paired-samples t-test rows=(Condition==&quot;New&quot;)) # first level of between-subjects variable ## ## ## ------ Describe ------ ## ## Difference: n.miss = 0, n = 25, mean = 22.040, sd = 11.238 ## ## ## ------ Normality Assumption ------ ## ## Null hypothesis is a normal distribution of Difference. ## Shapiro-Wilk normality test: W = 0.9641, p-value = 0.502 ## ## ## ------ Infer ------ ## ## t-cutoff for 95% range of variation: tcut = 2.064 ## Standard Error of Mean: SE = 2.248 ## ## Hypothesized Value H0: mu = 0 ## Hypothesis Test of Mean: t-value = 9.806, df = 24, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.639 ## 95% Confidence Interval for Mean: 17.401 to 26.679 ## ## ## ------ Effect Size ------ ## ## Distance of sample mean from hypothesized: 22.040 ## Standardized Distance, Cohen&#39;s d: 1.961 ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for 6.726 Let’s repeat the process for the those individuals who participated in the Old training condition. # Within-subjects simple effect: # Paired-samples t-test for just Old training condition ttest(PreTest, # first level of within-subjects variable PostTest, # second level of within-subjects variable data=df, # data frame object paired=TRUE, # paired-samples t-test rows=(Condition==&quot;Old&quot;)) # second level of between-subjects variable ## ## ## ------ Describe ------ ## ## Difference: n.miss = 0, n = 25, mean = 13.280, sd = 12.785 ## ## ## ------ Normality Assumption ------ ## ## Null hypothesis is a normal distribution of Difference. ## Shapiro-Wilk normality test: W = 0.9541, p-value = 0.309 ## ## ## ------ Infer ------ ## ## t-cutoff for 95% range of variation: tcut = 2.064 ## Standard Error of Mean: SE = 2.557 ## ## Hypothesized Value H0: mu = 0 ## Hypothesis Test of Mean: t-value = 5.194, df = 24, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 5.277 ## 95% Confidence Interval for Mean: 8.003 to 18.557 ## ## ## ------ Effect Size ------ ## ## Distance of sample mean from hypothesized: 13.280 ## Standardized Distance, Cohen&#39;s d: 1.039 ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for 7.655 Both paired-samples t-tests met the normality assumption. Further, the within-subjects simple effects gleaned from these paired-samples t-tests showed a statistically significant mean of the differences (PostTest minus PreTest); however, the mean of the differences for those who participated in the New training condition (Mean of the Differences = 22.040, t = 9.806, df = 24, p &lt; .001, Cohen’s d = 1.961) was notably larger than the mean of the differences for those who participated in the Old training condition (Mean of the Differences = 13.280, t = 5.194, df = 24, p &lt; .001, Cohen’s d = 1.039). This further elucidates what we observed in our initial independent-samples t-test in which the difference score variable served as the outcome; specifically, we now know that the average increase in test scores for those in the New training program is statistically significantly different from zero and very large, whereas the average increase in test scores for those in the Old training program is statistically significant and large – but not nearly as large in a relative sense. Together, the between-subjects and within-subjects simple effects help us understand the nature of the implied interaction we observed in our focal independent-samples t-test with difference score outcome variable. In sum, we found that (a) Individuals started in approximately the same place in terms of pre-test scores prior to participating in their respective training programs; (b) Those who participated in the New training program showed, on average, a greater increase in scores from pre-test to post-test than those who participated in the Old training program; more specifically, those who participated in both the New and Old training programs showed large increases in test scores, with those in the New training programs showing what can be described as a much larger increase; (c) and those who participated in the New training program showed, on average, higher post-test scores than those who participated in the Old training program, which indicates that those in the New training program ended up with higher post-test scores. 72.2.5 Summary In this chapter, we learned how to estimate an independent-samples t-test with a difference score outcome variable to evaluate a 2x2 mixed-factorial design like a pre-test/post-test with control group design; importantly, this approach is statistically equivalent to a 2x2 mixed-factorial ANOVA when there is a balanced design. We also learned how to estimate the between-subjects and within-subjects simple effects. 72.3 Chapter Supplement In this chapter supplement, we will learn how, under these conditions, a simple linear regression model with difference score outcome variable, biserial correlation with difference score outcome variable, 2x2 mixed-factorial ANOVA model, and random-coefficients model will yield results that are statistically equivalent to the independent-samples t-test with difference score outcome variable that was introduced in the main portion of this chapter. 72.3.1 Functions &amp; Packages Introduced Function Package factor base R lm base R summary base R as.numeric base R cor.test base R pivot_longer tidyr aov base R model.tables base R t.test base R interaction.plot base R lme nlme avova base R 72.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;TrainingEvaluation_PrePostControl.csv&quot;) ## Rows: 50 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Condition ## dbl (3): EmpID, PreTest, PostTest ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PreTest&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(df) ## spc_tbl_ [50 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:50] 26 27 28 29 30 31 32 33 34 35 ... ## $ Condition: chr [1:50] &quot;New&quot; &quot;New&quot; &quot;New&quot; &quot;New&quot; ... ## $ PreTest : num [1:50] 41 56 50 45 41 48 64 46 47 47 ... ## $ PostTest : num [1:50] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PreTest = col_double(), ## .. PostTest = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # View first 6 rows of data frame head(df) ## # A tibble: 6 × 4 ## EmpID Condition PreTest PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 New 41 66 ## 2 27 New 56 74 ## 3 28 New 50 62 ## 4 29 New 45 84 ## 5 30 New 41 78 ## 6 31 New 48 73 As an initial step, we must create a difference score variable. To do so, we must first specify the name of the existing data frame we wish to add this new variable to (df), followed by the $ symbol and what we would like to name this new difference variable; in this example, I name the new variable diff. Second, type the &lt;- symbol to indicate that you are creating and naming a new object. Third, write simple arithmetic equation wherein PreTest scores are subtracted from PostTest scores, which means that a positive difference will indicate an increase in assessment scores from PreTest to PostTest. # Create difference score variable &amp; add to data frame object df$diff &lt;- df$PostTest - df$PreTest 72.3.3 Estimating a Simple Linear Regression Model with a Difference Score Outcome Variable Prior to estimating a simple linear regression model, let’s re-order the default levels of the Condition variable from alphabetical (“New”, “Old”) to “Old” preceding “New”. This will ensure that the sign of the effect is comparable to what we found with the independent-samples t-test with a difference score outcome variable. To do so, we’ll convert the Condition variable to type factor using the factor function from base R. As the first argument, we’ll specify the name of the data frame object from the main portion of the chapter (df) followed by the $ operator and the name of the Condition variable. As the second argument, we’ll specify the levels= argument followed by the c function; as the two arguments within the c function, we’ll specify “Old” as the first argument and “New” as the second argument. # Re-order Condition variable and convert to a factor df$Condition &lt;- factor(df$Condition, levels=c(&quot;Old&quot;, &quot;New&quot;)) To estimate a simple linear regression model, we will use the lm function from base R. We’ll assign the model to an object we name lm_mod using the &lt;- assignment operator. As the first argument in the lm function, we’ll specify our model, which is the diff (difference score variable) regressed on the Condition variable. As the second argument, we’ll specify data= followed by the name of the data frame object to which the variables in the model belong (df). Finally, we will print a summary of the results using the summary function from base R with the name of our model object as the sole parenthetical argument (lm_mod). # Estimate simple linear regression model lm_mod &lt;- lm(diff ~ Condition, data=df) # Print a summary of the model results summary(lm_mod) ## ## Call: ## lm(formula = diff ~ Condition, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.28 -10.22 -0.66 9.47 21.72 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.280 2.407 5.517 0.00000136 *** ## ConditionNew 8.760 3.404 2.573 0.0132 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.04 on 48 degrees of freedom ## Multiple R-squared: 0.1212, Adjusted R-squared: 0.1029 ## F-statistic: 6.621 on 1 and 48 DF, p-value: 0.01322 In the output, the unstandardized regression coefficient associated with the Condition variable is 8.760, the standard error is 3.404, the t-value is 2.573, the degrees of freedom (df) is equal to 48, and the p-value is .0132, all of which are equal to the corresponding statistics found using the independent-samples t-test. In fact, the coefficient value of 8.760 is the mean difference between the two “Old” and “New” conditions that we observed in the independent-samples t-test with a difference score outcome variable. Finally, please note that unadjusted R-squared (R2) for the model is .1212, which we will revisit in the context of the biserial correlation in the following section. 72.3.4 Estimating a Biserial Correlation with a Difference Score Outcome Variable Under these conditions, a biserial correlation is also statistically equivalent to an independent-samples t-test with a difference score outcome variable. Before we estimate the biserial correlation, however, we need to convert the Condition variable to type numeric, as the correlation function we will use expects numeric variables. To do so, we will use the as.numeric function from base R. Let’s name this converted variable Condition_numeric and attach it to our df data frame object using the $ operator. The &lt;- assignment operator will allow us to perform this assignment. To the right of the &lt;- assignment operator, let’s type the name of the as.numeric function, and as the sole parenthetical argument, we will specify the data frame df followed by the $ operator and the Condition variable. # Convert Condition variable to numeric and assign to new variable df$Condition_numeric &lt;- as.numeric(df$Condition) We will estimate the biserial correlation using the cor.test function base R. As the first argument, we’ll specify the data frame df followed by the $ operator and the Condition_numeric variable. As the second argument, we’ll specify the data frame df followed by the $ operator and the diff (difference score) variable. # Estimate biserial correlation cor.test(df$Condition_numeric, df$diff) ## ## Pearson&#39;s product-moment correlation ## ## data: df$Condition_numeric and df$diff ## t = 2.5731, df = 48, p-value = 0.01322 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.07730703 0.57115937 ## sample estimates: ## cor ## 0.3481629 Like the independent-samples t-test and simple linear regression model with a difference score outcome variable, the following model statistics are the same for the biserial correlation: t = 2.573, df = 48, and p = .0132. Further, the correlation coefficient is .3481629, and if we square that value, we get an unadjusted R-squared (R2) equal to .1212, which is the same R-squared value as our simple linear regression model. Thus, under these conditions, the biserial correlation is also statistically equivalent to the independent-samples t-test and simple linear regression model with a difference score outcome variable. Before moving on, let’s remove the Condition_numeric variable from our df data frame object to avoid confusion when we proceed to the following section. # Remove Condition_numeric variable df$Condition_numeric &lt;- NULL 72.3.5 Estimating a 2x2 Mixed-Factorial ANOVA Model When there is a balanced design, a 2x2 mixed-factorial ANOVA model will also be statistically equivalent to a independent-samples t-test, simple linear regression model, and biserial correlation with a difference score outcome variable. To estimate the 2x2 mixed-factorial ANOVA model, we will use the same data frame object as in the main portion of the chapter, except we will need to perform data manipulation to pivot the data frame from wide to long format, and we will need to convert some of the variables to factors and re-order their levels. The data frame from the main portion of the chapter (df) is in wide format, as each substantive variable has its own column. To restructure the data from wide to long format, we will use the pivot_longer function from the tidyr package. Along with readr and dplyr (as well as other useful packages), the tidyr package is part of the tidyverse of packages. Let’s begin by installing and accessing the tidyr package so that we can use the pivot_longer function. # Install tidyr package if you haven&#39;t already install.packages(&quot;tidyr&quot;) # Access tidyr package library(tidyr) Note. If you received an error when attempting to access the tidyr package using the library function, you may need to install the following packages using the install.packages function: rlang and glue. Alternatively, you may try installing the entire tidyverse package. Now that we’ve accessed the tidyr package, I will demonstrate how to manipulate the data from wide-to-long format using the pipe operator (%&gt;%). The pipe operator comes from a package called magrittr, on which the tidyr package is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemund’s chapter on pipes. Using the pipe (%&gt;%) operator technique, let’s apply the pivot_longer function to manipulate the df data frame object from wide format to long format. We can specify the wide-to-long manipulation as follows. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object df_long. Use the &lt;- operator to assign the new long-form data frame object to the object named df_long in the step above. Type the name of the original data frame object (df), followed by the pipe (%&gt;%) operator. Type the name of the pivot_longer function. As the first argument in the pivot_longer function, type cols= followed by the c (combine) function. As the arguments within the c function, list the names of the variables that you wish to pivot from separate variables (wide) to levels or categories of a new variable, effectively stacking them vertically. In this example, let’s list the names of pre-test and post-test variables: PreTest and PostTest. As the second argument in the pivot_longer function, type names_to= followed by what you would like to name the new stacked variable (see previous) created from the four survey measure variables. Let’s call the new variable containing the names of the pre-test and post-test variables the following: \"Test\". As the third argument in the pivot_longer function, type values_to= followed by what you would like to name the new variable that contains the scores for the two variables that are now stacked vertically for each case. Let’s call the new variable containing the scores on the two variables the following: \"Score\". # Apply pivot_longer function to restructure data to long format (using pipe) df_long &lt;- df %&gt;% pivot_longer(cols=c(PreTest, PostTest), names_to=&quot;Test&quot;, values_to=&quot;Score&quot;) # Print first 12 rows of new data frame head(df_long, n=12) ## # A tibble: 12 × 5 ## EmpID Condition diff Test Score ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 New 25 PreTest 41 ## 2 26 New 25 PostTest 66 ## 3 27 New 18 PreTest 56 ## 4 27 New 18 PostTest 74 ## 5 28 New 12 PreTest 50 ## 6 28 New 12 PostTest 62 ## 7 29 New 39 PreTest 45 ## 8 29 New 39 PostTest 84 ## 9 30 New 37 PreTest 41 ## 10 30 New 37 PostTest 78 ## 11 31 New 25 PreTest 48 ## 12 31 New 25 PostTest 73 As you can see, in the output, each employee now has two rows of data – one row for each test measure and the associated score. The giveaway is that each respondent’s unique EmpID value is repeated two times. Because we no longer need the diff (difference score variable), let’s remove it. # Remove diff (difference score) variable df_long$diff &lt;- NULL We also need to convert the EmpID variable to type factor. To do so, we’ll use the factor function from base R. # Convert EmpID variable to a factor df_long$EmpID &lt;- factor(df_long$EmpID) Finally, we will override the default ordering of the “PostTest” and “PreTest” levels of the Test variable from alphabetical to “PreTest” preceding “PostTest”. In doing so, we will also convert the Test variable to type factor using the factor function from base R. We will also override the default ordering of the Condition variable’s from alphabetical to “Old” preceding “New”. # Re-order Test variable and convert to a factor df_long$Test &lt;- factor(df_long$Test, levels=c(&quot;PreTest&quot;, &quot;PostTest&quot;)) # Re-order Condition variable and convert to a factor df_long$Condition &lt;- factor(df_long$Condition, levels=c(&quot;Old&quot;, &quot;New&quot;)) We will use the aov function from base R to estimate our ANOVA. Specifically, we will estimate a 2x2 mixed-factorial ANOVA because there is a within-subjects variable (Test) with two levels (“PreTest”, “PostTest”) and a between-subjects variable (Condition) with two levels (“Old”, “New”). We will begin by creating a model object name of our choosing (aov_mod), and we will use the &lt;- assignment operator to assign our fitted model to the object. To the right of the &lt;- assignment operator, we will type the name of the aov function. As the first argument in the function, we will specify the name of our outcome variable (Score) followed by the ~ operator, and to the right of the ~ operator, we will specify the name of our between-subjects variable (Condition) followed by the * interaction operator and then the name of the within-subjects variable (Test); after the within-subjects variable, we will type the + operator followed by the Error function. Within the Error function parentheses, we specify the name of the grouping variable (EmpID, which is the unique employee identifier), followed by a forward slash / and the name of the within-subjects variable (Test); More specifically, the Error function indicates that there is nonindependence in the data, such that we have repeated measures of the Test variable levels nested within employees. As the second argument in the aov function, we will specify the data= argument followed by the name of our long-format df_long data frame object, to which all of the variables specified in our model belong. Finally, we will print the model results using the summary function from base R with the name of the fitted model object in the parentheses (aov_mod). # Estimate 2x2 mixed factorial ANOVA model aov_mod &lt;- aov(Score ~ # outcome variable Condition*Test + # within- &amp; between-subjects interaction Error(EmpID/Test), # nesting of within-subjects variable data=df_long) # data frame object # Print a summary of the results summary(aov_mod) ## ## Error: EmpID ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Condition 1 1109 1108.9 29.11 0.00000207 *** ## Residuals 48 1829 38.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: EmpID:Test ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Test 1 7797 7797 107.636 0.0000000000000749 *** ## Condition:Test 1 480 480 6.621 0.0132 * ## Residuals 48 3477 72 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For the 2x2 mixed-factorial ANOVA model, the interaction term involving the between-subjects Condition variable and within-subjects Test variable produces the following key statistics: F = 6.621 and p = .0132. If we compute the square root of the F-value (6.621), we get 2.573, which is the same as the t-value we observed in the independent-samples t-test, simple linear regression model, and biserial correlation with a difference score outcome variable; plus, the associated p-value (.0132) and the residuals degrees of freedom (48) are also the same. We can apply the model.tables function from base R with the fitted model object (aov_mod) and “means” as the two function arguments. Doing so provides us with tables of means. # Print a table of means model.tables(aov_mod, &quot;means&quot;) ## Tables of means ## Grand mean ## ## 58.01 ## ## Condition ## Condition ## Old New ## 54.68 61.34 ## ## Test ## Test ## PreTest PostTest ## 49.18 66.84 ## ## Condition:Test ## Test ## Condition PreTest PostTest ## Old 48.04 61.32 ## New 50.32 72.36 Given the statistically significant interaction term between the between-subjects Condition variable and within-subjects Test variable, we proceed forward with follow-up analyses of the between-subjects and within-subjects simple effects. Given our 2x2 design, our job examining the simple effects is made easier. Let’s begin with the between-subjects simple effects. To compute the between-subjects simple effects, we will apply independent-samples t-tests to each level of the within-subjects factor. To do so, we will use the t.test function from base R. As the first argument, we’ll specify our model, which is our outcome variable (Score) followed by the ~ operator and the predictor variable (Condition). As the second argument, we’ll specify data= followed by the name of our long-format data frame object (df_long). As the third argument, we’ll specify paired=FALSE to request an independent-samples t-test. As the fourth argument, we’ll assume the variances are equal by applying var.equal=TRUE. As the fifth argument, we’ll specify subset=(Test==\"PreTest\") to filter the data down to just the “PreTest” scores. We’ll then repeat the process for the PostTest scores in a separate t.test function. # Between-subjects simple effects for the Condition variable # by each level of the Test variable # By PreTest level of Test variable t.test(Score ~ Condition, data=df_long, paired=FALSE, var.equal=TRUE, subset=(Test==&quot;PreTest&quot;)) ## ## Two Sample t-test ## ## data: Score by Condition ## t = -1.2106, df = 48, p-value = 0.232 ## alternative hypothesis: true difference in means between group Old and group New is not equal to 0 ## 95 percent confidence interval: ## -6.066903 1.506903 ## sample estimates: ## mean in group Old mean in group New ## 48.04 50.32 # By PostTest level of Test variable t.test(Score ~ Condition, data=df_long, paired=FALSE, var.equal=TRUE, subset=(Test==&quot;PostTest&quot;)) ## ## Two Sample t-test ## ## data: Score by Condition ## t = -4.7976, df = 48, p-value = 0.000016 ## alternative hypothesis: true difference in means between group Old and group New is not equal to 0 ## 95 percent confidence interval: ## -15.666791 -6.413209 ## sample estimates: ## mean in group Old mean in group New ## 61.32 72.36 For the independent-samples t-test involving just the “PreTest” scores, we find that there is not a statistically significant difference for scores on the “PreTest” between the “New” and “Old” levels of the Condition variable (t = -1.2106, df = 48, p = .232). This tells us that those in the “New” and “Old” training conditions started at about the same place in terms of the scores prior to the training intervention. For the independent-samples t-test involving just the “PostTest” scores, we find that there is a statistically significant difference for scores on the “PostTest” between the “New” and “Old” levels of the Condition variable (t = -4.7976, df = 48, p &lt; .001). Examining the means informs us that those in the “New” training condition had statistically significantly higher scores on the “PostTest” than those in the “Old” training condition. To compute the within-subjects simple effects, we will apply paired-samples t-tests to each level of the between-subjects fact. To do so, we will use the t.test function from base R once more. As the first argument, we’ll specify our model, which is our outcome variable (Score) followed by the ~ operator and the predictor variable (Test). As the second argument, we’ll specify data= followed by the name of our long-format data frame object (df_long). As the third argument, we’ll specify paired=TRUE to request an paired-samples t-test. As the fourth argument, we’ll specify subset=(Condition==\"Old\") to filter the data down to just the “Old” condition scores. We’ll then repeat the process for the “New” condition scores in a separate t.test function. # Within-subjects simple effects for the Test variable # by each level of the Condition variable # By Old level of Condition variable t.test(Score ~ Test, data=df_long, paired=TRUE, subset=(Condition==&quot;Old&quot;)) ## ## Paired t-test ## ## data: Score by Test ## t = -5.1935, df = 24, p-value = 0.00002548 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -18.55745 -8.00255 ## sample estimates: ## mean difference ## -13.28 # By Old level of Condition variable t.test(Score ~ Test, data=df_long, paired=TRUE, subset=(Condition==&quot;New&quot;)) ## ## Paired t-test ## ## data: Score by Test ## t = -9.8061, df = 24, p-value = 0.0000000007195 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -26.67877 -17.40123 ## sample estimates: ## mean difference ## -22.04 Both within-subjects simple effects show a statistically significant mean of the differences. If we combine this information with the table of means from our model.tables function output (see above), we see that scores on both the “Old” condition and the “New” condition increased to a statistically significant extent from before to after training; however, the increase in scores for those in the “New” condition was notably larger. Visualizing the simple effects using a bar chart or line chart can be useful for understanding the nature of the interaction, which is what we’ll do next. With long-format data, we can produce a bar chart of the results using the BarChart function from the lessR package. If you haven’t already, be sure to install and access the lessR package using the install.packages and library functions, respectively. # Create bar chart BarChart(x=Test, # within-subjects variable y=Score, # outcome variable by=Condition, # between-subjects variable stat=&quot;mean&quot;, # specify means be computed by category beside=TRUE, # plot bars side-by-side data=df_long, # name of data frame object xlab=&quot;Test Time&quot;, # specify x-axis label ylab=&quot;Average Test Score&quot;) # specify y-axis label ## Summary Table of Score ## ---------------------- ## ## Test ## Condition PreTest PostTest ## Old 48.040 61.320 ## New 50.320 72.360 Alternatively, we can plot an interaction line plot using the interaction.plot function from base R. interaction.plot(x.factor=df_long$Test, # within-subjects variable trace.factor=df_long$Condition, # between-subjects variable response=df_long$Score, # outcome variable fun=mean, # specify means be computed type=&quot;b&quot;, # request lines and points legend=TRUE, # request legend pch=1, # request bubble points ylab=&quot;Average Test Score&quot;, # specify y-axis label xlab=&quot;Test Time&quot;, # specify x-axis label trace.label=&quot;Condition&quot;) # specify legend label 72.3.6 Estimating a Random-Coefficients Multilevel Model Under these balanced-design conditions, a random-coefficients model (i.e., linear mixed-effects model, multilevel model) will also be statistically equivalent to a independent-samples t-test, simple linear regression model, biserial correlation with a difference score outcome variable, as well as a 2x2 mixed-factorial ANOVA model. We can also refer to this type of model as a linear growth model. To estimate a random-coefficients model, we will use the lme function from the nlme package. If you haven’t already, please install and access the nlme package. # Install nlme package if you haven&#39;t already install.packages(&quot;nlme&quot;) # Access nlme package library(nlme) Using the &lt;- assignment operator, we will assign our model to an object that we name lme_mod. To the right of the &lt;- assignment operator, we will type the name of the lme model. As the first argument, we will type the name of the outcome variable (Score) followed by the ~ operator. As the second argument, we will type the name of the within-subjects/time-variant predictor variable (Test). As the third argument, we will type the name of the time-invariant/between-subjects variable (Condition). As the fourth argument, we will specify the interaction term between the time-variant and time-invariant variables using the * operator. As the fifth argument, we will insert random= followed by the ~ operator, the time-variant random-effects variable (Test), the | operator and the grouping variable (EmpID). As the sixth argument, we will specify data= followed by the name of the long-format data frame object (df_long). Finally, we will type the name of the summary function from base R with the sole parenthetical argument lme_mod to print a summary of the model results. # Estimate random-coefficients multilevel model lme_mod &lt;- lme(Score ~ # outcome variable (Score) Test + # time-variant variable (Test) Condition + # time-invariant variable (Condition) Test * Condition, # interaction term random= ~ Test | # time-variant random-effects (Test) EmpID, # grouping variable (EmpID) data=df_long) # data frame object name # Print a summary of the model results summary(lme_mod) ## Linear mixed-effects model fit by REML ## Data: df_long ## AIC BIC logLik ## 679.4852 700 -331.7426 ## ## Random effects: ## Formula: ~Test | EmpID ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 5.776079 (Intr) ## TestPostTest 11.086832 -0.789 ## Residual 3.313394 ## ## Fixed effects: Score ~ Test + Condition + Test * Condition ## Value Std.Error DF t-value p-value ## (Intercept) 48.04 1.331791 48 36.07173 0.0000 ## TestPostTest 13.28 2.407281 48 5.51660 0.0000 ## ConditionNew 2.28 1.883437 48 1.21055 0.2320 ## TestPostTest:ConditionNew 8.76 3.404409 48 2.57313 0.0132 ## Correlation: ## (Intr) TstPsT CndtnN ## TestPostTest -0.767 ## ConditionNew -0.707 0.543 ## TestPostTest:ConditionNew 0.543 -0.707 -0.767 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.18371820 -0.30749376 -0.00838071 0.24550886 1.23636767 ## ## Number of Observations: 100 ## Number of Groups: 50 When reviewing the estimated values associated with the fixed-effects interaction term between Test and Condition, we see the following: coefficient = 8.760, standard error = 3.404, t = 2.573, df = 48, and p = .0132. These values should look familiar. In fact, the coefficient value of 8.760 is the mean difference between the two “Old” and “New” conditions that we observed in the independent-samples t-test with a difference score outcome variable. The other estimate values should look familiar as well given that this random-coefficients multilevel model (i.e., growth model) is statistically equivalent to the other models we have estimated in this chapter. In fact, when we apply the anova function from base R to the fitted lme_mod object, we reproduce the results from our 2x2 mixed-factorial ANOVA model interaction term. Specifically, please note the F-value and associated p-value for the Condition:Test line of the output. # Print a summary of the ANOVA model results anova(lme_mod) ## numDF denDF F-value p-value ## (Intercept) 1 48 8813.691 &lt;.0001 ## Test 1 48 107.636 &lt;.0001 ## Condition 1 48 24.688 &lt;.0001 ## Test:Condition 1 48 6.621 0.0132 References "],["references.html", "References", " References "]]
