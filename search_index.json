[["index.html", "Preface 0.1 Growth of HR Analytics 0.2 Skills Gap 0.3 Project Life Cycle Perspective 0.4 Overview of HRIS &amp; HR Analytics 0.5 My Philosophy for This Book 0.6 Structure 0.7 About the Author 0.8 Acknowledgements", " R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION David E. Caughlin 2022-01-01 Preface This book is free to read and is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The contents of this book may not be used for commercial purposes. [NOTE: This book is currently under construction, and I anticipate that I will complete a full beta version by January 2022.] 0.1 Growth of HR Analytics The term human resource analytics can mean different things to different people and to different organizations. Further, human resource analytics sometimes goes by other names like people analytics, talent analytics, workforce analytics, and human capital analytics. While some may argue for distinctions between these different names, for this book, I will treat them as interchangeable labels. Moreover, for the purposes of this book, human resource (HR) analytics is defined as the process of collecting, analyzing, interpreting, and reporting people-related data for the purpose of improving decision making, achieving strategic objectives, and sustaining a competitive advantage (Bauer et al. 2020, 34). The foundation of HR analytics formed over a century ago with the emergence of disciplines like industrial and organizational (I/O) psychology. In recent decades, advances in information technology and systems have reduced the time HR professionals spend on transactional and administrative activities, thereby creating more time and opportunity for transformational activities supporting the realization of strategic organizational objectives. HR analytics has the potential to play an integral role in such transformational activities, as it can inform HR system design (e.g., selection tool selection, validation, and process) and high-stakes decision making involving people-related data from the organization. A 2018 survey of companies highlighted the perceived importance of HR analytics but a relative lack of readiness to adopt and integrate HR analytics (Deloitte 2018). 0.2 Skills Gap Although many organizations regard HR analytics as strategically important for organizational success, today many of those same organizations face an HR analytics talent shortage. To some extent, the talent shortage can be attributed to data literacy  or the lack thereof. Historically, academic and professional HR training and development opportunities did not emphasize data-literacy skills, and this omission has left organizations today scrambling to hire external talent or to close the skills gap of existing HR professionals. To address the HR analytics talent shortage and skills gap, organizations have, broadly speaking, two options. First, for some organizations, closing the skills gap may be as straightforward as hiring a quant (e.g., data scientist, statistician), provided the individual works closely with HR professionals when working with data associated with HR systems, polices, and procedures, and identifying HR-specific legal and ethical issues. Second, I would argue that for most organizations perhaps a better alternative is to close the skills gap with among current HR professionals, as their HR-specific knowledge, skills, abilities, and other characteristics (KSAOs) offer tremendous value when deriving insights from HR data as well as a solid domain-specific foundation for subsequently layering on data-literacy KSAOs. Importantly, those with existing HR domain expertise presumably have working knowledge of prevailing employment and labor laws and experience with anticipating and uncovering ethical issues, both of which are necessary when acquiring, managing, analyzing, visualizing, and reporting HR data. At the most basic level, proficiency in HR analytics involves the integration of knowledge, skills, abilities, and other characteristics (KSAOs) associated with HR expertise and data literacy. 0.3 Project Life Cycle Perspective When building efficacy in HR analytics, I have found that its helpful to envision where and how contributions can be made at the project level and which specific KSAOs are required at each phase. To that end, I developed the HR Analytics Project Life Cycle (HRAPLC) as a way to conceptualize the prototypical phases of a generic project life cycle. These phases include: Question Formulation, Data Acquisition, Data Management, Data Analysis, Data Interpretation and Storytelling, and Deployment and Implementation. I dedicate Part 1 of this book to providing a conceptual overview of the HRPLC in Chapters 1-7. The Human Resource Analytics Project Life Cycle (HRAPLC) offers a way to conceptualize the prototypical phases of a generic HR analytics project life cycle. 0.4 Overview of HRIS &amp; HR Analytics If you are just looking for a basic overview of HR information systems (HRIS) and HR analytics, consider checking out the following introductory video. Link to video: https://youtu.be/3X7qmb1M39A If you are looking for an introduction to human resource management with supplementary Excel-based tutorials and data exercises, I recommend checking out one of the textbooks I have co-authored: Bauer, T. N., Erdogan, B., Caughlin, D. E., &amp; Truxillo, D. M. (2019). Human resource management: People, data, and analytics. Thousand Oaks, CA: Sage Bauer, T. N., Erdogan, B., Caughlin, D. E., &amp; Truxillo, D. M. (2020). Fundamentals of human resource management: People, data, and analytics. Thousand Oaks, CA: Sage. 0.5 My Philosophy for This Book Working with data does not need to be scary or intimidating; yet, over the years, I have interacted with students and professionals who carry with them what I refer to as a numerical phobia or quantitative trauma. Unfortunately, at some point in their lives, some people begin to believe that they are not suited for mathematics, statistics, and/or generally working with data. Given these psychological barriers, a primary objective of this book is to make data analytics  and HR analytics specifically  relevant, accessible, and maybe even a little fun. In early chapters, my intention is to ease the reader into foundational concepts, applications, and tools in order to incrementally build self-efficacy in HR analytics. The tutorials in each chapter are grounded in common and meaningful HR contexts (e.g., validating employee selection tools). As the book progresses, more challenging statistical concepts and data-analytic techniques are introduced. Reading this book and following along with the in-chapter tutorials will not lead to expert-level knowledge and skill; however, my hope is that completing all or portions of this book will do the following: Build excitement for working with data to inform decision making. Instill a sense of intellectual curiosity about data and a hunger to expand boundaries of expertise. Inspire further in-depth training, education, and learning in areas and topics introduced in this book. Enhance data literacy, including knowledge and skills related to (a) critical thinking and logic, (b) mathematics, statistics, and data analysis, and (c) data visualization and storytelling with data. 0.5.1 Rationale for Using R Today, we have the potential to access and use a remarkable number of statistical and data-analytic tools. Examples of such tools include (in no particular order) R, Python, SPSS, SAS, Stata, MatLab, Mplus, Alteryx, Tableau, PowerBI, and Microsoft Excel. Notably, some of these programs can be quite expensive when it comes to lifetime or annual user licensing costs, which can be a barrier to access for many. Programming languages like R and Python have several desirable qualities when it comes to managing, analyzing, and visualizing data. Namely, both are free to use, and both have an ever-growing number of free (add-on) packages with domain- or area-specific functions (e.g., data visualizations). It is beyond the scope of this Preface to provide an exhaustive comparison of the relative merits of R versus Python; however, when it comes to the statistical analysis of data, specifically, I argue that R provides a more user-friendly entry point for beginners as well as more advanced capabilities desired by expert users, especially for ad-hoc analyses. Moreover, the integrated development environment program called RStudio (which sits on top of base R) offers useful workflow tools and generally makes for an inviting environment within which the R engine to run. With all that said, Python has been catching up in these regards, and I wouldnt be surprised if Python closes these gaps relative to R in the next few years. I would be remiss if I didnt mention that the Python language is powerful and has capabilities that extend far beyond the management, analysis, and visualization of data. Fortunately, learning R makes learning Python easier (and vice versa), which means that this book can serve as springboard for learning Python or other programming languages. Finally, I believe it to be unlikely that one tool (e.g., program, language) will emerge that is ideal for every task, and thus, I encourage you to build familiarity with multiple tools so that you develop a toolbox of sorts, thereby allowing you to choose the best (or at least better) tool for each task. 0.5.2 Audience I have written this book with current or soon-to-be HR professionals in mind, particularly those who have an interest in upskilling their data-analytic knowledge and skills. With that said, I believe this book can provide a meaningful context for learning key data-analytic concepts, applications, and tools that are applicable beyond the HR context. Relatedly, this book may serve as a user-friendly gateway and introduction to the programming language called R. 0.6 Structure This under-construction version of the book consists of the following parts and associated chapters: HR Analytics Project Life Cycle: Overview of HR Analytics Project Life Cycle Question Formulation Data Acquisition Data Management Data Analysis Data Intrepretation &amp; Storytelling Deployment &amp; Implementation Introduction to R: Overview of R &amp; RStudio Installing R &amp; RStudio Getting Started with R &amp; RStudio Basic Features &amp; Operations of the R Language Setting a Working Directory Data Acquisition &amp; Management: Reading Data into R Removing, Adding, &amp; Changing Variable Names Writing Data from R Arranging (Sorting) Data Joining (Merging) Data Filtering (Subsetting) Data Cleaning Data Manipulating &amp; Restructuring Data Centering &amp; Standardizing Variables Removing Objects from the R Environment Employee Demographics: Introduction to Employee Demographics Describing Employee Demographics Using Descriptive Statistics Summarizing Two or More Categorical Variables Using Cross-Tabulations Applying Pivot Tables to Explore Employee Demographic Data Employee Surveys: Introduction to Employee Surveys Aggregating &amp; Segmenting Employee Survey Data Estimating Internal Consistency Reliability Using Cronbachs alpha Creating a Composite Variable Based on a Multi-Item Measure Employee Training: Introduction to Employee Training Evaluating a Post-Test/Post-Test without Control Group Design Using Paired-Samples t-test Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA Employee Selection: Introduction to Employee Selection Investigating Disparate Impact Estimating Criterion-Related Validity of a Selection Tool Using Correlation Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method Testing for Differential Prediction Using Moderated Multiple Linear Regression Statistically &amp; Empirically Cross-Validating a Selection Tool Employee Separation &amp; Retention: Introduction to Employee Separation &amp; Retention Computing Monthly &amp; Annual Turnover Rates Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence Identifying Predictors of Turnover Using Logistic Regression Applying k-Fold Cross-Validation to Logistic Regression Understanding Length of Service Using Survival Analysis Employee Performance Management: Introduction to Employee Performance Management Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations Investigating Nonlinear Associations Using Polynomial Regression Predicting Criterion Scores Using Lasso Regression Odds &amp; Ends: Creating a Data Analytics Portfolio Conducting a Literature Search &amp; Review 0.7 About the Author David Caughlin works for Portland State Universitys School of Business where he teaches and researches topics related to organizational behavior, human resource management, and data analytics. David received his B.S. in psychology and B.A. in Spanish from Indiana University, M.S. in industrial and organizational psychology from Indiana University - Purdue University at Indianapolis, and Ph.D. in industrial and organizational psychology from Portland State University with concentrations in quantitative methodology and occupational health psychology. His research interests are generally focused on supervisor support, work motivation, and occupational safety and health. David has co-authored research published in peer-reviewed outlets such as Journal of Applied Psychology, Journal of Management, Human Resource Management, Journal of Vocational Behavior, Journal of Occupational Health Psychology, and Psychology, Public Policy, and the Law. In addition, he has co-authored the textbooks Human Resource Management: People, Data, and Analytics and Fundamentals of Human Resource Management. David teaches undergraduate and graduate courses on topics related to organizational behavior, human resource management, information systems, and data analytics. In his HR analytics courses, David teaches students how to apply the statistical programming language R to manage, analyze, and visualize HR data to improve strategic decision making; in the process, students build data-literacy, critical-thinking, and logical-reasoning skills. He has received the following teaching awards from the School of Business: Teaching Innovation Award (2018), Extra Mile Teaching Excellence Award (2019), and Teaching Innovation Award (2020). In his free time, David enjoys outdoor activities like trail running, cycling, skiing, and paddle boarding. 0.8 Acknowledgements My inspiration for writing and compiling the contents of this book stems from interactions with countless colleagues, professional acquaintances, and undergraduate and graduate students, and a broad thank you is in order for anyone with whom I have taught or had a conversation about HR analytics specifically or data analytics in general. Finally, I created this book using the following programs and packages: R (R Core Team 2021), RStudio (RStudio Team 2020), rmarkdown (Xie, Allaire, and Grolemund 2018; Allaire et al. 2021), knitr (Xie 2015, 2014, 2021), and bookdown (Xie 2016, 2020). "],["overviewhraplc.html", "Chapter 1 Overview of HR Analytics Project Life Cycle", " Chapter 1 Overview of HR Analytics Project Life Cycle The Human Resource Analytics Project Life Cycle (HRAPLC) offers a framework for conceptualizing the prototypical phases of a generic project life cycle. When learning how to apply HR analytics, Ive found that its helpful to envision where and how contributions can be made at the project level and which specific knowledge, skills, abilities, and other characteristics (KSAOs) are required at each phase of a project. The HRAPLC phases are as follows. Question Formulation: process of identifying and posing strategy-inspired and -aligned problems and/or questions that can be solved or answered using data. Data Acquisition: process of collecting, retrieving, gathering, and sourcing data that can be used to solve problems and answer questions. Data Management: process of wrangling, cleaning, manipulating, and structuring data. Data Analysis: process of applying mathematical, statistical, and/or computational analyses to data to identify associations, differences, changes, or classes, as well as to predict the likelihood of future events, values, differences, or changes. Data Interpretation and Storytelling: process of making sense of data-analysis findings in the context of the focal problem and/or question, and of disseminating and communicating the findings to different stakeholders. Deployment and Implementation: process of prescribing or taking action based on interpretation of data-analysis findings. The HR Analytics Project Life Cycle (HRAPLC) offers a way to conceptualize the prototypical phases of a generic HR analytics project life cycle. Notably, the phases of the HRAPLC generally align with the steps of the classic scientific process, which include formulating a hypothesis, designing a study, collecting data, analyzing data, and reporting findings. This similarity underscores how HR analytics is consistent with a scientific approach to HR management. and, more generally, to an empirical approach aimed at uncovering truth based on data. Like the scientific process, the HRAPLC is predicated on empiricism, which means truth comes from data. That is, the engine of the HRAPLC runs on data, and these data serve as evidence on which we build knowledge and glean insights. The phases of the HR Analytics Project Life Cycle (HRAPLC) generally align with the classic steps of the scientific process. In the following six chapters, I provide a detailed conceptual overview of each HRAPLC phase, beginning with Question Formulation. "],["questionformulation.html", "Chapter 2 Question Formulation 2.1 Adopting a Strategic Mindset 2.2 Defining Problems &amp; Formulating Questions 2.3 Summary", " Chapter 2 Question Formulation The first phase of the HR Analytics Project Life Cycle (HRAPLC) is Question Formulation. Question formulation refers to the process of posing strategy-inspired and -aligned questions or hypotheses (that can be answered using data) in order to investigate why or how a problem occurs, what a problem might lead to or be associated with, or who is affected by the problem. Thoughtful question formulation results in (a) more effective and efficient data acquisition, management, and analysis, and (b) answers that are more useful to stakeholders. Question formulation is closely associated with problem definition, which refers to the process of framing and diagnosing a problem (e.g., challenge, opportunity, threat) for which finding a solution will bring value. The Question Formulation phase of the HR Analytics Project Life Cycle (HRAPLC) involves defining problems and formulating questions. 2.1 Adopting a Strategic Mindset When our goal is to improve the organization and the employee experience, adopting a strategic mindset sets us up for defining the right problems and formulating the right questions. A strategic mindset involves: Familiarity with strategic goals and roles, which requires recognizing and understanding strategic objects of the department and the organization, and the roles that key decision makers and stakeholders play. Understanding of organization systems, which requires the application of systems thinking when defining problems and formulating questions. Focus on opportunities for innovation, which requires thinking broadly and openly prior to narrowing focus. Application of (intellectual) curiosity, which entails asking why, why not, and how type questions. Before defining a problem and formulating a question, it is important to adopt a strategic mindset and to engage in divergent and convergent thinking processes. 2.1.1 Strategy Because adopting a strategy necessitates and understanding of strategy, lets take a moment to review the concept of a strategy. Simply put, a strategy refers to a well-devised and thoughtful plan for achieving an objective (Bauer et al. 2019, 35). Notably, a strategy is future oriented and provides a roadmap towards completion of a desired objective. Moreover, realizing a strategy requires coordinating business activities to achieve both short- and long-term objectives. When creating a new strategy, we often focus on two distinguishable phases: (a) strategy formulation and (b) strategy implementation. 2.1.2 Strategy Formulation Strategy formulation involves planning what to do to achieve organizational objectives  or in other words, the development and/or refinement of a strategy (Bauer et al. 2019, 3536). The prototypical strategy formulation often includes the following steps: Creating a mission statement, articulating a vision, and defining core values. The mission, vision, and values can serve as a compass by guiding organizational actions in the direction of a desired future state and by providing parameters and guidelines for reaching that future state. Analyzing the internal and external environments. Decision-making tools like a SWOT (strengths, weaknesses, opportunities, threats) analysis provide frameworks for understanding and describing the characteristics that are internal to the organization (e.g., employees) and characteristics that are external to the organization (e.g., labor market). Selecting a broad type of strategy. The type of strategy refers to the general approach an organization takes when bringing the mission, vision, and values to life, such as differentiation or cost leadership. Defining specific strategic objectives aimed at satisfying relevant stakeholders. This involves considering the needs of key stakeholders (e.g., customers, investors, shareholders, employees, broader communities) and considering what will result in a sustainable competitive advantage. Finalizing and temporarily crystallizing the strategy prior to strategy implementation. This step often results in a clear strategic plan that summarizes the previous four steps. 2.1.3 Strategy Implementation Once weve completed the strategy formulation phase, were ready to move onto strategy implementation, where strategy implementation refers to the process of following through on the strategic plan developed and/or refined during the strategy formulation phase (Bauer et al. 2019). This phase involves building and leveraging the organizations human capital capabilities in order to enact and realize the strategic plan, and aligning HR policies and practices with the strategic plan. For example, if a strategic objective outlined in the strategic plan requires acquiring, managing, or retaining the best software engineers, then the organization should focus on how HR systems like recruitment, selection, performance management, and reward systems can help support that requirement. 2.1.4 Strategic Human Resource Management When HR management aligns with and supports the realization of organizational strategic objects, strategic human resource management emerges. In other words, strategic human resource management involves a strategy-oriented approach to human resource management. While most specific HR policies and practices will vary across organizations, there are practices that are generally strategically relevant for all organizations, and examples include selectively hiring new employees to ensure sufficiently high qualifications and good fit, and training employees to enhance job- and work-relevant knowledge, skills, abilities, and other characteristics (KSAOs) (Pfeffer 1998). 2.1.4.1 Video Lecture Link to video lecture: https://youtu.be/08whYkgFiQI 2.2 Defining Problems &amp; Formulating Questions The overarching purpose of the Question Formulation phase of the HRAPLC is to define problems and formulate questions for which solutions and answers, respectively, can improve the organization and push it towards its goals. In the sections that follow, we will focus on the processes of problem definition and question formulation, and ultimately learn the value of applying both divergent and convergent thinking during both of these processes. 2.2.1 Defining a Problem With a departments and/or organizations strategy in mind, were ready to define an organizationally relevant problem in need of a solution. While we typically know a problem when we see one, I found its helpful to take a step back and think about what a problem actual is. For our purposes, we can think of a problem as a gap between the current state of affairs and a desired state of affairs. Given that, problem definition refers to the process of framing and diagnosing a problem (e.g., challenge, opportunity, threat) for which finding a solution will be of value. As shown below, key problem-definition components often include articulating the problem statement, decision makers and stakeholders, scope, and goals (Chevallier 2016; Conn and McLean 2018). A fully formed problem definition often involves articulating key problem definition components, such as articulating the problem statement, decision makers and stakeholders, scope, and goals. With respect to the HR context, in the figure below, I provide an example focused on problem of voluntary turnover among new customer service representatives, and provide hypothetical examples of key problem-definition components. Articulating the problem definition components is appropriate for most problems, including those specific to HR, such as the one in this example. 2.2.2 Formulating a Question Once a general problem has been defined, were ready to begin question formulation, which refers to the process of posing a question or hypothesis to investigate why or how a problem occurs, what a problem might lead to or be associated with, or who is affected by the problem. Question formulation can even help us flesh out the typical problem definition components outlined in the previous section. Just as we did with problem definition, we should continue to apply our strategic mindset when formulating questions. Moreover, we should focus on posing questions for which finding answers hold value for the organization, as doing so can contribute to the following (for example): a better understanding of the focal problem, the identification of potential solutions to the focal problem, solutions that meet the needs of key decision makers and stakeholders, more efficient and targeted literature searches and reviews, and more effective and focused storytelling. So what is a question in this context? A question can be described as general line of inquiry regarding a problem or phenomenon of interest. Examples of questions are as follows. What factors influence voluntary turnover? Does engagement predict voluntary turnover? How does engagement relate to voluntary turnover? Which work characteristics predict voluntary turnover? Why do new employees turn over in the first 6 months? If, based on prior research or theory, an informed prediction can be made, we might choose to restate a question a s hypothesis, where a hypothesis can be thought of as a statement about an expected association, difference, change, or classification. Examples of hypotheses are as follows. Engagement is negatively related to voluntary turnover. Turnover intention mediates the relationship between engagement and voluntary turnover. Autonomy and task significance negatively predict voluntary turnover. New employees who participate in the formal onboarding program are less likely to quit during their first 6 months. Regardless of whether we pose a question or state a hypothesis, its important to remember that question formulation is often an iterative process, which means that answering one question (or testing one hypothesis) often leads to additional questions (or hypotheses). Question formulation is often an iterative process, meaning that answering one question (or testing hypothesis) often leads to additional questions (or hypotheses). Further, we can draw upon existing theory and research to inform the types of questions we pose (or hypotheses we state). A theory provides a way to understand or explain a phenomenon of interest in parsimonious manner. As an example, the theory of planned behavior (Ajzen 1991) is based on a voluminous body of research, and some of the basic tenets of the theory are as follows. First, an individuals attitude towards a behavior, perception of the norms associated with the behavior, and sense of control over enacting a behavior contribute to their intention to perform a behavior. Second, an individuals intention to perform a behavior often leads them to perform a behavior. This theory can be applied to any number of behaviors. Given that HR professionals (and organizational leadership) are often concerned with voluntary turnover (i.e., an employee-initiated organizational separation), lets focus on voluntary turnover. Using the theoretical tenets, we can reason that an individuals voluntary turnover behavior might be explained by their attitudes, norms, and sense of control related to voluntary turnover, and ultimately to their intentions to turn over voluntarily. Thus, based on the theory of planned behavior, we might first focus on the proposed link between attitudes and behavioral intention, and formulate the following question: Are more positive attitudes towards leaving the organization associated with a stronger intention to voluntarily turn over? Further, by focusing on the proposed link between behavioral intention and actually enacting the behavior, we might hypothesize the following: Stronger turnover intention is associated with a higher probability of quitting. Existing theories, such as the theory of planned behavior (Azjen, 1991) can be used to inform and direct the types of questions or hypotheses that are posed during question formulation. 2.2.3 Thinking Divergently &amp; Convergently Throughout the processes of defining a problem and formulating a question, it is advisable to apply both divergent and convergent thinking. On the one hand, divergent thinking refers to the process of adopting a broadened and imaginative mindset in which many ideas, possibilities, and alternatives are considered, which can lead to a large quantity of creative or novel ideas; on the other hand, convergent thinking refers to the process of adopting a critical and evaluative mindset in which various alternatives are judged, which can lead to a more refined set of high-quality ideas (Basadur, Graen, and Scandura 1986; Basadur, Runco, and Vega 2000; Reiter-Palmon and Illies 2004). When used effectively, these two ways of thinking can improve problem definition and question formulation. More specifically, once we have adopted a strategic mindset, the identification, framing, and defining of problems should involve: (a) a phase of ideation with divergent thinking, such that many potential ideas, possibilities, and ultimately problems are entertained and considered - with the primary focus being on novelty, creativity, and quantity; and (b) a phase with deliberate convergent thinking, such that the list of potential problems generated during the previous phase is winnowed to those that will contribute the most to the organizations strategic goals, ends, or initiatives. A similar two-phase process can be applied when formulating questions. 2.2.3.1 Video Lecture Link to video lecture: https://youtu.be/w1104yS-zJU 2.3 Summary In this chapter, we did a conceptual deep dive into the Question Formulation phase of the HR Analytics Project Life Cycle. In doing so, we reviewed the importance of adopting a strategic mindset, and then engaging in thoughtful problem definition and question formulation. Finally, we learned how a two-phase process of deliberate divergent and convergent thinking can help us identify the most important problems and questions for which solutions and answers, respectively, would provide the most value to the organization. "],["dataacquisition.html", "Chapter 3 Data Acquisition 3.1 Employee Surveys 3.2 Rating Forms 3.3 Surveillance &amp; Monitoring 3.4 Database Queries 3.5 Scraping 3.6 Summary", " Chapter 3 Data Acquisition Data acquisition refers to the process of collecting, retrieving, gathering, and sourcing data that can be used to solve problems, answer questions, and test hypotheses that were identified during the Question Formulation phase of the HR Analytics Project Life Cycle. Various tools can be used for data acquisition, such as employee surveys, (performance) rating forms, surveillance and monitoring, database queries, and scraping or crawling. In some instances, the required data may already reside in an HR information system (HRIS) or enterprise resource planning (ERP) platform, and such data are often referred to as archival. From ethical, legal, and practical perspectives, my general advice is to acquire data with a purpose. That is, if we dont have a compelling and well-thought-out rationale to collect certain data  especially data about people  then we should probably should resist collecting such data. The Data Acquisition phase of the Human Resource Analytics Project Life Cycle (HRAPLC) involves gathering the data necessary for solving the problem or answering the question from the Question Formulation phase. 3.1 Employee Surveys When it comes to acquiring data about employee attitudes, behaviors, and feelings, the employee survey is perhaps one of the most common (if not the most common) tools. If youre unfamiliar with employee surveys, simply put, they consist of some number of items (e.g., questions) to which employees are asked to respond. Survey items can be open-ended (e.g., Please describe our onboarding experience.) or close-ended with fixed response options (e.g., I am satisfied with my job. [1 = Strongly Disagree, 5 = Strongly Agree]), and they can vary in length, ranging from shorter yet more frequent pulse surveys to longer yet less frequent annual engagement surveys. Further, surveys can be used to deploy multi-item measures of multi-faceted and nuanced concepts (i.e., constructs) such as engagement and organizational citizenship behaviors. The quality of the data acquired using an employee survey depends largely on the quality of the survey content (e.g., quality of item writing), the appropriateness of the survey for the target population, and respondents motivation (or lack thereof) for taking the survey. To learn more about writing high-quality items, avoiding common pitfalls, and other design and administration considerations, I recommend checking out the Google re:Work guide for developing and administering employee surveys (https://rework.withgoogle.com/guides/analytics-run-an-employee-survey), as it distills many best practices into a user-friendly and efficient format. Below, I list some potential advantages and disadvantages of using employee surveys for data acquisition. Advantages: If designed well, they can be efficient and effective tools for acquiring self- or observer-report data on employee personality, attitudes, individual differences, and behaviors as well as perceptions of work, working environment, work-family interface, supervisor behavior, coworker behavior, and client behavior. They tend to be relatively affordable to administer and a variety of platforms exist today to facilitate this process (e.g., Qualtrics, SurveyMonkey). Employees are typically familiar with the concept of a survey and can exert more control over the information that is collected. Disadvantages: Some may argue that the date acquired may be more subjective in nature than the data acquired by other tools, as respondents may succumb to perceived social desirability pressures and/or fake or distort their responses. They can be time-consuming and resource-intensive to respond to and to develop. If surveyed too frequently, employees may experience survey fatigue. 3.2 Rating Forms Rating forms often share some of the same characteristics as employee surveys (e.g, multiple close-ended items) but tend to be more focused on measuring work-related behaviors and job performance. Examples of common types of ratings forms include the behavioral observation scale and the behavioral-anchored rating scale. Given the breadth of the performance domain for most jobs, when targeting performance, ratings forms tend to consist of multiple items or dimensions. For example, the performance domain for the prototypical customer service representative job will involve interacting with customers but will likely also involve administrative tasks, for example, involving the documentation of customer complaints. Below, I offer some advantages and disadvantages of using ratings forms for data acquisition. Advantages: If designed well, they allow raters to produce data efficiently. They can offer a standardized and consistent format that ultimately results in cleaner and more structured data than using no rating form at all to collect the same data. Disadvantages: Achieving sufficiently high reliability across raters can be challenging, even when they are using the same rating form. Some types of ratings forms likely the behavioral-anchored rating scale can be very time-consuming and resource-intensive to develop. Ratings may be influenced by office politics and idiosyncratic rater motivations (e.g., I scored this person lower than they deserved to send a message.) 3.3 Surveillance &amp; Monitoring Surveillance and monitoring offer a more discrete and less obtrusive approach to data acquisition. Examples include tracking system login information (e.g., dates, times), recording video or audio of employees, examining email correspondence, and deploying sensors and other wearable technologies (e.g., sociometric badges). Below, I offer some advantages and disadvantages of using surveillance and monitoring to acquire data. Advantages: They tend to be nonintrusive and operate behind the scenes which can lead to the acquisition of more realistic and authentic employee behavior. Technological advances continue to expand surveillance and monitoring capabilities, such as those designed to measure geolocation, tone of voice, interactions, heart rate, sleep quantity and quality, and exposure to noxious chemicals. Disadvantages: Without clear and transparent communication regarding the use of such tools, employees may perceive a violation of trust. The technologies behind many surveillance and monitoring tools can produce truly big data (e.g., high velocity, massive amounts, unstructured) which can make wrangling and managing the data challenging and time-consuming. Employees may have ethical and privacy concerns about these tools, including about how they data are going to be used by the organization and how they are going to be protected. 3.4 Database Queries When acquiring data that already reside in an information system or enterprise resource planning platform, a database query is often the tool of choice, where A a database query refers to an action in which a request is made to access, acquire, restructure, and/or manipulate data housed in a database. Structured query language (SQL) is an example of a language that is commonly used to access data from a relational database. In many instances, the data retrieved from a database via a query meet the definition of archival data. Below, I offer some potential advantages and disadvantages of using database queries to acquire data. Advantages: They can be an efficient way to gather archival data already residing in an information system or enterprise resource planning platform. They provide an opportunity to leverage data that an organization acquired previously. Disadvantages: Just because data reside in a database does not necessarily mean they are of high quality or are trustworthy. Unless carefully documented, important characteristics and definitions regarding the data residing in a database may be challenging to locate, which means even when queried, the definition and purpose of certain fields (i.e., variables) may remain unclear. 3.5 Scraping Scraping refers to the process of extracting data from websites, documents, and other sources of information. In many cases, we use scraping to gather data that were not originally intended to be used in the way that we plan to use them. For example, to predict changes in the stock market, an analyst might scraped tweets from Twitter over some period of time, use text analysis to code their sentiment, and then correlation tweet sentiment with market performance indicators. Scraping might also be applied to emails, internal company chat applications, and even electronic documents like applicant resumes. Below, I suggest some potential advantages and disadvantages of using scraping as a data-acquisition tool. Advantages: New scraping tools and R packages have made it easier than ever to scrape data. Scraping tools can offer new insights based on previously difficult-to-reach or difficult-to-acquire text data that are rich with contextual information. Disadvantages: Scraping data that arent public or that werent intended to be used in the manner we plan to use them, can raise ethical and privacy concerns. Once scraped, the data often need to be structured into a format for subsequent, which can be a labor-intensive and exhausting process in terms of effort and time. 3.6 Summary In this chapter, we reviewed the Data Acquisition phase of the HR Analytics Project Life Cycle, which included overviews of common data-acquisition tools or techniques like employee surveys, rating forms, surveillance and monitoring, database queries, and scraping. "],["datamanagement.html", "Chapter 4 Data Management 4.1 Data Cleaning 4.2 Data Manipulation &amp; Structuring 4.3 Common Data-Management Tools 4.4 Summary", " Chapter 4 Data Management Data management refers to the process of wrangling, cleaning, manipulating, and structuring the data attained during the Data Acquisition phase of the HR Analytics Project Life Cycle. In many instances, the overarching goal of data management is to prepare the data for subsequent analysis during the Data Analysis phase. In general, you can expect to spend 80% of your time managing data and about 20% of your time analyzing data; however, upon reflection of my own personal experiences, I generally spend about 90% of my time managing data for a project and only 5% of my time actually analyzing the data. My point is that we shouldnt underestimate how long the data manage process will take. The Data Management phase of the Human Resource Analytics Project Life Cycle (HRAPLC) involves wrangling, cleaning, manipulating, and structuring the data gathered during the Data Acquisition phase. Data analysts and database administrators manage data to complete tasks like: Defining the characteristics of the data (e.g., quantitative vs. qualitative; structured vs. unstructured); Organizing the data in a manner that promotes integration, data quality, and accessibility, particularly when data are longitudinal or multisource; Cleaning the data by addressing data-validation issues, missing data, and untrustworthy data or variables; Manipulating and joining (i.e., merging) data, and adding structure if needed; Maintaining the data security and privacy, including restricting access to only authorized individuals. 4.1 Data Cleaning Data cleaning is an essential part of data management and is discussed in greater detail in the Data Cleaning chapter. Broadly speaking, when we clean data, we attempt to identify, correct, or remove problematic observations, scores, or variables. More specifically, data cleaning often entails (but is not limited to): Identifying and correcting data-entry errors and inconsistent coding; Evaluating missing data and determining how to handle them; Flagging and correcting out-of-bounds scores for variables; Addressing open-ended and/or other responses from employee surveys; Flagging and potentially removing untrustworthy variables. By default, different spellings and cases (e.g., Beaverton, beverton, beaverton) for what is supposed to be the same category (e.g., Beaverton) will be treated as separate categories by many programs; therefore, it is important to clean the data by creating consistent category labels. 4.2 Data Manipulation &amp; Structuring Often, data require manipulation or (re)structuring prior to analysis or visualization. A common type of data manipulation is to pivot data from wide to long format or from long to wide format. When data are in wide format, each row contains all of the variable values for a single case (i.e., entity). For example, imagine a data table containing data related to each employees performance on the exact same tests administered at two points in time (e.g., pre-test, post-test) as well as their employee unique identifier number (e.g., employee ID). If this table is in wide format, then each employees data will be contained within a single row, such that their scores one each test belong to a separate variable (i.e., column, field). In contrast, when data are in long format, a single cases data may be spread across multiple rows of data, where commonly each row belonging to a given case represents the a different observation for that case. Separation observations for a single case may have to do with the time (e.g., Time 1 vs. Time 2) or source (e.g., employee rating vs. supervisor rating) of measurement for common target. For example, the same imaginary data described above could be manipulated into long format, such that each employee would have up to two rows of data: one for each test (i.e., pre-test vs. post-test). Again, our decision regarding whether to manipulate data to wide or long format often has to do with what type of analysis or visualization we plan to apply to the data subsequently. As a prerequisite for applying many common types of analyses and visualizations, we need to apply structure to the data, which often means putting the data into rows and columns (e.g., table). Some data-acquisition tools (e.g., survey platforms) make this process relatively easy for us, as they typically export data in the form of a spreadsheet. Other data-acquisition tools (e.g., data scraping), however, will require that the data be structured prior to analysis. For example, imagine we wish to scrape data from employee emails (with their consent). In their raw form, the data contained within the typical email does not inherently fall within rows and columns, where columns represent variables and rows represent cases or observations. When data are acquired, they dont always arrive in a structure that is amenable for subsequent analysis or visualization. For example, the data from these four emails are not yet structured in neat rows and columns. We might restructure the email data by creating separate variables, such as an email unique identifier number, sender, receiver, and subject line. Each row might then contain a single emails data for those variables. By structuring the data into a table, we are now one step closer to applying perhaps some sort of text analysis or to construct a social network analysis. Structure can be applied to the same unstructured email data shown in the previous figure by organizing the data into columns (e.g., variables, fields) and rows (e.g., cases)  or in other words, organizing the data into a table. 4.3 Common Data-Management Tools A variety of tools can facilitate data management, including database management systems and information systems, data analysis software programs, and programming languages. Even widely available spreadsheet tools like Microsoft Excel and Google Sheets offer some data management capabilities. Enterprise database management systems and information system platforms often feature functions that enable joining and arranging data stored in tables. For example, Microsoft Access is perhaps one of the simplest relational database management tools, and it is not uncommon for instructors to teach beginners fundamental database management concepts using the program given its easy-to-understand graphic interface. Other examples of database management systems and vendors include MySQL, Microsoft SQL Server, SQLite, MongoDB, Oracle, and SAP. A number of common database management platforms can be accessed using Structured Query Language (SQL). Finally, programming languages like R and Python can be used to manage data. 4.4 Summary In this chapter, we reviewed the Data Management phase of the HR Analytics Project Life Cycle, which included discussions of data cleaning, manipulating and structuring data, and common data-management tools. "],["dataanalysis.html", "Chapter 5 Data Analysis 5.1 Tools &amp; Techniques 5.2 Continuum of Data Analytics 5.3 Summary", " Chapter 5 Data Analysis Data analysis refers to the process of applying mathematical, statistical, and/or computational techniques to data to identify associations, differences or changes, or classes (categories), as well as to predict the likelihood of future events, values, or differences or changes. This phase draws upon the data that was acquired and managed during the Data Acquisition and Data Management phases, respectively, of the HR Analytics Project Life Cycle. In this chapter, we will review different data-analysis tools and techniques and the continuum of data analytics. The Data Analysis phase of the Human Resource Analytics Project Life Cycle (HRAPLC) refers to the process of applying mathematical, statistical, and/or computational techniques to data to identify associations, differences or changes, or classes (categories), as well as to predict the likelihood of future events, values, or differences or changes. 5.1 Tools &amp; Techniques A variety of tools and techniques can be used for data analysis, including: Mathematics, Statistics, Machine learning, Computational modeling and simulations, and Text analyses and qualitative analyses. 5.1.1 Mathematics With the exception of some forms of qualitative analysis, mathematics acts as the foundation for most data-analysis tools and techniques. Mathematics encompasses a broad array of topics ranging from basic arithmetic to algebra, geometry, and calculus. Even the application of arithmetic and simple counting to data can yield important and valuable insights. For example, a colleague of mine works as an executive coach and shared the following anecdote about a client who was the CEO of a small business. This CEO expressed concern about spending an inordinate amount of time in meetings. In response, my colleague asked how the CEO: How much time do you spend in meetings each week? The CEO responded: I dont know. How can figure that out? Finding the answer to the question ended up being relatively straightforward. My colleague asked if the CEO to share their work calendar for the past four weeks, and together they counted up the total number of hours the CEO spent in meetings over the four-week period and then divided by four to compute the weekly average. The issue wasnt that the CEO was unfamiliar with basic counting and arithmetic; rather, the CEO had never considered that useful data might be acquired from a work calendar and then analyzed using simple math. To me, this anecdote serves as a reminder that even simple forms of data analysis that we learned in primary school can yield valuable insights. Of course, the world of mathematics extends well beyond basic arithmetic to other more-advanced areas like geometry, algebra, and calculus. 5.1.2 Statistics When we think of data analysis in the context of HR analytics, we often focus on statistics, and thus well spend a bit more time addressing this type of data analysis. Statistics is a branch of applied mathematics used to describe or infer something about a population, where a population could be, for example, all of the employees in an organization. In general, we can differentiate between descriptive and inferential statistics. Descriptive statistics are used to describe the characteristics of a sample drawn from a population; often, when dealing with data about human beings in organizations, its not feasible to attain data for the entire population, so instead we settle for what is hopefully a representative sample of individuals from the focal population. Common types of descriptive statistics include counts (i.e., frequencies), measures of central tendency (e.g., mean, median, mode), and measures of dispersion (e.g., variance, standard deviation, interquartile range). When we analyze employee demographic data, for example, we often compute descriptive statistics like the number of employees who identify with each race/ethnicity category or the average employee age and standard deviation. Its important to remember that descriptive statistics are, well, descriptive. That is, they help us describe characteristics of a sample, but when computed with data from a sample, they dont allow us to make inferences about the underlying population from which the sample was drawn. In contrast, we apply inferential statistics when we wish to make inferences about a particular population based on a sample of data we have collected from that population. A common approach to making these inferences is the application of null hypothesis significance testing (NHST). If youre familiar with p-values, then you may also be familiar with NHST. For example, in the HR context, we commonly pilot new training programs with a sample of employees and collect outcome data, and then apply an inferential statistical analysis (e.g., independent-samples t-test) to infer whether the new training program will result in better outcomes than the older training program if applied to the overall population of employees. Two specific types of inferential statistics include parametric and nonparametric statistics. Simply put, parametric statistics make certain assumptions about population parameters and distributions (e.g., normally distributed), whereas nonparametric statistics dont make these assumptions. Thus, when determining which inferential statistical analysis to apply to a sample of data, we often begin by investigating whether the assumptions for a parametric statistical analysis have been met, and if we dont find that the data satisfy those assumptions, we then move on to selecting an appropriate nonparametric statistical analysis. Classic examples of parametric statistics include: t-tests, analyses of variance, Pearson product-moment correlations, and ordinary least squares linear regression. Examples of nonparametric statistics include: Mann-Whitney U-test, Kruskal-Wallis one-way analysis, and McNemars test. 5.1.3 Machine Learning Machine learning allows us to extend mathematical and statistical analysis of data - often to very large amounts of data. Specifically, machine learning is form of artificial intelligence in which data are statistically analyzed in ways that allow the machine to learn when presented with new data (for a brief yet broad overview, see Jordan and Mitchell 2015). Machine-learning algorithms (i.e., models) are often developed for the purposes of making more accurate predictions or classifications  or for finding patterns. For example, an HR data analyst might train, test, and validate a model aimed at accurately predicting who is at-risk for leaving the organization within a given time period. These algorithms can also be applied more broadly to automate processes and systems, with a famous example being how spam emails are detected and assigned to a spam folder (Guzella and Caminhas 2009; Gan 2018). Lets distinguish between two types of machine learning: supervised and unsupervised (Oswald and Putka 2016). In supervised learning, the model is trained, tested, and validated on data with labeled variables and with particular types of patterns, predictions, or classifications specified (e.g., decision tree, Lasso regression). In contrast, in unsupervised learning, variables are unlabeled, and structures, patterns, or clusters are inferred from the data, making this type of machine learning more exploratory in nature (e.g., K-means). 5.1.4 Computational Modeling &amp; Simulations To understand and predict how complex systems behave, we can apply computational modeling and simulations. These tools are particularly useful in contexts in which it might be impractical, difficult (e.g., expensive, resource-intensive), or just not feasible to acquire and analyze real data (for a brief yet broad overview, see Calder et al. 2018). For example, employee interactions are complex and dynamic, and although technologies like sociometric badges enable us to monitor and surveil employee interactions in a mostly unobtrusive manner, such technologies may not be able to capture the specific type of interaction were interested in  and they may pose ethical and legal issues. Further, some types of interactions and relationship building may take months  or even years  to emerge, and it may not be practical to wait that long. To address these limitations and concerns, a tool like agent-based modeling (Bonabeau 2002; Smith and Conrey 2007) would allow us to specify evidence-informed characteristics and interaction decision rules (i.e., parameters) for each employee (i.e., agent) within a multi-employee system and to then observe that simulated system over a period of time. About a decade ago, I developed an agent-based model to simulate how employee emotional contagion and aggressive behavior propagate through a social network, which allowed me to understand how different social-network structures might affect the spread of emotion and aggressive behavior. In some sense, these types of simulations allow use to ask and answer what if questions. Other prominent examples of computational modeling and simulations include dynamical systems modeling and Monte Carlo methods, and the data generated by computational models can also be analyzed using the aforementioned data-analysis tools and techniques. Like any modeling technique, computational models are not without their limitations. For starters, the omission of relevant parameters will influence how closely the model approximates reality, and such models may fail to account for real-world unanticipated shocks to a system. Still, computation modeling can be a useful tool for helping us to test theory and to project what can or will happen in the future in complex systems. 5.1.5 Text Analyses &amp; Qualitative Analyses Text analysis is a large umbrella term for a vast array of analyses aimed at analyzing text data, including quantitative and computer-algorithmic approaches and qualitative-analysis approaches. Lets start by describing quantitative and computer-algorithmic approaches. These approaches are computer driven and tend to efficient and scalable for large (or very large) amounts of text, and include popular toots and techniques like text mining, sentiment analysis, natural language processing, latent semantic analysis, and (broadly) computational linguistics (see Banks et al. 2018 for a review). As the label implies, quantitative and computer-algorithmic approaches often focus on translating rich sources of qualitative data into quantitative data and metrics. These approaches continue to evolve, which has led to improvements in classification and meaning-meaning extraction accuracy. Compared to a human being, quantitative and computer-algorithmic approaches can struggle to incorporate context and to detect and interpret tone and idiomatic expressions appropriately and accurately. Qualitative-analysis approaches differ substantially from quantitative and computer-algorithmic approaches in that the former rely primarily on human beings to analyze and interpret text data. The human brain still offers one of the best tools for detecting subtle contextual influences and interpreting tone and idiomatic expressions. With their reliance on the human brain, qualitative-analysis approaches can lead to a richer and more robust understanding of a phenomenon of interest captured in text data. In terms of human work hours, rigorous qualitative analysis can be quite resource-intensive, as it requires iterative coding and sensemaking by (typically) multiple independent human coders. This can make qualitative-analysis approaches less practical and feasible for large amounts of text data. Examples of qualitative analysis include content analysis, thematic analysis, narrative analysis, discourse analysis, and grounded theory (Strauss and Corbin 1997). 5.2 Continuum of Data Analytics When discussing the Data Analysis phase of the HRAPLC, I would be remiss if I failed to introduce and review the continuum of data analytics. The extent to which an organization does (or does not) engage in the full continuum of data analytics can be an indicator of that organizations data-analytics maturity. In the following sections, well work from descriptive analytics to predict-ish analytics to predictive analytics to prescriptive analytics. The continuum of data analytics classically includes descriptive, predictive, and prescriptive analytics; though, I like to place predict-ish analytics between descriptive and predictive analytics. 5.2.1 Descriptive Analytics Lets begin with descriptive analytics, which falls at the beginning of the data-analytics continuum. We apply descriptive analytics when we wish to apply analyses that describe what has already happened, and thus, descriptive analytics offer a snapshot of the past. Common analyses include descriptive statistics (e.g., mean, standard deviation), HR metrics (e.g., turnover rate, time-to-fill), and various types of inferential statistics that we might use to infer whether differences or associations exist in a population (e.g., correlation, t-test). Some hypothetical examples of descriptive analytics include: Last year, Unit X had a 34% annual turnover rate while Unit Y had a 5% annual turnover rate. A recent survey indicated that 20% of respondents were strongly considering leaving the organization in the next 6 months. On average, employees used 15.3 days of paid time off (PTO) last year. Higher employee engage was strongly and significantly associated with higher task performance (r = .53, p = .01). Descriptive analytics involves analyzing past data and describing what has happened in the past. 5.2.2 Predict-ish Analytics Most discussions of the continuum of data analytics move straight from descriptive analytics to predictive analytics. Lets, however, take the road less traveled by addressing the space between descriptive and predictive analytics: predict-ish analytics. You probably havent heard of predict-ish analytics before because, well, its a term I made up, as many ad-hoc data-analyses in HR analytics end up in this space. I define predict-ish analytics as the estimation of models that can predict what will happen in the future based on a snapshot of data from the past  but that dont actually verify what will happen in the future and how accurate the original models predictions were. In other words, predict-ish analytics stops a bit shy of proper predictive analytics. Often predict-ish analytics take the form of inferential statistical analyses without cross-validation of any kind, and a major shortcoming is that this approach is prone to model overfitting on past data, making it unlikely that the model will fit future data well. As a hypothetical example of predict-ish analytics, imagine the following. An HR analytics team collected data from job incumbents as part of a criterion-related validation study. The team estimated a simple linear regression model by regressing scores for a job performance rating tool on scores on a structured interview selection tool. The results indicated that higher scores on the structured interview were positively and significantly associated with higher job performance ratings (b = 2.98, p = .02), such that for every 1-point increase in structured interview scores is associated with increase of 2.98 points in job performance ratings. The team has not, however, evaluated the accuracy of their model by applying it to a new sample of data from the intended population. Predict-ish analytics involves analyzing past data and building a predictive model but stopping short of testing or validating that model with a new sample of data. 5.2.3 Predictive Analytics Unlike descriptive analytics, predictive analytics focuses on the future. That being said, predictive analytics initially looks to the past. More precisely, predictive analytics refers to the process of estimating a model based on past data in order to predict what will happen in the future, and then testing and validating how accurate the models predictions are when the model is applied to new data. One of the classic examples of predictive analytics is the cross-validation of an inferential statistical model (e.g., linear regression model). Other more-advanced forms of predictive analytics involve applying machine-learning procedures or computational modeling. As a hypothetical example of predictive analytics, imagine the following scenario. Using available data from the previous year, an HR analyst estimated a logistic regression model with voluntary turnover (i.e., stayed vs. quit) as the outcome of interest and employee engagement and task performance as predictors. The results indicated that engagement and task performance both significant predictors of who would quit over the subsequent 6 months based on the initial sample of data, and more specifically, both engagement and task performance were negatively associated with turnover, such that those with lower engagement and task performance were more likely to quit. At the end of the following year, the analyst applied the model to new data that had been acquired for employee engagement, task performance, and voluntary turnover, and the analyst found that the model predicted who would quit and who would stay with 85% accuracy. Predictive analytics involves estimating a model based on past data, and then applying that model to new data in order to evaluate the accuracy of the models predictions. 5.2.4 Prescriptive Analytics At the far end of the continuum of data analytics sits prescriptive analytics. Prescriptive analytics takes predictive analytics to the next level by prescribing action and making targeted recommendations based on predictive-analytics findings. That is, in addition to interpreting the results of the predictive-analytics process, prescriptive analytics involves taking those results and coming up with evidence-informed interventions, recommendations, and actions. To illustrate how prescriptive analytics might work in practice, lets continue with the hypothetical example that I provided for predictive analytics (see above). As a reminder, the model was focused on predicting who would voluntarily turnover based on engagement and task performance scores. Because the model predicted who would quit and how would stay with 85% accuracy, the HR analyst prescribed the following actions: The model should be applied at the beginning of each year to identify those individuals who have a higher probability of voluntarily turning over, allowing managers to check in with at-risk employees with greater frequency. Where feasible, the organization should redesign jobs to improve employee engagement, as lower engagement was found to be one of the drivers of voluntary turnover. In order to improve employee task performance, the organization should train managers how to deliver constructive feedback, as lower task performance was found to be one of the drivers of voluntary turnover. Prescriptive analytics expands upon predictive analytics by taking the results of the predictive-analytics process and recommending evidence-informed interventions and actions. 5.3 Summary In this chapter, we took a tour of the Data Analysis phase of the HR Analytics Project Life Cycle. Along the way, we learned about common tools and techniques for data analysis, including mathematics, statistics, machine learning, computational modeling and simulations, and text analyses and qualitative analyses. We concluded our tour by reviewing the continuum of data analytics, ranging from descriptive analytics to predict-ish analytics to predictive analytics to prescriptive analytics. "],["datainterpretationstorytelling.html", "Chapter 6 Data Interpretation &amp; Storytelling 6.1 Data Interpretation 6.2 Storytelling 6.3 Storytelling with Data 6.4 Summary", " Chapter 6 Data Interpretation &amp; Storytelling Data interpretation and storytelling refers to the process of making sense of data analysis findings and evaluating questions and hypotheses, as well as disseminating the findings to different stakeholders. To support data interpretation and storytelling with data, data visualizations are commonly used (e.g., graphs, charts, plots, dashboards). This phase draws upon the results of the Data Analysis phase of the HR Analytics Project Life Cycle. In this chapter, we will review different data-analysis tools and techniques and the continuum of data analytics. The Data Interpretation &amp; Storytelling phase of the Human Resource Analytics Project Life Cycle (HRAPLC) refers to the process of making sense of data analysis findings and evaluating questions and hypotheses, as well as disseminating the findings to different stakeholders. Effective data interpretation and storytelling relies on the following: Recognizing the role of human decision making and judgment (including biases) when interpreting data-analytic findings; Understanding the context in which the data used for analysis were acquired and the context in which the story about the data will be told; Connecting the data-analytic findings back to the original problem or question to evaluate what (if any) solutions or answers may be informed by the findings; Acknowledging and honoring the needs and knowledge of the stakeholders to whom the story will be told; Focusing on effective communication, connection, and clarity with stakeholders when telling a story about data-analytic findings and interpretations. 6.1 Data Interpretation At this moment in time, data interpretation is still mostly a human endeavor. That is, human beings are still one of the best resources we have when it comes to interpreting the results of data analyses. In particular, humans are capable using their potentially rich knowledge of the context (e.g., population from which data were sampled; organizational history) and theory with data-analytic findings. With that being said, two people who come from different backgrounds and who have different knowledge, skills, and abilities may interpret the same findings differently, especially when it comes to the perceived practical significance of the findings. A difference in interpretation need not necessarily be a bad thing, though, as it can lead to meaningful conversations and potentially new problems that need solutions and questions that need answers. Although I treat Data Analysis and Data Interpretation &amp; Storytelling as distinct phases in the HR Analytics Project Life Cycle, analysis and interpretation often occur iteratively, such that an analyst jumps back and forth between the two phases. For example, an analyst might analyze the association between employees levels of job satisfaction and their level of supervisor-rated performance, and then interpret the statistical and practical significance of that finding; based on their interpretation, the analyst may decide to then analyze whether that association between job satisfaction and job performance is the same or different across organizational units, followed by interpretation of the results. 6.2 Storytelling Storytelling is an innate part of the human experience and is often used to facilitate shared learning and knowledge acquisition. Humans often construct stories to make sense of and explain their experiences  even when theyre not instructed to do so. For example, the story model from the juror decision making literature posits that jurors construct verbal narratives and mental representations of trial-related events to make sense of the information theyre presented with and to arrive at a verdict (Bennett and Feldman 1981; Devine 2012; Denning 1986; Pennington and Hastie 1993). In fact, jurors own life events and perceptions shape the story they construct. We can define a story as an account of a connected succession of events that is intended to instruct, inform, spur interest, and/or entertain (LiteraryTerms.net n.d.; Merriam-Webster.com n.d.; Dictionary.com n.d.). And by extension, storytelling is the process of communicating a story ourselves or to others. Telling a story can be done used any mode of communication, such as written words, spoken words, imagery, body language, music, and physical performance. In the business context, storytelling may take on a strategic purpose, which can be called strategic storytelling. To that end, an Organizational Performance Consultant named Seth Kahan described strategic storytelling as follows (Denning 2006). In my work at NASA, I coached leaders on how to tell stories that accelerate positive change. The first thing these leaders needed to realize is that their primary goal is to make change happen, not to be seen as a good storyteller. Its immaterial whether or not listeners are aware that a story is being told, much less that they admire the story. However, the listeners reactions in response to the story are critical. We are looking for changes in behavior that align with the leaders objectives. When that happens you have a powerful change agent equipped with a powerful tool.  Seth Kahan, Organizational Performance Consultant 6.2.1 Structure Whether intentional or not, a story unfolds with a narrative structure, which includes the key parts or components of the story and the order in which they are presented. The linear structure is perhaps the simplest, as it partitions the story into a beginning, middle, and end  in that order. That being said, often even the simplest linear structures deploy tension in some manner, and storytellers often create tension that is in need of a resolution, leading to a climax of some kind (e.g., Freytags Dramatic Structure). Using tension effectively has the potential to foster greater audience engagement and interest. The mountain structure is a common storytelling structure (Knaflic 2015, 2020). This structure begins with the introduction of the plot. The plot includes providing needed context for the story and describing a gap between the current state of affairs and a desired state of affairs is introduced to the audience. To draw the audience into the story, its often helpful to provide eye-catching, provocative, or shocking information, as this can signal to the audience why they should care about the story youre beginning to tell. After introducing the plot, the story builds tension through rising action to ultimately arrive at the climax, where the tension between the current and desired states of affairs reaches its peak. Arriving at the climax can be an opportunity to articulate the problem that needs a solution or the question that needs an answer. After the climax, the story resolves the tension until the ending  and in the case of strategic storytelling, falling action leads to a call to action wherein the story attempts to persuade the audience that some action is needed to achieve the desired state of affairs. If a problem (or question) was set up earlier in the story, the ending is where solutions to the problem can be offered (or answers to the question). The mountain structure builds tension (rising action) and resolves tension (falling tension). This is an example of the mountain structure applied to the HR context. 6.2.2 Clarity &amp; Parsimony Particularly in the case of strategic storytelling, its advisable to tell a story that is both clear and parsimonious. That is, avoid convoluted and complicated narratives that distract from the central message that the story is intended to communicate. Generally, I recommend identifying 1-3 points that you would really like for the audience to remember and/or act on  and telling a parsimonious story with deliberate and thoughtful repetition can facilitate this process. When telling stories using visual a visual medium (e.g., data visualization, PowerPoint presentation), sometimes there is a tendency to fill every available space with words or images. Often, however, it is more effective to leave unused visual space and to avoid unnecessary visual clutter. Similarly, in written and oral presentations, I recommend prioritizing brevity over verbosity and using silence before or after key ideas to create a sense of dramatic importance. To help achieve clarity and parsimony, I recommend focusing on a particular problem that needs a solution or a question that needs an answer. Doing so can help to ensure that your story will stay focused and on message. Further, towards the end of a story, sometimes it is helpful to remind the audience of the focal problem or question  and then tie the key takeaway points (e.g., call to action) back to the problem or question. 6.2.3 Influence &amp; Persuasion In the case of strategy storytelling, our goal is often to influence and persuade the audience to feel, think, or do something, and thus its worthwhile to unpack what influence and persuasion entail. Influence  or social influence  refers to change in preferences or behavior that one individual or group causes in another, and as a specific form of influence, persuasion is the active attempt by an individual, group, or social entity (e.g., government, political party, business) to change a persons beliefs, attitudes or behaviors by conveying information, feelings, or reasoning (Cacioppo, Cacioppo, and Petty 2018). For example, in HR we might try to persuade: decision makers to invest in a data-informed employee retention initiative; HR professionals to build data-literacy skills; decision makers to support the development or acquisition of selection methods that leverage artificial intelligence (AI); front-line managers to check in and provide feedback regularly with their employees regarding their performance. When used effectively, influence and persuasion tactics can help HR analytics teams move from the Data Interpretation and Storytelling phase of the HR Analytics Project Life Cycle to the Deployment and Implementation phase. More specifically, influence and persuasion can facilitate the leap from knowledge acquired through data interpretation and storytelling to prescribed action that can be deployed and implemented in the organization  a process that is sometimes referred to as closing the knowing-doing gap (Pfeffer and Sutton 2000). For persuasion to lead to meaningful change in the audience, it is important to (a) recognize the boundaries of your own expertise, (b) understand the audience, (c) craft the message, and (d) tell the story. 6.3 Storytelling with Data XXXXX 6.4 Summary XXXXX "],["deploymentimplementation.html", "Chapter 7 Deployment &amp; Implementation", " Chapter 7 Deployment &amp; Implementation Deployment and implementation refers to the process of prescribing or taking action based on interpretation of data-analysis findings. This phase requires an (a) understanding of stakeholder needs, (b) an understanding of the business context, and (c) knowledge of change management theories and practices. "],["overviewR.html", "Chapter 8 Overview of R &amp; RStudio 8.1 R Programming Language 8.2 RStudio 8.3 Packages 8.4 Summary", " Chapter 8 Overview of R &amp; RStudio Advances in technology have paved the way for increasingly powerful, sophisticated, and expansive data-analytic tools. Examples of such tools include enterprise solutions by IBM and SAP as well as alteryx, Microsoft Power BI, Tableau, SAS, and Google TensorFlow. At the same time, barriers to entry for using powerful programming languages like R and Python have fallen as the number and quality of educational programs aimed at teaching programming has grown and the amount of free online content has increased. As time marches forward, the power, functionality, and capabilities of our data-analytics technologies have increased rapidly, with prime examples including tools like Tableau, Python, R, and TensorFlow. Compared to working with off-the-shelf platforms (e.g., Tableau, SAP), by working directly with programming languages, analysts can design and implement operations, models, and other tools that meet their specific needs. Programming languages allow analysts to define or customize their own functions or apply functions developed by experts around the world. In fact, these languages can have the advantage of getting us closer to our data and understanding and seeing the myriad decisions we must make when acquiring, managing, analyzing, and visualizing data. In this book, we focus specifically on the R programming language (R Core Team 2021). In the following sections, we will learn about about the R programming language itself as well as RStudio (RStudio Team 2020), where the latter is a integrated development environment for R. 8.1 R Programming Language In the following sections, we will learn answers to the following questions about R: What is R? Why use R? Who uses R? 8.1.1 What Is R? R is an open-source and freely available statistical programming language and environment that can be used for data management, analysis, and visualization. R is similar to the S language, where the latter was developed at Bell laboratories. You can learn more about R on the R Project website. R software can be freely downloaded for Windows, MacOS, and Linux operating systems via a Comprehensive R Archive Network (CRAN) mirror. Each CRAN mirror is a server that acts as a repository and distribution site that allows users to download copies of R-related software. Currently, CRAN mirrors are hosted at institutions all over the world, such as the University of Science and Technology Houari Boumediene in Algeria, Univerisiti Putra Malaysia in Malaysia, University of Bergen in Norway, and Indiana University in the United States. Often, R is referred to as base R, which can be useful for distinguishing it from add-on software like RStudio. To learn how to install the base R software, please refer to the following chapter. 8.1.2 Why Use R? R is a popular tool for managing, analyzing, and visualizing data. Some characteristics that make R particularly attractive are: R and its packages are free! R allows users to define new functions. Due to its open-source nature, R is often fast to react to new advances in data analysis and visualization. R is a powerful and constantly evolving language that many employers and educational institutions value. R is especially well-suited for ad hoc statistical analysis of data and data visualization. 8.1.3 Who Uses R? Data analysts and data scientists all over the world use R, including at well-known organizations like Google, Facebook, NASA, and Janssen. In addition, many scientists at academic institutions use R to statistically analyze data from research projects. 8.2 RStudio In the following sections, we will learn answers to the following questions about RStudio: What is RStudio? Why use RStudio? Who uses RStudio? 8.2.1 What is RStudio? RStudio is an integrated development environment (IDE) for R. RStudio uses base R as its engine and layers on additional features. Although an IDE like RStudio is not required to use R, using R via RStudio has a number of benefits. Namely, RStudio provides a user-friendly interface as well as easy-access to and integration with RMarkdown (Xie, Allaire, and Grolemund 2018; Allaire et al. 2021) and Shiny web applications (Chang et al. 2021), for example. Further, by default, RStudio includes window panes designated for R scripts, the Console, the Environment, and Plots. You can learn more about RStudio at the official website. Free desktop and server versions of RStudio are available, and you can learn how to download and install the desktop version in the following chapter. 8.2.2 Why RStudio? RStudio is a popular tool for implementing the R language and environment. Some characteristics that make RStudio particularly attractive are: The open-source versions of RStudio Desktop and Server are free to download and use. RStudio makes working in R easier, especially for beginners. RStudio facilitates report generation, particularly when the the report format remains generally the same over time but the data are updated. The RStudio developers hold conferences regularly, which can be great venues to connect with other R users. 8.2.3 Who Uses RStudio? Many people who use base R choose to use RStudio as well. Ive found that its relatively rare to find people who work directly from base R these days. 8.3 Packages Although a base R installation comes standard with a number of very useful standard functions, a major advantage of using R is the availability packages with more specialized functions. A package contains a collection of functions, generally with an overarching theme or purpose. For example, the psych package (Revelle 2021) includes a suite of functions that are well-suited to conducting the types of analyses that are common in psychology. As I type this sentence, there are currently over 17,000 available R packages, and you can view the current list of available packages sorted by name by clicking here. Examples of packages that I demonstrate in this book include: apaTables (Stanley 2021), dplyr (Wickham et al. 2021), ggplot2 (Wickham 2016), lavaan (Rosseel 2012), lessR (Gerbing, Business, and University 2021), psych (Revelle 2021), readr (Wickham and Hester 2020), and tidyr (Wickham 2021a). 8.4 Summary In summary, R is a powerful and widely used programming language and environment, and RStudio is an integrated development environment that layers user-friendly features and helpful tools onto R. Together, they help data analysts and data scientists manage, analyze, visualize, and report data. "],["install.html", "Chapter 9 Installing R &amp; RStudio 9.1 Video Tutorial 9.2 Downloading &amp; Installing R 9.3 Downloading &amp; Installing RStudio 9.4 Summary", " Chapter 9 Installing R &amp; RStudio If you have a Windows, Mac, or Linux operating system, you have several ways in which you can begin working in R. Commonly, users install R on their computer along with an integrated development environment (IDE) software application like RStudio. Recently, RStudio Cloud (https://rstudio.cloud) has emerged as an alternative to installing R and RStudio by allowing users to use R and RStudio via the cloud, which notably allows those using the Google Chrome operating system to access R and RStudio. 9.1 Video Tutorial When it comes to learning how to use and apply R Ive found that some people prefer written tutorials, others prefer video tutorials, and some like to learn using a combination of written and video tutorials. When first starting out, you might find it easier to follow along with a video tutorial, and as you get more comfortable with R, you may begin to prefer the written tutorials that are integrated into each chapter. Generally, the written tutorials contain more information and often more functions and operations, whereas the video tutorial provides the need to know information, functions, and operations. Because this first chapter is just focused on downloading and installing the R and RStudio programs, the written tutorial provided below may suffice; however, if you get stuck, you might find it useful to check out the video tutorial. As a final note, I created the video tutorial embedded in this book using a Windows computer, and thus there might be some minor aesthetic differences in the RStudio interface  as well as differences in hot keys (e.g., Ctrl+C vs. Command+C). Link to video tutorial: https://youtu.be/b18IHQERT4A 9.2 Downloading &amp; Installing R In the following sections, you will learn how to download and install the R program for Windows and Mac operating systems. The base R program must be installed prior to installing the RStudio program. R is open-source software and free to download. 9.2.1 For Windows Operation Systems R can currently run under operating systems as old as Windows Vista (circa 2007). To download R for your Windows operating system for the first time, click on this link: https://cran.r-project.org/bin/windows/base/. Once you are on the R download page, click on the hyperlink to download the current version of R for Windows. Once the file has downloaded, follow the installation prompts. 9.2.2 For Mac Operating Systems The current version of R works with Mac OS X (release 10.6 and higher). To download R for Mac OS X operating system for the first time, click on this link: https://cran.r-project.org/bin/macosx/. If you have Mac OS X 10.11 or higher, click on the hyperlink (with .pkg extension) under the Latest release section to begin your download. If you have Mac OS X 10.10 or lower, click on the appropriate hyperlink (with .pkg extension) under the Binaries for legacy OS X systems section. Once the file has downloaded, follow the installation prompts. I dont advise using a Mac operating system that is older than Mac OS X 10.6 (which came out in 2009), as you may run into issues when using certain R packages for data analysis and visualization. 9.3 Downloading &amp; Installing RStudio RStudio is not required to use R; however, RStudio offers a number of helpful features and a user-friendly interface. More specifically, RStudio is an integrated development environment (IDE) for R. The desktop version of RStudio is free to download. To do so, click on this link: https://rstudio.com/products/rstudio/download/#download. Make sure that you have already installed the R program to your computer (see above). Click on the download button below the RStudio Desktop followed by the version number. Again, this version is free. Under the heading Installers for Supported Platforms, click on the link that corresponds to your operating system. Once the file has downloaded, follow the installation prompts. 9.4 Summary In this chapter, we learned how to install R and RStudio for our Windows or Mac operating systems. "],["gettingstarted.html", "Chapter 10 Getting Started with R &amp; RStudio 10.1 Orientation to RStudio 10.2 Creating &amp; Saving an R Script 10.3 Creating an RStudio Project 10.4 Orientation to Written Tutorials 10.5 Summary", " Chapter 10 Getting Started with R &amp; RStudio Like any program, there is bound to be a learning curve. Personally, one of the initial barriers I face when working with a new program is navigating the graphic user interface (GUI). Given that, in this chapter we will begin with a brief orientation to the RStudio program. Subsequent sections will focus on creating and saving R scripts, creating and working with RStudio projects, and how to follow along with written tutorials like the ones presented throughout this book. 10.1 Orientation to RStudio Rather than describe a series of screenshots, I believe one of the best ways to learn how to navigate the RStudio interface is to watch someone else. In the following video, I walk through key (but not all) features of the RStudio interface. This is one of the rare occasions in this book in which only a video tutorial is provided. Please note that in the video I walk through the Windows version of RStudio Desktop; the MacOS version may have slight differences in layout. Link to video tutorial: https://youtu.be/cq4wixfCuhQ 10.2 Creating &amp; Saving an R Script An R Script is a text editor file in which you can create, edit, and save your R code for a particular task or project. An R Script file has the .R file extension. It is advisable that you type code directly into an R Script file if you wish to use the code again in the future or if you wish to save the code for another session. In general, try to avoid writing code directly into the Console using the command line if you wish to later reproduce your work. An R Script also allows you to make and save annotations (using the # symbol) to explain your code and decision making. Once you typed code (and annotations) into an R Script, you can highlight all of it (or chunks of it) and then click the Run button (or CTRL+Enter for Windows users or Command+Enter for Mac users), which is located in the upper right hand corner of the R Script editor window. In essence, an R Script allows you to save your code and to tell a story about what you have done. As much as you believe youll never forget what you were doing in a particular R session, you will likely forget important details as time passes. Or, imagine a scenario in which someone else inherits your data project; a well-written and -documented R Script file will help them retrace your footsteps and onboard them onto the project. If you prefer to follow along with screenshots when learning how to navigate a programs interface, feel free to follow along with the written instructions for creating and saving an R script in this section. If, however, you prefer a voiceover of me walking through the interface menus in a video, then by all means follow along with the video below. Link to video tutorial: https://youtu.be/6_CFx5-KmMI 10.2.1 Creating a New R Script To create a new R Script in RStudio, in the drop-down menu, select File &gt; New File &gt; R Script (as shown below). 10.2.2 Using an R Script To use an R Script, simply type into the script interface. To illustrate how to do this, lets type # Adding 2 plus 3 on the first line; note that I began the line with the # symbol, which tells R that any text written to the right is annotation and thus wont be interpreted by R when you select it and click Run. On the next line, lets type 2 + 3. Highlight both lines of script you typed and click the Run button (or CTRL+Enter for Windows users or Command+Enter for Mac users) (as shown below). # Adding 2 plus 3 2 + 3 ## [1] 5 Your Console window should show your output (as shown above). 10.2.3 Saving an R Script Always remember to save your R Script, and do so frequently. To save an R Script in RStudio, in the drop-down menu, select File &gt; Save As (as shown below). After that, a window will open, and you can save the R Script file in a location of your choosing and with a name of your choosing. 10.2.4 Opening a Saved R Script To open a saved R Script in RStudio, in the drop-down menu, select File &gt; Open File (as shown below). After that, a window will open, and you can select the R Script file to open. 10.3 Creating an RStudio Project An RStudio project (or R project) file (.Rproj) is specific to RStudio and allows one to cluster associated scripts and data files into into a single workflow. For example, if you were evaluating a new onboarding program for your company, you could create an RStudio project with a common working directory that ties together any data files and R scripts that are relevant for evaluating the program. Creating an R project is not required for data management, analysis, and visualization work in RStudio, but it can be helpful. For more information on the value of RStudio projects, check out Wickham and Grolemunds (2017) section on RStudio projects. To learn how to create an RStudio project, you have the choice between following along with screenshots and written explanations or the voiceover video tutorial below. Link to video tutorial: https://youtu.be/WyrJmJWgPiU 10.3.1 Creating a New RStudio Project First, to create a new project in RStudio, in the drop-down menu, select File &gt; New Project. Second, when the Create Project window pops up, select the New Directory option if you have not yet created a working directory that can be used for your project (see Figure 2). [Alternatively, select the Existing Directory option if already have a working directory in place that can be used for your project.] Third, in the Project Type window, select New Project. Fourth, in the Create New Project window, input what you would like to name the new project (in the field under Directory name) and select the location of your working directory. Finally, click the Create Project button. 10.3.2 Opening an Existing RStudio Project To open an existing RStudio project, in the drop-down menu, select File &gt; Open Project. 10.4 Orientation to Written Tutorials Throughout this book, I have included sample R code embedded in chapter tutorials, which I created using RMarkdown. This approach to demonstrating R tools and techniques is common, and thus its good to orient yourself to written tutorials in this format (which can be displayed in HTML or PDF formats). The following video provides an orientation to the in-chapter written tutorials involving R code that you will have the opportunity to follow along with in subsequent chapters. To learn how to create an RStudio project, you have the choice between following along with screenshots and written explanations or the voiceover video tutorial below. Link to video tutorial: https://youtu.be/1Wh6eUYAoZc 10.5 Summary In this chapter, you learned how to set a working directory, create an R script, create an RStudio project, and orient yourself to written R tutorials. First, setting the working directory is often an important step when reading (importing) and writing (exporting objects) in R. You can use the getwd function to check where your current working directory is, whereas the setwd can be used to set a new working directory. Second, writing and saving your R code in an R Script file (.R) is an important step towards reproducible data management, analysis, and visualization. Third, creating an RStudio project can streamline data-analytic projects and provides some user-friendly features. Finally, written R tutorials are commonly presented in either printed (PDF) and web-based (HTML) formats, and thus its worthwhile to familiarize yourself with how to follow along with these tutorial formats. "],["gentleintro.html", "Chapter 11 Basic Features and Operations of the R Language 11.1 Video Tutorial 11.2 Functions &amp; Packages Introduced 11.3 R as a Calculator 11.4 Functions 11.5 Packages 11.6 Variable Assignment 11.7 Types of Data 11.8 Vectors 11.9 Lists 11.10 Data Frames 11.11 Annotations 11.12 Summary", " Chapter 11 Basic Features and Operations of the R Language In this chapter, you will learn about basic features of the R language along with key bits of terminology. Think of this chapter as the gentle introduction to R that nearly every book on R includes. Also, it is completely fine if you dont fully grasp certain concepts and functions upon completing this chapter. We will revisit many of these concepts and functions in the HR context in subsequent chapters. Until then, use this chapter as an opportunity to practice writing R code. 11.1 Video Tutorial When exploring the basic features, operations, and functions feel free to follow along with the written tutorial below or to check out the video. In the video, I offer an abbreviated version of whats covered in the written tutorial and focus on what I think most beginners need to know and understand early on about R. In the written tutorial, I get into some functions and operations that probably wont become relevant until further along in your learning of using R as tool for HR analytics. Link to video tutorial: https://youtu.be/yHbVbHEjhLQ 11.2 Functions &amp; Packages Introduced Function Package print base R class base R str base R install.packages base R library base R is.numeric base R is.integer base R is.character base R is.logical base R as.Date base R as.POSIXct base R c base R data.frame base R names base R 11.3 R as a Calculator In its simplest form, R is a calculator. You can use R to carry out basic arithmetic, algebra, and other mathematical operations. The arithmetic operators in R are + (addition), - (subtraction), * (multiplication), / (division), ^ (exponent), and sqrt (square root). Below, you will find an example of these different arithmetic operators in action. In this book, lines of output are preceded by double hashtags (##); however, in your own R Console, you will not see the double hashtags before your output  unless, that is, you use double hashtags before your lines of script annotations. 3 + 2 ## [1] 5 3 - 2 ## [1] 1 3 * 2 ## [1] 6 3 / 2 ## [1] 1.5 3 ^ 2 ## [1] 9 sqrt(3) ## [1] 1.732051 Note how the six lines of output we generated (see above) appear in the same order in your Console; relatedly, remember that in R (like many other languages) the order of operations is important. In R it doesnt matter whether there are spaces between the numeric values and the arithmetic operators. As such, we can write our code as follows and arrive at the same output. 3+2 ## [1] 5 3-2 ## [1] 1 3*2 ## [1] 6 3/2 ## [1] 1.5 3^2 ## [1] 9 sqrt(3) ## [1] 1.732051 11.4 Functions A function refers to an integrated set of instructions that can be applied consistently. Some functions also accept arguments, where an argument is used to further refine the instructions and resulting operations of the function. In R we can use functions that come standard from base R or functions that come from downloadable packages. Lets take a look at the print function that comes standard with base R, which means that we dont need a special package to access the function. This wont be terribly exciting, but we can enter 3 as an argument within the print function parentheses; in general, arguments will appear within the inclusive parentheses. print(3) ## [1] 3 Note how the print function simply printed the numeric value 3 that we entered. We can also do the classic - yet super cliche - Hello world! example to illustrate how R and the print function handle text/character/string data; except, lets change it to \"Hello HR Analytics!\". print(&quot;Hello HR Analytics!&quot;) ## [1] &quot;Hello HR Analytics!&quot; Note how we have to put text/character/string data in quotation marks. We can use double (\" \") or single quotes (' '). Some people prefer double quotes and some prefer single quotes. I happen to prefer double quotes. Now, lets play around with the class function. The class function is used for determining the data type represented by a datum or by multiple data that are contained in a vector or variable. By entering 3 as an argument in the class variable, we find that the data type is numeric. class(3) ## [1] &quot;numeric&quot; If you would like to learn more about a function and the types of arguments that can be used within the function, you can access the help feature in R to access documentation on the function. The easiest way to do this is to enter ? before the name of the function. Upon doing so, a help window will open; if youre using RStudio, a specific window pane dedicated to Help will open. ?class 11.5 Packages A package is a collection of functions with a common theme or that can be applied to address a similar set of problems. R packages go through a rigorous and laborious development and vetting process before being posted on the CRAN website (https://cran.r-project.org/). There are two functions that are important when it comes to installing and using packages. First, the install.packages function is used to install a package. The name of the package you wish to install should be surrounded with quotation marks (\" \" or ' ') and entered as an argument in the function. For example, if we wish to install the lessR package (Gerbing, Business, and University 2021), we type install.packages(\"lessR\"), as shown below. Please note that the names of packages (and functions, arguments, and objects) are case sensitive in R. install.packages(&quot;lessR&quot;) Once you have installed a package, you use the library function to check out the package from your library of functions. To use the function, enter the exact name of the function without quotation marks. library(lessR) ## ## lessR 4.1.3 feedback: gerbing@pdx.edu web: lessRstats.com/new ## --------------------------------------------------------------- ## &gt; d &lt;- Read(&quot;&quot;) Read text, Excel, SPSS, SAS, or R data file ## d is default data frame, data= in analysis routines optional ## ## Learn about reading, writing, and manipulating data, graphics, ## testing means and proportions, regression, factor analysis, ## customization, and descriptive statistics from pivot tables. ## Enter: browseVignettes(&quot;lessR&quot;) ## ## View changes in this new version of lessR. ## Enter: help(package=lessR) Click: Package NEWS ## Enter: interact() for access to interactive graphics 11.6 Variable Assignment Variable assignment is the process of assigning a value or multiple values to a variable. There are two assignment operators that can be used for variable assignment as well as for (re)naming objects such as tables and data frames: &lt;- and =. Both work the same way. I prefer to use &lt;-, but others prefer =. In the example below, we assign the value 3 to a variable (i.e., object) we are naming x. x &lt;- 3 x = 3 Both functions achieved the same end, and the function that was run most recently overrides the previous attempt at assigning 3 to x. Using the print function we check with this worked. print(x) ## [1] 3 Or, instead of using the print function , we can simply run x by itself. x ## [1] 3 11.7 Types of Data In general, there are four different types of data in R: numeric, character, Date, and logical. 11.7.1 numeric Data numeric data are numbers or numeric values. This data type is ready-made for quantitative analysis. We can apply the is.numeric function to determine whether a value or variable is numeric; if the value or variable entered as an argument is numeric, R will return TRUE, and if it is not numeric, R will return FALSE. [Note that TRUE and FALSE statements dont require quotation marks like text/character/string data, as they are handled differently in R.] Finally, lets see if that \"Hello data science!\" phrase is numeric. is.numeric(3) ## [1] TRUE is.numeric(TRUE) ## [1] FALSE is.numeric(&quot;Hello data science!&quot;) ## [1] FALSE An integer is a special type of numeric data. An integer does not have any decimals, and thus is a whole number. To specify that numeric data are of type integer, L must be appended to the value. For example, to specify that 3 is an integer, it should be written as 3L. To verify that a value is in fact of type integer, we can apply the as.integer function. is.integer(3L) ## [1] TRUE is.integer(3) ## [1] FALSE Alternatively, we can use the class or str functions to determine whether a value or variable is integer or numeric. The function str is used to identify the structure of an object (e.g., data frame, variable, value). class(3L) ## [1] &quot;integer&quot; str(3L) ## int 3 class(3) ## [1] &quot;numeric&quot; str(3) ## num 3 Finally, if we assign a numeric or integer value to a variable, the resulting variable will take on the numeric or integer data type (respectively). x &lt;- 3 class(x) ## [1] &quot;numeric&quot; x &lt;- 3L class(x) ## [1] &quot;integer&quot; 11.7.2 character Data Data of type character do not explicitly or innately have quantitative properties. Sometimes this type of data is called string or text data. Data of type factor is similar to character but handled differently by R; this distinction becomes more important when working with vectors and analyses. That said, many analysis functions automatically convert character to factor for analyses, but when it comes to working with and manipulating data frames, this character versus factor distinction becomes more important. When data are of type character, we place quotation marks (\" \" or ' ') around the text. For example, if the character of interest is old, then we place quotation marks around text like this \"old\". Also note that character data are case sensitive, which means that \"old\" is not the same as \"Old\". Using the function is.character, we can determine whether data are in fact of type character. is.character(&quot;old&quot;) ## [1] TRUE Note how omitting the \" \" results in an error message. is.character(old) ## Error in eval(expr, envir, enclos): object &#39;old&#39; not found Finally, if we assign a numeric or integer value to a variable, the resulting variable will take on the numeric or integer data types. y &lt;- &quot;old&quot; class(y) ## [1] &quot;character&quot; 11.7.3 Date Data When working with dates in R, there are two different types: Date and POSIXct. Date captures just the date, whereas POSIXct captures the date and time. Behind the scenes, R treats Date numerically as the number of days since January 1, 1970, and POSIXct as the number of seconds since January 1, 1970. To specify a value as a date, we can use the as.Date function. z &lt;- as.Date(&quot;1970-03-01&quot;) class(z) ## [1] &quot;Date&quot; If we convert a variable of type Date to numeric using the as.numeric function, the result is the number of days since January 1, 1970. z &lt;- as.Date(&quot;1970-03-01&quot;) as.numeric(z) ## [1] 59 Now we can use the as.POSIXct function to specify a value as a date and time. Note the very specific format in which the data and time are to be written. z &lt;- as.POSIXct(&quot;1970-03-01 13:10&quot;) class(z) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; If we convert a variable of type POSIXct to numeric using the as.numeric function, the result is the number of seconds since January 1, 1970. z &lt;- as.POSIXct(&quot;1970-03-01 13:10&quot;) as.numeric(z) ## [1] 5173800 11.7.4 logical Data Data that are of type logical can take on values of either TRUE or FALSE, which correspond to the integers 1 and 0, respectively. As mentioned above, although TRUE and FALSE appear to be character or factor data, they are actually logical data, which means they do not require quotation marks (\" \" or ' '). w &lt;- FALSE class(w) ## [1] &quot;logical&quot; is.logical(w) ## [1] TRUE 11.8 Vectors A vector is a group of data elements in a particular order that are all the same data type. To create a vector, we can use the c function, which stands for combine. Within the c function parentheses, we can list the data elements and separate them by commas, as commas separate arguments within a functions parentheses. We can also assign a vector to a variable using either the &lt;- or = operator. We can create vectors for all of the data types: numeric, character, Date, and logical. As an example, lets create a vector of numeric values, and lets call it a. a &lt;- c(1, 4, 7, 11, 19) Using the class and print functions, we can determine the class of our new a object and print its values, respectively. class(a) ## [1] &quot;numeric&quot; print(a) ## [1] 1 4 7 11 19 Lets repeat this process by creating vectors containing integer, character, Date, and logical values. b &lt;- c(3L, 10L, 2L, 5L, 5L) class(b) ## [1] &quot;integer&quot; print(b) ## [1] 3 10 2 5 5 c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) class(c) ## [1] &quot;character&quot; print(c) ## [1] &quot;old&quot; &quot;young&quot; &quot;young&quot; &quot;old&quot; &quot;young&quot; d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) class(d) ## [1] &quot;Date&quot; print(d) ## [1] &quot;2018-06-01&quot; &quot;2018-06-01&quot; &quot;2018-10-31&quot; &quot;2018-01-01&quot; &quot;2018-06-01&quot; e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) class(e) ## [1] &quot;logical&quot; print(e) ## [1] TRUE TRUE TRUE FALSE FALSE We can also perform mathematical operations on vectors. For instance, we can multiply vector a (which we created above) by a numeric value, and as a result each vector value will be multiplied by that value. This is an important type of operation to remember when it comes time to transform a variable. a * 11 ## [1] 11 44 77 121 209 Note that performing mathematical operations on a vector does not automatically change the properties of the vector itself. If you inspect the a vector, you will see that the original data (e.g., 1, 4, 7, 11, 19) remain. print(a) ## [1] 1 4 7 11 19 If we want to overwrite a vector with new values based on our operations, we can use &lt;- or = to name the new vector (which, if named the same thing as the old vector, will override the old vector) and, ultimately, to create a vector with the operations applied to the original values. a &lt;- a * 11 print(a) ## [1] 11 44 77 121 209 To revert back to the original vector values for object a, we can simply specify the original values using the c function once more. a &lt;- c(1, 4, 7, 11, 19) Lets now apply subtraction, addition, and division operators to the vector. Note that R adheres to the standard mathematical orders of operation. (3 + a) / 2 - 1 ## [1] 1.0 2.5 4.0 6.0 10.0 We can also perform mathematical operations on vectors of the same length (i.e., with the same number of data elements). In order, the mathematical operator will be applied to each pair of vector values from the respective vectors. Lets begin by creating a new vector called f. f &lt;- c(3, 1, 3, 5, 3) Both a and f are the same length, which means we can multiply, add, divide, subtract, and exponentiate a * f ## [1] 3 4 21 55 57 a + f ## [1] 4 5 10 16 22 a / f ## [1] 0.3333333 4.0000000 2.3333333 2.2000000 6.3333333 a - f ## [1] -2 3 4 6 16 a ^ f ## [1] 1 4 343 161051 6859 11.9 Lists If we wish to combine data elements into a single list that with different data types, we can use the list function. The list function orders each data element and retains its value. g &lt;- list(1, &quot;dog&quot;, TRUE, &quot;2018-05-30&quot;) print(g) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;dog&quot; ## ## [[3]] ## [1] TRUE ## ## [[4]] ## [1] &quot;2018-05-30&quot; class(g) ## [1] &quot;list&quot; 11.10 Data Frames A data frame is a specific type of table in which columns represent variables (i.e., fields) and rows represent cases (i.e., observations). We can create a simple data frame object by combining vectors of the same length. Lets begin by creating six vector objects, which we will label a through f. a &lt;- c(1, 4, 7, 11, 19) b &lt;- c(3L, 10L, 2L, 5L, 5L) c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) f &lt;- c(3, 1, 3, 5, 3) Using the data.frame function from base R we can combine the six vectors to create a data frame object. All we need to do is enter the names of the six vectors as separate arguments in the function parentheses. Just as we did with the vectors, we can name the data frame object using the &lt;- operator (or = operator). Lets name this data frame object r. r &lt;- data.frame(a, b, c, d, e, f) Using the print function, we can view the contents of our new data frame object called r. print(r) ## a b c d e f ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 We can also rename the columns (i.e., variables) of the data frame object by using the names function from base R along with the c function from base R. names(r) &lt;- c(&quot;TenureSup&quot;, &quot;TenureOrg&quot;, &quot;Age&quot;, &quot;HireDate&quot;, &quot;FTE&quot;, &quot;NumEmp&quot;) To view the changes to our data frame object, use the print function once more. print(r) ## TenureSup TenureOrg Age HireDate FTE NumEmp ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 Finally, we can use the class function to verify that the object is in fact a data frame. class(r) ## [1] &quot;data.frame&quot; 11.11 Annotations Part of the value of using a code/script-based program like R is that you can leave notes and explain your decisions and operations. When preceding text, the # symbol indicates that all text that follows on that line is a comment or annotation; as a result, R knows not to interpret or analyze the text that follows. To illustrate annotations, lets repeat the steps from the previous section; however, this time, lets include annotations. # Create six vectors a &lt;- c(1, 4, 7, 11, 19) # Vector a b &lt;- c(3L, 10L, 2L, 5L, 5L) # Vector b c &lt;- c(&quot;old&quot;, &quot;young&quot;, &quot;young&quot;, &quot;old&quot;, &quot;young&quot;) # Vector c d &lt;- as.Date(c(&quot;2018-06-01&quot;, &quot;2018-06-01&quot;, &quot;2018-10-31&quot;, &quot;2018-01-01&quot;, &quot;2018-06-01&quot;)) # Vector d e &lt;- c(TRUE, TRUE, TRUE, FALSE, FALSE) # Vector e f &lt;- c(3, 1, 3, 5, 3) # Vector f # Combine vectors into data frame r &lt;- data.frame(a, b, c, d, e, f) # Print data frame print(r) ## a b c d e f ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 # Rename columns in data frame names(r) &lt;- c(&quot;TenureSup&quot;, &quot;TenureOrg&quot;, &quot;Age&quot;, &quot;HireDate&quot;, &quot;FTE&quot;, &quot;NumEmp&quot;) # Print data frame print(r) ## TenureSup TenureOrg Age HireDate FTE NumEmp ## 1 1 3 old 2018-06-01 TRUE 3 ## 2 4 10 young 2018-06-01 TRUE 1 ## 3 7 2 young 2018-10-31 TRUE 3 ## 4 11 5 old 2018-01-01 FALSE 5 ## 5 19 5 young 2018-06-01 FALSE 3 # Determine class of object class(r) ## [1] &quot;data.frame&quot; Can you start to envision how annotated code might help to tell a story about data-related decision-making processes? 11.12 Summary In this chapter, we learned the basics of working with the R statistical programming language. This chapter is by no means comprehensive, and there were probably some concepts and functions that still dont quite make sense to you. Nonetheless, hopefully, this chapter provided you with a basic understanding of the basic operations and building blocks of R. Well practice applying many of the operations and functions from this chapter in subsequent chapters, which means youll have many more opportunities to learn and practice. "],["setwd.html", "Chapter 12 Setting a Working Directory 12.1 Video Tutorial 12.2 Functions &amp; Packages Introduced 12.3 Identify the Current Working Directory 12.4 Set a New Working Directory 12.5 Summary", " Chapter 12 Setting a Working Directory A working directory refers to the location of a folder within a hierarchical file system. For our purposes, a working directory contains data files associated with a particular task or project. Ideally, a single working directory contains all of the data files you need for a task or project, but in some instances, it might make sense to have multiple working directories for a single project. From our designated working directory, we can read in data files (i.e., import files) to the R environment without adding long paths as prefixes in front of the variable names. Further, anytime you save a plot, data frame, or other object created in R, the default will be to save it to the folder you have set as your working directory (i.e., export files). 12.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial demonstrate how to identify what your current working directory is and how to set a new working directory. Link to video tutorial: https://youtu.be/oSqOqvMkhSE 12.2 Functions &amp; Packages Introduced Function Package getwd base R setwd base R 12.3 Identify the Current Working Directory To determine if a working directory has already been set, and if so, what that working directory is, use the getwd (get working directory) function from base R. Because this function comes standard with our R download, we dont need to install an additional package to access it. For this function, you dont need any arguments within the parentheses; in other words, leave the function parentheses empty. Alternatively, if you are using RStudio, you will see your current working directory next to the word Console in your Console window. # Find your current working directory getwd() 12.4 Set a New Working Directory Lets assume that the current working directory is not what we want; meaning, we need to set a new or different working directory. If you need to set a new working directory, you can use the setwd function from base R. Within the parentheses, your only argument will be the working directory in quotation marks. I recommend typing your setwd function into an R Script (.R) file so that it can be saved for future sessions. I also recommend using the # to annotate your script so that you can remind yourself (and others) what you are doing. When it comes to working directories, R likes the forward slash (/) (as opposed to backslash). Remember, the working directory is the location of the data files you wish to access and bring into the R environment. You can access any folder you would like and set it as your working directory. For example, in the code below, I set my working directory to H:/RWorkshop, as that folder at the end of that path contains the data files I would like to work with. The folder (and associated path) you set as your working directory will almost certainly be different than the one I set below. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Alternatively, you may use the drop-down menus to select a working directory folder. To do so, go to Session &gt; Set Working Directory &gt; Choose Directory, and select the folder where your files live. Upon doing so, your working directory will appear in the Console. You can copy and paste the working directory into your setwd function. Once you have set your working directory, you can verify that it was set to the correct folder by (a) typing getwd() into your console or (b) looking at the working directory listed next to the word Console in your Console window. 12.5 Summary In this chapter, you learned how to get and set a working directory using the getwd and setwd functions from base R. "],["read.html", "Chapter 13 Reading Data into R 13.1 Conceptual Overview 13.2 Tutorial 13.3 Chapter Supplement", " Chapter 13 Reading Data into R In this chapter, we will learn what reading data means in the context of the R language, and how to go about reading data into R so that we can begin managing, analyzing, and visualizing the data. 13.1 Conceptual Overview Reading data refers to the process of importing data from a (working) directory or website into R. When we read a data file into R, we often read it in as a data frame (df) object, where a data frame is a tabular display with columns representing variables and rows representing cases. For additional information on data frames, please refer to this section from a previous chapter. Many different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS). In this chapter, you will learn how to read .csv and .xlsx files into R; however, in the Chapter Supplement, you will have an opportunity to learn how to use the Read function from the lessR package, which can read in .sas7bdat (SAS) and .sav (SPSS) files. 13.2 Tutorial This chapters tutorial demonstrates how to read data files into R, such as those in .csv or .xlsx format. 13.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial demonstrate how to read a .csv file into R; however, in the video tutorial I demonstrate multiple functions that can read in .csv files (read.csv, read_csv, Read), whereas in the written tutorial, I demonstrate just the function I prefer to use (read_csv). In this written tutorial, I also demonstrate how to read in a .xlsx file using the read_excel function as well as some additional operations, and for time considerations, I dont demonstrate those approaches in the video. Link to video tutorial: https://youtu.be/smWjqhaxHY8 13.2.2 Functions &amp; Packages Introduced Function Package read_csv readr excel_sheets readxl read_excel readxl View base R print base R head base R tail base R names base R colnames base R 13.2.3 Initial Steps Please note, that any function that appears in the Initial Steps section has been covered in a previous chapter. If you need a refresher, please view the relevant chapter. In addition, a previous chapter may show you how to perform the same action using different functions or packages. To get started, please save the following data files into a folder on your computer that you will set as your working directory: PersData.csv and PersData_Excel.xlsx. As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, set your working directory by using the setwd function (see below) or by doing it using drop-down menus. Your working directory folder will likely be different than the one shown below; H:/RWorkshop just happens to be the name of the folder that I save my data files to and that I set as my working directory. You can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. If you need a refresher on how to set a working directory, please refer to Setting a Working Directory. # Set your working directory to the folder containing your data file setwd(&quot;H:/RWorkshop&quot;) Finally, I highly recommend that you create a new R Script file (.R), which will allow you to edit and save your script and annotations. To learn more, please refer to Creating &amp; Saving an R Script. 13.2.4 Read a .csv File https://youtu.be/xsnOGUKtECo One of the easiest data file formats to work with when reading data into R is the .csv (comma-separated values) file format. Many HR analysts and other types of data analysts regularly work with .csv files, and .csv files can be created in Microsoft Excel and Google Sheets (as well as using many other programs). For example, many survey, data-analysis, and data-acquisition platforms allow data to be exported to .csv files. When getting started in R, the way in which the .csv file is formatted can make your life easier. Specifically, the most straightforward .csv file format to read is structured such that (a) the first row contains the names of the variables (i.e., columns, fields), and (b) the second, third, fourth, and fifth rows (and so on) contain the observed scores on the variables (i.e., data), where each row represents a case (i.e., observation, employee). In the chapter supplement section of this chapter, you will have an opportunity to read in .csv files in which the observed values do not begin until the third row or later. As part of the tidyverse of R packages (Wickham 2021b; Wickham et al. 2019), the readr package (Wickham and Hester 2020) and its functions can be used to read in a few different data file formats (as long as they are rectangular), including .csv files. To read in .csv files, we will use the read_csv function from the readr package, as it tends to be faster than some of the other functions developed to read in data. There are several other R functions that can read in .csv files (e.g., read.csv, Read), and if youre interested in learning two of those functions, feel free to check out the end-of-book supplement called Reading Data: Chapter Supplement. By default, the read_csv function reads data in as a data frame, where a data frame is a specific type of table in which columns contain variables and rows contain cases. Well, technically, the function reads data in as a tibble (as opposed to a data frame), where a tibble behaves a lot like a data frame. Thus, from here on out in the book, Ill just use the term data frame. If you would like more information about tibbles, check out Wickham and Grolemunds (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. To use the read_csv function, the readr package must be installed and accessed using the install.packages and library functions, respectively. Type \"readr\" (note the quotation marks) into the parentheses of the install.packages function, and run that line of code. # Install readr package install.packages(&quot;readr&quot;) Next, type readr (without quotation marks) into the parentheses of the library function. In other words, include readr as the library functions sole parenthetical argument. Run that line of code. # Access readr package library(readr) Type the name of the read_csv function, and note that all of the letters in the function name are lowercase. As the sole argument within the functions parentheses and within quotation marks (\" \"), type the exact name of the .csv data file as it is named in your working directory (PersData.csv), and be sure to follow it immediately with the .csv extension. Remember, R is a language where spaces matter in the context of file names; meaning, if there are spaces in your file name, there needs to be spaces when the file name appears in your R code. Remember, the file called PersData.csv should already be saved in your working directory folder (see Initial Steps). # Read .csv file into R as data frame read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see in your Console, the data frame that appears contains only a handful of rows and columns; nonetheless, this gives you an idea of how the read_csv function works. Often, you will want to assign a data frame to an object that will be stored in your (Global) Environment for subsequent use; once the data are assigned, the object becomes a data frame object. By creating a data frame object, you can manipulate and/or analyze the data within the object using a variety of functions (and without changing the data in the original .csv file). To assign the data frame to an object, we simply (a) use the same read_csv function and argument as above, (b) add either the &lt;- or = operator to the left of the read_csv function, and (c) create a name of our choosing for the data frame object by entering that name to the left of the &lt;- or = operator. You can name your data frame object whatever you would like as long as it doesnt include spaces, doesnt start with a numeral, and doesnt include special characters like * or - (to name a few). I recommend choosing a name that is relatively short but descriptive, and that is not the same as another R function or variable name that you plan to use. Below, I name the new data frame object personaldata; note, however, that I could have just have easily called PersonalData, pd, df, or any other single-word name that doesnt begin with a special character or a numeral. # Read .csv data file into R and name data frame object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) Using the head function from base R, lets print just the first 6 rows of our data frame object that we named personaldata. This will allow us to verify that everything worked as planned. # Print just the first 6 rows of the data frame object in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male If you are working in RStudio, you will see the data frame object appear in your Global Environment window panel, as shown below. If you click on the name of the data frame object in your Global Environment, a new tab will open up next to your R script editor tab, which will allow you to view the data. Alternatively, you can use the View function from base R with the exact name of the data frame object we just created as the sole parenthetical argument. Note that the View function begins with an uppercase letter. Remember, R is case and space sensitive when it comes to function names. Further, the name of the data frame object you enter into the parentheses of the function must be exactly the same as the name of the object you created. That is, R wont recognize the data frame object if you type it as PersonalData, but R will recognize it if you type it as personaldata. Sometimes it helps to copy and paste the exact names of functions and variables into the function parentheses. # View data within data frame object View(personaldata) Instead of using the View function, you could just run the name of the data frame object by highlighting personaldata in your R Script and clicking Run (or you can enter the name of the data frame object directly into your Console command line and click Enter). To print an object to the Console, another option is to use the print function (from base R) with the name of the data frame object as the sole argument in the parentheses. Similarly, if you have many rows of data, you can use the head function from base R to print just the first 6 rows of data, or you can use the tail function from base R to print the last 6 rows of data. # Highlight the name of data frame object and run the code to view in Console personaldata ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male # Use print function with the name of the data frame object to view in Console print(personaldata) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male # Print just the first 6 rows of the data frame object in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male # Print just the last 6 rows of the data frame object in Console tail(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 125 Franklin Benjamin 1/5/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 201 Providence Cindy 1/9/2016 female ## 6 282 Legend John 1/9/2016 male If your data file resides in a folder other than your set working directory, then you can type the exact name of the path directory where the file resides followed by a forward slash (/) before the file name. Please note that your path directory will almost certainly be different than the one I show below. # Read data and name data frame object personaldata &lt;- read_csv(&quot;H:/RWorkshop/PersData.csv&quot;) Note that by assigning this data frame to an object called personaldata, we have overwritten the previous version of the object with that same name. In this case, this isnt a big deal because we just read in the exact data using two different methods. If you dont wish to overwrite an existing object, just name the object something unique. When naming objects, I suggest that you avoid the names of functions that you plan to use. When needed, you can also use the read_csv function to read in .csv data from a website. For example, rather than save the .csv file to a folder on your computer, you can read in the raw data directly from my GitHub site. Within the quotation marks (\" \"), simply paste in the following URL: https://raw.githubusercontent.com/davidcaughlin/R-Tutorial-Data-Files/master/PersData.csv. # Read .csv data file into R from a website personaldata &lt;- read_csv(&quot;https://raw.githubusercontent.com/davidcaughlin/R-Tutorial-Data-Files/master/PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) 13.2.5 Read a .xlsx File Reading in Excel workbook files with more than one worksheet requires a bit more work. To read in a .xlsx file with multiple worksheets, we will use the excel_sheets and read_excel functions from the readxl package (Wickham and Bryan 2019). Be sure to install and access the read_xl package if you havent already. # Install readxl package install.packages(&quot;readxl&quot;) # Access readxl package library(readxl) To print the worksheet names within an Excel workbook file, simply type the name of the excel_sheets function, and as the sole parenthetical argument, type the exact name of the data file with the .xlsx extension  all within quotation marks (i.e., \"PersData_Excel.xlsx\"). # Print worksheet names contained within .xlsx file excel_sheets(&quot;PersData_Excel.xlsx&quot;) ## [1] &quot;Year1&quot; &quot;Year2&quot; Note that the .xlsx file contains two worksheets called Year1 and Year2. We can now reference each of these worksheets when reading in the data from the Excel workbook file. To do so, we will use the read_excel function. As the first argument, enter the exact name of the data file (as named in your working directory), followed by .xlsx  and all within quotation marks (\" \"). As the second argument, type sheets= followed by the name of the worksheet containing the data you wish to read in; lets read in the data from the worksheet called Year1. Finally, either the &lt;- or = operator can be used to name the data frame object. Below, I name the data frame object personaldata_year1 to avoid overwriting the data frame object we created above called personaldata. Remember to type a comma (,) before the second argument, as this is how we separate arguments from one another when there are more than one. # Read data from .xlsx sheet called &quot;Year1&quot; as data frame and assign to object personaldata_year1 &lt;- read_excel(&quot;PersData_Excel.xlsx&quot;, sheet=&quot;Year1&quot;) # Print data frame object in Console print(personaldata_year1) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 2016-01-01 00:00:00 male ## 2 154 McDonald Ronald 2016-01-09 00:00:00 male ## 3 155 Smith John 2016-01-09 00:00:00 male ## 4 165 Doe Jane 2016-01-04 00:00:00 female ## 5 125 Franklin Benjamin 2016-01-05 00:00:00 male ## 6 111 Newton Isaac 2016-01-09 00:00:00 male ## 7 198 Morales Linda 2016-01-07 00:00:00 female ## 8 201 Providence Cindy 2016-01-09 00:00:00 female ## 9 282 Legend John 2016-01-09 00:00:00 male Lets repeat the process for the worksheet called Year2 and assign these data to a new object. # Read data from .xlsx sheet called &quot;Year2&quot; as data frame and assign to object personaldata_year2 &lt;- read_excel(&quot;PersData_Excel.xlsx&quot;, sheet=&quot;Year2&quot;) # Print data frame object in Console print(personaldata_year2) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 2016-01-01 00:00:00 male ## 2 155 Smith John 2016-01-09 00:00:00 male ## 3 165 Doe Jane 2016-01-04 00:00:00 female ## 4 125 Franklin Benjamin 2016-01-05 00:00:00 male ## 5 111 Newton Isaac 2016-01-09 00:00:00 male ## 6 201 Providence Cindy 2016-01-09 00:00:00 female ## 7 282 Legend John 2016-01-09 00:00:00 male ## 8 312 Ramos Jorge 2017-03-01 00:00:00 male ## 9 395 Lucas Nadia 2017-03-04 00:00:00 female 13.2.6 Summary In this chapter, we learned how to read data into the R environment. Reading data into R is an important first step, and often, it is the step that causes the most problems for new R users. We practiced applying the read_csv function from the readr pack and the read_excel function from the read_xl package to read .csv and .xlsx files, respectively, into the R environment. 13.3 Chapter Supplement In this chapter supplement, I demonstrate additional functions that can be used to read in .csv files and demonstrate how to list the names of data files located in a (working directory) folder and how to skip rows of data when reading in a .csv file. 13.3.1 Functions &amp; Packages Introduced Function Package read.csv base R Read lessR list.files base R 13.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) 13.3.3 Additional Functions for Reading a .csv File In addition to the read_csv function from the readr package covered earlier in the chapter, we can read .csv files into R using the read.csv function from base R and the Read function from the lessR package (Gerbing, Business, and University 2021), which we will review in this chapter supplement. 13.3.3.1 read.csv Function from Base R The read.csv file comes standard with base R, which means that you dont need to install a package to access the function. As the function name implies, this function is used when the source data file is in .csv format. To learn how to use the read.csv function, you have the choice to follow along with the video tutorial below or the subsequent written tutorial. Link to video tutorial: https://youtu.be/xsnOGUKtECo Typically, the read.csv function requires only a single argument within the parentheses, which will be the exact name of the data file enclosed with quotation marks; the file should be located your working directory folder. Remember, R is a language where case and space sensitivity matters when it comes to names; meaning, if there are spaces in your file name, there needs to be spaces when the file name appears in your R script, and if some letters are upper case in your file name, there needs to be corresponding upper-case letters in your R script. Lets practice reading in a file called PersData.csv by entering the exact name of the file followed by the .csv extension, all within in quotation marks. Remember, the file called PersData.csv should already be saved in your working directory folder (see Initial Steps). # Read data from working directory read.csv(&quot;PersData.csv&quot;) ## id lastname firstname startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see, the data that appear in your Console contains only a handful of rows and columns; nonetheless, this gives you an idea of how the read.csv function works. Often, you will want to assign your data frame to an object that is stored in your Global Environment for subsequent use. By creating a data frame object, you can manipulate and/or analyze the data within the object using a variety of functions (and without changing the data in the source file). To create a data frame object, we simply (a) use the same read.csv function from above, (b) add either a &lt;- or = operator to the left of the read.csv function, and (c) create a name of our choosing for the data frame object by entering that name to the left of the &lt;- or = operator. You can name your data frame object whatever you would like as long as it doesnt include spaces, doesnt start with a numeral, and doesnt include special characters like * or - (to name a few). I recommend choosing a name that is relatively short but descriptive, and that is not the same as another R function or variable name that you plan to use. Below, I name the new data frame object personaldata. # Read in data and name data frame object personaldata &lt;- read.csv(&quot;PersData.csv&quot;) 13.3.3.2 Read Function from lessR Package Just like the read.csv and read_csv functions, the Read function from the lessR package can read in .csv files; however, it can also read in other file formats like .xls/x, .sas7bdat (SAS), and .sav (SPSS). To use the Read function, the lessR package needs to be installed and accessed using the install.packages and library functions, respectively. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) When reading in a .csv file using the Read function, type the exact name of your data file from your working directory as an argument (followed by .csv and surrounded by quotation marks). Further, either the &lt;- or = operator can be used to name the data frame object. # Read data and assign to data frame object personaldata &lt;- Read(&quot;PersData.csv&quot;) ## ## &gt;&gt;&gt; Suggestions ## To read a csv or Excel file of variable labels, var_labels=TRUE ## Each row of the file: Variable Name, Variable Label ## Details about your data, Enter: details() for d, or details(name) ## ## Data Types ## ------------------------------------------------------------ ## character: Non-numeric data values ## integer: Numeric data values, integers only ## ------------------------------------------------------------ ## ## Variable Missing Unique ## Name Type Values Values Values First and last values ## ------------------------------------------------------------------------------------------ ## 1 id integer 9 0 9 153 154 155 ... 198 201 282 ## 2 lastname character 9 0 9 Sanchez McDonald ... Providence Legend ## 3 firstname character 9 0 8 Alejandro Ronald ... Cindy John ## 4 startdate character 9 0 5 1/1/2016 1/9/2016 ... 1/9/2016 1/9/2016 ## 5 gender character 9 0 2 male male male ... female female male ## ------------------------------------------------------------------------------------------ Lets print just the first six rows of the personaldata data frame object to the Console to verify that everything worked as intended. # Print just the first 6 rows of the data frame object in Console head(personaldata) ## id lastname firstname startdate gender ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male For more information on the Read function from the lessR package, check out David Gerbings website: http://www.lessrstats.com/videos.html. 13.3.4 Skip Rows of Data During Read Thus far, I have showcased some of the most common approaches to reading in data files, with an emphasis on reading in .csv files with the first row corresponding to the column (variable) names and the remaining rows containing the substantive data for cases. There are, however, other challenges and considerations you might encounter along the way. For example, some survey platforms like Qualtrics allow for data to be downloaded in .csv format; however, sometimes these platforms include variable name and label information in the second and even third rows of data as opposed to in just the first row. Fortunately, we can skip rows when reading in such data files. Well first learn how to skip rows with the read_csv function from the readr package, and then well learn to do so using the read.csv function from base R and the Read function from the lessR package. Lets pretend that the first row of the PersData.csv data file contains variable names, and the second and third rows contain variable label information and explanations. We can nest the read_csv function (from the readr package) within the names function, which will result in a vector of names from the first row of the data file. Using the &lt;- operator, lets name this vector var_names so that we can reference it in the subsequent step. # Read variable names from first row of data var_names &lt;- names(read_csv(&quot;PersData.csv&quot;)) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) Next, using the read_csv function, we will read in the data file, skip the variable names row and the first two rows of actual values (which adds to three rows), and add the variable names we pulled in the previous step. Notably, the read_csv function assumes that the first of data in your data file contain the variable names when you use the col_names argument, as we will do below. As usual, as the first argument of the read_csv function, type the exact name of the data file you wish to read in within quotation marks (\" \"). As the second argument, type skip=3 to indicate that you wish to skip the first three rows when reading in the data. As the third argument, type col_names= followed by the name of the var_names vector object we created in the previous step. Using the &lt;- operator, lets name this data frame object test. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- read_csv(&quot;PersData.csv&quot;, skip=3, col_names=var_names) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) Finally, lets see the fruits of our labor by printing the contents of the test data frame object to our Console. # Print data frame object in Console print(test) ## # A tibble: 7 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male The read.csv function from base R also allows for us to skip rows; however, to make the function operate like the read_csv function, we need to add the header=FALSE argument to pretend like the first row of data in the data file does not contain variable names. In doing so, we can keep the argument rows=3 the same as we did in the read_csv function above. Alternatively, if we were to set header=TRUE (which is the default setting for this function), then we would need to change the argument rows=3 to rows=2. Its up to you which makes more intuitive sense to you. Finally, instead of col_names, the read.csv function equivalent argument is col.names. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- read.csv(&quot;PersData.csv&quot;, header=FALSE, skip=3, col.names=var_names) # Print data frame object in Console print(test) ## id lastname firstname startdate gender ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male Finally, if we take the code from above for the read.csv function and swap read.csv out with Read function (assuming we have already accessed the lessR package using the library function), then we can keep all of the arguments the same. # Read data file (but skip the variable names &amp; rows 1-2) # &amp; introduce variable names test &lt;- Read(&quot;PersData.csv&quot;, header=FALSE, skip=3, col.names=var_names) ## ## &gt;&gt;&gt; Suggestions ## To read a csv or Excel file of variable labels, var_labels=TRUE ## Each row of the file: Variable Name, Variable Label ## Details about your data, Enter: details() for d, or details(name) ## ## Data Types ## ------------------------------------------------------------ ## character: Non-numeric data values ## integer: Numeric data values, integers only ## ------------------------------------------------------------ ## ## Variable Missing Unique ## Name Type Values Values Values First and last values ## ------------------------------------------------------------------------------------------ ## 1 id integer 7 0 7 155 165 125 ... 198 201 282 ## 2 lastname character 7 0 7 Smith Doe ... Providence Legend ## 3 firstname character 7 0 6 John Jane ... Cindy John ## 4 startdate character 7 0 4 1/9/2016 1/4/2016 ... 1/9/2016 1/9/2016 ## 5 gender character 7 0 2 male female ... female male ## ------------------------------------------------------------------------------------------ # Print data frame object in Console print(test) ## id lastname firstname startdate gender ## 1 155 Smith John 1/9/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 111 Newton Isaac 1/9/2016 male ## 5 198 Morales Linda 1/7/2016 female ## 6 201 Providence Cindy 1/9/2016 female ## 7 282 Legend John 1/9/2016 male 13.3.5 List Data File Names in Working Directory If youre like me, and you save a lot of data files into a single folder, sometimes you find yourself flipping back and forth from RStudio to your file folder to see the exact names of the files when youre attempting to read them into your R environment. If you would like to obtain the exact names of files located in a (working) directory, the list.files function from base R comes in handy. This function will return a list of all file names within a particular directory or file names that meet a particular pattern. For our purposes, lets identify all of the .csv data file names contained within our current working directory. As the first argument, type path= followed by the path associated with your working directory. Second, because we are only pulling the file names associated with .csv files, enter the argument all.files=FALSE. Third, type the argument full.names=FALSE to indicate that we do not want the path to precede the file names. Finally, type the argument pattern=\".csv\" to request the names of only those file names that match the regular expression of .csv will be returned. # List data file names in working directory list.files(path=&quot;H:/RWorkshop&quot;, all.files=FALSE, full.names=FALSE, pattern=&quot;.csv&quot;) In your Console, you should see the list of file names you requested. You could then copy specific file names that you wish to read into R. "],["addnames.html", "Chapter 14 Removing, Adding, &amp; Changing Variable Names 14.1 Conceptual Overview 14.2 Tutorial", " Chapter 14 Removing, Adding, &amp; Changing Variable Names In this chapter, we will learn how to remove, add, and change variable names in R. 14.1 Conceptual Overview After reading data into R as a data frame object, you may encounter situations in which it makes sense to remove the variable names (and not the data associated with the variable names), to add or replace variable names, or to just rename (change) certain variables. For example, perhaps the variable names from the original data file dont adhere to your preferred naming conventions, and thus you wish to change the variable names. As another example, sometimes the variable names are divorced from the associated data, and thus as an initial data management step, we need to add the variable names to the associated data in R. In the following tutorial, you will learn some simple techniques to achieve these objectives. 14.2 Tutorial This chapters tutorial demonstrates how remove, add, and change variable names in a data frame object. 14.2.1 Video Tutorial Link to video tutorial: https://youtu.be/3m32O9f8gAI 14.2.2 Functions &amp; Packages Introduced Function Package names base R c base R head base R rename dplyr 14.2.3 Initial Steps If you havent already, save the file called PersData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called PersData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a tibble object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemunds (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 14.2.4 Remove Variable Names from a Data Frame Object In some instances, you may wish to remove the variable names from a data frame. For example, I sometimes write (i.e., export) a data frame object Ive been cleaning in R so that I may use the data file with the statistical software program called Mplus (Muthn and Muthn 1998-2018). Because Mplus doesnt accept variable names within its data files, I may drop the variable names from the data frame object prior to writing to my working directory. To remove variable names, just apply the names function with the data frame name as the argument, and then use either the &lt;- operator with NULL to remove the variable names. # Remove variable names names(personaldata) &lt;- NULL # Print just the first 6 rows of the data frame object in Console head(personaldata) ## # A tibble: 6 x 5 ## `` `` `` `` `` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male As you can see, the variable names do not appear in the overwritten personaldata data frame object. 14.2.5 Add Variable Names to a Data Frame Object In other instances, you might find yourself with a dataset that lacks variable names (or has variable names that need to be replaced), which means that you will need to add those variable names to the data frame. Lets work with the personaldata data frame object from the previous section for practice. To add variable names, we can use the names function from base R, and enter the name of the data frame as the argument. Using the &lt;- operator, we can specify the variable names using the c (combine) function that contains a vector of variable names in quotation marks (\" \") as the arguments. Remember to type a comma (,) between the function arguments, as commas are used to separate arguments from one another when there are more than one. Please note that the its important that the vector of variable names contains the same number of names as the data frame object has columns. # Add (or replace) variable names to data frame object names(personaldata) &lt;- c(&quot;id&quot;, &quot;lastname&quot;, &quot;firstname&quot;, &quot;startdate&quot;, &quot;gender&quot;) # Print just the first 6 rows of data in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male Now the data frame object has variable names! 14.2.6 Change Specific Variable Names in a Data Frame Object Using the resulting data frame object from the previous section (personaldata), we can rename specific variables using the rename function from the dplyr package. To get started, well need to install the dplyr package so that we can access the rename function. If you havent already, install and access the dplyr package using the install.packages and library functions, respectively. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) Well begin by specifying the name of our data frame object personaldata, followed by the &lt;- operator so that we can overwrite the existing personaldata frame object with one that contains the renamed variables. Next, type the name of the rename function. As the first argument in the function, type the name of the data frame object (personaldata). As the second argument, lets change the lastname variable to Last_Name by typing the name of our new variable followed by = and, in quotation marks (\" \"), the name of the original variable (Last_Name=\"lastname\"). As the third argument, lets apply the same process as the second argument and change the firstname variable to First_Name by typing the name of our new variable followed by = and, in quotation marks (\" \"), the name of the original variable (First_Name=\"firstname\"). # Add (or replace) variable names to data frame object personaldata &lt;- rename(personaldata, Last_Name=&quot;lastname&quot;, First_Name=&quot;firstname&quot;) Using the head function from base R, lets verify that we renamed the two variables successfully. # View just the first 6 rows of data in Console head(personaldata) ## # A tibble: 6 x 5 ## id Last_Name First_Name startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male As you can see, the lastname and firstname variables are now named Last_Name and First_Name. It worked! 14.2.7 Summary In this chapter, we reviewed how to remove variable names from a data frame object; how to add variable names to a data frame object using the names, colnames, and c functions, which all come standard with your base R installation; and how to rename specific variables using the rename function from the dplyr package. "],["write.html", "Chapter 15 Writing Data from R 15.1 Conceptual Overview 15.2 Tutorial", " Chapter 15 Writing Data from R In this chapter, we will learn what writing data means in the context of the R language, and how to go about writing data from R so that we share data with non-R users. 15.1 Conceptual Overview Writing data refers to the process of exporting data from the R Environment to a (working directory) folder. If you collaborate with others who do not work in R, writing data will allow them to use the data you cleaned, managed, or manipulated in the R Environment in other software programs. In the following tutorial, we will learn how to write a data frame object and a table object to our working directory folder as .csv files. 15.2 Tutorial This chapters tutorial demonstrates how to write data from R into a .csv file that can be opened in programs like Microsoft Excel or Google Sheets  along with many other analytical software programs. 15.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/ORTe8vE7nzU 15.2.2 Functions &amp; Packages Introduced Function Package write.csv base R write.table base R table base R 15.2.3 Initial Steps If you havent already, save the file called PersData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called PersData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a tibble object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemunds (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 15.2.4 Write Data Frame to Working Directory The write.csv function from base R can be used to write a data frame object to your working directory or to a folder of your choosing. Lets write the personaldata data frame (that we read in and named above) to our working directory. Before doing so, however, lets make a minor change to the data frame to illustrate a scenario in which you clean your data in R and then write the data to a .csv file so that a colleague can work with the data in another program. Specifically, lets remove the lastname variable from the data frame. To do so, type the name of the data frame (personaldata), followed by the $ symbol and then the name of the variable in question (lastname). Next, type the &lt;- operator followed by NULL. This code will remove the variable from the data frame. # Remove variable from data frame object personaldata$lastname &lt;- NULL # Print data frame object print(personaldata) ## # A tibble: 9 x 4 ## id firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Alejandro 1/1/2016 male ## 2 154 Ronald 1/9/2016 male ## 3 155 John 1/9/2016 male ## 4 165 Jane 1/4/2016 female ## 5 125 Benjamin 1/5/2016 male ## 6 111 Isaac 1/9/2016 male ## 7 198 Linda 1/7/2016 female ## 8 201 Cindy 1/9/2016 female ## 9 282 John 1/9/2016 male As you can see in your Console output, the variable called lastname is no longer present in the data frame object. To write our cleaned data frame (personaldata) to our working directory, we use the write.csv function from base R. As the first argument in the parentheses, type the name of the data frame (personaldata). Remember to type a comma (,) before the second argument, as this is how we separate arguments from one another when there are more than one. As the second argument, lets type what we want to name the file that we will create in our working directory. Make sure that the name of the new .csv file is in quotation marks (\" \"). Here, I name the new file Cleaned PersData.csv; it is important that you keep the .csv extension at the end of the name you provide. # Write data frame object to working directory write.csv(personaldata, &quot;Cleaned PersData.csv&quot;) If you go to your working directory folder, you will find the file called Cleaned PersData.csv saved there. We can also specify which folder that we want to write our data to using the full path extension and what we would like to name the new .csv file. # Write data frame object to folder write.csv(personaldata, &quot;H:/RWorkshop/Cleaned PersData2.csv&quot;) If you go to your working directory folder, you will find the file called Cleaned PersData2.csv. 15.2.5 Write Table to Working Directory Sometimes we work with table objects in R. If we wish to write a table to our working directory, we can use the write.table function from base R. Before doing so, we need to create a data table object as an example, which we can do using the table function from base R. To create a table, first, come up with a name for your new table object; in this example, I name the table table_example (because Im so creative). Second, type the &lt;- operator to the right of your new table name to tell R that you are creating a new object. Third, type the name of the table-creation function, which is table. Fourth, in the functions parentheses, as the first argument, enter the name of first variable you wish to use to make the table, and use the $ symbol to indicate that the variable (gender) belongs to the data frame in question (personaldata), which should look like this: personaldata$gender. Fifth, as the second argument, enter the name of the second variable you wish to use to make the table, and use the $ symbol to indicate that the variable (startdate) belongs to the data frame in question (personaldata), which should look like this: personaldata$startdate. # Create table from gender and startdate variables from personaldata data frame table_example &lt;- table(personaldata$gender, personaldata$startdate) # Print table object in Console print(table_example) ## ## 1/1/2016 1/4/2016 1/5/2016 1/7/2016 1/9/2016 ## female 0 1 0 1 1 ## male 1 0 1 0 4 The table above shows how how many female versus male employees started working on a given date. Now we are ready to write the table called table_example to our working directory using the write.table function. As the first argument, type the name of the table object (table_example). Second, type what we would like to call the file when it is saved in our working directory (**\"Practice Table.csv\"**); be sure to include the .csv extension in the name and wrap it all in quotation marks. Third, use the sep=\",\" argument to specify that the values in the table are separated by commas, as this will be a comma separated values file. Fourth, add the argument col.names=NA to format the table such that the column names will be aligned with their respective values. The reason for this fourth argument is that in our table the first column will contain the row names of one of the variables; if we dont include this argument, the function will by default enter the name of the first column name associated with one of the levels of the variables in the first column, and because the first column actually contains the row names for the table, the row names will be off by one column. The col.names=NA argument simply leaves the first cell in the top row blank so that in the next column to the right, the first column name for one of the variables will appear. [To understand what the table would look like without this fourth argument, simply omit it, and open the resulting file in your working directory to see what happens.] # Write table object to working directory write.table(table_example, &quot;Practice Table.csv&quot;, sep=&quot;,&quot;, col.names=NA) If you go to your working directory, you will find the file called Practice Table.csv. 15.2.6 Summary Writing data from the R environment to your working directory or another folder can be useful, especially when collaborating with those who do not use R. The write.csv function writes a data frame object to a .csv file, whereas the write.table function writes a data table object to a .csv file. "],["arrange.html", "Chapter 16 Arranging (Sorting) Data 16.1 Conceptual Overview 16.2 Tutorial 16.3 Chapter Supplement", " Chapter 16 Arranging (Sorting) Data In this chapter, we will learn how to arrange (sort) data within a data frame object, which can be useful for identifying high or low numeric values or to alphabetize character values. 16.1 Conceptual Overview Arranging (sorting) data refers to the process of ordering rows numerically or alphabetically in a data frame or table by the values of one or more variables. Sorting can make it easier to visually scan raw data, such as for the purposes of identifying extreme or outlier values. Sorting can also make facilitate decision making when rank ordering applicants scores, for example, on different selection tools. 16.2 Tutorial This chapters tutorial demonstrates how to arrange (sort) data in R. 16.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to arrange (sort) data with or without the pipe (%&gt;%) operator. If youre unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Link to video tutorial: https://youtu.be/wVwJQsLNbmw 16.2.2 Functions &amp; Packages Introduced Function Package arrange dplyr desc dplyr 16.2.3 Initial Steps Please note, that any function that appears in the Initial Steps section has been covered in a previous chapter. If you need a refresher, please view the relevant chapter. In addition, a previous chapter may show you how to perform the same action using different functions or packages. If you havent already, save the file called PersData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called PersData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object personaldata ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male As you can see from the output generated in your console, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. Technically, the read_csv function reads in what is called a tibble object (as opposed to a data frame object), but for our purposes a tibble will behave similarly to a data frame. For more information on tibbles, check out Wickham and Grolemunds (2017) chapter on tibbles: http://r4ds.had.co.nz/tibbles.html. 16.2.4 Arrange (Sort) Data There are different functions we could use to arrange (sort) the data in the data frame, and in this chapter, we will focus on the arrange function from the dplyr package (Wickham et al. 2021). Please note that there are other functions we could use to sort data, and if youre interested, in the Arranging (Sorting) Data: Chapter Supplement, I demonstrate how to use the order function from base R to carry out the same operations we will cover below. Because the arrange function comes from the dplyr package, which is part of the tidyverse of R packages (Wickham 2021b; Wickham et al. 2019). If you havent already, install and access the dplyr package using the install.packages and library functions, respectively. # Install dplyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) Before diving into arranging the data, as a disclaimer, I will demonstrate two techniques for arranging (sorting) data using the arrange function. The first technique uses a pipe which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2020), on which the dplyr is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemunds (2017) chapter on pipes: https://r4ds.had.co.nz/pipes.html. This brings us to the second technique for arranging (sorting) data using the arrange function. The second technique uses a more traditional approach that some may argue lacks the efficiency and readability of the pipe. Conversely, others may argue against the use of pipes altogether. Im not here to settle any pipes versus no pipes debate, and youre welcome to use either technique. If you dont want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 16.2.4.1 With Pipe To use the with pipe technique, first, type the name of our data frame object, which we previously named personaldata, followed by the pipe (%&gt;%) operator. This will pipe our data frame into the subsequent function. Second, either on the same line or on the next line, type the name of the arrange function, and within the parentheses, enter the variable name startdate as the argument to indicate that we want to arrange (sort) the data by the start date of the employees. The default operation of the arrange function is to arrange (sort) the data in ascending order. If youre wondering where I found the exact names of the variables in the data frame, revisit the use of the names function, which I demonstrated previously in this chapter in the Initial Steps section. # Arrange (sort) data by variable in ascending order (single line) (with pipe) personaldata %&gt;% arrange(startdate) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male Alternatively, we can write this script over two lines and achieve the same output in our Console. # Arrange (sort) data by variable in ascending order (two lines) (with pipe) personaldata %&gt;% arrange(startdate) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male Please note that the operations we have performed thus far have not changed anything in the personaldata data frame object itself; rather, the output in the Console simply shows what it looks like if the data are sorted by the variable in question. We can verify this by viewing the first six rows of data in our data frame object using the head function. As you can see below, nothing changed in the data frame itself. # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object (with pipe) personaldata &lt;- personaldata %&gt;% arrange(startdate) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male As you can see in the Console output, now the personaldata data frame object has been changed such that the data are arranged (sorted) by the startdate variable. To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object (with pipe) personaldata &lt;- personaldata %&gt;% arrange(desc(startdate)) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female To arrange (sort) data by values/levels of two variables, we simply enter the names of two variables as consecutive arguments. Lets enter the gender variable first, followed by the startdate variable. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. As shown below, startdate is sorted within the sorted levels of the gender variable. As a reminder, the default operation of the arrange function is to arrange (sort) the data in ascending order. Remember, we use commas to separate arguments used in a function (if there are more than one arguments). # Arrange (sort) data by two variables in ascending order (with pipe) personaldata %&gt;% arrange(gender, startdate) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 201 Providence Cindy 1/9/2016 female ## 4 153 Sanchez Alejandro 1/1/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 154 McDonald Ronald 1/9/2016 male ## 7 155 Smith John 1/9/2016 male ## 8 111 Newton Isaac 1/9/2016 male ## 9 282 Legend John 1/9/2016 male Watch what happens when we switch the order of the two variables we are using to sort the data. # Arrange (sort) data by two variables in ascending order (with pipe) personaldata %&gt;% arrange(startdate, gender) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 201 Providence Cindy 1/9/2016 female ## 6 154 McDonald Ronald 1/9/2016 male ## 7 155 Smith John 1/9/2016 male ## 8 111 Newton Isaac 1/9/2016 male ## 9 282 Legend John 1/9/2016 male As you can see, the order of the two sorting variables matters. To arrange the data in descending order, just use the desc function from dplyr within the arrange function. # Arrange (sort) data by variable in descending order (with pipe) personaldata %&gt;% arrange(desc(gender,startdate)) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 282 Legend John 1/9/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 153 Sanchez Alejandro 1/1/2016 male ## 7 201 Providence Cindy 1/9/2016 female ## 8 198 Morales Linda 1/7/2016 female ## 9 165 Doe Jane 1/4/2016 female Or, we can sort one variable in the default ascending order and the other in descending order. # Arrange (sort) data by two variables in ascending &amp; descending order (with pipe) personaldata %&gt;% arrange(gender, desc(startdate)) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 201 Providence Cindy 1/9/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 165 Doe Jane 1/4/2016 female ## 4 154 McDonald Ronald 1/9/2016 male ## 5 155 Smith John 1/9/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 282 Legend John 1/9/2016 male ## 8 125 Franklin Benjamin 1/5/2016 male ## 9 153 Sanchez Alejandro 1/1/2016 male 16.2.4.2 Without Pipe We can achieve the same output without using the pipe (%&gt;%) operator as with the pipe operator; again, your choice of using or not using the pipe operator is up to you. To use the arrange function without the pipe operator, type the name of the arrange function, and within the parentheses, as the first argument, type the name of the personaldata data frame object, and as the second argument, type the startdate variable, where the latter indicates that we want to arrange (sort) the data frame object by the start date of the employees. The default operation of the arrange function is to arrange (sort) the data in ascending order. Remember, we use commas to separate arguments used in a function (if there are more than one arguments). If youre wondering where I found the exact names of the variables in the data frame, revisit the use of the names function, which I demonstrated previously in this chapter in the Initial Steps section. # Arrange (sort) data by variable in ascending order without pipe arrange(personaldata, startdate) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order and # overwrite existing data frame object without pipe personaldata &lt;- arrange(personaldata, startdate) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. # Arrange (sort) data by variable in descending order and # overwrite existing data frame object without pipe personaldata &lt;- arrange(personaldata, desc(startdate)) # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female To arrange (sort) data by values/levels of two variables, we simply enter the names of two variables as consecutive arguments (after the name of the data frame, which is the first argument). Lets enter the gender variable first, followed by the startdate variable. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. # Arrange (sort) data by variable in ascending order without pipe personaldata &lt;- arrange(personaldata, gender, startdate) As shown in the output above, startdate is sorted within the sorted levels of the gender variable. This also verifies that the default operation of the arrange function is to arrange (sort) the data in ascending order. To arrange the data in descending order, just use the desc function from dplyr within the arrange function as shown below. You can use the desc function on one or both sorting variables. # Arrange (sort) data by one variable in ascending order and # the other in descending order without pipe personaldata &lt;- arrange(personaldata, gender, desc(startdate)) Or we can apply the desc function to both variables. # Arrange (sort) data by both variables descending order without pipe personaldata &lt;- arrange(personaldata, desc(gender, startdate)) 16.2.5 Summary In this chapter, we learned how to arrange (sort) data by one or more variables using the arrange and desc functions from the dplyr package. This chapter also introduced the pipe (%&gt;%) operator, which can help make code easier to read in some contexts. 16.3 Chapter Supplement In addition to the arrange function from the dplyr package covered above, we can use the order function from base R to arrange (sort) data by values for one or more variable. Because this function comes from base R, we do not need to install and access an additional package like we do with the arrange functions, which some may find advantageous. 16.3.1 Functions &amp; Packages Introduced Function Package order base R c base R 16.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) 16.3.3 order Function from Base R To sort a data frame object in ascending order based on a single variable, we will use the order function from base R to do the following: Type the name of the data frame object that you wish to arrange (sort) (personaldata). Insert brackets ([ ]), which allow us to reference rows or columns depending on how we format the brackets. If we type a function or value before the comma, we are indicating that we wish to apply operations to row(s), and if we type a function or value after the comma, we are indicating that we wish to apply operations to column(s). To sort the data frame into ascending rows by the startdate variable, type the name of the order function before the comma in the brackets. As the sole parenthetical argument of the order function, type the name of the personaldata data frame object, followed by the $ operator and the name of the variable by which we wish to sort the data frame, which to reiterate is the startdate variable. The $ operator signals to R that a variable belongs to a particular data frame object. By default, the order function sorts in ascending order. # Arrange (sort) data by variable in ascending order personaldata[order(personaldata$startdate),] ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male ## 7 111 Newton Isaac 1/9/2016 male ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male To change the ordering of data in the personaldata data frame object itself, we will need to (re)name the data frame object using the &lt;- variable assignment operator. In this example, I will demonstrate how to overwrite the existing data frame object, and thus I give the data frame object the exact same name as it had originally (i.e., personaldata). To do so, to the left of the &lt;- operator, type what you would like to name the new (updated) sorted data frame object (personaldata). Next, to the right of the &lt;- operator, copy and paste the same code we wrote above. Finally, use the head function from base R to view the first six rows of the new data frame object. # Arrange (sort) data by variable in ascending order # and overwrite existing data frame object personaldata &lt;- personaldata[order(personaldata$startdate),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 165 Doe Jane 1/4/2016 female ## 3 125 Franklin Benjamin 1/5/2016 male ## 4 198 Morales Linda 1/7/2016 female ## 5 154 McDonald Ronald 1/9/2016 male ## 6 155 Smith John 1/9/2016 male To sort in descending order, add the argument decreasing=TRUE within the order function parentheses. Remember, we use commas to separate arguments used in a function (if there are two or more arguments). # Arrange (sort) data by variable in descending order personaldata &lt;- personaldata[order(personaldata$startdate, decreasing=TRUE),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 201 Providence Cindy 1/9/2016 female ## 5 282 Legend John 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female If we wish to sort a data frame object by two variables, as the second argument in the order function parentheses, simply add the name of the data frame object, followed by the $ operator and the name of the second second variable. We will sort the data frame in by gender and startdate. The ordering of the two variables matters; the function sorts initially by the values/levels of the first variable listed and sorts subsequently by the values/levels of the second variable listed, but does so within the values/levels of the first variable listed. As shown below, startdate is sorted within the sorted levels of the gender variable. The default operation of the arrange function is to arrange (sort) the data in ascending order. # Arrange (sort) data by two variables in ascending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 201 Providence Cindy 1/9/2016 female ## 4 153 Sanchez Alejandro 1/1/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 154 McDonald Ronald 1/9/2016 male To sort by one of the variables in descending order and the other variable by the default ascending order, we need to add the decreasing= argument, but because we have two variables, we need to provide a vector containing logical values (TRUE, FALSE) to indicate which variable we wish to apply a descending order. If the logical value is TRUE for the decreasing= argument, then we sort in descending variable. Using the c (combine) function from base R, we create a vector of two logical values whose order corresponds to the order in which we listed the two variables in the order function. For example, if the argument is decreasing=c(FALSE, TRUE), then we sort the first variable in the default ascending order and the second variable in descending order, which is what we do below. # Arrange (sort) data by gender in ascending order and # startdate in descending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate, decreasing=c(FALSE, TRUE)),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 female ## 2 198 Morales Linda 1/7/2016 female ## 3 201 Providence Cindy 1/9/2016 female ## 4 153 Sanchez Alejandro 1/1/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 154 McDonald Ronald 1/9/2016 male Or, you could sort by both variables in descending order by change the argument to decreasing=c(TRUE, TRUE). # Arrange (sort) data by gender and startdate variables descending order personaldata &lt;- personaldata[order(personaldata$gender, personaldata$startdate, decreasing=c(TRUE, TRUE)),] # Print just the first 6 rows of the data frame in Console head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 111 Newton Isaac 1/9/2016 male ## 4 282 Legend John 1/9/2016 male ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 153 Sanchez Alejandro 1/1/2016 male "],["join.html", "Chapter 17 Joining (Merging) Data 17.1 Conceptual Overview 17.2 Tutorial 17.3 Chapter Supplement", " Chapter 17 Joining (Merging) Data In this chapter, we will learn the fundamentals of joins (merges). Specifically, we will learn how to join (merge) data horizontally and vertically. 17.1 Conceptual Overview Joining (merging) refers to the process of matching two data frames by either one or more key variables (i.e., horizontal join) or by variable names or columns (i.e., vertical join). Sometimes a join is referred to as a merge and vice versa, and thus I will use these terms interchangeably throughout the chapter. Broadly speaking, there are two types of joins (merges): horizontal and vertical. 17.1.1 Review of Horizontal Joins (Merges) A horizontal join (merge) refers to the process of matching cases (i.e., rows, observations) between two data frames using a key variable (matching variable), which results in distinct sets of variables (i.e., fields, columns) being combined horizontally (laterally) across two data frames. The resulting joined data frame will be wider (in terms of the number of variables) than either of the original data frames in isolation. For example, imagine that we pull data from separate information systems, each with different variables (i.e., fields) but at least some employees (i.e., cases) in common; to combine these two data frames, we can perform a horizontal join. This is often a necessary step when creating a data frame that contains all of the variables we will need in subsequent data analyses. For instance, if we wish to estimate the criterion-related validities using the selection tool scores from one data frame with criterion (e.g., job performance) scores from another data frame, then we could perform a horizontal join, assuming we have a key variable with which to match the scores from the two data frames. In a horizontal join, cases (or observations) are matched between two data frames using one or more key variables. We will focus on four different types of horizontal joins: Inner join: All unmatched cases (or observations) are dropped, thereby retaining only those cases that are present in both the left (x, first) and right (y, second) data frames. In other words, a case is only included in the merged data frame if it appears in both of the original data data frames. In an inner join, all unmatched cases (or observations) are dropped, thereby retaining only those cases that are present in both the left (x, first) and right (y, second) data frames. Full join: All cases (or observations) are retained, including those cases that do not have a match in the other data data frame. In other words, a case is included in the merged data frame even if it only appears in one of the original data data frames. These type of join leads to the highest number of retained cases under conditions in which both data frames contain unique cases. In a full join, all cases (or observations) are retained, including those cases that do not have a match in the other data data frame. Left join: All cases (or observations) that appear in the left (x, first) data frame are retained, even if they lack a match in the right (y, second) data frame. Consequently, cases from the right data frame that lack a match in the left data frame are dropped in the merged data frame. In a left join, all cases (or observations) that appear in the left (x, first) data frame are retained, even if they lack a match in the right (y, second) data frame. Right join: All cases (or observations) that appear in the right (y, second) data frame are retained, even if they lack a match in the left (x, first) data frame. Consequently, cases from the left data frame that lack a match in the right data frame are dropped in the merged data frame. In a left join, only cases (or observations) that appear in the left (x, first) data frame are retained, even if they lack a match in the right (y, second) data frame. Please note that I have illustrated different types of horizontal joins using a single key variable. It is entirely possible to perform horizontal joins using two or more key variables. For example, imagine that each morning we administered a pulse survey to employees and each afternoon we afternoon we administered a different pulse survey to the same employees, and that we repeated this process for five consecutive workdays. In this instance, we would likely need to horizontally join the data frames using both a unique employee identifier variable and a unique day-of-week variable. 17.1.2 Review of Vertical Joins (Merges) A vertical join (merge) refers to the process of matching identical variables from two data frames, which results in distinct sets of cases or observations being combined vertically. The resulting joined data frame will be longer (in terms of the number of cases) than either of the original data frames in isolation. For example, imagine an organization administered the same survey to two facilities (i.e., independent groups) each with unique employees; we could combine the two resulting data frames by performing a vertical join. In a vertical join, identical variables are matched between two data frames, each with distinct sets of cases or observations. 17.2 Tutorial This chapters tutorial demonstrates how to join (merge) cases from two data frames. 17.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to join (merge) data with or without the pipe (%&gt;%) operator. If youre unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Finally, please note that in the chapter supplement you have an opportunity to learn how to join (merge) cases from data frames using a function from base R, which you may find preferable or more intuitive. Link to video tutorial: https://youtu.be/38zsLj-fWo0 17.2.2 Functions &amp; Packages Introduced Function Package merge base R right_join dplyr left_join dplyr inner_join dplyr full_join dplyr data.frame base R c base R rep base R rbind base R 17.2.3 Initial Steps If you havent already, save the files called PersData.csv and PerfData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data files for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called PersData.csv and PerfData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) performancedata &lt;- read_csv(&quot;PerfData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## perf_q1 = col_double(), ## perf_q2 = col_double(), ## perf_q3 = col_double(), ## perf_q4 = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male print(performancedata) ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 As you can see from the output generated in your Console, on the one hand, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. On the other hand, the performancedata data frame object contains the same id unique identifier variable as the personaldata data frame object, but instead of employee demographic information, this data frame object includes variables associated with quarterly employee performance: perf_q1, perf_q2, perf_q3, and perf_q4. In order to better illustrate certain join functions later on in this chapter, well begin by removing the case (i.e., employee) associated with the id variable value of 153 (i.e., Alejandro Sanchez); in terms of a rationale for doing so, lets imagine that Alejandro no longer works for the organization, and thus we would like to remove him from the personaldata data frame. If you dont completely understand the following process for removing this individual from the data frame, no need to worry, as you will learn more in the subsequent chapter on filtering data. Type the name of the data frame object (personaldata) followed by the &lt;- operator to overwrite the existing data frame object. Type the name of the original data frame object (personaldata) followed by brackets ([ ]). Within the brackets ([ ]), type the name of the data frame object (personaldata) again, followed by the $ operator and the name of the variable we wish to use to select the case that will be removed, which in this instance is the id unique identifier variable. The $ operator indicates to R that the id variable belongs to the personaldata data frame. Type the not equal to operator, which is != (the ! means not), followed by the id variable value we wish to use to remove the case (i.e., 153). Type a comma (,) to indicate that we are removing a row, not a column. When referencing rows and columns in R, as we are doing in the brackets ([ ]), rows are entered first (before a comma), and columns are entered second (after a comma). In doing so, we are telling R to retain all rows of data in personaldata except for the one corresponding to id equal to 153. # Remove case with id variable equal to 153 personaldata &lt;- personaldata[personaldata$id != 153,] Check out the first 6 rows of the updated data frame for personaldata, and note that the data corresponding to the case associated with id equal to 153 is gone. # Print first 6 rows of first data frame object once more head(personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female 17.2.4 Horizontal Join (Merge) Recall that a horizontal join (merge) means that cases are matched using one more more key variables, and as a result, variables (i.e., columns, fields) are combined across two data frames. We will review two options for performing horizontal joins. To perform horizontal joins, we will learn how to use the join functions from the the dplyr package (Wickham et al. 2021), which include: right_join, left_join, inner_join, and full_join. Please note that there are other functions we could use to perform horizontal joins, and if youre interested, in the Joining (Merging) Data: Chapter Supplement, I demonstrate how to use the merge function from base R to carry out the same operations we will cover below. Using the aforementioned join functions, we will match cases from the personaldata and performancedata data frames using the id unique identifier variable as a key variable. So how can we verify that id is an appropriate key variable? Well, lets use the names function from base R to retrieve the list of variable names from the two data frames, which we already did above. Nevertheless, lets call up those variable names once more. Simply enter the name of the data frame as a parenthetical argument in the names function. # Retrieve variable names from first data frame names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Retrieve variable names from second data frame names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; As you can see in the variable names listed above, the id variable is common to both data frames, and thus it will serve as our key variable. Now we are almost ready to begin joining the two data frames using the id unique identifier as a key variable. Before doing so, however, we should make sure that we have installed and accessed the dplyr package (if we havent already), as the join functions come from that package. # Install dplyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) I will demonstrate two techniques for applying the join function. The first technique uses the pipe operator (%&gt;%). The pipe operator comes from a package called magrittr (Bache and Wickham 2020), on which the dplyr is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemunds (2017) chapter on pipes: https://r4ds.had.co.nz/pipes.html. The second technique for applying the join function takes a more traditional approach in that it involves nested functions being nested parenthetically. If you dont want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 17.2.4.1 With Pipe Using the pipe (%&gt;%) operator technique, lets begin with what is referred to as an inner join by doing the following: Use the &lt;- operator to name the joined (merged) data frame that we will create using the one of the dplyr join functions. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Make sure you put the name of the new data frame object to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the first data frame, which we named personaldata, followed by the pipe (%&gt;%) operator. This will pipe our data frame into the subsequent function. On the same line or on the next line, type the inner_join function, and within the parentheses as the first argument, type the name of the second data frame, which we called performancedata. As the second argument, use the by= argument to indicate the name of the key variable, which in this example is id; make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join (with pipe) mergeddf &lt;- personaldata %&gt;% inner_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 5 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now, lets revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female ## 7 201 Providence Cindy 1/9/2016 female ## 8 282 Legend John 1/9/2016 male # Print the second original data frame performancedata ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we simply swap out the inner_join function from our previous code with the full_join function. # Full join (with pipe) mergeddf &lt;- personaldata %&gt;% full_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 9 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 9 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the left_join function instead, while keeping the rest of the previous code the same. # Left join (with pipe) mergeddf &lt;- personaldata %&gt;% left_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 8 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how the left_join function retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we will use the right_join function instead, while keeping the rest of the previous code the same. # Right join (with pipe) mergeddf &lt;- personaldata %&gt;% right_join(performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 6 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 6 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the right_join function retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. 17.2.4.2 Without Pipe In this section, I demonstrate the same dplyr join functions as above, except here I demonstrate how to specify the functions without the use of a pipe (%&gt;%) operator. Lets begin with what is referred to as an inner join by doing the following: Use the &lt;- operator to name the joined (merged) data frame that we will create using the one of the dplyr join functions. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Make sure you put the name of the new data frame object to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the inner_join function. As the first argument within the parentheses, type the name of the first data frame, which we named personaldata. As the second argument, type the name of the second data frame we named performancedata. As the third argument, use the by= argument to indicate the name of the key variable, which in this example is id; make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join (without pipe) mergeddf &lt;- inner_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 5 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now, lets revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 male ## 2 155 Smith John 1/9/2016 male ## 3 165 Doe Jane 1/4/2016 female ## 4 125 Franklin Benjamin 1/5/2016 male ## 5 111 Newton Isaac 1/9/2016 male ## 6 198 Morales Linda 1/7/2016 female ## 7 201 Providence Cindy 1/9/2016 female ## 8 282 Legend John 1/9/2016 male # Print the second original data frame performancedata ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we simply swap out the inner_join function from our previous code with the full_join function. # Full join (without pipe) mergeddf &lt;- full_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 9 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 9 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the left_join function instead, while keeping the rest of the previous code the same. # Left join (without pipe) mergeddf &lt;- left_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 8 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 2 155 Smith John 1/9/2016 male NA NA NA NA ## 3 165 Doe Jane 1/4/2016 female NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how the left_join function retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we use the right_join function instead, while keeping the rest of the previous code the same. # Right join (without pipe) mergeddf &lt;- right_join(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## # A tibble: 6 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 ## 6 153 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3.9 4.8 4.9 5 Note how the right_join function retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. 17.2.5 Vertical Join (Merge) To perform a vertical join (merge), we will use the rbind function from base R, which stands for row bind. As a reminder, with a horizontal join, our focus is on joining variables (i.e., columns, fields) from two data frames containing overlapping cases (i.e., rows). In contrast, with a vertical join, our focus is on joining cases from data frames with the same variables. To illustrate how to perform a vertical join, we take a slightly different approach than what we did with horizontal joins. Instead of reading in data files, we will create two toy employee demographic data frames with the exact same variables but different cases. We will use the data.frame function from base R to indicate that we wish to create a data frame object; we use the c (combine) function from base R to combine values into a vector; and we use the rep (replicate) function from base R to replicate the same value a specified number of times. Also note that the : operator, when used between two numbers, creates a vector of consecutive values, beginning with the first value and ending with the second. Please note, that using and understanding the data.frame, c, and rep functions is not consequential for understanding how to do a vertical merge; rather, I merely use these functions in this tutorial to create quick toy data frames that we can use to illustrate how to do a vertical join. For more information on the data.frame function and the c function, please refer to the chapter called Basic Features and Operations of the R Language. # Create data frames with same variables but arbitrary values df1 &lt;- data.frame(id=c(1:6), age=c(21:26), sex=c(rep(&quot;male&quot;, 6))) df2 &lt;- data.frame(id=c(7:10), age=c(27:30), sex=c(rep(&quot;female&quot;, 4))) # Print first data frame df1 ## id age sex ## 1 1 21 male ## 2 2 22 male ## 3 3 23 male ## 4 4 24 male ## 5 5 25 male ## 6 6 26 male # Print second data frame df2 ## id age sex ## 1 7 27 female ## 2 8 28 female ## 3 9 29 female ## 4 10 30 female Given that these two data frames (i.e., df1, df2) have the exact same variable names (id, age, and sex), we can easily perform a vertical join using the rbind function. To do so, enter the names of the two data frames as arguments, separated by a comma. Use the &lt;- operator to name the merged data frame something, which for this case, I arbitrarily named it mergeddf2. # Vertical merge mergeddf2 &lt;- rbind(df1, df2) # Print the merged data frame mergeddf2 ## id age sex ## 1 1 21 male ## 2 2 22 male ## 3 3 23 male ## 4 4 24 male ## 5 5 25 male ## 6 6 26 male ## 7 7 27 female ## 8 8 28 female ## 9 9 29 female ## 10 10 30 female Note how the two data frames are now stacked on one another. This was possible because they shared the same variables names and variables types (e.g., numeric and character). 17.2.6 Summary Joining (merging) data frames in R is a useful practice. In this chapter, we learned how to perform a horizontal join using the right_join, left_join, inner_join, and full_join functions from the dplyr package. We also learned how to perform a vertical join using the rbind function from base R. 17.3 Chapter Supplement In addition to the join functions from the dplyr package covered above, we can use the merge function from base R to perform a horizontal join. Because this function comes from base R, we do not need to install and access an additional package like we do with the join functions, which some may find advantageous. 17.3.1 Video Tutorial In addition to the written chapter supplement provided below, you can follow along with the following video tutorial to learn more about how to horizontally join data two data frames using the merge function from base R. Link to video tutorial: https://youtu.be/MLihEVEpJBg 17.3.2 Functions &amp; Packages Introduced Function Package names base R merge base R 17.3.3 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. Please note, however, that we are using two slightly different data files in this supplement, which will simply and clarify some of the different types of merges (joins) that well go over. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects # Note that these data files are different than the # ones we used in the main part of the chapter personaldata &lt;- read_csv(&quot;PersonalData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) performancedata &lt;- read_csv(&quot;PerformanceData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## perf_q1 = col_double(), ## perf_q2 = col_double(), ## perf_q3 = col_double(), ## perf_q4 = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 8 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man print(performancedata) ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 17.3.4 merge Function from Base R We will use the merge function to horizontally match cases from the personaldata and performancedata data frames using id as a key variable. To identify what the key variable is, lets use the names function from base R to print the list of variable names from the two data frames, which we already did above. Nevertheless, lets call up those variable names once more. Simply enter the name of the data frame as a parenthetical argument in the names function. # Print variable names from first data frame names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print variable names from second data frame names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; As you can see in the variable names listed above, the id variable is common to both data frames, and thus it will serve as our key variable. Lets begin with what is referred to as an inner join: Use the &lt;- operator to name the joined data frame that we create using the merge function. For this example, I name the new joined data frame mergeddf, which is completely arbitrary; you could name it whatever you would like. Type the name of the new joined data frame to the left of the &lt;- operator. To the right of the &lt;- operator, type the name of the merge function. Within the merge function parentheses, we will provide the arguments needed to make this join a reality. First, enter the name of one of the data frames (e.g., personaldata), followed by a comma. Second, enter the name of of the other data frame (e.g., performancedata), followed by a comma. Third, use the by= argument to indicate the name of the key variable (e.g., id); make sure the key variable is in quotation marks (\" \"), and remember, object and variable names in R are case and space sensitive. # Inner join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 3 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 5 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Now, lets revisit the original data frame objects that we read in initially. # Print the first original data frame personaldata ## # A tibble: 8 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man # Print the second original data frame performancedata ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 In the output, first, note how all of the variables from the original data frames (i.e., personaldata, performancedata) are represented in the merged data frame (i.e., mergeddf). Second, note how the cases are matched by the id key variable. Third, note that the personaldata data frame has 8 cases, the performancedata data frame has 6 cases, and the mergeddf data frame has 6 cases. By default, the merge function performs an inner join and retains only those matched cases that have data in both data frames. Because cases whose id values were 154, 155, and 165 had data in personaldata but not performancedata and because the case with an id value equal to 153 was in performancedata but not personaldata, only the 5 cases that had available data in both data frames were retained. To perform what is referred to as a full join in which we retain all cases and available data, we can add the all= argument to our previous code and specify the logical value TRUE. # Full join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 125 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2.1 1.9 2.1 2.3 ## 3 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 4 154 McDonald Ronald 1/9/2016 man NA NA NA NA ## 5 155 Smith John 1/9/2016 man NA NA NA NA ## 6 165 Doe Jane 1/4/2016 woman NA NA NA NA ## 7 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 8 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 9 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the full_join function retains all available cases that had available data in at least one of the data frames, which in this example is 9 cases. When in doubt, I recommend using the full_join function to retain all available data. To perform what is referred to as a left join in which we retain only those cases with data available in the first (left, x) data frame (personaldata), we use the all.x=TRUE argument instead. # Left join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all.x=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 3 154 McDonald Ronald 1/9/2016 man NA NA NA NA ## 4 155 Smith John 1/9/2016 man NA NA NA NA ## 5 165 Doe Jane 1/4/2016 woman NA NA NA NA ## 6 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 7 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 8 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the left join retains only those cases for which the first (left, x) data frame (i.e., personaldata) has complete data, which in this case happens to be 8 cases. Notably absent is the case associated with id equal to 153 because the first (left, x) data frame (i.e., personaldata) lacked that case. An NA appears for each case from the second (right, y) data frame that contained missing values on variables from that data frame. To perform what is referred to as a right join in which we retain only those cases with data available in the second (right, y) data frame (performancedata), we use the all.y=TRUE argument instead. # Right join mergeddf &lt;- merge(personaldata, performancedata, by=&quot;id&quot;, all.y=TRUE) # Print the joined data frame mergeddf ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## 1 111 Newton Isaac 1/9/2016 man 3.3 3.3 3.4 3.3 ## 2 125 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2.1 1.9 2.1 2.3 ## 3 153 Sanchez Alejandro 1/1/2016 man 3.9 4.8 4.9 5.0 ## 4 198 Morales Linda 1/7/2016 woman 4.9 4.5 4.4 4.8 ## 5 201 Providence Cindy 1/9/2016 woman 1.2 1.1 1.0 1.0 ## 6 282 Legend John 1/9/2016 man 2.2 2.3 2.4 2.5 Note how the right join retains only those cases for which the joined (second, right, y) data frame (i.e., performancedata) has complete data. Because the first (left, x) data frame lacks data for the case in which id is equal to 153, an NA appears for each case from the first data frame that contained missing values on variables from that data frame. "],["filter.html", "Chapter 18 Filtering (Subsetting) Data 18.1 Conceptual Overview 18.2 Tutorial 18.3 Chapter Supplement", " Chapter 18 Filtering (Subsetting) Data In this chapter, we will learn how to filter (subset) cases from a data frame and how to select or remove variables from a data frame. Well begin with a conceptual overview of filtering (subsetting) and variable selection/removal. In this chapter, well use the terms filter and subset interchangeably. 18.1 Conceptual Overview Filtering data (i.e., subsetting data) is an important data-management process, as it allows us to: Select or remove a subset of cases from a data frame based on their scores on one or more variables; Select or remove a subset of variables from a data frame. In this section, we will review logical operators, as it is through the application of logical operators that we will ultimately filter (subset) cases from a data frame. 18.1.1 Review of Logical Operators When our goal is to select or remove a subset of cases (i.e., observations) from a data frame, we typically do so by applying logical operators. You may already be comfortable with the use of logical operators and expressions like greater than (\\(&gt;\\)), less than (\\(&lt;\\)), equal to (\\(=\\)), greater than or equal to (\\(\\ge\\)), and less than or equal to (\\(\\le\\)), but you may be less comfortable with the use of the logical OR and the logical AND. Thus, before we start working with data, lets do a quick review. When we wish to apply a single logical statement, our job is relatively straightforward. To keep things simple, lets focus on a single vector, and well call that vector \\(X\\). For example, we might choose to select only those cases with scores on \\(X\\) that are greater than or equal to (\\(\\ge\\) or \\(&gt;\\)\\(=\\)) 3, thereby retaining scores that are equal to or greater than 3, as highlighted in blue below. In other words, we select only those cases for which their score on \\(X\\) would be true given the logical statement \\(X \\ge 3\\). The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we apply the logical statement that \\(X\\) is greater than or equal to (\\(\\ge\\)) 3, we select only those cases with scores that are equal to or greater than 3 and up to the maximum possible score of 7. As another example, we might choose to select only those cases with scores on \\(X\\) that are less than or equal to (\\(\\le\\) or \\(&lt;\\)\\(=\\)) 5, thereby retaining scores that are equal to or less than 5, as highlighted in red below. In other words, we select only those cases for which their score on \\(X\\) would be true given the logical statement \\(X \\le 5\\). The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we apply the logical statement that \\(X\\) is greater than or equal to (\\(\\ge\\)) 3, we select only those cases with scores that are equal to or greater than 3 and down to the minimum possible score of 1. If we apply the logical OR operator, things get a bit more interesting. The logical OR is used when our objective is to select cases based on two logical statements, and if either logical statement is true for a given case, then that case is selected. The logical OR is consistent with idea of a mathematical union (\\(\\cup\\)) from Set Theory. As an example, lets combine the two logical statements from above with the logical OR operator. We retain those cases for which their score on \\(X\\) is greater than or equal to 3 or less than or equal to 5. Given that \\(X\\) has possible scores ranging from 1-7, below, we see that this logic retains all cases, as all scores (1-7) would satisfy one or both of the logical statements. The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we wish to retain those cases for which \\(X\\) is greater than or equal to 3 or less than or equal to 5, we select those cases that satisfy either (or both) logical statements. Using the same to logical statements above, lets replace the logical OR with the logical AND. The logical AND is used when our objective is to select cases based on two logical statements, and if both logical statements are true for a given case, then that case is selected. The logical AND is consistent with idea of a mathematical intersection (\\(\\cap\\)) from Set Theory. In this example, we retain only those cases for which their score on \\(X\\) is greater than or equal to 3 and less than or equal to 5. Given that \\(X\\) has possible scores ranging from 1-7, below, we see that this logic retain only cases with scores within the range 3-5, as only scores of 3, 4, or 5 would satisfy both of the logical statements. The vector \\(X\\) has seven possible scores, ranging from 1 to 7. If we wish to retain those cases for which \\(X\\) is greater than or equal to 3 and less than or equal to 5, we select only those cases that satisfy both logical statements. 18.2 Tutorial This chapters tutorial demonstrates how to filter (subset) cases from a data frame and how to select or remove variables from a data frame. 18.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Both versions of the tutorial will show you how to filter (subset) data with or without the pipe (%&gt;%) operator. If youre unfamiliar with the pipe operator, no need to worry: I provide a brief explanation and demonstration regarding their purpose in both versions of the tutorial. Finally, please note that in the chapter supplement you have an opportunity to learn how to filter (subset) cases and select/remove variables using a function from base R, which you may find more intuitive or preferable for other reasons. Link to video tutorial: https://youtu.be/izVcbPmu0D0 18.2.2 Functions &amp; Packages Introduced Function Package str base R filter dplyr c base R as.Date base R select dplyr subset base R 18.2.3 Initial Steps If you havent already, save the files called PersData.csv and PerfData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called PersData.csv and PerfData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects personaldata &lt;- read_csv(&quot;PersData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) performancedata &lt;- read_csv(&quot;PerfData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## perf_q1 = col_double(), ## perf_q2 = col_double(), ## perf_q3 = col_double(), ## perf_q4 = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; names(performancedata) ## [1] &quot;id&quot; &quot;perf_q1&quot; &quot;perf_q2&quot; &quot;perf_q3&quot; &quot;perf_q4&quot; # Print data frame (tibble) objects print(personaldata) ## # A tibble: 9 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male ## 2 154 McDonald Ronald 1/9/2016 male ## 3 155 Smith John 1/9/2016 male ## 4 165 Doe Jane 1/4/2016 female ## 5 125 Franklin Benjamin 1/5/2016 male ## 6 111 Newton Isaac 1/9/2016 male ## 7 198 Morales Linda 1/7/2016 female ## 8 201 Providence Cindy 1/9/2016 female ## 9 282 Legend John 1/9/2016 male print(performancedata) ## # A tibble: 6 x 5 ## id perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 4.8 4.9 5 ## 2 125 2.1 1.9 2.1 2.3 ## 3 111 3.3 3.3 3.4 3.3 ## 4 198 4.9 4.5 4.4 4.8 ## 5 201 1.2 1.1 1 1 ## 6 282 2.2 2.3 2.4 2.5 As you can see from the output generated in your console, on the one hand, the personaldata data frame object contains basic employee demographic information. The variable names include: id, lastname, firstname, startdate, and gender. On the other hand, the personaldata data frame object contains the same id unique identifier variable as the personaldata data frame object, but instead of employee demographic information, this data frame object includes variables associated with quarterly employee performance: perf_q1, perf_q2, perf_q3, and perf_q4. To make this chapter more interesting (and for the sake of practice), lets use the full_join function from dplyr (Wickham et al. 2021) to join (merge) the two data frames we just read in (personaldata, performancedata) using the id variable as the key variable. Lets arbitrarily name the new joined (merged) data frame mergeddf using the &lt;- operator. For more information on joining data, check out the chapter called Joining (Merging) Data. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;dplyr&quot;) # Access package library(dplyr) # Full join (without pipe) mergeddf &lt;- full_join(personaldata, performancedata, by=&quot;id&quot;) # Print joined (merged) data frame object print(mergeddf) ## # A tibble: 9 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 8 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 9 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Now we have a joined data frame called mergeddf! 18.2.4 Filter Cases from Data Frame Sometimes we want to select only a subset of cases from a data frame or table. There are different functions that can achieve this end. For example, the subset function filter from base R will do the trick. With that said, the dplyr package offers the filter function which has some advantages (e.g., faster with larger amounts of data), and thus, we will focus on the filter function in this chapter. If you would like to learn how to use the subset function from base R, check out the chapter supplement called Joining (Merging) Data. In order to properly filter data by cases, we need to know the respective types (classes) of the variables in the data frame. Perhaps the quickest way to find out the type (class) of each variable in the data frame is to use the str (structure) function from base R, and the functions parentheses, just enter the name of the data frame (mergeddf). # Determine class of variables str(mergeddf) ## spec_tbl_df[,9] [9 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:9] 153 154 155 165 125 111 198 201 282 ## $ lastname : chr [1:9] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname: chr [1:9] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate: chr [1:9] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:9] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; ... ## $ perf_q1 : num [1:9] 3.9 NA NA NA 2.1 3.3 4.9 1.2 2.2 ## $ perf_q2 : num [1:9] 4.8 NA NA NA 1.9 3.3 4.5 1.1 2.3 ## $ perf_q3 : num [1:9] 4.9 NA NA NA 2.1 3.4 4.4 1 2.4 ## $ perf_q4 : num [1:9] 5 NA NA NA 2.3 3.3 4.8 1 2.5 ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) Note that the id variable is of type integer; the lastname, firstname, startdate, and gender variables are of type character (string); and the perf_q4, perf_q4, perf_q4, and perf_q4 variables are of type numeric. The variable type will have important implications for how use use the filter function from dplyr. In R, we can apply any one of the following logical operators when filtering our data: Logical Operator Definition &lt; less than &gt; greater than &lt;= less than or equal to &gt;= greater than or equal to == equal to != not equal to | or &amp; and ! not To get started, install and access the dplyr package. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) I will demonstrate two approaches for applying the filter function from dplyr. The first option uses pipe(s), which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2020), on which the dplyr is partially dependent. In short, a pipe allows one to more efficiently code/script and to improve the readability of the code/script under certain conditions. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. The second option is more traditional and lacks the efficiency and readability of pipes. You can use either approach, and if dont you want to use pipes, skip to the section below called Without Pipes. For more information on the pipe operator, check out this link: https://r4ds.had.co.nz/pipes.html. 18.2.4.1 With Pipes Using an approach with pipes, first, use the &lt;- operator to name the filtered data frame that we will create. For this example, I name the new joined data frame filterdf; you could name it whatever you would like. Second, type the name of the first data frame, which we named mergeddf (see above), followed by the pipe (%&gt;%) operator. This will pipe our data frame into the subsequent function. Third, either on the same line or on the next line, type the filter function. Fourth, within the function parentheses, type the name of the variable we wish to filter the data frame by, which in this example is gender. Fourth, type a logical operator, which for this example is ==. Fifth, type a value for the filter variable, which in this example is female; because the gender variable is of type character, we need to put quotation marks (\" \") around the value of the variable that we wish to filter by. Remember, object names in R are case and space sensitive; for instance, gender is different from Gender, and female is different from Female. # Filter in by gender with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot;) # Print filtered data frame filterdf ## # A tibble: 3 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 165 Doe Jane 1/4/2016 female NA NA NA NA ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 3 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 Note how the data frame above contains only those cases with female as their gender variable designation. The filter worked as expected. Alternatively, we could filter out those cases in which gender is equal to female using the != (not equal to) logical operator. # Filter out by gender with pipe filterdf &lt;- mergeddf %&gt;% filter(gender!=&quot;female&quot;) # Print filtered data frame filterdf ## # A tibble: 6 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 5 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 6 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note how cases with gender equal to female are no longer in the data frame, while every other case is retained. Lets now filter by a variable of type numeric (or integer). Specifically, lets select those cases in which the perf_q2 variable is greater than (&gt;) 4.0. Because the perf_q2 variable is of type numeric, we dont use quotation marks (\" \") around the value we wish to filter by, which in this case is 4.0. # Filter by perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 2 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 If we wish to filter by two variables, we can apply the logical or (|) operator or and (&amp;) operator. First, lets select those cases in which either gender is equal to female or perf_q2 is greater than 4.0 using the or (|) operator. # Filter by gender or perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot; | perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 4 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 165 Doe Jane 1/4/2016 female NA NA NA NA ## 3 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 Watch what happens if we apply the logical and (&amp;) operator with the same syntax as above. # Filter by gender and perf_q2 with pipe filterdf &lt;- mergeddf %&gt;% filter(gender==&quot;female&quot; &amp; perf_q2&gt;4.0) # Print filtered data frame filterdf ## # A tibble: 1 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 We can also use the logical or (|) operator to select two values of the same variable. # Filter by two values of firstname with pipe filterdf &lt;- mergeddf %&gt;% filter(firstname==&quot;John&quot; | firstname==&quot;Jane&quot;) # Print filtered data frame filterdf ## # A tibble: 3 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 155 Smith John 1/9/2016 male NA NA NA NA ## 2 165 Doe Jane 1/4/2016 female NA NA NA NA ## 3 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Or we can select two ranges of values from the same variable using the logical or (|) operator, assuming the variable is of type numeric, integer, or date. # Filter by two ranges of values of perf_q1 with pipe filterdf &lt;- mergeddf %&gt;% filter(perf_q1&lt;=2.5 | perf_q1&gt;=4.0) # Print filtered data frame filterdf ## # A tibble: 4 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 2 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 3 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 4 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 The filter function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when youve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, lets pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out id of 198 and 201 with pipe filterdf &lt;- mergeddf %&gt;% filter(!id %in% c(198,201)) # Print filtered data frame filterdf ## # A tibble: 7 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 Note that in the output above cases with id variable values equal to 198 and 201 are no longer present. If you remove the ! in front of the filter variable, only cases 198 and 201 are retained. # Filter in id of 198 and 201 with pipe filterdf &lt;- mergeddf %&gt;% filter(id %in% c(198,201)) # Print filtered data frame filterdf ## # A tibble: 2 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 198 Morales Linda 1/7/2016 female 4.9 4.5 4.4 4.8 ## 2 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 And if you wanted to remove just a single case, you could use the unique identifier variable (id) and the following script/code. # Filter out id of 198 with pipe filterdf &lt;- mergeddf %&gt;% filter(id!=198) # Print filtered data frame filterdf ## # A tibble: 8 x 9 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 153 Sanchez Alejandro 1/1/2016 male 3.9 4.8 4.9 5 ## 2 154 McDonald Ronald 1/9/2016 male NA NA NA NA ## 3 155 Smith John 1/9/2016 male NA NA NA NA ## 4 165 Doe Jane 1/4/2016 female NA NA NA NA ## 5 125 Franklin Benjamin 1/5/2016 male 2.1 1.9 2.1 2.3 ## 6 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 ## 7 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 ## 8 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. First, type the name of the data frame object (mergeddf), followed by the $ operator and the name of whatever you want to call the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Second, type the &lt;- operator. Third, type the name of the as.Date function. Fourth, in the function parentheses, as the first argument, enter the as.character function with the name of the data frame object (mergeddf), followed by the $ operator and the name the original variable (startdate) as the sole argument. Fifth, as the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. # Convert character startdate variable to the Date type startdate2 variable mergeddf$startdate2 &lt;- as.Date(as.character(mergeddf$startdate), format=&quot;%m/%d/%Y&quot;) To verify that the new startdate2 variable is of type date, use the str function from base R, and enter the name of the data frame object (mergeddf) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Verify that the startdate2 variable is now a variable of type Date str(mergeddf) ## spec_tbl_df[,10] [9 x 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:9] 153 154 155 165 125 111 198 201 282 ## $ lastname : chr [1:9] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname : chr [1:9] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate : chr [1:9] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:9] &quot;male&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; ... ## $ perf_q1 : num [1:9] 3.9 NA NA NA 2.1 3.3 4.9 1.2 2.2 ## $ perf_q2 : num [1:9] 4.8 NA NA NA 1.9 3.3 4.5 1.1 2.3 ## $ perf_q3 : num [1:9] 4.9 NA NA NA 2.1 3.4 4.4 1 2.4 ## $ perf_q4 : num [1:9] 5 NA NA NA 2.3 3.3 4.8 1 2.5 ## $ startdate2: Date[1:9], format: &quot;2016-01-01&quot; &quot;2016-01-09&quot; &quot;2016-01-09&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) Now we are ready to filter using the new startdate2 variable. When specify the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I filter for those cases in which their startdate2 values are greater than 2016-01-07. # Filter by startdate2 with pipe filterdf &lt;- mergeddf %&gt;% filter(startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered data frame filterdf ## # A tibble: 5 x 10 ## id lastname firstname startdate gender perf_q1 perf_q2 perf_q3 perf_q4 startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 154 McDonald Ronald 1/9/2016 male NA NA NA NA 2016-01-09 ## 2 155 Smith John 1/9/2016 male NA NA NA NA 2016-01-09 ## 3 111 Newton Isaac 1/9/2016 male 3.3 3.3 3.4 3.3 2016-01-09 ## 4 201 Providence Cindy 1/9/2016 female 1.2 1.1 1 1 2016-01-09 ## 5 282 Legend John 1/9/2016 male 2.2 2.3 2.4 2.5 2016-01-09 18.2.4.2 Without Pipes We can also filter using the filter function from the dplyr package without using the pipe (%&gt;%) operator. Note how I simply move the name of the data frame object from before the pipe (%&gt;%) operator to the first argument in the filter function. Everything else remains the same. For simplicity, I dont display the output below as it is the same as the output as above using pipes. Your decision whether to use a pipe operator is completely up to you. Lets filter the mergeddf data frame object such that only those cases for which the gender variable is equal to female are retained. Note how we apply the equal to (==) logical operator. A table of logical operators is presented towards the beginning of this tutorial. # Filter in by gender without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot;) # Print filtered data frame filterdf Now lets filter out those cases in which gender is not equal to female. We apply the not equal to (!=) logical operator to do so. # Filter in by gender without pipe filterdf &lt;- filter(mergeddf, gender!=&quot;female&quot;) # Print filtered data frame filterdf Filter the data frame such that we retain those cases for which the perf_q2 variable is greater than (&gt;) 4.0. Because the perf_q2 variable is numeric, we dont put the value 4.0 in quotation marks. # Filter by perf_q2 without pipe filterdf &lt;- filter(mergeddf, perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical or operator (|), select those cases for which gender is equal to female or for which perf_q2 is greater than 4.0. # Filter by gender or perf_q2 without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot; | perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical and operator (&amp;), select those cases for which gender is equal to female and for which perf_q2 is greater than 4.0. Note the difference in the resulting filtered data frame. # Filter by gender and perf_q2 without pipe filterdf &lt;- filter(mergeddf, gender==&quot;female&quot; &amp; perf_q2&gt;4.0) # Print filtered data frame filterdf Using the logical or operator (|), select those cases for which firstname is equal to John or for which firstname is equal to Jane. In other words, select those individuals whose names are either John or Jane. # Filter by two values of firstname without pipe filterdf &lt;- filter(mergeddf, firstname==&quot;John&quot; | firstname==&quot;Jane&quot;) # Print filtered data frame filterdf Using the logical or operator (|), select the range of cases for which perf_q1 is less than equal to (&lt;=) 2.5 or for which perf_q1 is greater than or equal (&gt;=) to 4.0. # Filter by two ranges of values of perf_q1 without pipe filterdf &lt;- filter(mergeddf, perf_q1&lt;=2.5 | perf_q1&gt;=4.0) # Print filtered data frame filterdf The filter function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when youve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, lets pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out id of 198 and 201 without pipe filterdf &lt;- filter(mergeddf, !id %in% c(198,201)) # Print filtered data frame filterdf Or if you wish to retain only those cases for which the id variable is equal to 198 and 201, drop the not operator (!) from the previous script. # Filter in id of 198 and 201 without pipe filterdf &lt;- filter(mergeddf, id %in% c(198,201)) # Print filtered data frame filterdf You can also drop specific cases one by one using the not equal to operator (!=) and the a unique identifier value associated with the case you wish to remove. We accomplish the same result as above but use two steps instead. Also, note that in the second step below, the new data frame object (filterdf) is used as the first argument because we want to retain the changes we made in the prior step (i.e., dropping case with id equal to 198). # Filter in id of 198 without pipe filterdf &lt;- filter(mergeddf, id!=198) # Filter in id of 201 without pipe filterdf &lt;- filter(filterdf, id!=201) # Print filtered data frame filterdf When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. First, type the name of the data frame object (mergeddf), followed by the $ operator and the name of whatever you want to call the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Second, type the &lt;- operator. Third, type the name of the as.Date function. Fourth, in the function parentheses, as the first argument, enter the as.character function with the name of the data frame object (mergeddf), followed by the $ operator and the name the original variable (startdate) as the sole argument. Fifth, as the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. To verify that the new startdate2 variable is of type date, on the next line, use the str function from base R, and enter the name of the data frame object (mergeddf) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Convert character startdate variable to the date type startdate2 variable mergeddf$startdate2 &lt;- as.Date(as.character(mergeddf$startdate), format=&quot;%m/%d/%Y&quot;) # Verify that the startdate2 variable is now a variable of type date str(mergeddf) Now we are ready to filter using the new startdate2 variable. When specify the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I filter for those cases in which their startdate2 values are greater than 2016-01-07. # Filter by startdate2 without pipe filterdf &lt;- filter(mergeddf, startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered data frame filterdf 18.2.5 Remove Single Variable from Data Frame If you just need to remove a single variable from a data frame, using the NULL object in R in conjunction with the &lt;- operator can designate which variable to drop. For example, if we wish to drop the startdate variable from the mergeddf data frame, we simply note that startdate belongs to mergeddf by joining them with $. Next, we set &lt;- NULL adjacent to mergeddf$startdate to indicate that we wish to remove that variable from that data frame. # Remove variable mergeddf$startdate &lt;- NULL # Print updated data frame mergeddf ## # A tibble: 9 x 9 ## id lastname firstname gender perf_q1 perf_q2 perf_q3 perf_q4 startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; ## 1 153 Sanchez Alejandro male 3.9 4.8 4.9 5 2016-01-01 ## 2 154 McDonald Ronald male NA NA NA NA 2016-01-09 ## 3 155 Smith John male NA NA NA NA 2016-01-09 ## 4 165 Doe Jane female NA NA NA NA 2016-01-04 ## 5 125 Franklin Benjamin male 2.1 1.9 2.1 2.3 2016-01-05 ## 6 111 Newton Isaac male 3.3 3.3 3.4 3.3 2016-01-09 ## 7 198 Morales Linda female 4.9 4.5 4.4 4.8 2016-01-07 ## 8 201 Providence Cindy female 1.2 1.1 1 1 2016-01-09 ## 9 282 Legend John male 2.2 2.3 2.4 2.5 2016-01-09 18.2.6 Select Multiple Variables from Data Frame If you wish to select multiple variables from a data frame (and remove all others), the select function from the dplyr package is quite useful and intuitive. Below, I demonstrate how to select multiple variables with and without pipes. If you dont want to use pipes, feel free to skip down to the section called Without Pipes. 18.2.6.1 With Pipe Using the pipe (%&gt;%) operator, first, decide whether you want to override an existing data frame or create a new data frame based on our selection; here, I override the mergeddf data frame using the &lt;- operator, which results in mergeddf &lt;-. Second, type the name of the original data frame (mergeddf), followed by the pipe (%&gt;%) operator. Third, type the name of the select function. Fourth, in the parentheses, list the names of the variables you wish to select as arguments; all variables that are not listed will be dropped. Here, we are selecting (to retain) the id, perf_q1, gender, lastname, and firstname variables. Note that the updated date frame includes the selected variables in the order in which you listed them. # Select multiple variables with pipe mergeddf &lt;- mergeddf %&gt;% select(id, perf_q1, gender, lastname, firstname) # Print updated data frame mergeddf ## # A tibble: 9 x 5 ## id perf_q1 gender lastname firstname ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 3.9 male Sanchez Alejandro ## 2 154 NA male McDonald Ronald ## 3 155 NA male Smith John ## 4 165 NA female Doe Jane ## 5 125 2.1 male Franklin Benjamin ## 6 111 3.3 male Newton Isaac ## 7 198 4.9 female Morales Linda ## 8 201 1.2 female Providence Cindy ## 9 282 2.2 male Legend John 18.2.6.2 Without Pipe If you decide not to use the pipe (%&gt;%) operator, the syntax remains almost the same except the name of the original data frame object (mergeddf) is moved from before the pipe (%&gt;%) operator to the first argument in the select function. Everything else remains the same. # Select multiple variables without pipe mergeddf &lt;- select(mergeddf, id, gender, lastname, firstname) # Print updated data frame mergeddf 18.2.7 Remove Multiple Variables from Data Frame If you wish to remove multiple variables from a data frame, the select function from dplyr will work just fine. I demonstrate how to use this function with and without pipes. If you dont want to use pipes, feel free to skip down to the section called Without Pipes. 18.2.7.1 With Pipe Using the pipe (%&gt;%) operator, first, decide whether you want to override an existing data frame or create a new data frame from the subset; here, I override the mergeddf data frame using the &lt;- operator, which results in mergeddf &lt;-. Second, type the name of the original data frame (mergeddf), followed by the pipe (%&gt;%) operator. Third, enter the select function. Fourth, use the c (combine) function with - in front of it to note that you want to select all other variables except the ones listed in the c function. # Remove multiple variables with pipe mergeddf &lt;- mergeddf %&gt;% select(-c(lastname, firstname)) # Print updated data frame mergeddf ## # A tibble: 9 x 3 ## id perf_q1 gender ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 153 3.9 male ## 2 154 NA male ## 3 155 NA male ## 4 165 NA female ## 5 125 2.1 male ## 6 111 3.3 male ## 7 198 4.9 female ## 8 201 1.2 female ## 9 282 2.2 male Removing a single variable can also be done using the select function. To do so, just list a single variable with - in front of it (as the sole argument) to indicate that you wish to drop that variable. # Remove single variable with pipe mergeddf &lt;- mergeddf %&gt;% select(-gender) # Print updated data frame mergeddf ## # A tibble: 9 x 2 ## id perf_q1 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 153 3.9 ## 2 154 NA ## 3 155 NA ## 4 165 NA ## 5 125 2.1 ## 6 111 3.3 ## 7 198 4.9 ## 8 201 1.2 ## 9 282 2.2 18.2.7.2 Without Pipe If you decide not to use the pipe (%&gt;%) operator, the syntax remains mostly the same except the name of the original data frame object (mergeddf) is moved from before the pipe (%&gt;%) operator to the first argument in the select function. Everything else remains the same. # Remove multiple variables without pipe mergeddf &lt;- select(mergeddf, -c(lastname, firstname)) # Print updated data frame mergeddf And heres the non-pipe equivalent to removing a single variable using this approach. # Remove single variable without pipe mergeddf &lt;- mergeddf %&gt;% select(-gender) # Print updated data frame mergeddf 18.2.8 Summary Applying filters and creating subsets of cases (rows) and variables (columns) from a data frame is an important part of data management. The dplyr package has two useful functions that can be used for these purposes: filter and select. 18.3 Chapter Supplement In addition to the filter function from the dplyr package covered above, we can use the subset function from base R to subset cases from a data frame and to select cases from a data frame. Because this function comes from base R, we do not need to install and access an additional package like we do with the filter function, which some may prefer or find advantageous. Further, we can also apply the str_detect function from the stringr package with either the subset or the filter function to filter by a text pattern contained within a string (e.g., character) variable. 18.3.1 Video Tutorials In addition to the written chapter supplement provided below, you can follow along with the following video tutorials to learn more about how to subset cases and select/remove variables using the subset function from base R. Link to video tutorial: https://youtu.be/iM1e0wxUMrs Link to video tutorial: https://youtu.be/kNpezEOx70g 18.3.2 Functions &amp; Packages Introduced Function Package subset base R str_detect stringr 18.3.3 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object # Note that we will only be reading in one # data frame object for this supplement # ones we used in the main part of the chapter personaldata &lt;- read_csv(&quot;PersonalData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## lastname = col_character(), ## firstname = col_character(), ## startdate = col_character(), ## gender = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(personaldata) ## [1] &quot;id&quot; &quot;lastname&quot; &quot;firstname&quot; &quot;startdate&quot; &quot;gender&quot; # Print data frame (tibble) object print(personaldata) ## # A tibble: 8 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 198 Morales Linda 1/7/2016 woman ## 7 201 Providence Cindy 1/9/2016 woman ## 8 282 Legend John 1/9/2016 man 18.3.4 subset Function from Base R As an alternative to the filter function from the dplyr package, we will learn how to use the subset function from base R to filter cases from a data frame and to select or remove variables from a data frame. 18.3.4.1 Filter (Subset) Cases from Data Frame Well begin by filtering cases from a data frame object. As a reminder, in R, we can apply any one of the following logical operators when filtering cases from a data frame or table object. Logical Operator Definition &lt; less than &gt; greater than &lt;= less than or equal to &gt;= greater than or equal to == equal to != not equal to | or &amp; and ! not To filter (subset) cases from a data frame object using the subset function from base R, we will take the following steps: Well use the &lt;- assignment operator to name the filtered data frame that we are about to create. For this example, I chose to create a new data frame object (sub_personaldata), which I specified to the left of the &lt;- operator; that being said, you could name the new data frame object whatever you would like. To the right of the &lt;- operator, type the name of subset function from base R. As the first argument in the function, type the name of the data frame we created above (personaldata). As the second argument, type the name of the variable we wish to filter the data frame by, which in this example is gender followed by a logical (conditional) argument. For this example, we wish to to retain only those cases in which gender is equal to woman, and we do so using this logical argument gender == \"woman\". Because the gender variable is of type character, we need to put quotation marks (\" \") around the variable value (i.e., text) that we wish to filter by. Remember, object names in R are case and space sensitive; for instance, gender is different from Gender, and woman is different from Woman. # Filter (subset) by gender sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 woman ## 2 198 Morales Linda 1/7/2016 woman ## 3 201 Providence Cindy 1/9/2016 woman Note how the data frame above contains only those cases with woman as their gender variable designation. The filter worked as expected. Alternatively, we could filter out (subset out) those cases in which gender is equal to woman using the != (not equal to) logical operator. Instead of overwriting the existing data frame object (personaldata), lets assign the filtered data frame object to a new object that well call sub_personaldata. # Filter (subset) by gender sub_personaldata &lt;- subset(personaldata, gender != &quot;woman&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 111 Newton Isaac 1/9/2016 man ## 5 282 Legend John 1/9/2016 man Note how cases with gender equal to woman are no longer in the data frame, while every other case is retained. Lets now filter (subset) by a variable of type numeric/integer. Specifically, lets select those cases in which the id variable is greater than (&gt;) 154. Because the id variable consists of type numeric/integer, we wont use quotation marks (\" \") around the value we wish to filter by, which in this case is 154. As we did above, lets assign the filtered data frame object to an object that well call sub_personaldata; this will overwrite the existing object called sub_personaldata in our R Environment. # Filter (subset) by id sub_personaldata &lt;- subset(personaldata, id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 198 Morales Linda 1/7/2016 woman ## 4 201 Providence Cindy 1/9/2016 woman ## 5 282 Legend John 1/9/2016 man If we wish to filter (subset) by two variables, we can apply the logical or (|) operator or and (&amp;) operator. First, lets select those cases in which either gender is equal to woman or id is greater than 154 using the logical or (|) operator. This will retain those cases for whom at least one logical statement is true. Once again, lets assign the filtered data frame object to an object that well call sub_personaldata; this will overwrite the existing object called sub_personaldata in our R Environment. # Filter (subset) by gender OR id (application of logical OR operator) sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot; | id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 198 Morales Linda 1/7/2016 woman ## 4 201 Providence Cindy 1/9/2016 woman ## 5 282 Legend John 1/9/2016 man Now watch what happens if we apply the logical and (&amp;) operator by keeping everything the same but swapping out the logical or (|) operator with the logical and (&amp;) operator. This will retain only those cases for whom both logical statements are true. # Filter (subset) by gender AND id (application of logical AND operator) sub_personaldata &lt;- subset(personaldata, gender == &quot;woman&quot; &amp; id &gt; 154) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 165 Doe Jane 1/4/2016 woman ## 2 198 Morales Linda 1/7/2016 woman ## 3 201 Providence Cindy 1/9/2016 woman We can also use the logical or (|) operator to select two values of the same variable. In this example, we will select cases for whom either firstname is equal to John or firstname is equal to Jane. # Filter (subset) by two values of firstname using logical OR sub_personaldata &lt;- subset(personaldata, firstname == &quot;John&quot; | firstname == &quot;Jane&quot;) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 3 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 155 Smith John 1/9/2016 man ## 2 165 Doe Jane 1/4/2016 woman ## 3 282 Legend John 1/9/2016 man We can select two ranges of values from the same variable using the logical or (|) operator, assuming the variable is of type numeric, integer, or date. In this example, we will retain the cases for whom either logical statement is true: id is less than or equal to 154 or id is greater than or equal to 198. # Filter (subset) by two ranges of values for id using logical OR sub_personaldata &lt;- subset(personaldata, id &lt;= 154 | id &gt;= 198) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 111 Newton Isaac 1/9/2016 man ## 4 198 Morales Linda 1/7/2016 woman ## 5 201 Providence Cindy 1/9/2016 woman ## 6 282 Legend John 1/9/2016 man Alternatively, can select a single range of values within lower and upper bounds from the same variable by using the logical and (&amp;) operator, assuming the variable is of type numeric, integer, or date. In this example, we will retain the cases for whom both logical statements are true: id is greater than or equal to 154 and id is less than or equal to 198. # Filter (subset) by single range of values for id using logical AND sub_personaldata &lt;- subset(personaldata, id &gt;= 154 &amp; id &lt;= 198) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 4 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 154 McDonald Ronald 1/9/2016 man ## 2 155 Smith John 1/9/2016 man ## 3 165 Doe Jane 1/4/2016 woman ## 4 198 Morales Linda 1/7/2016 woman The subset function can also be used to remove multiple specific cases (such as from a unique identifier variable), which might be useful when youve identified outliers that need to be removed. As a first step, identify a vector of values that need to be removed. In this example, lets pretend that cases with id variable values of 198 and 201 no longer work for this company, so they should be removed from the sample. To create a vector of these two values, use the c function like this: c(198,201). Next, because you are now filtering by a vector, you will need to use the %in% operator, which is an operator that instructs R to go through each value of the filter variable (id) and identify instances of 198 and 201 (c(198,201)); if the values match, then those cases are retained. However, because we entered ! in front of the filter variable, this actually reverses our logic and instructs R to remove those cases in which a value of the filter variable matches a value contained in the vector. # Filter out (subset out) id of 198 and 201 sub_personaldata &lt;- subset(personaldata, !id %in% c(198,201)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 282 Legend John 1/9/2016 man Note that in the output above cases with id variable values equal to 198 and 201 are no longer present. If we remove the ! in front of the filter (subset) variable, only cases 198 and 201 are retained. # Filter in (subset in, select) id of 198 and 201 sub_personaldata &lt;- subset(personaldata, id %in% c(198,201)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 2 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 198 Morales Linda 1/7/2016 woman ## 2 201 Providence Cindy 1/9/2016 woman We can also drop specific cases one by one using the not equal to logical operator (!=) and the a unique identifier value associated with the case you wish to remove. We accomplish the same result as above but use two steps instead. Also, note that in the second step below, the new data frame object (sub_personaldata) is used as the first argument in the subset function because we want to retain the changes we made in the prior step (i.e., dropping case with id equal to 198). # Filter out (subset out) id of 198 sub_personaldata &lt;- subset(personaldata, id != 198) # Filter out (subset out) id of 201 sub_personaldata &lt;- subset(sub_personaldata, id != 201) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 6 x 5 ## id lastname firstname startdate gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 153 Sanchez Alejandro 1/1/2016 man ## 2 154 McDonald Ronald 1/9/2016 man ## 3 155 Smith John 1/9/2016 man ## 4 165 Doe Jane 1/4/2016 woman ## 5 111 Newton Isaac 1/9/2016 man ## 6 282 Legend John 1/9/2016 man When working with variables of type Date, things can get a bit trickier. When we applied the str function from base R (see above), we found that the startdate variable was read in and joined as a character variable as opposed to a date variable. As such, we need to convert the startdate variable using the as.Date function from base R. Begin by typing the name of the data frame object (personaldata), followed by the $ operator and the name of whatever you would like to name the new variable (startdate2); remember, the $ operator tells R that a variable belongs to (or will belong to) a particular data frame. Type the &lt;- assignment operator. Type the name of the as.Date function. As the first argument in the function in the as.Date function, type the name of the as.character function with the name of the data frame object (personaldata), followed by the $ operator and the name the original variable (startdate) as the sole argument within the as.character function. Note that we are nesting the as.character function within the as.Date function, and due to order of operations, the as.character function will be run first, followed by the startdate function. As the second argument in the as.Date function, type format=\"%m/%d/%Y\" to indicate the format for the data variable; note that the capital Y in %Y implies a 4-digit year, whereas a lower case would imply a 2-digit year. # Convert character startdate variable to the Date type startdate2 variable personaldata$startdate2 &lt;- as.Date(as.character(personaldata$startdate), format=&quot;%m/%d/%Y&quot;) To verify that the new startdate2 variable is of type Date, use the str function from base R, and type the name of the data frame object (personaldata) as the sole argument. As you will see, the new startdate2 variable is now of type Date. # Verify that the startdate2 variable is now a variable of type Date str(personaldata) ## spec_tbl_df[,6] [8 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:8] 153 154 155 165 111 198 201 282 ## $ lastname : chr [1:8] &quot;Sanchez&quot; &quot;McDonald&quot; &quot;Smith&quot; &quot;Doe&quot; ... ## $ firstname : chr [1:8] &quot;Alejandro&quot; &quot;Ronald&quot; &quot;John&quot; &quot;Jane&quot; ... ## $ startdate : chr [1:8] &quot;1/1/2016&quot; &quot;1/9/2016&quot; &quot;1/9/2016&quot; &quot;1/4/2016&quot; ... ## $ gender : chr [1:8] &quot;man&quot; &quot;man&quot; &quot;man&quot; &quot;woman&quot; ... ## $ startdate2: Date[1:8], format: &quot;2016-01-01&quot; &quot;2016-01-09&quot; &quot;2016-01-09&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. lastname = col_character(), ## .. firstname = col_character(), ## .. startdate = col_character(), ## .. gender = col_character() ## .. ) Now we are ready to filter (subset) using the new startdate2 variable. When specifying the value of the startdate2 variable by which you wish to filter by, make sure to use the as.Date function once more with the date (formatted as YYYY-MM-DD) in quotation marks (\" \") as the sole argument. Here, I select those cases for whom their startdate2 values are greater than 2016-01-07  or in other words, those cases who started after January 1, 2016. # Filter (subset) by startdate2 sub_personaldata &lt;- subset(personaldata, startdate2 &gt; as.Date(&quot;2016-01-07&quot;)) # Print filtered (subsetted) data frame print(sub_personaldata) ## # A tibble: 5 x 6 ## id lastname firstname startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 154 McDonald Ronald 1/9/2016 man 2016-01-09 ## 2 155 Smith John 1/9/2016 man 2016-01-09 ## 3 111 Newton Isaac 1/9/2016 man 2016-01-09 ## 4 201 Providence Cindy 1/9/2016 woman 2016-01-09 ## 5 282 Legend John 1/9/2016 man 2016-01-09 18.3.4.2 Select Single Variable from Data Frame To display a single variable from a data frame in our Console, within the subset function, we can do the following: Well use the &lt;- assignment operator to name the data frame that we are about to create. For this example, I chose to create a new data frame object called tempdf, which I placed to the left of the &lt;- assignment operator; that being said, you could name the new data frame object whatever you would like  or you could overwrite the existing data frame object. To the right of the &lt;- operator, type the name of subset function from base R. As the first argument in the function, type the name of the data frame object from which we wish to select a single variable (personaldata) As the second argument, type select= followed by the name of a single variable (startdate) we wish to select. # Select only one variable from a data frame (startdate) tempdf &lt;- subset(personaldata, select=startdate) # Print data frame print(tempdf) ## # A tibble: 8 x 1 ## startdate ## &lt;chr&gt; ## 1 1/1/2016 ## 2 1/9/2016 ## 3 1/9/2016 ## 4 1/4/2016 ## 5 1/9/2016 ## 6 1/7/2016 ## 7 1/9/2016 ## 8 1/9/2016 18.3.4.3 Select Multiple Variables from Data Frame If our goal is to select multiple variables from a data frame (and remove all others), we can use the subset function as follows. As we did above, name new data frame object using the &lt;- operator, and we will overwrite the data frame object we created above called tempdf. As the first argument in the subset function, type the name of your original data frame object (mergeddf). As the second argument, type select= followed by a vector of variable name you wish to select/retain. The order in which you enter the variable names will correspond to the order in which they appear in the new data frame object. Use the c (combine) function from base R with each variable name you wish to select as arguments separated by commas. Here we select the lastname, firstname, and gender variables. # Select multiple variables (lastname, firstname, gender) tempdf &lt;- subset(personaldata, select=c(lastname, firstname, gender)) # Print data frame print(tempdf) ## # A tibble: 8 x 3 ## lastname firstname gender ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sanchez Alejandro man ## 2 McDonald Ronald man ## 3 Smith John man ## 4 Doe Jane woman ## 5 Newton Isaac man ## 6 Morales Linda woman ## 7 Providence Cindy woman ## 8 Legend John man 18.3.4.4 Remove Single Variable from Data Frame If you need to remove a single variable from a data frame, you can simply type the minus (-) operator before the name of the variable you wish to remove. Here, we remove the startdate variable. # Remove one variable from a data frame (startdate) tempdf &lt;- subset(personaldata, select=-startdate) # Print data frame print(tempdf) ## # A tibble: 8 x 5 ## id lastname firstname gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 153 Sanchez Alejandro man 2016-01-01 ## 2 154 McDonald Ronald man 2016-01-09 ## 3 155 Smith John man 2016-01-09 ## 4 165 Doe Jane woman 2016-01-04 ## 5 111 Newton Isaac man 2016-01-09 ## 6 198 Morales Linda woman 2016-01-07 ## 7 201 Providence Cindy woman 2016-01-09 ## 8 282 Legend John man 2016-01-09 18.3.4.5 Remove Multiple Variables from Data Frame If you wish to remove multiple variables from a data frame, you can apply the same syntax as you did when selecting multiple variables, except insert a minus (-) operator in from the of the c function. This tells the function to not select those variables. Here, we remove the lastname and firstname variables from the personaldata data frame object and assign the resulting data frame to an object called tempdf. # Remove multiple variables (lastname, firstname) tempdf &lt;- subset(personaldata, select= -c(lastname, firstname)) # Print data frame print(tempdf) ## # A tibble: 8 x 4 ## id startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 153 1/1/2016 man 2016-01-01 ## 2 154 1/9/2016 man 2016-01-09 ## 3 155 1/9/2016 man 2016-01-09 ## 4 165 1/4/2016 woman 2016-01-04 ## 5 111 1/9/2016 man 2016-01-09 ## 6 198 1/7/2016 woman 2016-01-07 ## 7 201 1/9/2016 woman 2016-01-09 ## 8 282 1/9/2016 man 2016-01-09 18.3.5 Filter by Pattern Contained within String In some cases, we may wish to filter cases from a data frame object based on a pattern contained within a string (i.e., text, characters). For example, using the personaldata data frame object we created above, perhaps we would like to select those cases for which their firstname string (i.e., value) contains a capital (J). To do so, we can use the str_detect function from the stringr package within either the subset function from base R or the filter function from the dplyr package. To get started, make sure that you have installed and accessed the stringr package. # Install stringr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;stringr&quot;) # Access stringr package library(stringr) Given that this chapter supplement has focused thus far on the subset function from base R, lets continue to use that function, but please note that you could just as easily use the filter function from the dplyr package. Using the &lt;- operator, well assign the resulting subset data frame to an object that Im calling sub_personaldata. To the right of the &lt;- operator, type the name of the subset function As the first argument in the subset function, type the name of the original data frame object that weve been working with called personaldata. As the second argument, type the name of the str_detect function. As the first argument within the str_detect function, type the name of the variable we wish to filter by, which in this example is firstname; as the second argument and within quotation marks (\" \"), type a pattern you would like to detect within text strings from the firstname variable. For this example, lets detect any text string containing an uppercase J while noting that case sensitivity matters (e.g., J vs. j). In other words, we are filtering the data frame such that we will retain only those cases for which their firstname variable text strings (i.e., values) contain an uppercase J. # Select cases for which firstname variable contains a &quot;J&quot; # Note that case sensitivity matters (e.g., &quot;j&quot; vs. &quot;J&quot;) sub_personaldata &lt;- subset(personaldata, str_detect(firstname, &quot;J&quot;)) Now lets print the new data frame object we created. # Print the data frame object print(sub_personaldata) ## # A tibble: 3 x 6 ## id lastname firstname startdate gender startdate2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 155 Smith John 1/9/2016 man 2016-01-09 ## 2 165 Doe Jane 1/4/2016 woman 2016-01-04 ## 3 282 Legend John 1/9/2016 man 2016-01-09 "],["clean.html", "Chapter 19 Cleaning Data 19.1 Conceptual Overview 19.2 Tutorial", " Chapter 19 Cleaning Data In this chapter, we will learn how to clean data, such as correcting data-entry errors or removing out-of-bounds scores. 19.1 Conceptual Overview Cleaning data is an essential part of the Data Management phase of the HR Analytics Project Life Cycle. When we clean data, broadly speaking, we identify, correct, or remove problematic observations, scores, or variables. More specifically, data cleaning often entails (but is not limited to): Identifying and correcting data-entry errors and inconsistent coding; Evaluating missing data and determining how to handle them; Flagging and correcting out-of-bounds scores for variables; Addressing open-ended and/or other responses from employee surveys; Flagging and potentially removing untrustworthy variables. With categorical (i.e., nominal, ordinal) variables containing text values, sometimes different spellings or formatting (e.g., uppercase, lowercase) are used (mistakenly) to represent the same category. Such issues broadly fall under data-entry errors and inconsistent coding reason for data cleaning. While the human eye can usually discern what the intended category is, many software programs and programming languages will be unable to automatically or correctly determine which text values represent which category. For example, in the table below, the facility variable is categorical and contains text values meant to represent different facilities at this hypothetical organization. Human eyes can quickly pick out that there are two facility locations represented in this table: Beaverton and Portland. With that said, for the R programming language, without direction, the different spellings and different cases (i.e., lowercase vs. uppercase B) for the Beaverton facility (i.e., Beaverton, beaverton, beverton) will be treated as unique categories (i.e., facilities in this context). Often this is the result of data-entry errors and/or the lack of data validation. To clean the Facility variable, we could convert all instances of beaverton and beverton to Beaverton. Data-entry errors and inconsistent coding: In this table, different spelling and letter cases (e.g., uppercase vs. lowercase) appear for what is supposed to be the same facility location: Beaverton. Missing data (i.e., missing scores) for certain observations (i.e., cases) should also be addressed during data cleaning. For the Facility variable in the table shown below, note how facility location data are missing for the employees with IDs EP9746 and EP9952. In this example, we could likely find other employee records or contact the employees (or their supervisors) in question to verify the facility location where these to employees work. Provided we find the facility locations for these two employees, we could then replace the missing values with the correct facility location information. In other instances, it may prove to be more difficult to replace missing data, such as when organization administers an anonymous employee survey and certain respondents have missing responses to certain questions or items. In such instances, we may decide to tolerate a small percentage of missing data (e.g., &lt; 10%) when we go to analyze the data; however, if the percentage of missing data is sufficiently large and if we plan to analyze the data to make inferences about underlying population from which the sample data were attained, we may begin to think more seriously about whether the data are missing completely at random, missing at random, or missing at not at random. A proper discussion of missing data theory is beyond the scope of this chapter. Missing data: In this table, data are missing for the employees with IDs EP9746 and EP9952. In some instances, we might encounter out-of-bounds scores, which refer to values that simply are too high or low, or that are just too unrealistic or implausible to be correct. In the table below, the Base Salary variable includes salaries that are all below 75,000  with the notable exception of salary associated with employee ID EP0214. Lets imagine that this table is only supposed to include data for a specific job category; knowing that, a base salary of 789,120,000 seems excessively high in a global sense and extraordinarily high in a local sense. It could be that someone entered the base salary data incorrectly for this employee, perhaps by adding four extra zeroes at the end of the actual base salary amount. In this case, we would try to find verify the correct base salary for this individual and then make the correction to the data. Out-of-bounds scores: In this table, the base salary for the individual with employee ID EP0214 seems extraordinarily high and is almost certainly a data entry error. If the example above seems a bit far-fetched to you, Ill provide you with a personal example of just a single zero being mistakenly added to the end of a paycheck. After my fourth year of graduate school, I decided to teach a summer course as an adjunct faculty member, which happened to be an introductory human resource management course. During the week in which I was covering employee compensation, I received a paycheck with one extra zero added to my pay for that month, which of course increased my monthly pay 10-fold. As much as I would have enjoyed holding onto that extra money, I quickly reached out to the a representative from the universitys HR department, and the person addressed the error very quickly. At the end of the conversation, the HR representative said jokingly, The irony is not lost on us that we made this payroll error for someone who is teaching a unit on employee compensation. When an open-ended response or other response field is provided (as opposed to a close-ended response field with predetermined response options), the individual who enters the data can type in whatever they would like (in most instances) provided that they limit their response to the allotted space. This challenge crops up frequently in employee surveys, such as when employees may select an other response for one survey question that then branches them to a follow-up question that is open-ended. In the table below, survey respondents close-ended response options for the Number of Direct Reports variable include an Other option; respondents who responded with Other had an opportunity to indicate their number of direct reports using an open-ended survey question associated with the Number of Direct Reports (Other) variable. When cleaning such data, we often must determine what to do with the affect variables on a case-by-case basis. For example, the individual who responded to the survey associated with a unique identifier of 2 responded with Not a supervisor. If this survey was intended to acquire data from supervisors only, then we might decide to remove the row of data associated with that individuals response, as they likely do not fit our definition of the target population. Open-ended and/or other responses: In this table, survey respondents close-ended response options for the Number of Direct Reports variable include an Other option; for respondents who responded with Other, they then had an opportunity to indicate their number of direct reports using an open-ended survey question associated with the Number of Direct Reports (Other) variable. Finally, sometimes scores for a variable (or even missing scores for a variable) seem off, incorrect, or implausible  or in other words, untrustworthy. For example, in the table below, a variable called Training Post Test is meant to include the scores on a post-training assessment; yet, we can see that the individual with employee ID EP1475 has a score of 99 even though the adjacent variable indicates that the individual did not complete the training. Now, its entirely possible that this person (and others) were part of a control group (i.e., comparison group) intended to be used as part of a training evaluation design, but that then begs the question why only one individual in this table has a training post-test score. At first glance, data associated with the Training Post Test variable seem untrustworthy, and they very well may be in reality. As a next step, we would want to do some sleuthing to figure out what errors or issues may be at work in these data, and whether we should remove the potentially untrustworthy variable in question. Untrustworthy variables: In this table, data are missing for all but employee ID EP1475 on the Training Post Test variable, and furthermore, the Completed Training variable indicates that none of the employees who appear in the table received training. 19.2 Tutorial This chapters tutorial demonstrates different techniques for cleaning data in R. 19.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/mGQvJ3FuNa8 19.2.2 Functions &amp; Packages Introduced Function Package View base R count dplyr str base R mutate dplyr replace base R match base R ifelse base R is.na base R toupper base R tolower base R names base R function base R clean_names janitor 19.2.3 Initial Steps If you havent already, save the file called DataCleaningExample.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called DataCleaningExample.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_character(), ## Facility = col_character(), ## JobLevel = col_double(), ## StartDate = col_date(format = &quot;&quot;), ## Org_Tenure_Yrs = col_double(), ## OnboardingCompleted = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;JobLevel&quot; &quot;StartDate&quot; ## [5] &quot;Org_Tenure_Yrs&quot; &quot;OnboardingCompleted&quot; # Print data frame (tibble) objects df ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Note in the data frame that the EmpID field/variable is a unique identifier variable, which means that each case/observation has a unique value on this field/variable. This will become useful later on in this tutorial when we replace values on variables for specific cases. 19.2.4 Review Data There are different tools and techniques we can use to review the cleanliness or integrity of the available data. Often, its a good idea to give the data a once over with what I refer to as the ocular test, which simply means to scan the raw data using your eyes. The ocular test can give you an idea of the types of data cleaning issues youll need to address. The View function from base R is a great tool for this, as it allows you to look at the data frame object in a viewer tab, and using the arrows at the top of each column, you can sort each field (i.e., variable) manually. # View data frame object using View function View(df) Reviewing data using the View function from base R. In addition, applying the count function from the dplyr package (Wickham et al. 2021) can provide us with an understanding of the values (or lack thereof) associated with each variable in your data frame object. The count function groups and tallies the number of observations (i.e., frequencies) by value/level within a variable. Using this function we can (hopefully) identify any values that might be considered out of bounds or erroneous. Because the count function comes from the dplyr package, if you havent already, install and access the dplyr package using the install.packages and library functions, respectively. # Install dplyr package if you haven&#39;t already install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) We can achieve the same output with and without the use of the pipe (%&gt;%) operator because the dplyr package is built upon the magrittr (Bache and Wickham 2020) package. For more information on the pipe operator, check out this link: https://r4ds.had.co.nz/pipes.html. If you dont want to use the pipe operator with the dplyr functions, just skip down below to see how to specify the function without using pipes. Using the pipe (%&gt;%) operator, first, type the name of the data frame object to which a variable belongs (e.g., df). Second, type the %&gt;% operator. Third, type the name of the count function, and within the parentheses, enter the exact name of the variable (e.g., Facility) as the sole argument. # Apply count function to Facility variable using pipe df %&gt;% count(Facility) ## # A tibble: 5 x 2 ## Facility n ## &lt;chr&gt; &lt;int&gt; ## 1 beaverton 1 ## 2 Beaverton 4 ## 3 beverton 1 ## 4 Portland 3 ## 5 &lt;NA&gt; 1 Note that the output yields a table object (or more specifically a tibble, which is used in the tidyverse collection of functions). There are two columns: (a) the variable name, below which appears all values/levels of that variable, and (b) the n column, which displays the number of cases/observations associated with each value/level of the variable in question. As you can see, there appear to be some errors. Perhaps the database used to gather these data lacked data validation rules for certain variables, which allowed users to enter different spellings or formatting of the same value/level of the variables. For example, the Beaverton facility is spelled/formatted in three ways: Beaverton, beaverton, and beverton. Because R is case sensitive, Beaverton and beaverton are treated as to distinct levels/values of the Facility variable. Further, beverton is a misspelling of Beaverton and lacks the capital B. Finally, note that there is one NA value, which indicates that someone forgot to enter the name of the facility where that employee works. Clearly, we need to correct these errors and clean up the data residing within this variable; later in the tutorial, you will learn how to correct/replace these values in the R environment. To perform the operation above without the pipe (%&gt;%) operator,type the name of the count function, and within the parentheses, type the name of the data frame object (df) as the first argument, followed by a comma. As the second argument, type the exact name of the variable (e.g., Facility). # Apply count function to Facility variable without using pipe count(df, Facility) ## # A tibble: 5 x 2 ## Facility n ## &lt;chr&gt; &lt;int&gt; ## 1 beaverton 1 ## 2 Beaverton 4 ## 3 beverton 1 ## 4 Portland 3 ## 5 &lt;NA&gt; 1 As you can see, the output is the same with and without the use of the pipe (%&gt;%) operator. Now lets apply the count function to the JobLevel variable. To save space, I only show how to do this using the pipe operator (%&gt;%). All we need to do is replace Facility with JobLevel in the syntax/code we wrote previously. # Apply count function to JobLevel variable using pipe df %&gt;% count(JobLevel) ## # A tibble: 7 x 2 ## JobLevel n ## &lt;dbl&gt; &lt;int&gt; ## 1 -9999 1 ## 2 1 4 ## 3 2 1 ## 4 3 1 ## 5 4 1 ## 6 5 1 ## 7 11 1 For the sake of this example, lets assume that the company has only five job levels (1 = lowest, 5 = highest). In the output, it is apparent that there are two out-of-bounds values: -9999 and 11. Rather than leave cell blank when entering data, some people prefer to use extreme negative values, such as -9999, to flag missing data, and lets assume that is the case in this example. A potential issue with this approach to coding missing data is that R assumes -9999 is a real value, and in R, NA is often used to indicate missing data to avoid this issue. Regarding the 11 value, it is likely that someone made an error when entering the data value in the database by typing 1 twice by mistake; lets assume that we verified that this was the case and that 11 should be replaced with 1. Lastly, lets apply the count function to the OnboardingCompleted variable. # Apply count function to OnboardingCompleted variable using pipe df %&gt;% count(OnboardingCompleted) ## # A tibble: 1 x 2 ## OnboardingCompleted n ## &lt;chr&gt; &lt;int&gt; ## 1 No 10 In this example, employees either completed (Yes) or did not complete (No) the onboarding for new employees. As you can see in the output, there are no missing data (i.e., all 10 cases have a value); however, note that every single case/observation (i.e., employee) has the value No when in actuality (lets assume) every single case/observation should have the value Yes because we learned that every single employee in this data frame completed the onboarding program. In the next section, you will learn how to clean the dirty data we identified within the Facility, JobLevel, and OnboardingCompleted variables. 19.2.5 Clean Data As is often the case in R (and in life), there are multiple approaches to cleaning data. I demonstrate two approaches in this tutorial: (a) replacing a value for specific cases/observations with the correct value and (b) replacing a specific value for all cases that have the same value on that variable. Both approaches can come in handy given the particular data-cleaning circumstance you are facing. 19.2.5.1 Replace a Specific Value for a Specific Case To begin, lets replace a specific value for a specific case/observation. To do so, well apply three functions: mutate from the dplyr package and replace and match from base R. If you havent already, be sure that you have installed and accessed the dplyr package using the install.packages and library functions, respectively. Using the pipe (%&gt;%) operator, we can replace a specific value for a specific case by: Type the name of a new data frame object, as it is generally a good practice to preserve the original data frame you read in earlier; here, I arbitrarily name the new data frame object newdf1. Type the &lt;- operator to the right of the new data frame object so that the results of the subsequent operations can be assigned to the new object. Type the name of the original data frame object (df), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. Within the mutate function parentheses, provide the name of an existing or new variable; in this case, we will overwrite the existing Facility variable by typing the same variable name. Next, enter the = operator to the right of the mutate function to specify how values for the Facility variable will be determined. Type the name of the replace function, which is a function that allows one to replace values in specific location within a vector or data frame. Within the replace function parentheses, as the first argument, type the name of the variable for which you wish to replace specific values (Facility). As the second argument, type the name of the match function, which is a function that allows us to find matched value(s) within a variable (i.e., vector); s the first argument within the match function, enter the value you wish to find a match for (EP1202), and as the second argument, enter the name of the variable (EmpID) to which that value belongs. Essentially, this match function and its two arguments will instruct R to identify the case in which EmpID (variable name) is equal to EP1202. Because the variable EmpID is of type character (chr), the value identified from that variable should be placed in quotation marks (\" \"); if the variable were of type numeric, you would not include the quotation marks (\" \"), and as shown below, the str (structure) function can be used to identify the variable type for the variables within a data frame object (e.g., df); as the final (third) argument within the replace function, enter the new value (e.g., Beaverton) that you would like to replace the existing value with for the particular case you identified. # Identify variable type str(df) ## spec_tbl_df[,6] [10 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : chr [1:10] &quot;EP1201&quot; &quot;EP1202&quot; &quot;EP1203&quot; &quot;EP1204&quot; ... ## $ Facility : chr [1:10] &quot;Beaverton&quot; &quot;beaverton&quot; &quot;Beaverton&quot; &quot;Portland&quot; ... ## $ JobLevel : num [1:10] 1 1 -9999 5 2 ... ## $ StartDate : Date[1:10], format: &quot;2010-05-05&quot; &quot;2008-01-31&quot; &quot;2017-02-05&quot; ... ## $ Org_Tenure_Yrs : num [1:10] 8.6 10.9 1.9 0.3 0.3 8.8 7.6 8.6 7.7 6.5 ## $ OnboardingCompleted: chr [1:10] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_character(), ## .. Facility = col_character(), ## .. JobLevel = col_double(), ## .. StartDate = col_date(format = &quot;&quot;), ## .. Org_Tenure_Yrs = col_double(), ## .. OnboardingCompleted = col_character() ## .. ) # For EmpID equal to EP1202, replace &quot;beaverton&quot; with &quot;Beaverton&quot; using pipe newdf1 &lt;- df %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1202&quot;, EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No In the new data frame object (newdf1), you should now see that beaverton has been replaced with Beaverton for the case in which EmpID is equal to EP1202. Assuming you save your script as an R script file (.R), you will have a paper trail that shows that you replaced an existing value with a new (correct) value. To apply the mutate function without a pipe (%&gt;%) operator, we simply remove the pipe (%&gt;%) operator and enter the name of our original data frame object (df) as the first argument of the mutate function. # For EmpID equal to EP1202, replace &quot;beaverton&quot; with &quot;Beaverton&quot; without using pipe newdf1 &lt;- mutate(df, Facility = replace(Facility, match(&quot;EP1202&quot;,EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Now lets take the newdf1 data frame object we just created and replace beverton with Beaverton for the case in which the EmpID variable value is equal to EP1207. All we need to do in this instance is to (a) swap out df with newdf1 as the original data frame name and (b) swap out EP1202 with EP1207 from the previous piped code we wrote. # For EmpID equal to EP1207, replace &quot;beverton&quot; with &quot;Beaverton&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1207&quot;, EmpID), &quot;Beaverton&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Lets assume that after looking through other organizational records we found that the employee with EmpID equal to EP1205 works at the Portland facility. Accordingly, we want to replace the NA value for the Facility variable for that employee with Portland. When adapting the previous script/code, all we need to do is replace EP1207 with EP1205 and Beaverton with Portland. # For EmpID equal to EP1205, replace NA with &quot;Portland&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(Facility = replace(Facility, match(&quot;EP1205&quot;, EmpID), &quot;Portland&quot;)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Now that the Facility variable has been cleaned, lets move on to the JobLevel variable. If you recall, in the previous section, we determined that there were two problematic values for two cases: -9999 (associated with EmpID equal to 1203) and 11 (associated with EmpID equal to 1210). We decided that -9999 should be replace with NA because it represents a missing value, and 11 should be replaced with 1 because it represents a data entry error. Check out the script/code below to see how these replacements were handled. Also, note that because the JobLevel variable is of type numeric, we dont put the replacement values of NA and 1 in direct quotations. # For EmpID equal to EP1203, replace -9999 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(JobLevel = replace(JobLevel, match(&quot;EP1203&quot;, EmpID), NA)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No # For EmpID equal to EP1210, replace 11 with 1 using pipe newdf1 &lt;- newdf1 %&gt;% mutate(JobLevel = replace(JobLevel, match(&quot;EP1210&quot;, EmpID), 1)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 Portland 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No Finally, lets assume that for the OnboardingCompleted variable, two of the cases have values that should in fact be NA (missing). Specifically, because the hypothetical onboarding program takes 1.0 year to complete, the two employees (cases) with Org_Tenure_Yrs (organizational tenure in years) equal to 0.3 have not worked in the company long enough to have completed the 1-year-long onboarding program. Consequently, we decide that it is not appropriate to put No or Yes for these two employees, which means treating those values as missing (NA) would be most appropriate. The two employees in question have EmpID variable values of EP1204 and EP1205. Note that we can enter NA as the final argument within the replace function parentheses, just as we would with an actual value. Check out the script/code below to see how we replace 0.3 with NA on the OnboardingCompleted variable for the cases with the EmpID unique identifier variable equal to EP1204 and EP1205. # For EmpID equal to EP1204, replace 0.3 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = replace(OnboardingCompleted, match(&quot;EP1204&quot;, EmpID), NA)) # For EmpID equal to EP1205, replace 0.3 with NA using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = replace(OnboardingCompleted, match(&quot;EP1205&quot;, EmpID), NA)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 19.2.5.2 Replace a Specific Value for All Cases with a Particular Value If you need to systematically clean multiple values for a given variable, there is another approach that is more efficient. Note that this approach is only appropriate if you have identified that the values in question all need to be changed to the same value. For the sake of this example, lets pretend that all of the No values for the OnboardingCompleted variable were entered incorrectly. Instead of No, the value should be Yes for each of these cases. First, you will learn how to replace these values in one fell swoop using the mutate function from the dplyr package and the ifelse function from base R. Second, you will learn how to do the same thing without the use of a specific function. Lets start with the first approach, which involves the mutate and ifelse functions. Using the pipe (%&gt;%) operator, we can do the following: Type the name of a new data frame object; here, I use the same name as the original name to overwrite the existing data frame object called newdf1. Type the &lt;- operator to the right of the new data frame object so that the results of the subsequent operations can be assigned to the new object. Type the name of the original data frame object (newdf1), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. Within the mutate function parentheses, provide the name of an existing or a new variable; in this case, we will overwrite the existing OnboardingCompleted variable by typing the same name as the existing variable. To the right of the mutate function, type the = operator to specify how values for the OnboardingCompleted variable will be determined. Type the name of the ifelse function, which is a function that can be used for conditional value (element) selection (identification) and replacement. As the first argument, type a conditional statement for the variable whose values you wish to replace; because we wish to replace all No values with Yes for the OnboardingCompleted variable, well specify the conditional statement as OnboardingCompleted==\"No\"; for a review of the different logical operators, please refer to the chapter called Filtering Data. As the second argument in the ifelse function, type the replacement value when the aforementioned conditional statement is true for a given value of the OnboardingCompleted variable, which is Yes in this example. Because the OnboardingCompleted variable is of type character, this value needs to be in quotation marks (\" \"). As the final argument function, instruct the ifelse function what the replacement value should be when the aforementioned conditional statement is false, which in this; in this example, we will replace all other values with the existing values of the OnboardingCompleted variable, and thus well enter that variable name as the third argument. # For OnboardingCompleted variable, replace &quot;No&quot; values with &quot;Yes&quot; using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = ifelse(OnboardingCompleted==&quot;No&quot;, &quot;Yes&quot;, OnboardingCompleted)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 Yes ## 2 EP1202 Beaverton 1 2008-01-31 10.9 Yes ## 3 EP1203 Beaverton NA 2017-02-05 1.9 Yes ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 Yes ## 7 EP1207 Beaverton 1 2011-06-01 7.6 Yes ## 8 EP1208 Portland 4 2010-05-15 8.6 Yes ## 9 EP1209 Portland 3 2011-04-29 7.7 Yes ## 10 EP1210 Beaverton 1 2012-07-11 6.5 Yes Now lets pretend that we wish to convert the Yes values for the OnboardingCompleted variable with the numeric value of 1. The format of the code remains virtually the same, except that we change the conditional statement to OnboardingCompleted==\"Yes\" as the first argument in the ifelse function, and in the second argument, we change the replacement value to 1. # For OnboardingCompleted variable, replace &quot;Yes&quot; values with 1 using pipe newdf1 &lt;- newdf1 %&gt;% mutate(OnboardingCompleted = ifelse(OnboardingCompleted==&quot;Yes&quot;, 1, OnboardingCompleted)) # Print new data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 1 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 1 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 1 ## 4 EP1204 Portland 5 2018-09-19 0.3 NA ## 5 EP1205 Portland 2 2018-09-19 0.3 NA ## 6 EP1206 Beaverton 1 2010-03-23 8.8 1 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 1 ## 8 EP1208 Portland 4 2010-05-15 8.6 1 ## 9 EP1209 Portland 3 2011-04-29 7.7 1 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 1 As an alternative approach, we can write the following code. First, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the variable in question (OnboardingCompleted). Second, type brackets ([ ]). Third, within the brackets, enter a conditional statement; for the sake of example, lets say that we want to identify all instances in which the OnboardingCompleted variable is equal to 1 (newdf1$OnboardingCompleted==1). Fourth, type the &lt;- operator, followed by the value you wish to use to replace the existing values for which the conditional statement you previously specified is true; in this example, we enter 2. Because the two values are numeric, we do not use quotation marks (\" \"). # For OnboardingCompleted variable, replace 1 values with 2 newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==1] &lt;- 2 # Print data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 2 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 2 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 2 ## 4 EP1204 Portland 5 2018-09-19 0.3 NA ## 5 EP1205 Portland 2 2018-09-19 0.3 NA ## 6 EP1206 Beaverton 1 2010-03-23 8.8 2 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 2 ## 8 EP1208 Portland 4 2010-05-15 8.6 2 ## 9 EP1209 Portland 3 2011-04-29 7.7 2 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 2 If we wish to change numeric values to character values, be sure to put the character values in quotation marks (\" \"). In the following example, all instances in which the OnboardingCompleted variable is equal to 2 are changed to Yes. # For OnboardingCompleted variable, replace 2 values with &quot;Yes&quot; newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==2] &lt;- &quot;Yes&quot; # Print data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 Yes ## 2 EP1202 Beaverton 1 2008-01-31 10.9 Yes ## 3 EP1203 Beaverton NA 2017-02-05 1.9 Yes ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 Yes ## 7 EP1207 Beaverton 1 2011-06-01 7.6 Yes ## 8 EP1208 Portland 4 2010-05-15 8.6 Yes ## 9 EP1209 Portland 3 2011-04-29 7.7 Yes ## 10 EP1210 Beaverton 1 2012-07-11 6.5 Yes Just for fun, lets change all of the Yes values for the OnboardingCompleted variable back to No using the code below. # For OnboardingCompleted variable, replace &quot;Yes&quot; values with &quot;No&quot; newdf1$OnboardingCompleted[newdf1$OnboardingCompleted==&quot;Yes&quot;] &lt;- &quot;No&quot; # Print data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 &lt;NA&gt; ## 5 EP1205 Portland 2 2018-09-19 0.3 &lt;NA&gt; ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No Finally, imagine we wish to replace the NA values in the OnboardingCompleted variable with the value 2 using this approach. To do so, instead of making our own conditional statement, within the brackets ([ ]), apply the is.na function from base R, and type the name of the data frame, followed by the $ operator and the name of the variable containing the NAs. To the right of the bracket, use the &lt;- operator followed by the value with which you wish to replace the NAs. # For OnboardingCompleted variable, replace NA values with 2 newdf1$OnboardingCompleted[is.na(newdf1$OnboardingCompleted)] &lt;- 2 # Print data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 2 ## 5 EP1205 Portland 2 2018-09-19 0.3 2 ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 19.2.6 Rename Variables In some cases you may wish to rename an existing variable. One of the simplest ways to do this is to create a new variable and then delete the old variable. Lets rename the Org_Tenure_Yrs variable as simply Tenure. First, specify the name of the data frame object (newdf1), followed by the $ operator and the new name of the variable (Tenure). Second, type the &lt;- operator. Third, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the old variable (Org_Tenure_Yrs). # Create new variable called Tenure based on Org_Tenure_Yrs variable newdf1$Tenure &lt;- newdf1$Org_Tenure_Yrs # Print data frame object print(newdf1) ## # A tibble: 10 x 7 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted Tenure ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No 8.6 ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No 10.9 ## 3 EP1203 Beaverton NA 2017-02-05 1.9 No 1.9 ## 4 EP1204 Portland 5 2018-09-19 0.3 2 0.3 ## 5 EP1205 Portland 2 2018-09-19 0.3 2 0.3 ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No 8.8 ## 7 EP1207 Beaverton 1 2011-06-01 7.6 No 7.6 ## 8 EP1208 Portland 4 2010-05-15 8.6 No 8.6 ## 9 EP1209 Portland 3 2011-04-29 7.7 No 7.7 ## 10 EP1210 Beaverton 1 2012-07-11 6.5 No 6.5 Note that there is now a new variable called Tenure and that the old variable called Org_Tenure_Yrs remains. To remove the old variable called Org_Tenure_Yrs, first, specify the name of the data frame object (newdf1), followed by the $ operator and the name of the old variable (Org_Tenure_Yrs). Second, type the &lt;- operator. Third, enter NULL. # Remove Org_Tenure_Yrs variable from data frame newdf1$Org_Tenure_Yrs &lt;- NULL # Print data frame object print(newdf1) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate OnboardingCompleted Tenure ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 EP1201 Beaverton 1 2010-05-05 No 8.6 ## 2 EP1202 Beaverton 1 2008-01-31 No 10.9 ## 3 EP1203 Beaverton NA 2017-02-05 No 1.9 ## 4 EP1204 Portland 5 2018-09-19 2 0.3 ## 5 EP1205 Portland 2 2018-09-19 2 0.3 ## 6 EP1206 Beaverton 1 2010-03-23 No 8.8 ## 7 EP1207 Beaverton 1 2011-06-01 No 7.6 ## 8 EP1208 Portland 4 2010-05-15 No 8.6 ## 9 EP1209 Portland 3 2011-04-29 No 7.7 ## 10 EP1210 Beaverton 1 2012-07-11 No 6.5 As you can see, the Org_Tenure_Yrs variable is now gone. 19.2.7 Other Approaches to Cleaning Data In the following sections, we will consider some other approaches to cleaning data, that under certain conditions, may be more elegant and efficient solutions. 19.2.7.1 Changing the Case of Variable Names In some instances, we may wish to systematically change the case of all variable or column names. There are a few functions that can be quite handy in this regard. Lets revert back the name of the data frame object we initially read in and names: df. # Access readr package library(readr) # Read in set of data as data frame df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) To change the case of variable or column names such that they are all lower case, we can use the tolower function from base R. Lets change the variable names to be all lower case. We will need to use the names function from base R as well to signal that we are referencing the column names as opposed to the values for each variable. First, type the name of the names function with the name of the new data frame object you are creating and naming as the sole argument; here, we will put in the same name of the existing data frame to overwrite it. Second, enter the &lt;- operator to create and name the new data frame object. Third, enter the tolower function, and as the sole argument, include the names function again with the name of the focal data frame object as its sole argument. # Change case of variable names to all lower case names(df) &lt;- tolower(names(df)) # Print data frame object print(df) ## # A tibble: 10 x 6 ## empid facility joblevel startdate org_tenure_yrs onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No We can use the toupper function to change the variable names to all upper case. # Change case of variable names to all upper case names(df) &lt;- toupper(names(df)) # Print data frame object print(df) ## # A tibble: 10 x 6 ## EMPID FACILITY JOBLEVEL STARTDATE ORG_TENURE_YRS ONBOARDINGCOMPLETED ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No If you wanted to capitalize just the first letter in each variable name, we can create our function to do so. Lets call this function firstletterupper and use the function called function from base R to program our own function. In this function, we will define what the function will do. Essentially, we are going to create two strings of text and then concatenate them using the paste function. As the first argument in the paste function and to select a string of text, we will pull just the first letter of each variable name using the substring function; the second and third numeric arguments in this function indicate the range of letters that we will pull out, and because we put 1 and 1, we are saying retain just the first letter in each name. We then enter this substring function as an argument in the toupper function so that we will just capitalize the first letter of each variable. We will then enter the second argument in the paste function, which is another substring function; this time, we will indicate that we simply wish to retain the second letter in each name followed by any remaining letters, and we do so by entering the numeral 2 as the second argument. We then enter this substring function as an argument in the tolower function to make all but the first letter in each variable name lower case. As the final argument in the paste function, we enter sep=\"\" to signify that we do not want any space between these letters when they are concatenated; note that there is no space within the quotation marks. We can now run this script to program our new function called firstletterupper. # Create function to change just first letter of variable name to upper case firstletterupper &lt;- function(x) {paste(toupper(substring(x, 1, 1)), tolower(substring(x, 2)), sep=&quot;&quot;)} With our new DIY function called firstletterupper we can apply it to the variable names using the same approach we did above with the toupper and tolower functions. # Change just first letter of variable name to upper case names(df) &lt;- firstletterupper(names(df)) # Print data frame object print(df) ## # A tibble: 10 x 6 ## Empid Facility Joblevel Startdate Org_tenure_yrs Onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No Alternatively, we could use the clean_names function from the janitor package (Firke 2021). Be sure to install and access the package if you havent already. # Install janitor package if you haven&#39;t already install.packages(&quot;janitor&quot;) # Access janitor package library(janitor) When coupled with the case=upper_camel argument, the clean_names function will capitalize the first letter in variable names, and if there is an underscore within the variable name (_), the function will capitalize the letter that comes immediately after the underscore. # Change just first letter of variable name to upper case df &lt;- clean_names(df, case=&quot;upper_camel&quot;) # Print data frame object print(df) ## # A tibble: 10 x 6 ## Empid Facility Joblevel Startdate OrgTenureYrs Onboardingcompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No There are many other case= arguments that you could use for the clean_name functions to different variations of capitalization. Just access the help menu for the function as shown below. # Access help information for clean_names function ?clean_names() 19.2.7.2 Changing the Case of Character Variable Values In other instances, we may wish to systematically change the case of values for a character variable. As before, lets revert back the name of the data frame object we initially read in and names: df. # Access readr package library(readr) # Read in set of data as data frame df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) To change the case of variable or column names such that they are all lower case, we can use the tolower function from base R. Lets change the variable names to be all lower case. We will need to use the names function from base R as well to signal that we are referencing the column names as opposed to the values for each variable. First, type the name of the names function with the name of the new data frame object you are creating and naming as the sole argument; here, we will put in the same name of the existing data frame to overwrite it. Second, enter the &lt;- operator to create and name the new data frame object. Third, enter the tolower function, and as the sole argument, include the names function again with the name of the focal data frame object as its sole argument. # Change case of Facility variable&#39;s values to all lower case df$Facility &lt;- tolower(df$Facility) # Print data frame object print(df) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 portland 4 2010-05-15 8.6 No ## 9 EP1209 portland 3 2011-04-29 7.7 No ## 10 EP1210 beaverton 11 2012-07-11 6.5 No We can similarly use the toupper function to change a character variables values to all upper case. # Change case of Facility variable&#39;s values to all upper case df$Facility &lt;- toupper(df$Facility) # Print data frame object print(df) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 BEAVERTON 1 2010-05-05 8.6 No ## 2 EP1202 BEAVERTON 1 2008-01-31 10.9 No ## 3 EP1203 BEAVERTON -9999 2017-02-05 1.9 No ## 4 EP1204 PORTLAND 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 BEAVERTON 1 2010-03-23 8.8 No ## 7 EP1207 BEVERTON 1 2011-06-01 7.6 No ## 8 EP1208 PORTLAND 4 2010-05-15 8.6 No ## 9 EP1209 PORTLAND 3 2011-04-29 7.7 No ## 10 EP1210 BEAVERTON 11 2012-07-11 6.5 No If you wanted to capitalize just the first letter in a character variables values, we can create our function to do so. Lets call this function firstletterupper and use the function called function from base R to program our own function. In this function, we will define what the function will do. Essentially, we are going to create two strings of text and then concatenate them using the paste function. As the first argument in the paste function and to select a string of text, we will pull just the first letter of the character variables values using the substring function; the second and third numeric arguments in this function indicate the range of letters that we will pull out, and because we put 1 and 1, we are saying retain just the first letter in each name. We then enter this substring function as an argument in the toupper function so that we will just capitalize the first letter of the variables values. We will then enter the second argument in the paste function, which is another substring function; this time, we will indicate that we simply wish to retain the second letter in each value followed by any remaining letters, and we do so by entering the numeral 2 as the second argument. We then enter this substring function as an argument in the tolower function to make all but the first letter in each variable name lower case. As the final argument in the paste function, we enter sep=\"\" to signify that we do not want any space between these letters when they are concatenated; note that there is no space within the quotation marks. We can now run this script to program our new function called firstletterupper. Note that this is the same function we created above for changing the case of variable names, but we will take it to another level in this instance. Specifically, we are now ready to insert the paste function we just specified into the ifelse function from base R. Specifically, in case there are any missing values (NAs), we want to exclude those when running this paste function. As the first argument in the ifelse function enter !is.na(x) to indicate we want to select those values for that are not NA. As the second argument, we enter what we wrote for the paste function. As the third argument, we address the else part of the ifelse function; meaning, we need to indicate how values that are NA should be treated. By entering the name of the object x as this final argument, we are indicating that the original value (which in this case is actually missing as NA) will be retained. # Create function to change just first letter of character variable&#39;s values to upper case firstletterupper &lt;- function(x) {ifelse(!is.na(x), paste(toupper(substring(x, 1, 1)), tolower(substring(x, 2)), sep=&quot;&quot;), x)} With our new DIY function called firstletterupper we can apply it to the character variable we wish to change (Facility). # Change just first letter of character variable&#39;s values to upper case df$Facility &lt;- firstletterupper(df$Facility) # Print data frame object print(df) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 Beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 Beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No 19.2.8 Summary In this tutorial, using the View function from base R and the count function from the dplyr package, we learned how to review data to identify problematic values or variables. In addition, we learned how to clean data by change values using the mutate function from the dplyr package and the replace, match, and ifelse functions from base R. Next, we learned how to rename variables and remove existing variables. Finally, we learned some alternative approaches to changing the case of character variables and values. "],["manipulate.html", "Chapter 20 Manipulating &amp; Restructuring Data 20.1 Conceptual Overview 20.2 Tutorial", " Chapter 20 Manipulating &amp; Restructuring Data In this chapter, we will learn how to manipulate and restructure data  and more specifically, convert a data frame object from wide to long format and from long to wide format. 20.1 Conceptual Overview Data manipulation and restructuring refers to the process in which data are restructured or formatted in a different manner. When we think about structured data, we often think in terms of wide versus long formats. Wide format data are structured such that there are more columns (i.e., variables) and fewer rows than with long format data. For example, most survey platforms allow you to download the survey responses in a wide format, such that each variable (that corresponds to a survey item/question) will have its own column. Long format data are structured such that that are (sometimes) fewer columns yet more rows than with wide-format data. For example, we can restructure survey data from wide format such that one variable (i.e., column) contains the names of the different survey items/questions, and another variable contains the responses/scores for each item/question. Often our decision to manipulate data into wide versus long formats has to do with the type of analysis or visualization we plan to perform. As such, understanding how to manipulate and restructure your data is an important data-science skill. Data can be manipulated into different structures to accomplish different goals. For example, a dataset in wide format might include employees scores on pre-test and post-test assessments as separate variables (i.e., columns), where one column includes the pre-test scores and one column includes the post-test scores. The same dataset could, alternatively, be manipulated or restructured into long format, such that the time of the test administration becomes a categorical variable and a separate variable contains the scores; In the wide-format dataset, each employee has a single row of data, whereas in the long-format dataset each employee has one row of data for their pre-test and one row of data for their post-test. 20.2 Tutorial This chapters tutorial demonstrates how to restructure data from wide to long format and from long to wide format. 20.2.1 Video Tutorial In the video tutorial below, I demonstrate how to use two data-manipulation functions that have since been deprecated by the tidyr package, which are called gather and spread. In this chapter, I review how to use the new data-manipulation functions from the tidyr package called pivot_longer and pivot_wider, respectively. I personally find the new functions to be more intuitive to use, but perhaps youll disagree. Regardless of which you choose, both will help you manipulate data from wide to long format and from long to wide format. Link to video tutorial: https://youtu.be/5DgEAKLRjhw 20.2.2 Functions &amp; Packages Introduced Function Package pivot_longer tidyr c base R pivot_wider tidyr 20.2.3 Initial Steps If you havent already, save the file called ManipulatingData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called ManipulatingData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object datam &lt;- read_csv(&quot;ManipulatingData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## SurveyID = col_double(), ## JobSatisfaction = col_double(), ## TurnoverIntentions = col_double(), ## OrgCommitment = col_double(), ## JobInvolvement = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(datam) ## [1] &quot;SurveyID&quot; &quot;JobSatisfaction&quot; &quot;TurnoverIntentions&quot; &quot;OrgCommitment&quot; ## [5] &quot;JobInvolvement&quot; # Print data frame (tibble) object print(datam) ## # A tibble: 20 x 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 ## 13 13 22.3 57.2 70.2 30.6 ## 14 14 23.3 52.9 64.9 30.5 ## 15 15 25.9 51 62.2 30.5 ## 16 16 29.5 51 67.3 30.5 ## 17 17 32.8 51 40.6 30.5 ## 18 18 35.4 51.4 74.7 30.5 ## 19 19 40.3 51.4 71.8 50.1 ## 20 20 56.7 56 84.3 69.5 Note in the data frame that the SurveyID variable (i.e., column, field) is a unique identifier variable, which means that each case (i.e., observation) has a unique value on this variable. Each row represents a unique employees composite score on four measures (i.e., JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), where higher scores indicate higher levels of the concept (i.e., construct) being measured. # Determine number of rows in data frame nrow(datam) ## [1] 20 Note how the data frame currently has 20 rows (cases) of data, as shown by the output to the nrow function from base R. 20.2.4 Wide-to-Long Format Data Manipulation The data frame we read in called datam is in wide format, as each substantive variable has its own column. To restructure the data from wide to long format, we will use the pivot_longer function from the tidyr package. Along with readr and dplyr (as well as other useful packages), the tidyr package is part of the tidyverse of packages. Lets begin by installing and accessing the tidyr package so that we can use the pivot_longer function. # Install tidyr package if you haven&#39;t already install.packages(&quot;tidyr&quot;) # Access tidyr package library(tidyr) If you received an error when attempting to access the tidyr package using the library function, you may need to install the following packages using the install.packages function: rlang and glue. Alternatively, you may try installing the entire tidyverse package. Now that weve accessed the tidyr package, I will demonstrate two techniques for applying the pivot_longer function. The first technique uses the pipe operator (%&gt;%). The pipe operator comes from a package called magrittr (Bache and Wickham 2020), on which the tidyr package is partially dependent. In short, a pipe allows a person to more efficiently write code and to improve the readability of the code and overall script. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. For more information on the pipe operator, check out Wickham and Grolemunds (2017) chapter on pipes. The second technique for applying the pivot_longer function takes a more traditional approach in that it involves nested functions being nested parenthetically. If you dont want to learn how to use pipes (or would like to learn how to use them at a later date), feel free to skip to the section below called Without Pipe. 20.2.4.1 With Pipe Using the pipe (%&gt;%) operator technique, lets apply the pivot_longer function to manipulate the datam data frame object from wide format to long format. We can specify the wide-to-long manipulation as follows. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object datam_long. Use the &lt;- operator to assign the new long-form data frame object to the object named datam_long in the step above. Type the name of the original data frame object (datam), followed by the pipe (%&gt;%) operator. Type the name of the pivot_longer function. As the first argument in the pivot_longer function, type cols= followed by the c (combine) function. As the arguments within the c function, list the names of the variables that you wish to pivot from separate variables (wide) to levels or categories of a new variable, effectively stacking them vertically. In this example, lets list the names of the four survey measures: JobSatisfaction, TurnoverIntentions, OrgCommitment, and JobInvolvement. As the second argument in the pivot_longer function, type names_to= followed by what you would like to name the new stacked variable (see previous) created from the four survey measure variables. Lets call the new variable containing the names of the measures the following: \"Measure\". As the third argument in the pivot_longer function, type values_to= followed by what you would like to name the new variable that contains the scores for the four survey variables that are now stacked vertically for each case. Lets call the new variable containing the scores on the four measures the following: \"Score\". # Apply pivot_longer function to restructure data in long format (using pipe) datam_long &lt;- datam %&gt;% pivot_longer(cols=c(JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), names_to=&quot;Measure&quot;, values_to=&quot;Score&quot;) # Print first 12 rows of new data frame head(datam_long, n=12) ## # A tibble: 12 x 3 ## SurveyID Measure Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 JobSatisfaction 55.4 ## 2 1 TurnoverIntentions 97.2 ## 3 1 OrgCommitment 32.3 ## 4 1 JobInvolvement 50.5 ## 5 2 JobSatisfaction 51.5 ## 6 2 TurnoverIntentions 96 ## 7 2 OrgCommitment 53.4 ## 8 2 JobInvolvement 50.3 ## 9 3 JobSatisfaction 46.2 ## 10 3 TurnoverIntentions 94.5 ## 11 3 OrgCommitment 63.9 ## 12 3 JobInvolvement 50.2 As you can see, in the output, each respondent now has four rows of data  one row for each measure and the associated score. The giveaway is that each respondents unique SurveyID value is repeated four times. # Print variable names names(datam_long) ## [1] &quot;SurveyID&quot; &quot;Measure&quot; &quot;Score&quot; Note how there are now just three variables: SurveyID, Measure, and Score. Further, by applying the nrow function to the new data frame (as shown below), we can see that the new data frame is now much longer in terms of the number of rows; specifically, the original datam data frame in wide format has 20 rows, and now the new datam_long data frame in long format has 80 rows. # Print number of rows in data frame nrow(datam_long) ## [1] 80 Now apply the View function from base R to view and scroll through the whole data frame. Note that the unique identifier variable (i.e., SurveyID) repeats multiple times to indicate that the same survey respondent has scores in multiple rows. # View entire data frame View(datam_long) 20.2.4.2 Without Pipe We can also apply the pivot_longer function without using the pipe (%&gt;%) operator. To do so, use the same code as above, except drop the pipe (%&gt;%) operator and type data= followed by the name of the original data frame (datam) as the first argument within the pivot_longer function parentheses. # Apply pivot_longer function to restructure data in long format (without using pipe) datam_long &lt;- pivot_longer(data=datam, cols=c(JobSatisfaction, TurnoverIntentions, OrgCommitment, JobInvolvement), names_to=&quot;Measure&quot;, values_to=&quot;Score&quot;) # Print first 12 rows of new data frame head(datam_long, n=12) ## # A tibble: 12 x 3 ## SurveyID Measure Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 JobSatisfaction 55.4 ## 2 1 TurnoverIntentions 97.2 ## 3 1 OrgCommitment 32.3 ## 4 1 JobInvolvement 50.5 ## 5 2 JobSatisfaction 51.5 ## 6 2 TurnoverIntentions 96 ## 7 2 OrgCommitment 53.4 ## 8 2 JobInvolvement 50.3 ## 9 3 JobSatisfaction 46.2 ## 10 3 TurnoverIntentions 94.5 ## 11 3 OrgCommitment 63.9 ## 12 3 JobInvolvement 50.2 Regardless of whether we use the pipe (%&gt;%) operator, we end up with the same wide-to-long format manipulation. 20.2.5 Long-to-Wide Format Data Manipulation Using the long-format data frame we just created called datam_long, lets manipulate it back to wide format using the pivot_wider function from the tidyr package. As we did above, well do the long-to-wide data manipulation with and without the pipe (%&gt;%) operator. 20.2.5.1 With Pipe Using the pipe (%&gt;%) operator technique, lets apply the pivot_wider function to manipulate the datam_long data frame object from long format back to wide format. We can specify the long-to-wide manipulation as follows. Create a name for a new data frame object to which we will eventually assign a long-format data frame object; here, I name the new data frame object datam_wide. Use the &lt;- operator to assign the new wide-form data frame object to the object named datam_wide in the step above. Type the name of the long-format data frame object (datam_long), followed by the pipe (%&gt;%) operator. Type the name of the pivot_wider function. As the first argument in the pivot_wider function, type names_from= followed by the name of the variable that contains the names of the different survey measures: Measure. The levels or categories of this Measure variable will become the names of separate columns (i.e., variables) when the long-format data frame object is converted to a wide-format data frame object. As the second argument in the pivot_wider function, type values_from= followed by the name of the variable that contains the values (i.e., scores) for each of the survey measures: Score. The values from the Score variable will provide the data for the new survey-measure variables when the data frame object is in wide format. # Apply pivot_wider function to restructure data in wide format (using pipe) datam_wide &lt;- datam_long %&gt;% pivot_wider(names_from=Measure, values_from=Score) # Print first 12 rows of new data frame head(datam_wide, n=12) ## # A tibble: 12 x 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 Now lets print the variable names in the new datam_wide data frame object. # Print variable names names(datam_wide) ## [1] &quot;SurveyID&quot; &quot;JobSatisfaction&quot; &quot;TurnoverIntentions&quot; &quot;OrgCommitment&quot; ## [5] &quot;JobInvolvement&quot; Note how we are now back to the same variables from our original wide-format data frame called datam: SurveyID, JobInvolvement, JobSatisfaction, OrgCommitment, and TurnoverIntentions. Further, by applying the nrow function to the new data frame (as shown below), we can see that the new wide-format data frame is now back to 20 rows. We have come full circle, as the pivot_wider function complements the pivot_longer function. # Print number of rows in data frame nrow(datam_wide) ## [1] 20 20.2.5.2 Without Pipe We can also apply the pivot_wider function without the pipe (%&gt;%) operator. Simply use the same code as above, except drop the pipe (%&gt;%) operator and type data= followed by the name of the long-format data frame object (datam_long) as the first argument within the pivot_wider function parentheses. # Apply pivot_wider function to restructure data in wide format (without using pipe) datam_wide &lt;- pivot_wider(data=datam_long, names_from=Measure, values_from=Score) # Print first 12 rows of new data frame head(datam_wide, n=12) ## # A tibble: 12 x 5 ## SurveyID JobSatisfaction TurnoverIntentions OrgCommitment JobInvolvement ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 55.4 97.2 32.3 50.5 ## 2 2 51.5 96 53.4 50.3 ## 3 3 46.2 94.5 63.9 50.2 ## 4 4 42.8 91.4 70.3 50.3 ## 5 5 40.8 88.3 34.1 50.5 ## 6 6 38.7 84.9 67.7 30.5 ## 7 7 35.6 79.9 53.3 30.5 ## 8 8 33.1 77.6 63.5 30.5 ## 9 9 29 74.5 68 30.5 ## 10 10 26.2 71.4 67.4 30.5 ## 11 11 23.1 66.4 15.6 30.5 ## 12 12 22.3 61.8 71.8 30.5 Regardless of whether we use the pipe (%&gt;%) operator, we end up with the same long-to-wide format manipulation. 20.2.6 Summary In this chapter, we learned how to manipulate a data frame from wide format to long format using the pivot_longer, and how to manipulate a data frame from long format to wide format using the pivot_wider. Both functions are from the tidyr package. Manipulating and restructuring data into different formats or structures is often an essential data-management step prior to running certain analyses or generating certain data visualizations. "],["center.html", "Chapter 21 Centering &amp; Standardizing Variables 21.1 Conceptual Overview 21.2 Tutorial", " Chapter 21 Centering &amp; Standardizing Variables In this chapter, we will learn how to center and standardize variables. 21.1 Conceptual Overview Centering or standardizing variables can be a useful data preparation step. For example, we often center predictor variables prior to specifying a product term (i.e., interaction term) when estimating moderation effects in a multiple linear regression model. 21.1.1 Review of Centering Variables Centering is the process of subtracting the variable mean (average) from each of the values of that same variable; in other words, its a linear rescaling of a variable. Centering variables is sometimes completed prior to including those variables as predictors in a regression model, and it is generally done for one or both of the following purposes: (a) to make the intercept valuable more interpretable, and (b) to reduce collinearity between two or more predictor variables that are subsequently multiplied to create an interaction term (product term) when estimating a moderated multiple linear regression model or polynomial regression model, for example. Regarding the first purpose, centering to enhance the interpretability of the intercept (constant) value in a regression model is relevant to the extent that we wish to interpret the intercept value. In an ordinary least squares (OLS) regression model, the intercept value represents the mean of the outcome variable when all predictor variables are set to zero. If the scaling of any of the predictor variables in our model does not include zero (e.g., predictor variables are on a 1-10 scale), then considering the intercept value when the predictors are zero, doesnt make much sense. Regarding the second purpose, it is important to center predictor variables prior to using them to create a multiplicative term (i.e., interaction variable), such as an interaction term (i.e., product term) or a polynomial term (e.g., quadratic term, cubic term). In the context of a moderated multiple linear regression model, centering does not affect the significance of the interaction term, but a lack of centering will affect the interpretation of the main effects. Thus far, I have been mostly referring to what is referred to as grand-mean centering. To grand-mean center a variable, we simply subtract the overall (grand) mean of the entire sample for that variable from each value of that variable, thereby creating a new variable in which the mean is zero and the standard deviation is the same as it was before centering. For example, lets assume that we have a variable called Age for a sample of individuals, where Age is measured in years. In its raw format, the mean of Age is 36.2 years with a standard deviation of 8.1. If we grand-mean center Age, then for each individual in our sample, we create a new variable in which take their Age and subtract the mean Age of 36.2. If an individual has an Age of 40.0, then their centered Age would be 3.8 (40.0 - 36.2 = 3.8). By centering each individuals Age relative to the grand-mean, we end up with a variable that has a mean of 0.0, but with a standard deviation that is equal to 8.1, which is equal to the standard deviation of the original Age variable. Why is this the case? Well, we have just performed a linear shift of Age, which affects only the mean and not the standard deviation. In the context of multilevel models, group-mean centering becomes relevant and important. In short, group-mean centering refers to the process of subtracting the respective group mean for a particular variable (based on another variable that acts as a grouping or clustering variable) for each cases score on that same variable. For example, if Employee A belongs to Work Team A, and Work Team A consists of 10 other employees, we would first calculate the mean of Work Team A employees scores on a continuous variable of interest, and second, we would subtract that group mean from each Work Team A employees score on that continuous variable. We would then repeat this process for all employees relative to their respective work teams. In the context of multilevel modeling, both grand-mean centering and group-mean centering can have pronounced on the estimated coefficients and the interpretation of those coefficients. For instance, group-mean centering can be used in multilevel models to separate out the within-group effects and the between-groups effects, if that is of interest. A full discussion of grand-mean centering and group-mean centering in the context of multilevel modeling is beyond the scope of this tutorial; for a more complete overview, please check out Professor Jason Newsoms handout: http://web.pdx.edu/~newsomj/mlrclass/ho_centering.pdf. 21.1.2 Review of Standardizing Variables Like centering variables, when standardizing (or scaling) variables, we center the variables around a mean of zero. However, when standardizing a variable, we are actually converting the variable to a z-score, which means we set the mean to 0 and the variance to 1; because the standard deviation is just the square root of the variance, then the standard deviation is also set to 1. So how do you interpret a variable that is standardized? Lets assume that we standardized a variable called Age for a sample of individuals, where Age is measured in years. In its raw format, the mean of Age is 36.2 years with a standard deviation of 8.1, which would mean, for example, that a person who is 44.* years old has an Age that is exactly 1 standard deviation higher than the mean (44.3 - 8.1 = 36.2). If we standardize the Age variable, then the mean becomes 0.0 and the standard deviation becomes 1.0. Accordingly, the standardized score for the person who has an Age of 44.3 years (which was 1 standard deviation above the mean) would become 1.0. If a person has an Age of 20.0, then that means they have a standardized score of -2.0, which represents 2.0 standard deviations below the mean (20.0 - 36.2 = -16.2 and -16.2 / 8.1 = -2.0). 21.2 Tutorial This chapters tutorial demonstrates how to center and standardize variables in R. 21.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Please note that the video shows how to grand-mean center and standardize variables  but not how to group-mean center variables. If your goal is to group-mean center variables, then check out the corresponding section below. Link to video tutorial: https://youtu.be/2_TxnvZGtV0 21.2.2 Functions &amp; Packages Introduced Function Package mean base R scale base R mutate dplyr 21.2.3 Initial Steps If you havent already, save the file called DiffPred.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called DiffPred.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DiffPred.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## emp_id = col_character(), ## perf_eval = col_double(), ## interview = col_double(), ## age = col_double(), ## gender = col_character(), ## race = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;age&quot; &quot;gender&quot; &quot;race&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,6] [377 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:377] &quot;MA322&quot; &quot;MA323&quot; &quot;MA324&quot; &quot;MA325&quot; ... ## $ perf_eval: num [1:377] 4.2 4.9 4.2 5.3 4.2 6.9 3.4 5.8 4.4 5.6 ... ## $ interview: num [1:377] 7.5 9.3 7.5 8 9.3 6.8 6.7 7 8.2 6.4 ... ## $ age : num [1:377] 29.7 31.7 29.4 37.9 30.9 46.2 43.9 47.8 31.7 44.6 ... ## $ gender : chr [1:377] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; ... ## $ race : chr [1:377] &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. age = col_double(), ## .. gender = col_character(), ## .. race = col_character() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 6 ## emp_id perf_eval interview age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 MA322 4.2 7.5 29.7 woman asian ## 2 MA323 4.9 9.3 31.7 man asian ## 3 MA324 4.2 7.5 29.4 woman asian ## 4 MA325 5.3 8 37.9 woman asian ## 5 MA326 4.2 9.3 30.9 man black ## 6 MA327 6.9 6.8 46.2 woman asian There are 6 variables and 377 cases (i.e., employees) in the DiffPred data frame: emp_id, perf_eval, interview, age, gender, and race. Per the output of the str (structure) function above, the variables perf_eval, interview, and age are of type numeric (continuous: interval/ratio), and the variables emp_id, gender, and race are of type character (nominal/categorical). The variable emp_id is the unique employee identifier. Imagine that these data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered a rated structured interview (interview) 90 days after entering the organization. The structured interview (interview) variable was designed to assess individuals interpersonal skills, and ratings can range from 1 (very weak interpersonal skills) to 10 (very strong interpersonal skills). The interviews were scored by untrained raters who were often the hiring managers but not always. The perf_eval variable is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, with possible ratings ranging from 1-7, with 7 indicating high performance. The age variable represents the job incumbents ages (in years). The gender variable represents the job incumbents tender identify and is defined by two levels/categories/values: man and woman. Finally, the race variable represents the job incumbents race/ethnicity and is defined by three levels/categories/values: asian, black, and white. 21.2.4 Grand-Mean Center Variables We only center variables that are of type numeric and that we conceptualize as having a continuous (interval/ratio) measurement scale. Further, if were centering variables prior to inclusion in a regression model, we often only center those variables that we plan on using as predictor variables (and not outcome variables). Thus, in our current data frame, we will grand-mean center just the interview and age variables; for more information on which variables to center, check out the chapter on estimating moderation effects in a multiple linear regression model. I will demonstrate three approaches, and you can try all three or just one, as any of the three will work. 21.2.4.1 Option 1: Basic Arithmetic and mean Function from Base R Well start with what is arguably the most intuitive approach for grand-mean centering. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variables names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each cases value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction symbol (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. # Grand-mean centering: Using basic arithmetic and the mean function from base R df$c_interview &lt;- df$interview - mean(df$interview, na.rm=TRUE) To admire your work, take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 7 ## emp_id perf_eval interview age gender race c_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 21.2.4.2 Option 2: scale Function from Base R An alternative approach to grand-mean centering is to use the scale function from base R. For some, this function might be preferable to the approach described above, but again, its really a matter of preference. As an initial step, start by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. As I mentioned above, I typically like to name the centered variable by simply adding the c_ prefix to the existing variables names - for example: c_interview. Now, enter the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, begin by typing the name of the scale function. As the first argument, type the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). As the second argument, type center=TRUE which instructs the function to grand-mean center the values. As the third argument, type scale=FALSE to inform the function that you do not which to scale or standardize the variable you are centering. # Grand-mean centering: Using scale function from base R df$c_interview &lt;- scale(df$interview, center=TRUE, scale=FALSE) Take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 7 ## emp_id perf_eval interview age gender race c_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 21.2.4.3 Option 3: mutate Function from dplyr and mean Function from Base R This third approach to grand-mean centering variables can come in handy when we want to grand-mean center multiple variables in a single step. With that said, I will begin by showing how to grand-mean center a single variable using this approach, and then we will extend the code/script to involve two variables. We will be using the mutate function from the dplyr package, so if you havent already, be sure to install and access the dplyr package using the functions below. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) In this example, I use the pipe (%&gt;%) operator from the dplyr package (and by extension, the magrittr package). For more information on using pipes with the mutate function, check out the chapter on cleaning data, and for more information on using pipes in general, check out this free eBook by one of the creators of the dplyr package and RStudio. To begin, type the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- assignment operator. Type the name of the original data frame to which the variable we wish to grand-mean center belongs (df), followed by the pipe (%&gt;%) operator. Type the name of the mutate function. As the first and only argument, begin by typing what you would like to name the new grand-mean centered variable (c_interview). After that, type the = operator to assign values to this new variable. Finally, we will specify a formula to inform the function how the new values will be calculated. Specifically, we type the name of the original variable (interview), type the subtraction (-) operator, and finally type the name of the mean function from base R. As the first argument in the mean function, type the name of the variable (interview) for which you would like to calculate the mean, and as the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when computing this grand mean for the sample. # Grand-mean centering: Using mutate function from dplyr and mean function from base R df &lt;- df %&gt;% mutate(c_interview = interview - mean(interview, na.rm=TRUE)) Take a look at the first six rows of your data frame object to inspect the new grand-mean centered variable called c_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 7 ## emp_id perf_eval interview age gender race c_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 ## 4 MA325 5.3 8 37.9 woman asian 0.745 ## 5 MA326 4.2 9.3 30.9 man black 2.04 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 One of the advantages of using this approach is that we can center multiple variables in a single step. To do so, we simply specify which additional variable we would like to grand-mean center by adding an additional argument to the mutate function. # Grand-mean centering: Multiple variables df &lt;- df %&gt;% mutate(c_interview = interview - mean(interview, na.rm=TRUE), c_age = age - mean(age, na.rm=TRUE)) 21.2.5 Group-Mean Center Variables When estimating multilevel models, there are certain contexts in which group-mean centering should be applied. For more information on centering in general and group-mean centering specifically, please check out this handout created by my colleague Jason Newsom. Like we did above with Option 3 for grand-mean centering, for group-mean centering we will also use the mutate function from dplyr; however, we will also go a step further by applying the group_by function from dplyr to group the data by a values of a categorical (nominal, ordinal) grouping variable. For more information grouping data, check out the chapter on aggregation and segmentation. Lets group-mean center interview scores by race variable categories. To begin, type the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- assignment operator. Type the name of the original data frame to which the variable we wish to grand-mean center belongs (df), followed by the pipe (%&gt;%) operator. Type the name of the group_by function, and as the functions argument(s), specify the name(s) of the grouping variable(s); in this example, we will group by the race variable. Follow this function with the the pipe (%&gt;%) operator. Type the name of the mutate function. As the first and only argument, begin by typing what you would like to name the new group-mean centered variable (gpmc_interview). After that, type the = operator to assign values to this new variable. Finally, we will specify a formula to inform the function how the new values will be calculated. Specifically, we type the name of the original variable (interview), type the subtraction (-) operator, and finally type the name of the mean function from base R. As the first argument in the mean function, type the name of the variable (interview) for which you would like to calculate the mean, and as the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when computing this grand mean for the sample. Follow the mutate function with the pipe (%&gt;%) operator. Finally, type the name of the ungroup function, and dont specify any arguments within the functions parentheses. This last step makes sure that we ungroup the grouping that we initially applied to the data frame. # Group-mean centering by race variable df &lt;- df %&gt;% group_by(race) %&gt;% mutate(gpmc_interview = interview - mean(interview, na.rm=TRUE)) %&gt;% ungroup() Take a look at the first six rows of your data frame object to inspect the new group-mean centered variable called gpmc_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 9 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 21.2.6 Standardize Variables There are different approaches we can use to standardize a variable. I provide two options below. You can try both options or one. Both will get you to the same end. I suggest picking the one that is most intuitive for you. 21.2.6.1 Option 1: scale Function from Base R As our first approach to standardizing (or scaling) a variable, we will use the scale function from base R. In fact, we can even apply this function within the lm (linear regression) function from base R to get standardized regression coefficients; for more information on standardized regression coefficients, check out the chapters on predicting criterion scores using simple linear regression and estimating incremental validity using multiple linear regression. Using the scale function on its own is fairly straightforward when the goal is to standardize. Start by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the standardized variable by simply adding the st_ prefix to the existing variables names (e.g., st_interview). Now, enter the name of the data frame object to which the new centered variable will be attached (df), followed by the $ operator and the name of the new variable we are creating (st_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To assign values to this new st_interview variable, begin by typing the name of the scale function. As the first and only argument, type the name of the data frame object (df), followed by the $ operator and the name of the original variable (interview). # Standardizing: Using scale function from base R df$st_interview &lt;- scale(df$interview) Take a look at your data frame object to inspect the new standardized variable called st_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 10 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview st_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 0.203 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 1.70 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 0.203 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 0.618 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 1.70 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 -0.378 21.2.6.2 Option 2: mutate Function from dplyr and scale Function from Base R This alternative approach to standardizing variables can come in handy when we want to standardize multiple variables in a single step. With that said, I will begin by showing how to standardize a single variable using this approach, and then we will try two variables. We will use the mutate function from the dplyr package, so if you havent already, be sure to install and access the dplyr package using the functions below. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) As the first step, enter the name of the data frame object you wish to create (or overwrite), and in this example, we are going to overwrite our existing data frame (df) by naming our new data frame object the same thing. To create and name the object, we use the &lt;- operator. Second, type the name of the original data frame to which the variable we wish to standardize belongs (df). Third, type the pipe (%&gt;%) operator. Fourth, type the name of the mutate function. As the first and only argument, begin by typing what you would like to call the new standardized variable, which in this case we will call the variable st_interview. After that, type the name of the scale function from base R. As the first and only argument in the scale function, type the name of the variable (interview) that you would like to standardize. # Standardizing: Using mutate function from dplyr and scale function from base R df &lt;- df %&gt;% mutate(st_interview = scale(interview)) Take a look at your data frame object to inspect the new standardized variable called st_interview. # Print first 6 rows of data frame head(df) ## # A tibble: 6 x 10 ## emp_id perf_eval interview age gender race c_interview c_age gpmc_interview st_interview[,1] ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA322 4.2 7.5 29.7 woman asian 0.245 -7.04 0.730 0.203 ## 2 MA323 4.9 9.3 31.7 man asian 2.04 -5.04 2.53 1.70 ## 3 MA324 4.2 7.5 29.4 woman asian 0.245 -7.34 0.730 0.203 ## 4 MA325 5.3 8 37.9 woman asian 0.745 1.16 1.23 0.618 ## 5 MA326 4.2 9.3 30.9 man black 2.04 -5.84 0.877 1.70 ## 6 MA327 6.9 6.8 46.2 woman asian -0.455 9.46 0.0305 -0.378 One of the advantages of using this approach is that we can standardize multiple variables in a single step. To do so, we simply specify which additional variable we would like to standardize by adding an additional argument to the mutate function. # Standardizing: Multiple variables df &lt;- df %&gt;% mutate(st_interview = scale(interview), st_age = scale(age)) 21.2.7 Summary In this chapter, we learned how to grand-mean center and standardize variables. For grand-mean centering variables, we used basic arithmetic and the mean function from base R, the scale function from base R, and a combination of the mutate function from dplyr and the mean function from base R. For standardizing variables, we used the scale function from base R and a combination of the mutate function from dplyr and the scale function from base R. "],["removeobjects.html", "Chapter 22 Removing Objects from the R Environment 22.1 Conceptual Overview 22.2 Tutorial", " Chapter 22 Removing Objects from the R Environment In this chapter, we will learn how to remove objects that weve created from the R Environment. 22.1 Conceptual Overview When working in R, we often assign vectors, matrices, tables, data frames, models, and values to objects so that we can reference them in subsequent operations. Such objects end up in our R (Global) Environment. If youre working in RStudio, you will see your objects in the Environment window, which is typically positioned in the upper right corner of the user interface. At times, we may wish to remove certain objects from our R Environment, which will be the focus of this chapters tutorial. 22.2 Tutorial This chapters tutorial demonstrates how to remove objects from the R Environment. 22.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/mUjSELUW-Ys 22.2.2 Functions &amp; Packages Introduced Function Package ls base R remove base R rm base R c base R 22.2.3 Initial Steps If you havent already, save the file called DataCleaningExample.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called DataCleaningExample.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DataCleaningExample.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_character(), ## Facility = col_character(), ## JobLevel = col_double(), ## StartDate = col_date(format = &quot;&quot;), ## Org_Tenure_Yrs = col_double(), ## OnboardingCompleted = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;JobLevel&quot; &quot;StartDate&quot; ## [5] &quot;Org_Tenure_Yrs&quot; &quot;OnboardingCompleted&quot; # Print data frame (tibble) object print(df) ## # A tibble: 10 x 6 ## EmpID Facility JobLevel StartDate Org_Tenure_Yrs OnboardingCompleted ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 EP1201 Beaverton 1 2010-05-05 8.6 No ## 2 EP1202 beaverton 1 2008-01-31 10.9 No ## 3 EP1203 Beaverton -9999 2017-02-05 1.9 No ## 4 EP1204 Portland 5 2018-09-19 0.3 No ## 5 EP1205 &lt;NA&gt; 2 2018-09-19 0.3 No ## 6 EP1206 Beaverton 1 2010-03-23 8.8 No ## 7 EP1207 beverton 1 2011-06-01 7.6 No ## 8 EP1208 Portland 4 2010-05-15 8.6 No ## 9 EP1209 Portland 3 2011-04-29 7.7 No ## 10 EP1210 Beaverton 11 2012-07-11 6.5 No To demonstrate how to remove objects from the R Environment, we first need to add some more objects to our R Environment. Using the xtabs (cross-tabulation) function from base R, lets create a two-way table called table1 from the JobLevel and Org_Tenure_Yrs variables from the df1 data frame object we just created and named. # Create table from JobLevel and Org_Tenure_Yrs variables from df data frame table1 &lt;- xtabs(~ JobLevel + Org_Tenure_Yrs, data=df) # Print table object print(table1) ## Org_Tenure_Yrs ## JobLevel 0.3 1.9 6.5 7.6 7.7 8.6 8.8 10.9 ## -9999 0 1 0 0 0 0 0 0 ## 1 0 0 0 1 0 1 1 1 ## 2 1 0 0 0 0 0 0 0 ## 3 0 0 0 0 1 0 0 0 ## 4 0 0 0 0 0 1 0 0 ## 5 1 0 0 0 0 0 0 0 ## 11 0 0 1 0 0 0 0 0 You should now see an object called table1 in your R Environment. Next, lets assign an arbitrary value to an object. Specifically, lets assign the value 3 to an object called a. # Assign the value 3 to the object a a &lt;- 3 # Print table object print(a) ## [1] 3 Finally, for good measure, lets add one more object called B to our R Environment. # Assign the character value &quot;example&#39; to the object B B &lt;- &quot;example&quot; # Print table object print(B) ## [1] &quot;example&quot; At the very least, we should now have the following objects in our R Environment: df, table1, a, and B. 22.2.4 List Objects in R Environment In addition to viewing objects in our R (Global) Environment in the Environment window in RStudio, we can also use the ls function from base R to retrieve the names of objects that are currently in our Environment. The function is simple to use. Simply, type the name of the function (ls) without any parenthetical arguments. # Print names of objects in R Environment ls() ## [1] &quot;a&quot; &quot;B&quot; &quot;df&quot; &quot;table1&quot; In your R console, a list of the objects in your global environment should have printed to your Console. Next, we will learn how to remove some of these objects. 22.2.5 Remove Objects from R Environment To remove specific objects from our R Environment, we can use the remove function from base R, which can be abbreviated as rm. To remove a specific object, just type the name of the function (remove), and as the sole parenthetical argument, type the name of the object you wish to remove. In this example, lets remove the data frame object called df. By default, this function first searches the R Environment that is currently active. # Remove df object from R Environment remove(df) Using the ls function, print the names of the objects that are currently in your R Environment. The df object should no longer be there. # Print names of objects in R Environment ls() ## [1] &quot;a&quot; &quot;B&quot; &quot;table1&quot; If our goal is to remove multiple objects from the R Environment, we could apply the remove function multiple times  one for each object we wish to remove. Alternatively, we could also use the list= argument from the remove function to specify a vector of objects we wish to remove. Using the c function from base R, which combines objects into a vector, we can specify a list of the objects we wish to remove. Lets remove the objects called table1 and a from the R Environment. Make sure to put the variable names in quotation marks (\" \") when you list them as arguments within the c function. # Remove multiple objects from R Environment remove(list=c(&quot;table1&quot;,&quot;a&quot;)) Using the ls function, print the names of the objects that are currently in your R Environment. The table1 and a objects should no longer appear in the R Environment. # Print names of objects in R Environment ls() ## [1] &quot;B&quot; Finally, if you wish to remove all objects from the environment, use the remove function, and within the function parentheses, set the list argument equal to ls(). # Remove all objects from R Environment remove(list=ls()) If you use the ls function once more without arguments, you will receive the following output, which indicates that there are no objects in the environment: character(0). # Print names of objects in R Environment ls() ## character(0) 22.2.6 Summary In this chapter, we practiced applying the ls and remove functions from base R to print the names of objects in the R Environment and to remove objects from the R Environment, respectively. "],["employeedemographics.html", "Chapter 23 Introduction to Employee Demographics", " Chapter 23 Introduction to Employee Demographics XXXXX "],["descriptives.html", "Chapter 24 Describing Employee Demographics Using Descriptive Statistics 24.1 Conceptual Overview 24.2 Tutorial 24.3 Chapter Supplement", " Chapter 24 Describing Employee Demographics Using Descriptive Statistics In this chapter, we will learn about how descriptive statistics can be used to describe employee-demographic variables. To determine which type of descriptive statistics is appropriate for a given variable, we will learn about measurement scales and how to distinguish from a construct and a measure. Finally, well conclude with a tutorial. 24.1 Conceptual Overview In this section, well review the four different types of measurement scales (i.e., nominal, ordinal, interval, ratio), the distinctions between constructs, measures, and measurement scales, and different types of descriptive statistics (e.g., counts, measures of central tendency, measures of dispersion). 24.1.1 Review of Measurement Scales When determining what type of descriptive statistics is appropriate for summarizing data contained within a particular variable, it is important to determine the measurement scale of the variable. Measurement scale (i.e., scale of measurement, level of measurement) refers to the type of information contained within a vector of data (e.g., variable), and the four measurement scales are: nominal, ordinal, interval, and ratio. 24.1.1.1 Nominal Variables with a nominal measurement scale have different category labels, which are sometimes referred to as levels. The category labels, however, do not have any inherent numeric properties. As an example, lets operationalize gender identity as having a nominal measurement scale, such that gender identity includes the following category labels: agender, man, nonbinary, trans man, trans woman, and woman. These category labels do not have any inherent numeric values, and although we could assign numeric values to the gender identity category labels (e.g., agender = 1, man = 2, nonbinary = 3, etc.), doing so wouldnt imply that one category label has a higher value than another. Variables with a nominal measurement scale are sometimes referred to as categorical variables. The Facility and Gender variables (i.e., columns) contain examples of nominal measurement scales, as each variable has category labels that lack any inherent numeric values and cannot be ordered in a meaningful way. 24.1.1.2 Ordinal Like variables with a nominal measurement scale, variables with an ordinal measurement scale are a specific type of categorical variable; however, unlike nominal variables, the category labels (i.e., levels) associated with ordinal variables can be ordered or ranked in a meaningful way. It should be noted that the gaps  or intervals  between categorical labels of an ordinal variable are unknown; in other words, we cant quantify the exact difference between adjacent category labels (i.e., levels). For example, lets operationalize employee education levels with the following ordered category labels: high school diploma, some college, and college degree. That is, completing some portion of a college degree is a higher level of education than earning a high school diploma, and completing a college degree is a higher level of education than completing some portion of a college degree. We dont know, though, the size of the interval between earning a high school diploma and completing some college, and between some completing some college and earning a college degree; thus, as operationalized in this example, employee education level demonstrates an ordinal measurement scale (as opposed to an interval measurement scale, which is described in the following section). A controversial example of an ordinal measurement scale is any type of Likert (or Likert-type) scale or response format. Examples of Likert scales include agreement response formats (e.g., Strongly Disagree, Disagree, Neither Disagree Nor Agree, Agree, Strongly Agree) and frequency response formats (e.g., Never, Rarely, Sometimes, Always). Likert scales are commonly used in employee surveys; for example, survey respondents might be asked to indicate their level of agreement with the following survey item that is designed to assess job satisfaction: In general, I am satisfied with my job. Just like any variable with an ordinal measurement scale, we dont know the exact quantitative intervals between adjacent category labels (i.e., response options) on a Likert scale. Nonetheless, in the social sciences, it is relatively common for analysts to apply numerical values to the ordered category labels on a Likert scale (e.g., 1 = Strongly Disagree, 2 = Disagree, 3 = Neither Disagree Nor Agree, 4 = Agree, 5 = Strongly Agree). After adding these numerical values, the analysts often treat Likert scales as though they were interval measurement scales for the purposes of data analysis, particularly when composite variables (i.e., overall scale score variables) are created by summing or averaging respondents scores across multiple survey items. The Education and Performance variables (i.e., columns) contain examples of ordinal measurement scales, as each variable has category labels can be ordered in a meaningful way but where the exact quantitative intervals between category labels are unknown or undefined. 24.1.1.3 Interval Variables with an interval measurement scale have a numeric scale (e.g., have inherent numeric values), and not only is there an order to the numeric values, equally sized intervals between values have the same meaning or interpretation  hence, the term interval measurement scale. With all that being said, interval variables lack a true or meaningful zero value; in other words, a value of zero is an arbitrary point on the scale  if it even appears in the possible range of values in the first place. Variables with an interval measurement scale are sometimes referred to as continuous variables. As an example, suppose we purchase a cognitive ability (i.e., intelligence) test that we plan to administer to job applicants. Lets now imagine that this test operationalizes cognitive ability, such that scores can range from 0 to 200, where 100 indicates the average level of cognitive ability in the population. Further, the test is designed such that every 1-point interval holds the same interpretation and is of equal quantitative size when compared to other 1-point intervals on the scale. For instance, lets imagine that the 1-point interval between 78 and 79 has the same meaning (and quantitative size) as the 1-point interval between 110 and 111. In other words, equally sized intervals between values have the same meaning or interpretation in terms of incremental differences in cognitive ability. Even though this cognitive ability test can produce a score of zero, the zero value is not meaningful, as it does not imply the absence of cognitive ability; rather, it just indicates the lowest point on the numeric scale used to assess cognitive ability happens to be zero, making the zero point on the scale somewhat arbitrary. The Cognitive Ability and BARS (Behaviorally Anchored Rating Scale) variables (i.e., columns) contain examples of interval measurement scales, as each variable has a numeric scale in which equally sized intervals between values have the same meaning or interpretation; however, both variables lack a meaningful or true zero. 24.1.1.4 Ratio Like variables with an interval measurement scale, variables with a ratio measurement scale are a specific type of continuous variable, as they have a numeric scale in which equally sized intervals between values have the same meaning or interpretation. Unlike interval variables, however, ratio variables have a true and meaningful zero value, such that zero indicates the absence of the construct being measured. Common examples of variables with a ratio measurement scale include those that measure (elapsed) time, where time is measured in standardized units like seconds, minutes, hours, days, months, years, decades, or centuries. Equally sized intervals between various time points have the same meaning, and a time of zero implies the absence of time having elapsed. In organizational settings, we often measure employee age and tenure as numeric elapsed time since a prior date. Because there is a true zero associated with ratio measurement scales, we can make statements like this individual is twice as old as that individual or this individual has worked here one third as long as that individual. Finally, I should note even if we do not observe a true-zero value in our acquired data, a variable can still have a ratio measurement scale. What matters is whether the scale used to measure the construct in question has a possible true-zero value. Using the example of employee age, we can safely assume that we wont observe any employees who have an age of exactly zero years; however, because age is measured as a standardized unit of time (i.e., years), we know that when measuring time in this way a value of zero years does exist on this scale  and it it would indicate the absence of time having passed. That is, a value of zero could hypothetically indicate the lack of time having passed since the exact moment of a persons birth. In sum, even if we dont observe a zero score in our data, a variable can still be classified as having a ratio measurement scale, so long as the scale used to measure the underlying construct could theoretically include a true or meaningful zero value. The Age and Monthly Pay variables (i.e., columns) contain examples of ratio measurement scales, as each variable has a numeric scale in which equally sized intervals between values have the same meaning or interpretation; in addition, both variables have a meaningful or true zero, where zero implies the absence of whatever is being measured. 24.1.2 Constructs, Measures, &amp; Measurement Scales Importantly, we use measures to assess constructs (i.e., concepts), and often there are different ways in which we can measure or operationalize the same construct. Consequently, different measures might have a different measurement scale, even though they are each designed to assess the same construct. For example, if wish to assess the construct of job performance for sales professionals, we could have supervisors rate employee performance using a three-point scale, ranging from Does Not Meet Expectations to Meets Expectations to Exceeds Expectations, which could be described as an ordinal measurement scale. Alternatively, we might also assess the construct of job performance for sales professionals based on how much revenue they generate (in US dollars), which could be described as a ratio measurement scale. 24.1.3 Types of Descriptive Statistics Once we have determined the measurement scale of a variable, were ready to choose an appropriate type of descriptive statistics to summarize the data associated with that variable. Descriptive statistics are used to describe the characteristics of a sample drawn from a population; often, when dealing with data about human beings in organizations, its not feasible to attain data for the entire population, so instead we settle for what is hopefully a representative sample of individuals from the focal population. Common types of descriptive statistics include counts (i.e., frequencies), measures of central tendency (e.g., mean, median, mode), and measures of dispersion (e.g., variance, standard deviation, interquartile range). Note that descriptive statistics are not tests of statistical significance; for tests of statistical significance, we need to look to inferential statistics (e.g., independent-samples t-test, multiple linear regression). When we analyze employee demographic data, for example, we often compute descriptive statistics like the number of employees who identify with each race/ethnicity category or the average employee age and standard deviation. Its important to remember that descriptive statistics are, well, descriptive. That is, they help us summarize characteristics of a sample, which is why they are sometimes referred to as summary statistics. As discussed in the chapter on the Data Analysis phase of the HR Analytics Project Life Cycle, descriptive statistics are a specific type of descriptive analytics, as they summarize data that were collected in the past. Broadly speaking, when describing just a single variable (i.e., applying univariate descriptive statistics), we can distinguish between descriptive statistics that are appropriate for describing categorical versus continuous variables, where categorical variables have a nominal or ordinal measurement scale and continuous variable have an interval or ratio measurement scale. Often, counts (i.e., frequencies) are used to describe data associated with a categorical variable, and measures of central tendency and dispersion are used to describe data associated with a continuous variable. 24.1.3.1 Counts Counts are useful descriptive statistics when a variable has a nominal or ordinal measurement scale. Counts are also referred to as frequencies, so Ill use those two terms interchangeably. As an added benefit, counts tend to be understood by a broad audience, as they simply refer to counting or tallying how many instances of each discrete instances of a category label (i.e., level) of a nominal or ordinal variable have occurred. In fact, sometimes it can be quite amazing what insights we can gleaned just by counting things. A common example of counts in the HR context is headcount by department, facility, or unit. Imagine if you will an organization with facilities in three locations: Beaverton, Hillsboro, and Portland. After tallying up how many employees work at each location, we might find that 15 work at the Beaverton facility, 5 at the Hillsboro facility, and 10 at the Portland facility. In this example, Beaverton, Hillsboro, and Portland are our category labels for this nominal variable, and the values 15, 5, and 10, respectively, are the counts associated with each of those category labels. 24.1.3.2 Measures of Central Tendency &amp; Dispersion Measures of central tendency (e.g., mean, median, mode) summarize the center or most common scores from a distribution of numeric scores, whereas measures of dispersion (e.g., variance, standard deviation, range, interquartile range) summarize variation in numeric scores. Typically, one would apply these specific types of descriptive statistics to describe or summarize variables that have an interval or ratio measurement scale. For example, we might compute the median pay (in US dollars) and the interquartile range in pay for a sample of workers, where pay in this example has a ratio measurement scale. In some instances, however, numeric values could be assigned to category labels of a variable that can be most accurately described as having an ordinal measurement scale  and upon doing so, the variable might be reclassified as having an interval measurement scale. Such a numeric conversion from ordinal to ratio allows for measures of central tendency and dispersion to be computed. For example, a variable with five Likert responses options ranging from Strongly Disagree to Strongly Agree would technically have an ordinal measurement scale because there are unknown intervals between each of the levels (i.e., category labels); in other words, the interval distance between Strongly Disagree and Disagree might not be equal to the interval distance between Disagree and Neither Disagree Nor Agree. Yet, in order to perform certain analyses, sometimes such variables are reconceptualized as having equal intervals and thus having an interval measurement scale. To do so, we would typically assign numeric values to each of the Likert response options, such as 1 = Strongly Disagree and 5 = Strongly Agree  which gives the illusion of equal intervals. Perhaps a more compelling case for treating a variable with Likert responses as a having an interval measurement scale is when we create a composite variable (i.e., overall scale score) based on the sum or average of scores from multiple Likert variables (e.g., multiple survey items from a measure). 24.1.4 Sample Write-Up Based on data stored in the organizations HR information system, we sought out to describe the organizations employee demographics. The employee gender and race/ethnicity variables have nominal measurement scales, and thus we computed counts to describe these variables. Specifically, 321 employees identified as women, 300 as men, 25 as nonbinary, 8 as trans women, and 7 as trans men. Further, 192 employees identified as Hispanic/Latino, 145 as White, 132 as Asian, 119 as Black, 40 as Native American, and 33 as Native Hawaiian. Given that employee age was measured in years since birth, we classified the variable as having a ratio measurement scale, meaning that measures of central tendency and dispersion would be appropriate for describing the variable. We found that employee ages were normally distributed, and that the average employee age was 42.13 years with a standard deviation of 7.71, indicating that roughly two-thirds of employees ages fall between 34.42 and 49.84 years. 24.2 Tutorial This chapters tutorial demonstrates how to compute various types of descriptive statistics and how to present the findings visually. 24.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch one of the following video tutorials below. Note that in the videos below, I show how to read in the data using the read.csv function from base R, whereas in the written tutorial portion of this chapter, I show how to read in the data using the read_csv function from the readr package. Link to video tutorial: https://youtu.be/Xg0wiBofjCU Link to video tutorial: https://youtu.be/10jYstRPDAU 24.2.2 Functions &amp; Packages Introduced Function Package table base R levels base R factor base R c base R barplot base R pie base R colors base R abline base R hist base R boxplot base R c base R mean base R median base R var base R sd base R min base R max base R range base R IQR base R quantile base R summary base R 24.2.3 Initial Steps If you havent already, save the file called employee_demo.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called employee_demo.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demo &lt;- read_csv(&quot;employee_demo.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_character(), ## Facility = col_character(), ## Education = col_character(), ## Performance = col_double(), ## Age = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(demo) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;Education&quot; &quot;Performance&quot; &quot;Age&quot; # Print number of rows in data frame (tibble) object nrow(demo) ## [1] 30 # Print data frame (tibble) object print(demo) ## # A tibble: 30 x 5 ## EmpID Facility Education Performance Age ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE123 Beaverton College Degree 3.8 25 ## 2 EE124 Beaverton Some College 9 30 ## 3 EE125 Portland High School Diploma 8.3 32 ## 4 EE126 Beaverton Some College 9.8 28 ## 5 EE127 Beaverton Some College 5.7 30 ## 6 EE128 Beaverton College Degree 8.2 30 ## 7 EE129 Beaverton College Degree 7.3 28 ## 8 EE130 Beaverton College Degree 7.7 28 ## 9 EE131 Portland Some College 6.3 28 ## 10 EE132 Hillsboro Some College 8.4 27 ## # ... with 20 more rows The demo data frame object contains five variables. EmpID, Facility, Education, Performance, and Age. The EmpID variable is the employee unique identifier, and in this data frame, each row corresponds to a unique employee. The Facility variable contains the name of the facility where each employee works. The Education variable includes the highest level of education each employee attained (i.e., High School Diploma, Some College, College Degree). The Performance variable includes the employees annual performance scores (as derived by a proprietary algorithm), where a score of 0.0 would indicate exceptionally low job performance and a score of 10 would indicate exceptionally high job performance. The Age variable includes employees age (in years). 24.2.4 Determine the Measurement Scale As described above, we have four employee-demographic variables at our disposal in the data frame object we named demo: Facility, Education, Performance, and Age. Now its time to determine which measurement scale best describes each variable  and spoiler alert: These four variables correspond to nominal, ordinal, interval, and ratio measurement scales respectively. Below, I describe why a particular measurement scale maps onto each variable. By viewing our the data frame object called demo using the print, head, or View functions (as show above in the Initial Steps), we can see that the Facility variable consists of the following categories (i.e., levels): Beaverton, Hillsboro, and Portland. These categories do not have inherent numeric properties, and they cant be ordered meaningfully given that they just represent different facility locations for this fictitious organization. Given all that, the Facility variable can best be described as having a nominal measurement scale. The Education variable contains three levels (i.e., categories): High School Diploma, Some College, and College Degree. These three discrete categories do not have inherent numeric properties but can be ordered in terms of a conventional educational progression, where earning a high school diploma would be the lowest level and earning a college degree would be the highest level (of the three). Furthermore, although the three variable levels can be ordered, they do not necessarily have equal intervals between the levels; in other words, the distance (e.g., time) between a high school diploma and completing some college is not necessarily the same as the distance between completing some college and a college degree. Given all of those characteristics, the Education variable in these data can best be described as having an ordinal measurement scale. The Performance variable includes the annual performance score for each employee (as derived from a proprietary algorithm), where a score of 0.0 would indicate exceptionally low job performance and a score of 10 would indicate exceptionally high job performance. We can assume in this case that intervals between integers are equal, such that the distance between scores of 1 and 2 is the same as the distance between scores 2 and 3; however, because a value of zero (0.0) does not indicate the absence of performance for this variable (but rather exceptionally low job performance), we must conclude that it has an interval measurement scale as opposed to a ratio measurement scale. Finally, the Age variable includes the age of each employee measured in years. Because Age has ordered numeric values and because there are equal intervals between years as a standard measure of time, we can conclude that the variable does not have a nominal or ordinal measurement scale. Whats more, hypothetically, a value of zero when measuring something in years would imply the absence of years  which is to say Age as measured in years has a meaningful zero value. Given all that, the Age variable can be most accurately described as having a ratio measurement scale. 24.2.5 Describe Nominal &amp; Ordinal (Categorical) Variables We can describe variables with nominal or ordinal measurement scales by computing counts (i.e., frequencies) and by creating univariate bar charts (or pie charts), and well work through each of these descriptive approaches in the following sections. 24.2.5.1 Compute Counts &amp; Frequencies Fortunately, its quite easy to run counts in R, and well begin by running counts for the Facility variable. One of the simplest approaches is to use the table function from base R. As the sole parenthetical argument, just type the name of the data frame object (demo) followed by the $ operator and the name of the variable that belongs to that data frame object (Facility). # Compute counts for Facility variable (which has nominal measurement scale) table(demo$Facility) ## ## Beaverton Hillsboro Portland ## 15 5 10 As we can see, 15 employees work at the Beaverton facility, 5 at the Hillsboro facility, and 10 at the Portland facility. Simply put, the most employees work in Beaverton, followed by Portland and Hillsboro. Of course, we also would hope that these data are accurate and timely, and point-in-time headcount data in organizations can be surprisingly difficult to estimate accurately in some organizations, but thats a story for another time. Because we have classified the Education variable as ordinal, we want to make sure that it has ordered levels. That is, High School Diploma should be the lowest level and College Degree should be the highest. To check to see if the variable is a factor with ordered levels, we can apply the levels function from base R and, as the sole parenthetical argument, type the name of the data frame object (demo) followed by the $ operator and the name of the variable that belongs to that data frame object (Education). # Determine whether the Education variable is a factor with ordered levels levels(demo$Education) ## NULL Running the levels function for the Education variable returns NULL, which indicates that this variable is not a factor variable with ordered levels. Never fear, we can fix that by using the factor function from base R. To convert the Education variable to an ordered factor variable, we will overwrite the existing Education variable from the demo data frame object. Thus, we will start by typing the name of the data frame object (demo) followed by the $ operator and the name of the variable (Education), and to the right, we will type the &lt;- operator so that we can perform the variable assignment. To the right of the &lt;- operator, we will type the name of the factor function. As the first argument, we will type the name of the data frame object (demo) followed by the $ operator and the name of the variable (Education). As the second argument, we will type ordered=TRUE to signify that this variable will have ordered levels. As the third argument, well type levels= followed by a vector of the variable levels in ascending order. Note that we use the c (combine) function from base R to construct the vector, and we need to put each level within quotation marks (\" \"). # Convert Education variable to ordered factor demo$Education &lt;- factor(demo$Education, ordered=TRUE, levels=c(&quot;High School Diploma&quot;, &quot;Some College&quot;, &quot;College Degree&quot;)) Now that weve converted the Education variable to an ordered factor variable, lets verify that we did so correctly by running the same levels function that we did above. # Determine whether the Education variable is a factor with ordered levels levels(demo$Education) ## [1] &quot;High School Diploma&quot; &quot;Some College&quot; &quot;College Degree&quot; Instead of NULL, now we see the levels of the variable in ascending order. Good for us! With the Education variable now an ordered factor, it now makes sense to run the table function to compute the counts. # Compute counts for Education variable table(demo$Education) ## ## High School Diploma Some College College Degree ## 4 15 11 Descriptively, we see that the most people completed some college (15), followed closely by 11 people who completed a full college degree. Relatively few employees in this sample had just a high school diploma (4). 24.2.5.2 Create Data Visualizations When interpreting descriptive statistics, its often useful to create some kind of data visualization to display the findings in a pictorial or graphical format. A bar chart is a simple data visualization that many potential audience members will be familiar with, making it a good choice. In addition, when the different categories (e.g., levels) are mutually exclusive and sum to a whole, we might also choose to create a pie chart. Well begin by creating a bar chart for the Facility variable and follow that up with creating a pie chart for the Education variable  though, we just as easily could make a bar chart for the Education variable and a pie chart for the Facility variable. Create Bar Charts: Using the barplot function from base R, we can create a very simple and straightforward bar chart without too many frills and embellishments. Lets start with the Facility variable. As the sole parenthetical argument in the barplot function, simply, enter the table(demo$Facility) code that we wrote in the previous section. # Create a bar chart based on Facility counts barplot(table(demo$Facility)) As you can see, a very simple (and not super aesthetically pleasing) bar chart appears in our Plots window. When exploring data on our own, it is often fine to just complete a simple bar chart like this one, as opposed to fine-tuning the aesthetics (e.g., size, color, font) of the plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. If youre feeling adventurous and would like to learn how to fine-tune the bar chart, feel free to continue on with this tutorial. Additional attention paid to aesthetics might be worthwhile if you plan to present the plot to others in a formal presentation or report. Using the barplot code we wrote above, we can add a second argument in which we apply ylim= followed by a vector (using the c function) of the lower and upper limits for the y-axis. In this example, I set the lower and upper y-axis limits to 0 and 20. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20)) Building on the previous code, we add additional arguments in which we provide more meaningful labels for the x- and y-axes. To do so, we use the xlab argument for the x-axis label and the ylab argument for the y-axis label. Just make sure to put quotation marks (\" \") around whatever text you come up with for your axis labels. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;) We can change the colors of the bars by adding the col (color) argument. There are many, many different colors that can be used in R, and one of my favorites is dodgerblue. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;, col=&quot;dodgerblue&quot;) If youd like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and youll get a (huge) list of the color options. # List names of base R color choices colors() ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; ## [5] &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; ## [9] &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; &quot;azure3&quot; ## [17] &quot;azure4&quot; &quot;beige&quot; &quot;bisque&quot; &quot;bisque1&quot; ## [21] &quot;bisque2&quot; &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; ## [25] &quot;blanchedalmond&quot; &quot;blue&quot; &quot;blue1&quot; &quot;blue2&quot; ## [29] &quot;blue3&quot; &quot;blue4&quot; &quot;blueviolet&quot; &quot;brown&quot; ## [33] &quot;brown1&quot; &quot;brown2&quot; &quot;brown3&quot; &quot;brown4&quot; ## [37] &quot;burlywood&quot; &quot;burlywood1&quot; &quot;burlywood2&quot; &quot;burlywood3&quot; ## [41] &quot;burlywood4&quot; &quot;cadetblue&quot; &quot;cadetblue1&quot; &quot;cadetblue2&quot; ## [45] &quot;cadetblue3&quot; &quot;cadetblue4&quot; &quot;chartreuse&quot; &quot;chartreuse1&quot; ## [49] &quot;chartreuse2&quot; &quot;chartreuse3&quot; &quot;chartreuse4&quot; &quot;chocolate&quot; ## [53] &quot;chocolate1&quot; &quot;chocolate2&quot; &quot;chocolate3&quot; &quot;chocolate4&quot; ## [57] &quot;coral&quot; &quot;coral1&quot; &quot;coral2&quot; &quot;coral3&quot; ## [61] &quot;coral4&quot; &quot;cornflowerblue&quot; &quot;cornsilk&quot; &quot;cornsilk1&quot; ## [65] &quot;cornsilk2&quot; &quot;cornsilk3&quot; &quot;cornsilk4&quot; &quot;cyan&quot; ## [69] &quot;cyan1&quot; &quot;cyan2&quot; &quot;cyan3&quot; &quot;cyan4&quot; ## [73] &quot;darkblue&quot; &quot;darkcyan&quot; &quot;darkgoldenrod&quot; &quot;darkgoldenrod1&quot; ## [77] &quot;darkgoldenrod2&quot; &quot;darkgoldenrod3&quot; &quot;darkgoldenrod4&quot; &quot;darkgray&quot; ## [81] &quot;darkgreen&quot; &quot;darkgrey&quot; &quot;darkkhaki&quot; &quot;darkmagenta&quot; ## [85] &quot;darkolivegreen&quot; &quot;darkolivegreen1&quot; &quot;darkolivegreen2&quot; &quot;darkolivegreen3&quot; ## [89] &quot;darkolivegreen4&quot; &quot;darkorange&quot; &quot;darkorange1&quot; &quot;darkorange2&quot; ## [93] &quot;darkorange3&quot; &quot;darkorange4&quot; &quot;darkorchid&quot; &quot;darkorchid1&quot; ## [97] &quot;darkorchid2&quot; &quot;darkorchid3&quot; &quot;darkorchid4&quot; &quot;darkred&quot; ## [101] &quot;darksalmon&quot; &quot;darkseagreen&quot; &quot;darkseagreen1&quot; &quot;darkseagreen2&quot; ## [105] &quot;darkseagreen3&quot; &quot;darkseagreen4&quot; &quot;darkslateblue&quot; &quot;darkslategray&quot; ## [109] &quot;darkslategray1&quot; &quot;darkslategray2&quot; &quot;darkslategray3&quot; &quot;darkslategray4&quot; ## [113] &quot;darkslategrey&quot; &quot;darkturquoise&quot; &quot;darkviolet&quot; &quot;deeppink&quot; ## [117] &quot;deeppink1&quot; &quot;deeppink2&quot; &quot;deeppink3&quot; &quot;deeppink4&quot; ## [121] &quot;deepskyblue&quot; &quot;deepskyblue1&quot; &quot;deepskyblue2&quot; &quot;deepskyblue3&quot; ## [125] &quot;deepskyblue4&quot; &quot;dimgray&quot; &quot;dimgrey&quot; &quot;dodgerblue&quot; ## [129] &quot;dodgerblue1&quot; &quot;dodgerblue2&quot; &quot;dodgerblue3&quot; &quot;dodgerblue4&quot; ## [133] &quot;firebrick&quot; &quot;firebrick1&quot; &quot;firebrick2&quot; &quot;firebrick3&quot; ## [137] &quot;firebrick4&quot; &quot;floralwhite&quot; &quot;forestgreen&quot; &quot;gainsboro&quot; ## [141] &quot;ghostwhite&quot; &quot;gold&quot; &quot;gold1&quot; &quot;gold2&quot; ## [145] &quot;gold3&quot; &quot;gold4&quot; &quot;goldenrod&quot; &quot;goldenrod1&quot; ## [149] &quot;goldenrod2&quot; &quot;goldenrod3&quot; &quot;goldenrod4&quot; &quot;gray&quot; ## [153] &quot;gray0&quot; &quot;gray1&quot; &quot;gray2&quot; &quot;gray3&quot; ## [157] &quot;gray4&quot; &quot;gray5&quot; &quot;gray6&quot; &quot;gray7&quot; ## [161] &quot;gray8&quot; &quot;gray9&quot; &quot;gray10&quot; &quot;gray11&quot; ## [165] &quot;gray12&quot; &quot;gray13&quot; &quot;gray14&quot; &quot;gray15&quot; ## [169] &quot;gray16&quot; &quot;gray17&quot; &quot;gray18&quot; &quot;gray19&quot; ## [173] &quot;gray20&quot; &quot;gray21&quot; &quot;gray22&quot; &quot;gray23&quot; ## [177] &quot;gray24&quot; &quot;gray25&quot; &quot;gray26&quot; &quot;gray27&quot; ## [181] &quot;gray28&quot; &quot;gray29&quot; &quot;gray30&quot; &quot;gray31&quot; ## [185] &quot;gray32&quot; &quot;gray33&quot; &quot;gray34&quot; &quot;gray35&quot; ## [189] &quot;gray36&quot; &quot;gray37&quot; &quot;gray38&quot; &quot;gray39&quot; ## [193] &quot;gray40&quot; &quot;gray41&quot; &quot;gray42&quot; &quot;gray43&quot; ## [197] &quot;gray44&quot; &quot;gray45&quot; &quot;gray46&quot; &quot;gray47&quot; ## [201] &quot;gray48&quot; &quot;gray49&quot; &quot;gray50&quot; &quot;gray51&quot; ## [205] &quot;gray52&quot; &quot;gray53&quot; &quot;gray54&quot; &quot;gray55&quot; ## [209] &quot;gray56&quot; &quot;gray57&quot; &quot;gray58&quot; &quot;gray59&quot; ## [213] &quot;gray60&quot; &quot;gray61&quot; &quot;gray62&quot; &quot;gray63&quot; ## [217] &quot;gray64&quot; &quot;gray65&quot; &quot;gray66&quot; &quot;gray67&quot; ## [221] &quot;gray68&quot; &quot;gray69&quot; &quot;gray70&quot; &quot;gray71&quot; ## [225] &quot;gray72&quot; &quot;gray73&quot; &quot;gray74&quot; &quot;gray75&quot; ## [229] &quot;gray76&quot; &quot;gray77&quot; &quot;gray78&quot; &quot;gray79&quot; ## [233] &quot;gray80&quot; &quot;gray81&quot; &quot;gray82&quot; &quot;gray83&quot; ## [237] &quot;gray84&quot; &quot;gray85&quot; &quot;gray86&quot; &quot;gray87&quot; ## [241] &quot;gray88&quot; &quot;gray89&quot; &quot;gray90&quot; &quot;gray91&quot; ## [245] &quot;gray92&quot; &quot;gray93&quot; &quot;gray94&quot; &quot;gray95&quot; ## [249] &quot;gray96&quot; &quot;gray97&quot; &quot;gray98&quot; &quot;gray99&quot; ## [253] &quot;gray100&quot; &quot;green&quot; &quot;green1&quot; &quot;green2&quot; ## [257] &quot;green3&quot; &quot;green4&quot; &quot;greenyellow&quot; &quot;grey&quot; ## [261] &quot;grey0&quot; &quot;grey1&quot; &quot;grey2&quot; &quot;grey3&quot; ## [265] &quot;grey4&quot; &quot;grey5&quot; &quot;grey6&quot; &quot;grey7&quot; ## [269] &quot;grey8&quot; &quot;grey9&quot; &quot;grey10&quot; &quot;grey11&quot; ## [273] &quot;grey12&quot; &quot;grey13&quot; &quot;grey14&quot; &quot;grey15&quot; ## [277] &quot;grey16&quot; &quot;grey17&quot; &quot;grey18&quot; &quot;grey19&quot; ## [281] &quot;grey20&quot; &quot;grey21&quot; &quot;grey22&quot; &quot;grey23&quot; ## [285] &quot;grey24&quot; &quot;grey25&quot; &quot;grey26&quot; &quot;grey27&quot; ## [289] &quot;grey28&quot; &quot;grey29&quot; &quot;grey30&quot; &quot;grey31&quot; ## [293] &quot;grey32&quot; &quot;grey33&quot; &quot;grey34&quot; &quot;grey35&quot; ## [297] &quot;grey36&quot; &quot;grey37&quot; &quot;grey38&quot; &quot;grey39&quot; ## [301] &quot;grey40&quot; &quot;grey41&quot; &quot;grey42&quot; &quot;grey43&quot; ## [305] &quot;grey44&quot; &quot;grey45&quot; &quot;grey46&quot; &quot;grey47&quot; ## [309] &quot;grey48&quot; &quot;grey49&quot; &quot;grey50&quot; &quot;grey51&quot; ## [313] &quot;grey52&quot; &quot;grey53&quot; &quot;grey54&quot; &quot;grey55&quot; ## [317] &quot;grey56&quot; &quot;grey57&quot; &quot;grey58&quot; &quot;grey59&quot; ## [321] &quot;grey60&quot; &quot;grey61&quot; &quot;grey62&quot; &quot;grey63&quot; ## [325] &quot;grey64&quot; &quot;grey65&quot; &quot;grey66&quot; &quot;grey67&quot; ## [329] &quot;grey68&quot; &quot;grey69&quot; &quot;grey70&quot; &quot;grey71&quot; ## [333] &quot;grey72&quot; &quot;grey73&quot; &quot;grey74&quot; &quot;grey75&quot; ## [337] &quot;grey76&quot; &quot;grey77&quot; &quot;grey78&quot; &quot;grey79&quot; ## [341] &quot;grey80&quot; &quot;grey81&quot; &quot;grey82&quot; &quot;grey83&quot; ## [345] &quot;grey84&quot; &quot;grey85&quot; &quot;grey86&quot; &quot;grey87&quot; ## [349] &quot;grey88&quot; &quot;grey89&quot; &quot;grey90&quot; &quot;grey91&quot; ## [353] &quot;grey92&quot; &quot;grey93&quot; &quot;grey94&quot; &quot;grey95&quot; ## [357] &quot;grey96&quot; &quot;grey97&quot; &quot;grey98&quot; &quot;grey99&quot; ## [361] &quot;grey100&quot; &quot;honeydew&quot; &quot;honeydew1&quot; &quot;honeydew2&quot; ## [365] &quot;honeydew3&quot; &quot;honeydew4&quot; &quot;hotpink&quot; &quot;hotpink1&quot; ## [369] &quot;hotpink2&quot; &quot;hotpink3&quot; &quot;hotpink4&quot; &quot;indianred&quot; ## [373] &quot;indianred1&quot; &quot;indianred2&quot; &quot;indianred3&quot; &quot;indianred4&quot; ## [377] &quot;ivory&quot; &quot;ivory1&quot; &quot;ivory2&quot; &quot;ivory3&quot; ## [381] &quot;ivory4&quot; &quot;khaki&quot; &quot;khaki1&quot; &quot;khaki2&quot; ## [385] &quot;khaki3&quot; &quot;khaki4&quot; &quot;lavender&quot; &quot;lavenderblush&quot; ## [389] &quot;lavenderblush1&quot; &quot;lavenderblush2&quot; &quot;lavenderblush3&quot; &quot;lavenderblush4&quot; ## [393] &quot;lawngreen&quot; &quot;lemonchiffon&quot; &quot;lemonchiffon1&quot; &quot;lemonchiffon2&quot; ## [397] &quot;lemonchiffon3&quot; &quot;lemonchiffon4&quot; &quot;lightblue&quot; &quot;lightblue1&quot; ## [401] &quot;lightblue2&quot; &quot;lightblue3&quot; &quot;lightblue4&quot; &quot;lightcoral&quot; ## [405] &quot;lightcyan&quot; &quot;lightcyan1&quot; &quot;lightcyan2&quot; &quot;lightcyan3&quot; ## [409] &quot;lightcyan4&quot; &quot;lightgoldenrod&quot; &quot;lightgoldenrod1&quot; &quot;lightgoldenrod2&quot; ## [413] &quot;lightgoldenrod3&quot; &quot;lightgoldenrod4&quot; &quot;lightgoldenrodyellow&quot; &quot;lightgray&quot; ## [417] &quot;lightgreen&quot; &quot;lightgrey&quot; &quot;lightpink&quot; &quot;lightpink1&quot; ## [421] &quot;lightpink2&quot; &quot;lightpink3&quot; &quot;lightpink4&quot; &quot;lightsalmon&quot; ## [425] &quot;lightsalmon1&quot; &quot;lightsalmon2&quot; &quot;lightsalmon3&quot; &quot;lightsalmon4&quot; ## [429] &quot;lightseagreen&quot; &quot;lightskyblue&quot; &quot;lightskyblue1&quot; &quot;lightskyblue2&quot; ## [433] &quot;lightskyblue3&quot; &quot;lightskyblue4&quot; &quot;lightslateblue&quot; &quot;lightslategray&quot; ## [437] &quot;lightslategrey&quot; &quot;lightsteelblue&quot; &quot;lightsteelblue1&quot; &quot;lightsteelblue2&quot; ## [441] &quot;lightsteelblue3&quot; &quot;lightsteelblue4&quot; &quot;lightyellow&quot; &quot;lightyellow1&quot; ## [445] &quot;lightyellow2&quot; &quot;lightyellow3&quot; &quot;lightyellow4&quot; &quot;limegreen&quot; ## [449] &quot;linen&quot; &quot;magenta&quot; &quot;magenta1&quot; &quot;magenta2&quot; ## [453] &quot;magenta3&quot; &quot;magenta4&quot; &quot;maroon&quot; &quot;maroon1&quot; ## [457] &quot;maroon2&quot; &quot;maroon3&quot; &quot;maroon4&quot; &quot;mediumaquamarine&quot; ## [461] &quot;mediumblue&quot; &quot;mediumorchid&quot; &quot;mediumorchid1&quot; &quot;mediumorchid2&quot; ## [465] &quot;mediumorchid3&quot; &quot;mediumorchid4&quot; &quot;mediumpurple&quot; &quot;mediumpurple1&quot; ## [469] &quot;mediumpurple2&quot; &quot;mediumpurple3&quot; &quot;mediumpurple4&quot; &quot;mediumseagreen&quot; ## [473] &quot;mediumslateblue&quot; &quot;mediumspringgreen&quot; &quot;mediumturquoise&quot; &quot;mediumvioletred&quot; ## [477] &quot;midnightblue&quot; &quot;mintcream&quot; &quot;mistyrose&quot; &quot;mistyrose1&quot; ## [481] &quot;mistyrose2&quot; &quot;mistyrose3&quot; &quot;mistyrose4&quot; &quot;moccasin&quot; ## [485] &quot;navajowhite&quot; &quot;navajowhite1&quot; &quot;navajowhite2&quot; &quot;navajowhite3&quot; ## [489] &quot;navajowhite4&quot; &quot;navy&quot; &quot;navyblue&quot; &quot;oldlace&quot; ## [493] &quot;olivedrab&quot; &quot;olivedrab1&quot; &quot;olivedrab2&quot; &quot;olivedrab3&quot; ## [497] &quot;olivedrab4&quot; &quot;orange&quot; &quot;orange1&quot; &quot;orange2&quot; ## [501] &quot;orange3&quot; &quot;orange4&quot; &quot;orangered&quot; &quot;orangered1&quot; ## [505] &quot;orangered2&quot; &quot;orangered3&quot; &quot;orangered4&quot; &quot;orchid&quot; ## [509] &quot;orchid1&quot; &quot;orchid2&quot; &quot;orchid3&quot; &quot;orchid4&quot; ## [513] &quot;palegoldenrod&quot; &quot;palegreen&quot; &quot;palegreen1&quot; &quot;palegreen2&quot; ## [517] &quot;palegreen3&quot; &quot;palegreen4&quot; &quot;paleturquoise&quot; &quot;paleturquoise1&quot; ## [521] &quot;paleturquoise2&quot; &quot;paleturquoise3&quot; &quot;paleturquoise4&quot; &quot;palevioletred&quot; ## [525] &quot;palevioletred1&quot; &quot;palevioletred2&quot; &quot;palevioletred3&quot; &quot;palevioletred4&quot; ## [529] &quot;papayawhip&quot; &quot;peachpuff&quot; &quot;peachpuff1&quot; &quot;peachpuff2&quot; ## [533] &quot;peachpuff3&quot; &quot;peachpuff4&quot; &quot;peru&quot; &quot;pink&quot; ## [537] &quot;pink1&quot; &quot;pink2&quot; &quot;pink3&quot; &quot;pink4&quot; ## [541] &quot;plum&quot; &quot;plum1&quot; &quot;plum2&quot; &quot;plum3&quot; ## [545] &quot;plum4&quot; &quot;powderblue&quot; &quot;purple&quot; &quot;purple1&quot; ## [549] &quot;purple2&quot; &quot;purple3&quot; &quot;purple4&quot; &quot;red&quot; ## [553] &quot;red1&quot; &quot;red2&quot; &quot;red3&quot; &quot;red4&quot; ## [557] &quot;rosybrown&quot; &quot;rosybrown1&quot; &quot;rosybrown2&quot; &quot;rosybrown3&quot; ## [561] &quot;rosybrown4&quot; &quot;royalblue&quot; &quot;royalblue1&quot; &quot;royalblue2&quot; ## [565] &quot;royalblue3&quot; &quot;royalblue4&quot; &quot;saddlebrown&quot; &quot;salmon&quot; ## [569] &quot;salmon1&quot; &quot;salmon2&quot; &quot;salmon3&quot; &quot;salmon4&quot; ## [573] &quot;sandybrown&quot; &quot;seagreen&quot; &quot;seagreen1&quot; &quot;seagreen2&quot; ## [577] &quot;seagreen3&quot; &quot;seagreen4&quot; &quot;seashell&quot; &quot;seashell1&quot; ## [581] &quot;seashell2&quot; &quot;seashell3&quot; &quot;seashell4&quot; &quot;sienna&quot; ## [585] &quot;sienna1&quot; &quot;sienna2&quot; &quot;sienna3&quot; &quot;sienna4&quot; ## [589] &quot;skyblue&quot; &quot;skyblue1&quot; &quot;skyblue2&quot; &quot;skyblue3&quot; ## [593] &quot;skyblue4&quot; &quot;slateblue&quot; &quot;slateblue1&quot; &quot;slateblue2&quot; ## [597] &quot;slateblue3&quot; &quot;slateblue4&quot; &quot;slategray&quot; &quot;slategray1&quot; ## [601] &quot;slategray2&quot; &quot;slategray3&quot; &quot;slategray4&quot; &quot;slategrey&quot; ## [605] &quot;snow&quot; &quot;snow1&quot; &quot;snow2&quot; &quot;snow3&quot; ## [609] &quot;snow4&quot; &quot;springgreen&quot; &quot;springgreen1&quot; &quot;springgreen2&quot; ## [613] &quot;springgreen3&quot; &quot;springgreen4&quot; &quot;steelblue&quot; &quot;steelblue1&quot; ## [617] &quot;steelblue2&quot; &quot;steelblue3&quot; &quot;steelblue4&quot; &quot;tan&quot; ## [621] &quot;tan1&quot; &quot;tan2&quot; &quot;tan3&quot; &quot;tan4&quot; ## [625] &quot;thistle&quot; &quot;thistle1&quot; &quot;thistle2&quot; &quot;thistle3&quot; ## [629] &quot;thistle4&quot; &quot;tomato&quot; &quot;tomato1&quot; &quot;tomato2&quot; ## [633] &quot;tomato3&quot; &quot;tomato4&quot; &quot;turquoise&quot; &quot;turquoise1&quot; ## [637] &quot;turquoise2&quot; &quot;turquoise3&quot; &quot;turquoise4&quot; &quot;violet&quot; ## [641] &quot;violetred&quot; &quot;violetred1&quot; &quot;violetred2&quot; &quot;violetred3&quot; ## [645] &quot;violetred4&quot; &quot;wheat&quot; &quot;wheat1&quot; &quot;wheat2&quot; ## [649] &quot;wheat3&quot; &quot;wheat4&quot; &quot;whitesmoke&quot; &quot;yellow&quot; ## [653] &quot;yellow1&quot; &quot;yellow2&quot; &quot;yellow3&quot; &quot;yellow4&quot; ## [657] &quot;yellowgreen&quot; Finally, the barplot function does not provide a horizontal line where the y-axis is equal to 0. If youd like to add such a line, simply follow up your barplot function with the abline function, and as the sole argument, type h=0. # Create a bar chart based on Facility counts barplot(table(demo$Facility), ylim=c(0,20), xlab=&quot;Facility&quot;, ylab=&quot;Counts&quot;, col=&quot;dodgerblue&quot;) abline(h=0) And finally, heres a quick example of how you might visualize the Education variable using the barplot function. # Create a bar chart for Education variable barplot(table(demo$Education), ylim=c(0,20), xlab=&quot;Education Level&quot;, ylab=&quot;Counts&quot;, col=&quot;orange&quot;) abline(h=0) Create Pie Charts: Using the pie function from base R, we can create a very simple and straightforward bar chart without too many frills and embellishments. Lets start with the Education variable. As the sole parenthetical argument in the barplot function, simply, enter the table(demo$Education) code that we wrote in the section called Compute Counts &amp; Frequencies. # Create a bar chart based on Education counts pie(table(demo$Education)) A very simple and generic pie chart appears in our Plots window. When exploring data on our own, it is often fine to just complete a simple pie chart like this one, as opposed to fine-tuning the aesthetics (e.g., size, color, font) of the plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. If youre feeling adventurous and would like to learn how to adjust the colors pie chart, feel free to continue on with this tutorial. Using the pie code we wrote above, lets add the col= argument followed by the c (combine) function containing a vector of colors  one color for each slice of the pie. Here, I chose the primar colors of red, yellow, and blue. # Create a bar chart based on Education counts pie(table(demo$Education), col=c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) If youd like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and youll get a (huge) list of the color options. # List names of base R color choices colors() ## [1] &quot;white&quot; &quot;aliceblue&quot; &quot;antiquewhite&quot; &quot;antiquewhite1&quot; ## [5] &quot;antiquewhite2&quot; &quot;antiquewhite3&quot; &quot;antiquewhite4&quot; &quot;aquamarine&quot; ## [9] &quot;aquamarine1&quot; &quot;aquamarine2&quot; &quot;aquamarine3&quot; &quot;aquamarine4&quot; ## [13] &quot;azure&quot; &quot;azure1&quot; &quot;azure2&quot; &quot;azure3&quot; ## [17] &quot;azure4&quot; &quot;beige&quot; &quot;bisque&quot; &quot;bisque1&quot; ## [21] &quot;bisque2&quot; &quot;bisque3&quot; &quot;bisque4&quot; &quot;black&quot; ## [25] &quot;blanchedalmond&quot; &quot;blue&quot; &quot;blue1&quot; &quot;blue2&quot; ## [29] &quot;blue3&quot; &quot;blue4&quot; &quot;blueviolet&quot; &quot;brown&quot; ## [33] &quot;brown1&quot; &quot;brown2&quot; &quot;brown3&quot; &quot;brown4&quot; ## [37] &quot;burlywood&quot; &quot;burlywood1&quot; &quot;burlywood2&quot; &quot;burlywood3&quot; ## [41] &quot;burlywood4&quot; &quot;cadetblue&quot; &quot;cadetblue1&quot; &quot;cadetblue2&quot; ## [45] &quot;cadetblue3&quot; &quot;cadetblue4&quot; &quot;chartreuse&quot; &quot;chartreuse1&quot; ## [49] &quot;chartreuse2&quot; &quot;chartreuse3&quot; &quot;chartreuse4&quot; &quot;chocolate&quot; ## [53] &quot;chocolate1&quot; &quot;chocolate2&quot; &quot;chocolate3&quot; &quot;chocolate4&quot; ## [57] &quot;coral&quot; &quot;coral1&quot; &quot;coral2&quot; &quot;coral3&quot; ## [61] &quot;coral4&quot; &quot;cornflowerblue&quot; &quot;cornsilk&quot; &quot;cornsilk1&quot; ## [65] &quot;cornsilk2&quot; &quot;cornsilk3&quot; &quot;cornsilk4&quot; &quot;cyan&quot; ## [69] &quot;cyan1&quot; &quot;cyan2&quot; &quot;cyan3&quot; &quot;cyan4&quot; ## [73] &quot;darkblue&quot; &quot;darkcyan&quot; &quot;darkgoldenrod&quot; &quot;darkgoldenrod1&quot; ## [77] &quot;darkgoldenrod2&quot; &quot;darkgoldenrod3&quot; &quot;darkgoldenrod4&quot; &quot;darkgray&quot; ## [81] &quot;darkgreen&quot; &quot;darkgrey&quot; &quot;darkkhaki&quot; &quot;darkmagenta&quot; ## [85] &quot;darkolivegreen&quot; &quot;darkolivegreen1&quot; &quot;darkolivegreen2&quot; &quot;darkolivegreen3&quot; ## [89] &quot;darkolivegreen4&quot; &quot;darkorange&quot; &quot;darkorange1&quot; &quot;darkorange2&quot; ## [93] &quot;darkorange3&quot; &quot;darkorange4&quot; &quot;darkorchid&quot; &quot;darkorchid1&quot; ## [97] &quot;darkorchid2&quot; &quot;darkorchid3&quot; &quot;darkorchid4&quot; &quot;darkred&quot; ## [101] &quot;darksalmon&quot; &quot;darkseagreen&quot; &quot;darkseagreen1&quot; &quot;darkseagreen2&quot; ## [105] &quot;darkseagreen3&quot; &quot;darkseagreen4&quot; &quot;darkslateblue&quot; &quot;darkslategray&quot; ## [109] &quot;darkslategray1&quot; &quot;darkslategray2&quot; &quot;darkslategray3&quot; &quot;darkslategray4&quot; ## [113] &quot;darkslategrey&quot; &quot;darkturquoise&quot; &quot;darkviolet&quot; &quot;deeppink&quot; ## [117] &quot;deeppink1&quot; &quot;deeppink2&quot; &quot;deeppink3&quot; &quot;deeppink4&quot; ## [121] &quot;deepskyblue&quot; &quot;deepskyblue1&quot; &quot;deepskyblue2&quot; &quot;deepskyblue3&quot; ## [125] &quot;deepskyblue4&quot; &quot;dimgray&quot; &quot;dimgrey&quot; &quot;dodgerblue&quot; ## [129] &quot;dodgerblue1&quot; &quot;dodgerblue2&quot; &quot;dodgerblue3&quot; &quot;dodgerblue4&quot; ## [133] &quot;firebrick&quot; &quot;firebrick1&quot; &quot;firebrick2&quot; &quot;firebrick3&quot; ## [137] &quot;firebrick4&quot; &quot;floralwhite&quot; &quot;forestgreen&quot; &quot;gainsboro&quot; ## [141] &quot;ghostwhite&quot; &quot;gold&quot; &quot;gold1&quot; &quot;gold2&quot; ## [145] &quot;gold3&quot; &quot;gold4&quot; &quot;goldenrod&quot; &quot;goldenrod1&quot; ## [149] &quot;goldenrod2&quot; &quot;goldenrod3&quot; &quot;goldenrod4&quot; &quot;gray&quot; ## [153] &quot;gray0&quot; &quot;gray1&quot; &quot;gray2&quot; &quot;gray3&quot; ## [157] &quot;gray4&quot; &quot;gray5&quot; &quot;gray6&quot; &quot;gray7&quot; ## [161] &quot;gray8&quot; &quot;gray9&quot; &quot;gray10&quot; &quot;gray11&quot; ## [165] &quot;gray12&quot; &quot;gray13&quot; &quot;gray14&quot; &quot;gray15&quot; ## [169] &quot;gray16&quot; &quot;gray17&quot; &quot;gray18&quot; &quot;gray19&quot; ## [173] &quot;gray20&quot; &quot;gray21&quot; &quot;gray22&quot; &quot;gray23&quot; ## [177] &quot;gray24&quot; &quot;gray25&quot; &quot;gray26&quot; &quot;gray27&quot; ## [181] &quot;gray28&quot; &quot;gray29&quot; &quot;gray30&quot; &quot;gray31&quot; ## [185] &quot;gray32&quot; &quot;gray33&quot; &quot;gray34&quot; &quot;gray35&quot; ## [189] &quot;gray36&quot; &quot;gray37&quot; &quot;gray38&quot; &quot;gray39&quot; ## [193] &quot;gray40&quot; &quot;gray41&quot; &quot;gray42&quot; &quot;gray43&quot; ## [197] &quot;gray44&quot; &quot;gray45&quot; &quot;gray46&quot; &quot;gray47&quot; ## [201] &quot;gray48&quot; &quot;gray49&quot; &quot;gray50&quot; &quot;gray51&quot; ## [205] &quot;gray52&quot; &quot;gray53&quot; &quot;gray54&quot; &quot;gray55&quot; ## [209] &quot;gray56&quot; &quot;gray57&quot; &quot;gray58&quot; &quot;gray59&quot; ## [213] &quot;gray60&quot; &quot;gray61&quot; &quot;gray62&quot; &quot;gray63&quot; ## [217] &quot;gray64&quot; &quot;gray65&quot; &quot;gray66&quot; &quot;gray67&quot; ## [221] &quot;gray68&quot; &quot;gray69&quot; &quot;gray70&quot; &quot;gray71&quot; ## [225] &quot;gray72&quot; &quot;gray73&quot; &quot;gray74&quot; &quot;gray75&quot; ## [229] &quot;gray76&quot; &quot;gray77&quot; &quot;gray78&quot; &quot;gray79&quot; ## [233] &quot;gray80&quot; &quot;gray81&quot; &quot;gray82&quot; &quot;gray83&quot; ## [237] &quot;gray84&quot; &quot;gray85&quot; &quot;gray86&quot; &quot;gray87&quot; ## [241] &quot;gray88&quot; &quot;gray89&quot; &quot;gray90&quot; &quot;gray91&quot; ## [245] &quot;gray92&quot; &quot;gray93&quot; &quot;gray94&quot; &quot;gray95&quot; ## [249] &quot;gray96&quot; &quot;gray97&quot; &quot;gray98&quot; &quot;gray99&quot; ## [253] &quot;gray100&quot; &quot;green&quot; &quot;green1&quot; &quot;green2&quot; ## [257] &quot;green3&quot; &quot;green4&quot; &quot;greenyellow&quot; &quot;grey&quot; ## [261] &quot;grey0&quot; &quot;grey1&quot; &quot;grey2&quot; &quot;grey3&quot; ## [265] &quot;grey4&quot; &quot;grey5&quot; &quot;grey6&quot; &quot;grey7&quot; ## [269] &quot;grey8&quot; &quot;grey9&quot; &quot;grey10&quot; &quot;grey11&quot; ## [273] &quot;grey12&quot; &quot;grey13&quot; &quot;grey14&quot; &quot;grey15&quot; ## [277] &quot;grey16&quot; &quot;grey17&quot; &quot;grey18&quot; &quot;grey19&quot; ## [281] &quot;grey20&quot; &quot;grey21&quot; &quot;grey22&quot; &quot;grey23&quot; ## [285] &quot;grey24&quot; &quot;grey25&quot; &quot;grey26&quot; &quot;grey27&quot; ## [289] &quot;grey28&quot; &quot;grey29&quot; &quot;grey30&quot; &quot;grey31&quot; ## [293] &quot;grey32&quot; &quot;grey33&quot; &quot;grey34&quot; &quot;grey35&quot; ## [297] &quot;grey36&quot; &quot;grey37&quot; &quot;grey38&quot; &quot;grey39&quot; ## [301] &quot;grey40&quot; &quot;grey41&quot; &quot;grey42&quot; &quot;grey43&quot; ## [305] &quot;grey44&quot; &quot;grey45&quot; &quot;grey46&quot; &quot;grey47&quot; ## [309] &quot;grey48&quot; &quot;grey49&quot; &quot;grey50&quot; &quot;grey51&quot; ## [313] &quot;grey52&quot; &quot;grey53&quot; &quot;grey54&quot; &quot;grey55&quot; ## [317] &quot;grey56&quot; &quot;grey57&quot; &quot;grey58&quot; &quot;grey59&quot; ## [321] &quot;grey60&quot; &quot;grey61&quot; &quot;grey62&quot; &quot;grey63&quot; ## [325] &quot;grey64&quot; &quot;grey65&quot; &quot;grey66&quot; &quot;grey67&quot; ## [329] &quot;grey68&quot; &quot;grey69&quot; &quot;grey70&quot; &quot;grey71&quot; ## [333] &quot;grey72&quot; &quot;grey73&quot; &quot;grey74&quot; &quot;grey75&quot; ## [337] &quot;grey76&quot; &quot;grey77&quot; &quot;grey78&quot; &quot;grey79&quot; ## [341] &quot;grey80&quot; &quot;grey81&quot; &quot;grey82&quot; &quot;grey83&quot; ## [345] &quot;grey84&quot; &quot;grey85&quot; &quot;grey86&quot; &quot;grey87&quot; ## [349] &quot;grey88&quot; &quot;grey89&quot; &quot;grey90&quot; &quot;grey91&quot; ## [353] &quot;grey92&quot; &quot;grey93&quot; &quot;grey94&quot; &quot;grey95&quot; ## [357] &quot;grey96&quot; &quot;grey97&quot; &quot;grey98&quot; &quot;grey99&quot; ## [361] &quot;grey100&quot; &quot;honeydew&quot; &quot;honeydew1&quot; &quot;honeydew2&quot; ## [365] &quot;honeydew3&quot; &quot;honeydew4&quot; &quot;hotpink&quot; &quot;hotpink1&quot; ## [369] &quot;hotpink2&quot; &quot;hotpink3&quot; &quot;hotpink4&quot; &quot;indianred&quot; ## [373] &quot;indianred1&quot; &quot;indianred2&quot; &quot;indianred3&quot; &quot;indianred4&quot; ## [377] &quot;ivory&quot; &quot;ivory1&quot; &quot;ivory2&quot; &quot;ivory3&quot; ## [381] &quot;ivory4&quot; &quot;khaki&quot; &quot;khaki1&quot; &quot;khaki2&quot; ## [385] &quot;khaki3&quot; &quot;khaki4&quot; &quot;lavender&quot; &quot;lavenderblush&quot; ## [389] &quot;lavenderblush1&quot; &quot;lavenderblush2&quot; &quot;lavenderblush3&quot; &quot;lavenderblush4&quot; ## [393] &quot;lawngreen&quot; &quot;lemonchiffon&quot; &quot;lemonchiffon1&quot; &quot;lemonchiffon2&quot; ## [397] &quot;lemonchiffon3&quot; &quot;lemonchiffon4&quot; &quot;lightblue&quot; &quot;lightblue1&quot; ## [401] &quot;lightblue2&quot; &quot;lightblue3&quot; &quot;lightblue4&quot; &quot;lightcoral&quot; ## [405] &quot;lightcyan&quot; &quot;lightcyan1&quot; &quot;lightcyan2&quot; &quot;lightcyan3&quot; ## [409] &quot;lightcyan4&quot; &quot;lightgoldenrod&quot; &quot;lightgoldenrod1&quot; &quot;lightgoldenrod2&quot; ## [413] &quot;lightgoldenrod3&quot; &quot;lightgoldenrod4&quot; &quot;lightgoldenrodyellow&quot; &quot;lightgray&quot; ## [417] &quot;lightgreen&quot; &quot;lightgrey&quot; &quot;lightpink&quot; &quot;lightpink1&quot; ## [421] &quot;lightpink2&quot; &quot;lightpink3&quot; &quot;lightpink4&quot; &quot;lightsalmon&quot; ## [425] &quot;lightsalmon1&quot; &quot;lightsalmon2&quot; &quot;lightsalmon3&quot; &quot;lightsalmon4&quot; ## [429] &quot;lightseagreen&quot; &quot;lightskyblue&quot; &quot;lightskyblue1&quot; &quot;lightskyblue2&quot; ## [433] &quot;lightskyblue3&quot; &quot;lightskyblue4&quot; &quot;lightslateblue&quot; &quot;lightslategray&quot; ## [437] &quot;lightslategrey&quot; &quot;lightsteelblue&quot; &quot;lightsteelblue1&quot; &quot;lightsteelblue2&quot; ## [441] &quot;lightsteelblue3&quot; &quot;lightsteelblue4&quot; &quot;lightyellow&quot; &quot;lightyellow1&quot; ## [445] &quot;lightyellow2&quot; &quot;lightyellow3&quot; &quot;lightyellow4&quot; &quot;limegreen&quot; ## [449] &quot;linen&quot; &quot;magenta&quot; &quot;magenta1&quot; &quot;magenta2&quot; ## [453] &quot;magenta3&quot; &quot;magenta4&quot; &quot;maroon&quot; &quot;maroon1&quot; ## [457] &quot;maroon2&quot; &quot;maroon3&quot; &quot;maroon4&quot; &quot;mediumaquamarine&quot; ## [461] &quot;mediumblue&quot; &quot;mediumorchid&quot; &quot;mediumorchid1&quot; &quot;mediumorchid2&quot; ## [465] &quot;mediumorchid3&quot; &quot;mediumorchid4&quot; &quot;mediumpurple&quot; &quot;mediumpurple1&quot; ## [469] &quot;mediumpurple2&quot; &quot;mediumpurple3&quot; &quot;mediumpurple4&quot; &quot;mediumseagreen&quot; ## [473] &quot;mediumslateblue&quot; &quot;mediumspringgreen&quot; &quot;mediumturquoise&quot; &quot;mediumvioletred&quot; ## [477] &quot;midnightblue&quot; &quot;mintcream&quot; &quot;mistyrose&quot; &quot;mistyrose1&quot; ## [481] &quot;mistyrose2&quot; &quot;mistyrose3&quot; &quot;mistyrose4&quot; &quot;moccasin&quot; ## [485] &quot;navajowhite&quot; &quot;navajowhite1&quot; &quot;navajowhite2&quot; &quot;navajowhite3&quot; ## [489] &quot;navajowhite4&quot; &quot;navy&quot; &quot;navyblue&quot; &quot;oldlace&quot; ## [493] &quot;olivedrab&quot; &quot;olivedrab1&quot; &quot;olivedrab2&quot; &quot;olivedrab3&quot; ## [497] &quot;olivedrab4&quot; &quot;orange&quot; &quot;orange1&quot; &quot;orange2&quot; ## [501] &quot;orange3&quot; &quot;orange4&quot; &quot;orangered&quot; &quot;orangered1&quot; ## [505] &quot;orangered2&quot; &quot;orangered3&quot; &quot;orangered4&quot; &quot;orchid&quot; ## [509] &quot;orchid1&quot; &quot;orchid2&quot; &quot;orchid3&quot; &quot;orchid4&quot; ## [513] &quot;palegoldenrod&quot; &quot;palegreen&quot; &quot;palegreen1&quot; &quot;palegreen2&quot; ## [517] &quot;palegreen3&quot; &quot;palegreen4&quot; &quot;paleturquoise&quot; &quot;paleturquoise1&quot; ## [521] &quot;paleturquoise2&quot; &quot;paleturquoise3&quot; &quot;paleturquoise4&quot; &quot;palevioletred&quot; ## [525] &quot;palevioletred1&quot; &quot;palevioletred2&quot; &quot;palevioletred3&quot; &quot;palevioletred4&quot; ## [529] &quot;papayawhip&quot; &quot;peachpuff&quot; &quot;peachpuff1&quot; &quot;peachpuff2&quot; ## [533] &quot;peachpuff3&quot; &quot;peachpuff4&quot; &quot;peru&quot; &quot;pink&quot; ## [537] &quot;pink1&quot; &quot;pink2&quot; &quot;pink3&quot; &quot;pink4&quot; ## [541] &quot;plum&quot; &quot;plum1&quot; &quot;plum2&quot; &quot;plum3&quot; ## [545] &quot;plum4&quot; &quot;powderblue&quot; &quot;purple&quot; &quot;purple1&quot; ## [549] &quot;purple2&quot; &quot;purple3&quot; &quot;purple4&quot; &quot;red&quot; ## [553] &quot;red1&quot; &quot;red2&quot; &quot;red3&quot; &quot;red4&quot; ## [557] &quot;rosybrown&quot; &quot;rosybrown1&quot; &quot;rosybrown2&quot; &quot;rosybrown3&quot; ## [561] &quot;rosybrown4&quot; &quot;royalblue&quot; &quot;royalblue1&quot; &quot;royalblue2&quot; ## [565] &quot;royalblue3&quot; &quot;royalblue4&quot; &quot;saddlebrown&quot; &quot;salmon&quot; ## [569] &quot;salmon1&quot; &quot;salmon2&quot; &quot;salmon3&quot; &quot;salmon4&quot; ## [573] &quot;sandybrown&quot; &quot;seagreen&quot; &quot;seagreen1&quot; &quot;seagreen2&quot; ## [577] &quot;seagreen3&quot; &quot;seagreen4&quot; &quot;seashell&quot; &quot;seashell1&quot; ## [581] &quot;seashell2&quot; &quot;seashell3&quot; &quot;seashell4&quot; &quot;sienna&quot; ## [585] &quot;sienna1&quot; &quot;sienna2&quot; &quot;sienna3&quot; &quot;sienna4&quot; ## [589] &quot;skyblue&quot; &quot;skyblue1&quot; &quot;skyblue2&quot; &quot;skyblue3&quot; ## [593] &quot;skyblue4&quot; &quot;slateblue&quot; &quot;slateblue1&quot; &quot;slateblue2&quot; ## [597] &quot;slateblue3&quot; &quot;slateblue4&quot; &quot;slategray&quot; &quot;slategray1&quot; ## [601] &quot;slategray2&quot; &quot;slategray3&quot; &quot;slategray4&quot; &quot;slategrey&quot; ## [605] &quot;snow&quot; &quot;snow1&quot; &quot;snow2&quot; &quot;snow3&quot; ## [609] &quot;snow4&quot; &quot;springgreen&quot; &quot;springgreen1&quot; &quot;springgreen2&quot; ## [613] &quot;springgreen3&quot; &quot;springgreen4&quot; &quot;steelblue&quot; &quot;steelblue1&quot; ## [617] &quot;steelblue2&quot; &quot;steelblue3&quot; &quot;steelblue4&quot; &quot;tan&quot; ## [621] &quot;tan1&quot; &quot;tan2&quot; &quot;tan3&quot; &quot;tan4&quot; ## [625] &quot;thistle&quot; &quot;thistle1&quot; &quot;thistle2&quot; &quot;thistle3&quot; ## [629] &quot;thistle4&quot; &quot;tomato&quot; &quot;tomato1&quot; &quot;tomato2&quot; ## [633] &quot;tomato3&quot; &quot;tomato4&quot; &quot;turquoise&quot; &quot;turquoise1&quot; ## [637] &quot;turquoise2&quot; &quot;turquoise3&quot; &quot;turquoise4&quot; &quot;violet&quot; ## [641] &quot;violetred&quot; &quot;violetred1&quot; &quot;violetred2&quot; &quot;violetred3&quot; ## [645] &quot;violetred4&quot; &quot;wheat&quot; &quot;wheat1&quot; &quot;wheat2&quot; ## [649] &quot;wheat3&quot; &quot;wheat4&quot; &quot;whitesmoke&quot; &quot;yellow&quot; ## [653] &quot;yellow1&quot; &quot;yellow2&quot; &quot;yellow3&quot; &quot;yellow4&quot; ## [657] &quot;yellowgreen&quot; 24.2.6 Describe Interval &amp; Ratio (Continuous) Variables We can describe variables with interval or ratio measurement scales (i.e., continuous variables) by computing measures of central tendency (e.g., mean, median) and dispersion (e.g., standard deviation, range); however, its often good practice to begin by creating data visualizations (e.g., histograms, box plots) that will enable us to understand the nature of each variables distribution. 24.2.6.1 Create Data Visualizations By visualizing the shape of a continuous variables distribution (e.g., normal distribution, positive skew, negative skew), we can make a more informed decision regarding how to select, interpret, and report measures of central tendency and dispersion. In this section, well focus on creating histograms and box plots. Create Histograms: A histogram visually approximates the distribution of a set of numerical scores. The scores are grouped into ranges (which by default are often equally sized), and the boundaries of these ranges are referred to as breaks or break points. The bars in a histogram fill these ranges, and their heights represent the frequency (i.e., count) of sources within each range. Lets begin with the Age variable. To create a histogram, we can use the hist function from base R. To get things started, lets enter a single argument: the name of the data frame object (demo), followed by the $ operator and the name of the variable we wish to visualize (Age). # Create a histogram hist(demo$Age) This histogram will do just fine for our purposes. Note that the histogram indicates that the scores from the Age variable appear to be roughly normally distributed. With smaller sample sizes (e.g., fewer than 30 observations or cases), were less likely to see a clean, normal distribution of scores, and this relates to the central limit theorem; though, an explanation of this theorem is beyond the scope of this tutorial. Nevertheless, the take-home message is that histograms provide rough approximations of the shapes of distributions, and a normal distribution is less likely when their are fewer observations (i.e., a smaller sample) and thus fewer scores on a variable. For your own internal data-exploration purposes, it is often fine to create a simple histogram like the one we created above, meaning that you would not need to worry about the aesthetics (e.g., size, color) of the histogram. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. As optional next steps, you can play around with arguments to adjust the y-axis limits (ylim), x-axis label (xlab), y-axis label (ylab), main title (main), and the bar color (col). [If youd like to explore additional colors, check out this website: https://www.r-graph-gallery.com/colors.html. Or, you can run the colors() function (without any arguments), and youll get a (huge) list of the color options.] A more in-depth description of these plot arguments is provided in the section above called Create Bar Charts. # Create a histogram and add style hist(demo$Age, ylim=c(0, 15), # y-axis limits xlab=&quot;Employee Age&quot;, # x-axis label ylab=&quot;Count&quot;, # y-axis label main=NULL, # main title col=&quot;dodgerblue&quot;) # bar color We can also specify a vector of the break points between the bars using the c function from base R. Just be sure that the lowest value in your vector is equal to or less than the minimum value for the variable and the the highest value is equal to or greater than the maximum value for the variable. To do so, we can add the breaks argument. # Create a histogram and add style hist(demo$Age, ylim=c(0, 25), # y-axis limits xlab=&quot;Employee Age&quot;, # x-axis label ylab=&quot;Count&quot;, # y-axis label main=NULL, # remove main title col=&quot;dodgerblue&quot;, # bar color breaks=c(20, 25, 30, 35)) # set break points between bars Create Box Plots: We could use a histogram to visualize the Performance variable, but lets use this opportunity to create a box plot instead. Like a histogram, a box plot (sometimes called a box and whiskers plot) also reveals information about the shape of a distribution, including the median, 25th percentile (i.e., lower quartile), 75th percentile (i.e., upper quartile), and the variation outside the 25th and 75th percentiles. Well use the boxplot function from base R. To kick things off, lets enter a single argument: the name of the data frame object (demo), followed by the $ operator and the name of the variable we wish to visualize (Performance). # Create a box plot boxplot(demo$Performance) The thick horizontal line in the middle of the box is the median score, the lower edge of the box represents the lower quartile (i.e., 25th percentile, median of lower half of the distribution), and the upper edge of the box represents the upper quartile (i.e., 75th percentile, median of the upper half of the distribution). The height of the box is the interquartile range. By default, the boxplot function sets the upper whisker (i.e., the horizontal line at the top of the upper dashed line) as the smaller of two values: the maximum value or 1.5 times the interquartile range. Further, the function sets the lower whisker (i.e., the horizontal line at the bottom of the lower dashed line) as the larger of two values: the minimum value or 1.5 times the interquartile range. In the box plot for Performance, we can see that the distribution of scores appears to be slightly negatively skewed, as the upper quartile is smaller than the lower quartile (i.e., the median is closer to the top of the box) and the upper whisker is shorter than the lower whisker. If there had been any outlier scores, these would appear beyond the upper and lower limits of the whiskers. If you plan to create a box plot for your own data-exploration purposes only, it is often fine to create a simple box plot like the one we created above, which means you would not need to proceed forward with subsequent steps in which I show how to refine the aesthetics of the box plot. If you want, you can export this plot as a PDF or PNG image file, or you can copy it and paste it in another document. To do so, just click on the Export button in the Plots window, which should appear in the lower right of your RStudio interface. As optional next steps, you can play around with arguments to adjust the y-axis label (ylab) and the box color (col). If youd like to explore additional colors, check out this website. Or, you can run the colors() function (without any arguments), and youll get a (huge) list of the color options. # Create a box plot and add style boxplot(demo$Performance, ylab=&quot;Employee Job Performance&quot;, # y-axis label col=&quot;orange&quot;) # bar color 24.2.6.2 Compute Measures of Central Tendency &amp; Dispersion Now that weve visualized our interval and ratio measurement scale variables, were ready to compute some measures of central tendency and dispersion. In R the process is quite straightforward, as the function names are fairly intuitive: mean (mean), var (variance), sd (standard deviation), median (median), min (minimum), max (maximum), range (range), and IQR (interquartile range). Within each functions parentheses, you will enter the same arguments. Specifically, you should include the name of the data frame (demo), followed by the $ operator and the name of the variable ofese measures of central tendency even if there are missing data for the varia interest (Age). Keep the na.rm=TRUE argument as is if you would like to calculate the variable of interest. Lets start with some measures of central tendency for the Age variable, specifically the mean (mean) and median (median). # Mean of Age mean(demo$Age, na.rm=TRUE) ## [1] 28 # Median of Age median(demo$Age, na.rm=TRUE) ## [1] 28 As you can, see both the median and the mode happen to be 28, which indicates that center of the Age distribution is about 28 years. Should we have a skewed distribution (positive or negative), the median is often a better indicator of central tendency given that it is less susceptible to influential cases (e.g., outliers). A class example of a skewed distribution in organizations involves pay variables, especially when executive pay is included. In U.S. organizations, executive pay often is far greater than average workers pay, which often leads us to report the median pay as an indicator of central tendency. Lets move on to some measures of dispersion, specifically the variance (var) and standard deviation (sd). # Variance of Age var(demo$Age, na.rm=TRUE) ## [1] 7.103448 # Standard deviation (SD) of Age sd(demo$Age, na.rm=TRUE) ## [1] 2.665229 The variance is a nonstandardized indicator of dispersion or variation, so we typically interpret the square root of the variance, which is called the standard deviation. Given that we found a mean age of 28 years for this sample of employees, the standard deviation of approximately 2.67 years indicates that approximately 68% of employees ages fall within 2.67 years (i.e., 1 SD) of 28 years (i.e., between 25.33 and 30.67 years), and 95% of employees ages fall within 5.34 years (i.e., 2 SD) of 28 years (i.e., between 22.66 and 33.34 years). As we saw in the histogram for Age, the variable has a roughly normal distribution. Lets compute the minimum and maximum score for Age using the min and max functions, respectively. # Minimum of Age min(demo$Age, na.rm=TRUE) ## [1] 22 # Maximum of Age max(demo$Age, na.rm=TRUE) ## [1] 34 The minimum age is 22 years for this sample, and the maximum age is 34 years. Next lets compute the range, which will give us the minimum and maximum scores using a single function. # Range of Age range(demo$Age, na.rm=TRUE) ## [1] 22 34 As you can see, the range functions provides both the minimum and maximum scores. Next, lets compute the interquartile range (IQR), which is the distance between the lower and upper quartiles (i.e., between the 25th and 75th percentile). As noted above in the section called Create Box Plots, the lower and upper quartiles correspond to the outer edges of the box, whereas the median (50th percentile) corresponds to the line within the box. # Interquartile range (IQR) of Age IQR(demo$Age, na.rm=TRUE) ## [1] 3 The IQR is 3 years, which indicates that middle 50% of ages spans 3 years. As a follow-up, lets compute the lower and upper quartiles (i.e., between the 25th and 75th percentiles) by using the quantile function from base R. As the first argument, type the name of the data frame (demo), followed by the $ operator and the name of the variable of interest (Age). As the second argument, type .25 if you would like to request the 25th percentile (lower quartile) and .75 if you would like to request the 75th percentile (upper quartile). Lets do both. # Request specific quartiles/percentiles quantile(demo$Age, .25) # lower quartile / 25th percentile ## 25% ## 27 quantile(demo$Age, .75) # upper quartile / 75th percentile ## 75% ## 30 Corroborating what we found with the IQR, the difference between the upper and lower quartiles is 3 years (30 - 27 = 3). The IQR and lower and upper quartiles are typically reported along with the median (as evidenced by the box plot we created above), so lets report them together. If you recall, the median age was 28 years for this sample, and the IQR spans 3 years from 27 years to 30 years. These measures indicate that the middle 50% of ages for this sample are between 27 and 30 years, and that the middle-most age (i.e., 50th percentile) is 28 years. Alternatively, if we wish to automatically compute the 0th, 25th, 50th, 75th, and 100th percentile all at once, we can simply type the name of the quantile function and then enter the name of the data frame object (df) followed by the $ operator and the name of the variable (Age). # Request 0, 25, 50, 75, and 100 percentiles quantile(demo$Age) ## 0% 25% 50% 75% 100% ## 22 27 28 30 34 Finally, one way to compute the minimum, lower quartile (1st quartile), median, mean, upper quartile (3rd quartile), and maximum all at once is to use the summary function from base R with the name of the data frame object (demo) followed by the $ operator and the name of the variable (Age) as the sole parenthetical argument. # Minimum, lower quartile (1st quartile), median, mean, upper quartile (3rd quartile), and maximum summary(demo$Age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 22 27 28 28 30 34 24.2.7 Summary In this chapter, we focused on descriptive statistics. First, we began by learning about four different measurements scales (i.e., nominal, ordinal, interval, ratio) and how identifying the measurement scale of a variable is an important first step in determining an appropriate descriptive statistic or data-visualization display type. Second, we learned how to compute counts (i.e., frequencies) for nominal and ordinal variables using the table function from base R. Further, you learned how to convert a variable to an ordered factor using the factor function from base R. Finally, you learned how to visualize counts data using the barplot function from base R. Finally, we learned how to visualize the distribution of a variable with an interval or ratio measurement scale using histograms (hist function from base R) and box plots (boxplot function from base R). In addition, we learned how to compute measures of central tendency and dispersion base R functions like mean (mean), var (variance), sd (standard deviation), median (median), min (minimum), max (maximum), range (range), and IQR (interquartile range). 24.3 Chapter Supplement In this chapter supplement, we will learn how to compute the coefficient of variation (CV). 24.3.1 Functions &amp; Packages Introduced Function Package mean base R sd base R 24.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demo &lt;- read_csv(&quot;employee_demo.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_character(), ## Facility = col_character(), ## Education = col_character(), ## Performance = col_double(), ## Age = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(demo) ## [1] &quot;EmpID&quot; &quot;Facility&quot; &quot;Education&quot; &quot;Performance&quot; &quot;Age&quot; 24.3.3 Compute Coefficient of Variation (CV) The coefficient of variation (CV) (also known as relative standard deviation) is a standardized indicator of dispersion that can be used to compare the relative variability of two or more variables with different scaling. Technically, it is really only appropriate and meaningful to compute the CV for variables that have a ratio measurement scale and thus a meaningful zero; however, sometimes people relax this assumption (albeit inappropriately) to allow for the CV to be computed for a variable with an interval measurement scale. I urge you to only compute CVs for variables with ratio measurement scales. As a hypothetical application of the CV, imagine you would like to compare the variability of these two measures  both having a ratio measurement scale: (a) monthly base pay measured in US dollars, and (b) monthly variable pay measured in US dollars. The formula to compute the CV for a variable is simple. In fact, its just the ratio of a variables standard deviation (SD) relative to its mean, which results in a proportion. If we multiply that proportion by 100, we can interpret the CV as a percentage. \\(CV = \\frac{SD}{mean} * 100\\) Lets imagine that for a given sample of employees the mean monthly base pay is 3,553 dollars, and the SD is 593 dollars. Further, the mean monthly variable pay for these same employees is 422 dollars, and the SD is 98 dollars. Lets compute the CV for each measure and then compare. \\(CV_{basepay} = \\frac{593}{3553} * 100 = 16.7\\) \\(CV_{variablepay} = \\frac{98}{422} * 100 = 23.2\\) Note that the CV for monthly base pay is 16.7%, and the CV for the monthly variable pay is 23.2%. We can interpret these descriptively as indicating that monthly variable pay shows higher variability around its mean relative to monthly base pay. In other words, monthly variable pay shows higher relative dispersion than monthly base pay. Its important to note that comparing CVs in this way is entirely descriptive, which means that we cannot conclude that the two CVs differ significantly from one another in a statistical sense; to make such a conclusion, we would need to estimate an appropriate inferential statistical analysis (Feltz and Miller 1996; Lewontin 1966; Miller 1991). Alternatively, CVs can be computed to compare the relative variability of the same measure assessed with two independent samples. For example, in a clinical setting, the CV for a measure can be computed for each clinical trial sample in which it was administered to evaluate whether its appropriate to combine data from multiple samples. Now that we understand what a coefficient of variation is, lets practice computing one by using the data frame we read in called called demo. Note that both the Performance and Age variables can be described as having interval and ratio measurement scales, respectively. Lets begin by computing the coefficient of variation (CV) for the Performance variable. As noted in the introduction, the formula is simply a ratio, such that we divide the standard deviation (SD) for the measure by the mean for that measure. We can then convert the resulting proportion to a percentage by multiplying the proportion by 100. To compute the SD, well use the sd function from base R, and to compute the mean, well use the mean function from base R. Within each function, we enter the name of the data frame object (demo) followed by the $ operator and the name of the variable in question that belongs to the aforementioned data frame object. To divide we use the forward slash (/), and to multiply we use the asterisk (*), as shown below. # Compute coefficient of variation (CV) for Performance variable sd(demo$Performance) / mean(demo$Performance) * 100 ## [1] 21.15746 In the Console, we should see that the CV for the Performance variable is approximately 21.2%. Next, lets compute the CV for the Age variable. Well use the same formula as above, except swap out the Performance variable for the Age variable. # Compute coefficient of variation (CV) for Age variable sd(demo$Age) / mean(demo$Age) * 100 ## [1] 9.518677 In the Console, we should see that the CV for the Age variable is approximately 9.5%. As noted in the introduction, we are being descriptive in our comparisons in this tutorial and are not applying an inferential statistical analysis. Given that, we cannot make statements indicating that one CV is significantly larger than the other. To make such a statement, we would need to apply an inferential statistical analysis (Lewontin 1966; Miller 1991), which is beyond the scope of this chapter on descriptive statistics. "],["crosstabs.html", "Chapter 25 Summarizing Two or More Categorical Variables Using Cross-Tabulations 25.1 Conceptual Overview 25.2 Tutorial", " Chapter 25 Summarizing Two or More Categorical Variables Using Cross-Tabulations In this chapter, we will learn about how to summarize two or three categorical (nominal, ordinal) employee-demographic variables using cross-tabulations, and well conclude the chapter with a tutorial. 25.1 Conceptual Overview In this section, well review the purpose of cross-tabulations and how they can be useful for summarizing data from two or three categorical (nominal, ordinal) variables, followed by a sample-write up of cross-tabulation results. 25.1.1 Review of Cross-Tabulation A cross-tabulation is a specific type of table. A table in its simplest form is simply an object in which data are stored in rows and columns, and sometimes a table is referred to as a tabular display in the context of data visualization. Broadly speaking, in the R environment you can think of a data frame as a specific type of data table. When we create a table involving two or more categorical variables, we often refer to the the table as a cross-tabulation (cross-tabs). A cross-tabulation can be described more specifically as the process of creating a table from two or more categorical variables (i.e., dimensions), which frequencies (i.e., counts) of observations are displayed per combination of variable categories or levels. The table resulting from a cross tabulation is sometimes referred to as a contingency table. Cross-tabulation is a relatively simple way in which we can summarize data, but it serves as the foundation for the chi-square test of independence and allows for deriving insights via segmentation. Cross-tabulation is often most useful when the variables involved are nominal or ordinal. In this chapter, we focus on creating cross-tabulations to describe or summarize frequency (i.e., count) data from one or more categorical (i.e., nominal, ordinal) variables. Specifically, we will learn how to create different types of two-way and three-way cross-tabulations, where two-way implies that we are summarizing two variables and three-way implies that we are summarizing three variables. As a side note, the Aggregating &amp; Segmenting Data chapter provides additional approaches for creating descriptive or summary data tables (well, technically tibbles) using functions from the dplyr package; in that chapter, you can learn how to create tables when one variable is continuous (i.e., interval ratio) and one variable is categorical. Finally, please note that there is a package called data.table that has functions that allow one to convert data frames to a special kind of data table object that allows for faster and enhanced manipulations; if youre interested, follow this link to learn more. 25.1.2 Sample Write-Up Based on data stored in the organizations HR information system, we sought out to describe the organizations employee demographics. The employee gender and race/ethnicity variables have nominal measurement scales, and thus we computed counts to describe these variables. Specifically, 321 employees identified as women and 300 as men. With respect to race/ethnicity, 192 employees identified as Hispanic/Latino and 429 as White. To describe how the gender and race/ethnicity variables relate to one another, we computed a two-way cross-tabulation. The results indicated that 77 (24%) women identified as Hispanic/Latino, and 244 (76%) women identified as White. Additionally, 115 (38%) men identified as Hispanic/Latino, and 185 (62%) identified as White. 25.2 Tutorial This chapters tutorial demonstrates how to compute cross-tabulations for combinations of two and three categorical (nominal, ordinal) variables. 25.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/Ja_CM253oDQ 25.2.2 Functions &amp; Packages Introduced Function Package table base R prop.table base R round base R ftable base R xtabs base R CrossTable gmodels 25.2.3 Initial Steps If you havent already, save the file called EmployeeDemographics.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called EmployeeDemographics.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demodata &lt;- read_csv(&quot;EmployeeDemographics.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## OrgTenureYrs_2019 = col_double(), ## JobLevel = col_double(), ## Sex = col_character(), ## RaceEthnicity = col_character(), ## AgeYrs_2019 = col_double(), ## Veteran = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(demodata) ## [1] &quot;EmployeeID&quot; &quot;OrgTenureYrs_2019&quot; &quot;JobLevel&quot; &quot;Sex&quot; &quot;RaceEthnicity&quot; ## [6] &quot;AgeYrs_2019&quot; &quot;Veteran&quot; Note in the data frame that the EmployeeID variable (i.e., column, field) is a unique identifier variable, and each row contains an individual employees demographic data on the following variables: organizational tenure (OrgTenureYrs_2019), job level (JobLevel), sex (Sex), race/ethnicity (RaceEthnicity), age (AgeYrs_2019), and veteran status (Veteran). 25.2.4 Two-Way Cross-Tabulation A two-way cross-tabulation summarizes the association between two categorical (i.e., nominal, ordinal) variables. Using the data found in the data frame we named demodata, we will begin by creating two-way cross-tabulations using the categorical JobLevel and Sex variables. Ill demonstrate how to create two-way cross-tabulations using three different functions (i.e., table, xtabs, CrossTable), and you can choose to follow along with all three or just one or two. 25.2.4.1 Option 1: Using the table Function Using the table function from base R, lets create a two-way cross-tabulation table containing frequencies based on the JobLevel and Sex variables contained in the demodata data frame object. To begin, type the name of the table function. As the first argument in the function, specify the name of the data frame object (demodata) followed by the $ operator and the name of the first variable (JobLevel). As the second argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the second variable (Sex). # Create two-way cross-tabulation table from JobLevel and Sex variables table(demodata$JobLevel, demodata$Sex) ## ## Female Male ## 1 47 35 ## 2 22 22 ## 3 7 11 ## 4 3 7 ## 5 1 8 As you can see, the levels of the JobLevel variable (i.e., 1-5) appear as the row labels, and the categories of the Sex variable (i.e., Female, Male) appear as the column labels. If we wanted the categories of the Sex variable to appear as the row labels and the levels of the JobLevel variable to appear as the column labels, we would reverse the order of the two variables in our table function. Each cell in the cross-tabulation table contains the frequency (i.e., count) of employees who are in the intersecting categories. For example, the table shows that 47 female employees are in job level 1, whereas 35 male employees are in job level 2. Because there are five levels associated with the JobLevel variable (i.e., 1-5) and two levels associated with the Sex variable (i.e., Female, Male), the 5x2 cross-tabulation table has a total of 10 cells. Using the same code as above, lets assign the cross-tabulation table we created to an object that well (arbitrarily) call table_2. Well use the &lt;- operator to do this. # Assign two-way cross-tabulation table to object table_2 &lt;- table(demodata$JobLevel, demodata$Sex) Using the cross-tabulation table object we created above (table_2), we can apply the prop.table function from base R to estimate the proportions in each table cell. To calculate the cell proportions, simply type the name of the prop.table function, and as the only parenthetical argument, type the name of the cross-tabulation table object we created above (table_2). # Present the cell proportions for the cross-tabulation table prop.table(table_2) ## ## Female Male ## 1 0.288343558 0.214723926 ## 2 0.134969325 0.134969325 ## 3 0.042944785 0.067484663 ## 4 0.018404908 0.042944785 ## 5 0.006134969 0.049079755 When inspecting the cell proportion table displayed above, you might find it challenging to read given that number of decimal places after zero that are reported. To round values to 2 places after the decimal, lets wrap the code from above in the round function from base R. As the first argument, lets copy in our prop.table code from above, and as the second argument, lets type the number of decimals after zero to which we wish to round (e.g., 2). # Round values to 2 places after decimal round(prop.table(table_2), 2) ## ## Female Male ## 1 0.29 0.21 ## 2 0.13 0.13 ## 3 0.04 0.07 ## 4 0.02 0.04 ## 5 0.01 0.05 If you were to sum all of the proportions in the table, you would get a total of 1 (100%). To demonstrate, lets apply the sum function base R by wrapping our prop.table function code in the sum function. # Round values to 2 places after decimal sum(prop.table(table_2)) ## [1] 1 As expected, all of the proportions in the table sum to 1 (100%). What if we wish to compute the proportions by row or by column? Well, to compute those row-by-row or column-by-column proportions, we simply add an argument to the prop.table function. To compute the row proportions, enter 1 as the second argument. # Print the row proportions for the cross-tabulation table prop.table(table_2, 1) ## ## Female Male ## 1 0.5731707 0.4268293 ## 2 0.5000000 0.5000000 ## 3 0.3888889 0.6111111 ## 4 0.3000000 0.7000000 ## 5 0.1111111 0.8888889 In the output, you can see that the proportions in each row now sum to 1 (100%). Now, lets round the row proportions to 2 places after the decimal. # Round cross-tabulation table values to 2 places after decimal round(prop.table(table_2, 1), 2) ## ## Female Male ## 1 0.57 0.43 ## 2 0.50 0.50 ## 3 0.39 0.61 ## 4 0.30 0.70 ## 5 0.11 0.89 Next, lets convert those proportions to percentages by multiplying the previous code by 100. If you recall, the multiplication operator in R is the * symbol. Just be sure to remember that the values presented in the subsequent output represent percentages and not raw frequencies (i.e., counts). # Convert proportions to percentages by multiplying by 100 100 * round(prop.table(table_2, 1), 2) ## ## Female Male ## 1 57 43 ## 2 50 50 ## 3 39 61 ## 4 30 70 ## 5 11 89 We can retain two digits after the decimal by re-specifying the code from above in the following way. # Convert proportions to percentages by multiplying by 100 round(100 * prop.table(table_2, 1), 2) ## ## Female Male ## 1 57.32 42.68 ## 2 50.00 50.00 ## 3 38.89 61.11 ## 4 30.00 70.00 ## 5 11.11 88.89 Alternatively, we can compute the column proportions by typing 2 instead of 1 in the second argument of the prop.table function. As you can see below, each column now adds up to 1 (100%). # Print the row proportions for the cross-tabulation table prop.table(table_2, 2) ## ## Female Male ## 1 0.58750000 0.42168675 ## 2 0.27500000 0.26506024 ## 3 0.08750000 0.13253012 ## 4 0.03750000 0.08433735 ## 5 0.01250000 0.09638554 Now, lets multiply by 100 to convert the proportions to percentages and round to 2 places after the decimal. # Round table values to 2 places after decimal and convert to percentages round(100 * prop.table(table_2, 2), 2) ## ## Female Male ## 1 58.75 42.17 ## 2 27.50 26.51 ## 3 8.75 13.25 ## 4 3.75 8.43 ## 5 1.25 9.64 25.2.4.2 Option 2: Using the xtabs Function The xtabs function from base R can also be used to create a two-way cross-tabulation table. To begin, type the name of the xtabs function. As the first argument in the function parentheses, insert the tilde (~) operator followed by the name of the first variable (JobLevel), the addition (+) operator, and the name of the second variable (Sex). As the second argument, type data= followed by the name of the data frame object two which the aforementioned variables belong (demodata). # Create cross-tabulation table from JobLevel and Sex variables xtabs(~ JobLevel + Sex, data=demodata) ## Sex ## JobLevel Female Male ## 1 47 35 ## 2 22 22 ## 3 7 11 ## 4 3 7 ## 5 1 8 Using the same code as above, lets assign the cross-tabulation table we created to an object that well (arbitrarily) call table_2. Well use the &lt;- operator to do this. # Assign two-way cross-tabulation table to object table_2 &lt;- xtabs(~ JobLevel + Sex, data=demodata) Using the cross-tabulation table object we created above (table_2), we can apply the prop.table function from base R to estimate the proportions in each table cell. To calculate the cell proportions, simply type the name of the prop.table function, and as the only parenthetical argument, type the name of the cross-tabulation table object we created above (table_2). # Print the cell proportions for the cross-tabulation table prop.table(table_2) ## Sex ## JobLevel Female Male ## 1 0.288343558 0.214723926 ## 2 0.134969325 0.134969325 ## 3 0.042944785 0.067484663 ## 4 0.018404908 0.042944785 ## 5 0.006134969 0.049079755 When inspecting the cell proportion table displayed above, you might find it challenging to read given that number of decimal places after zero that are reported. To round values to 2 places after the decimal, lets wrap the code from above in the round function from base R. As the first argument, lets copy in our prop.table code from above, and as the second argument, lets type the number of decimals after zero to which we wish to round (e.g., 2). # Round values to 2 places after decimal round(prop.table(table_2), 2) ## Sex ## JobLevel Female Male ## 1 0.29 0.21 ## 2 0.13 0.13 ## 3 0.04 0.07 ## 4 0.02 0.04 ## 5 0.01 0.05 If you were to sum all of the proportions in the table, you would get a total of 1 (100%). To demonstrate, lets apply the sum function base R by wrapping our prop.table function code in the sum function. # Round values to 2 places after decimal sum(prop.table(table_2)) ## [1] 1 As expected, all of the proportions in the table sum to 1 (100%). What if we wish to compute the proportions by row or by column? Well, to compute those row-by-row or column-by-column proportions, we simply add an argument to the prop.table function. To compute the row proportions, enter 1 as the second argument. # Print the row proportions for the cross-tabulation table prop.table(table_2, 1) ## Sex ## JobLevel Female Male ## 1 0.5731707 0.4268293 ## 2 0.5000000 0.5000000 ## 3 0.3888889 0.6111111 ## 4 0.3000000 0.7000000 ## 5 0.1111111 0.8888889 In the output, you can see that the proportions in each row now sum to 1 (100%). Now, lets round the row proportions to 2 places after the decimal. # Round cross-tabulation table values to 2 places after decimal round(prop.table(table_2, 1), 2) ## Sex ## JobLevel Female Male ## 1 0.57 0.43 ## 2 0.50 0.50 ## 3 0.39 0.61 ## 4 0.30 0.70 ## 5 0.11 0.89 Next, lets convert those proportions to percentages by multiplying the previous code by 100. If you recall, the multiplication operator in R is the * symbol. Just be sure to remember that the values presented in the subsequent output represent percentages and not raw frequencies (i.e., counts). # Convert proportions to percentages by multiplying by 100 100 * round(prop.table(table_2, 1), 2) ## Sex ## JobLevel Female Male ## 1 57 43 ## 2 50 50 ## 3 39 61 ## 4 30 70 ## 5 11 89 Alternatively, we can compute the column proportions by typing 2 instead of 1 in the second argument of the prop.table function. As you can see below, each column now adds up to 1 (100%). # Print the row proportions for the table prop.table(table_2, 2) ## Sex ## JobLevel Female Male ## 1 0.58750000 0.42168675 ## 2 0.27500000 0.26506024 ## 3 0.08750000 0.13253012 ## 4 0.03750000 0.08433735 ## 5 0.01250000 0.09638554 Now, lets round to 2 places after the decimal and multiply by 100 to convert the proportions to percentages. # Round table values to 2 places after decimal and convert to percentages 100 * round(prop.table(table_2, 2), 2) ## Sex ## JobLevel Female Male ## 1 59 42 ## 2 28 27 ## 3 9 13 ## 4 4 8 ## 5 1 10 25.2.4.3 Option 3: Using the CrossTable Function The CrossTable function from the gmodels package is another option when it comes to creating a two-way cross-tabulation table. The function includes multiple arguments that can eliminate additional steps (e.g., rounding) that would be required when using the table or xtabs function from base R. Before using the CrossTable function, we must install and access the gmodels package using the install.packages and library functions, respectively. The downside of the CrossTable function is that it can only be used to create two-way cross-tabulation tables. # Install gmodels package if you haven&#39;t already install.packages(&quot;gmodels&quot;) # Access gmodels package library(gmodels) To create a two-way cross-tabulation table using the JobLevel and Sex variables, type the name of the CrossTable function. As the first argument in the function parentheses, type the name of first variable you wish to use to make the table, and use the $ symbol to indicate that the variable (JobLevel) belongs to the data frame in question (demodata), which should look like this: demodata$JobLevel. As the second argument, type the name of the second variable you wish to use to make the table, and use the $ symbol to indicate that the variable (Sex) belongs to the data frame in question (demodata), which should look like this: demodata$Sex. Make sure you use a comma (,) to separate the two arguments. # Create cross-tabulation table from JobLevel and Sex variables from demodata data frame CrossTable(demodata$JobLevel, demodata$Sex) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 163 ## ## ## | demodata$Sex ## demodata$JobLevel | Female | Male | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 47 | 35 | 82 | ## | 1.134 | 1.093 | | ## | 0.573 | 0.427 | 0.503 | ## | 0.588 | 0.422 | | ## | 0.288 | 0.215 | | ## ------------------|-----------|-----------|-----------| ## 2 | 22 | 22 | 44 | ## | 0.008 | 0.007 | | ## | 0.500 | 0.500 | 0.270 | ## | 0.275 | 0.265 | | ## | 0.135 | 0.135 | | ## ------------------|-----------|-----------|-----------| ## 3 | 7 | 11 | 18 | ## | 0.381 | 0.367 | | ## | 0.389 | 0.611 | 0.110 | ## | 0.087 | 0.133 | | ## | 0.043 | 0.067 | | ## ------------------|-----------|-----------|-----------| ## 4 | 3 | 7 | 10 | ## | 0.742 | 0.715 | | ## | 0.300 | 0.700 | 0.061 | ## | 0.037 | 0.084 | | ## | 0.018 | 0.043 | | ## ------------------|-----------|-----------|-----------| ## 5 | 1 | 8 | 9 | ## | 2.644 | 2.548 | | ## | 0.111 | 0.889 | 0.055 | ## | 0.012 | 0.096 | | ## | 0.006 | 0.049 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 80 | 83 | 163 | ## | 0.491 | 0.509 | | ## ------------------|-----------|-----------|-----------| ## ## The resulting cross-tabulation table is packed with information! Fortunately, there is a key titled Cell Contents that explains how to interpret the value displayed in each row within each cell. As you can see, by default, the table displays the raw frequencies (i.e., counts), the proportions by row, the proportions by column, and the overall cell proportions  and we only had to use a single function! In some cases, you may wish to reduce the amount of information displayed. To find additional documentation, use the help (?) feature for the CrossTable function. # Access background information on function ?CrossTable Once the Help window opens, you can explore the different arguments that can be used within the function to change the default settings. The default number of digits displayed after the decimal is 3, but lets change to 2 by adding the argument digits=2. Next, lets add the following arguments: (a) prop.r=FALSE to hide the row proportions, (b) prop.c=FALSE to hide the column proportions, (c) prop.t=TRUE to keep the total proportions visible, and (d) prop.chisq=FALSE to hide the chi-square contribution of each cell. # Create cross-tabulation table from JobLevel and Sex variables from demodata data frame CrossTable(demodata$JobLevel, demodata$Sex, digits=2, prop.r=FALSE, prop.c=FALSE, prop.t=TRUE, prop.chisq=FALSE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 163 ## ## ## | demodata$Sex ## demodata$JobLevel | Female | Male | Row Total | ## ------------------|-----------|-----------|-----------| ## 1 | 47 | 35 | 82 | ## | 0.29 | 0.21 | | ## ------------------|-----------|-----------|-----------| ## 2 | 22 | 22 | 44 | ## | 0.13 | 0.13 | | ## ------------------|-----------|-----------|-----------| ## 3 | 7 | 11 | 18 | ## | 0.04 | 0.07 | | ## ------------------|-----------|-----------|-----------| ## 4 | 3 | 7 | 10 | ## | 0.02 | 0.04 | | ## ------------------|-----------|-----------|-----------| ## 5 | 1 | 8 | 9 | ## | 0.01 | 0.05 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 80 | 83 | 163 | ## ------------------|-----------|-----------|-----------| ## ## 25.2.5 Three-Way Cross-Tabulation A three-way cross-tabulation table summarizes the association between three categorical (i.e., nominal, ordinal) variables. Using the data found in the data frame we named demodata, we will begin by creating two-way cross-tabulation tables using the categorical JobLevel, Sex, and RaceEthnicity variables. Ill demonstrate how to create three-way cross-tabulation tables using two different functions (i.e., table, xtabs), and you can choose to follow along with all three or just one or two. 25.2.5.1 Option 1: Using the table Function Using the table function from base R, lets create a three-way cross-tabulation table containing frequencies based on the JobLevel, Sex, and RaceEthnicity variables contained in the demodata data frame object. To begin, type the name of the table function. As the first argument in the function, specify the name of the data frame object (demodata) followed by the $ operator and the name of the first variable (JobLevel). As the second argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the second variable (Sex). As the third argument, specify the name of the data frame object (demodata) followed by the $ operator and the name of the third variable (RaceEthnicity). # Create three-way cross-tabulation table from JobLevel, Sex, and RaceEthnicity variables table(demodata$JobLevel, demodata$Sex, demodata$RaceEthnicity) ## , , = Asian ## ## ## Female Male ## 1 14 11 ## 2 6 10 ## 3 1 6 ## 4 0 5 ## 5 0 3 ## ## , , = Black ## ## ## Female Male ## 1 2 0 ## 2 0 1 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## ## , , = HispanicLatino ## ## ## Female Male ## 1 14 14 ## 2 3 5 ## 3 3 2 ## 4 3 2 ## 5 0 3 ## ## , , = White ## ## ## Female Male ## 1 17 10 ## 2 13 6 ## 3 3 3 ## 4 0 0 ## 5 1 2 As you can see, when used to create a three-way cross-tabulation table, the table function creates one two-way cross-tabulation table based on the first two variables for each category or level of the third variable. Lets assign this table to an object that well call table3. # Assign three-way cross-tabulation table to object table_3 &lt;- table(demodata$JobLevel, demodata$Sex, demodata$RaceEthnicity) If you would prefer to view the frequencies (i.e., counts) in a single table, then use the ftable function from base R. Simply type the name of the table object we created in the previous step (table_3) as the sole argument in the ftable function. # Print three-way cross-tabulation table in a different format ftable(table_3) ## Asian Black HispanicLatino White ## ## 1 Female 14 2 14 17 ## Male 11 0 14 10 ## 2 Female 6 0 3 13 ## Male 10 1 5 6 ## 3 Female 1 0 3 3 ## Male 6 0 2 3 ## 4 Female 0 0 3 0 ## Male 5 0 2 0 ## 5 Female 0 0 0 1 ## Male 3 0 3 2 Just as we did with the two-way tables above, you can also apply the prop.table function and round functions to this three-way cross-tabulation table object. 25.2.5.2 Option 2: Using the xtabs Function The xtabs function from base R can also be used to create a three-way cross-tabulation table. To begin, type the name of the xtabs function. As the first argument in the function parentheses, insert the tilde (~) operator followed by the name of the first variable (JobLevel), the addition (+) operator, and the name of the second variable (Sex), the addition (+) operator, and the name of the third variable (RaceEthnicity). As the second argument, type data= followed by the name of the data frame object two which the aforementioned variables belong (demodata). # Create cross-tabulation table from JobLevel and Sex variables xtabs(~ JobLevel + Sex + RaceEthnicity, data=demodata) ## , , RaceEthnicity = Asian ## ## Sex ## JobLevel Female Male ## 1 14 11 ## 2 6 10 ## 3 1 6 ## 4 0 5 ## 5 0 3 ## ## , , RaceEthnicity = Black ## ## Sex ## JobLevel Female Male ## 1 2 0 ## 2 0 1 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## ## , , RaceEthnicity = HispanicLatino ## ## Sex ## JobLevel Female Male ## 1 14 14 ## 2 3 5 ## 3 3 2 ## 4 3 2 ## 5 0 3 ## ## , , RaceEthnicity = White ## ## Sex ## JobLevel Female Male ## 1 17 10 ## 2 13 6 ## 3 3 3 ## 4 0 0 ## 5 1 2 As you can see, when used to create a three-way cross-tabulation table, the xtabs function creates one two-way cross-tabulation table based on the first two variables for each category or level of the third variable. Lets assign this table to an object that well call table3. # Assign three-way cross-tabulation table to object table_3 &lt;- xtabs(~ JobLevel + Sex + RaceEthnicity, data=demodata) If you would prefer to view the frequencies (i.e., counts) in a single table, then use the ftable function from base R. Simply type the name of the table object we created in the previous step (table_3) as the sole argument in the ftable function. # Print three-way cross-tabulation table in a different format ftable(table_3) ## RaceEthnicity Asian Black HispanicLatino White ## JobLevel Sex ## 1 Female 14 2 14 17 ## Male 11 0 14 10 ## 2 Female 6 0 3 13 ## Male 10 1 5 6 ## 3 Female 1 0 3 3 ## Male 6 0 2 3 ## 4 Female 0 0 3 0 ## Male 5 0 2 0 ## 5 Female 0 0 0 1 ## Male 3 0 3 2 Just as we did with the two-way tables above, you can also apply the prop.table function and round functions to this three-way table object. 25.2.6 Summary In this chapter, we learned how to create two-way cross-tabulation tables using the table and xtabs functions from base R and the CrossTable function from the gmodels package. In addition, we learned how to create three-way cross-tabulation tables using the table and xtabs. "],["pivottables.html", "Chapter 26 Applying Pivot Tables to Explore Employee Demographic Data 26.1 Conceptual Overview 26.2 Tutorial", " Chapter 26 Applying Pivot Tables to Explore Employee Demographic Data If you are a Microsoft Excel user, you are probably familiar with the infamous pivot table (or PivotTable). If youre not familiar with Excel, you are likely asking yourself the question: What is a pivot table? A pivot table is an interactive type of table that allows one to manipulate, arrange, or pivot data and descriptive and statistics interactively. The pivot table can be a useful tool for exploring and processing data, particularly employee demographic data. 26.1 Conceptual Overview If youre looking for a conceptual overview of pivot tables, check out this resource created by Microsoft. 26.2 Tutorial This chapters tutorial demonstrates how to create pivot tables that you can share with others via interactive HTML files. 26.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/98UvbWW6fLo 26.2.2 Functions &amp; Packages Introduced Function Package N/A htmlwidgets N/A knitr rpivotTable rpivotTable c base R 26.2.3 Initial Steps If you havent already, save the file called EmployeeDemographics.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called EmployeeDemographics.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object demodata &lt;- read_csv(&quot;EmployeeDemographics.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## OrgTenureYrs_2019 = col_double(), ## JobLevel = col_double(), ## Sex = col_character(), ## RaceEthnicity = col_character(), ## AgeYrs_2019 = col_double(), ## Veteran = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(demodata) ## [1] &quot;EmployeeID&quot; &quot;OrgTenureYrs_2019&quot; &quot;JobLevel&quot; &quot;Sex&quot; &quot;RaceEthnicity&quot; ## [6] &quot;AgeYrs_2019&quot; &quot;Veteran&quot; Note in the data frame that the EmployeeID variable (i.e., column, field) is a unique identifier variable, and each row contains an individual employees demographic data on the following variables: organizational tenure (OrgTenureYrs_2019), job level (JobLevel), sex (Sex), race/ethnicity (RaceEthnicity), age (AgeYrs_2019), and veteran status (Veteran). 26.2.4 Create a Pivot Table Creating a pivot table is relatively simple. Before doing so, however, we need to make sure the following packages are installed: htmlwidgets, knitr, and rpivotTable. # Install htmlwidgets package if you haven&#39;t already install.packages(&quot;htmlwidgets&quot;) # Install knitr package if you haven&#39;t already install.packages(&quot;knitr&quot;) # Install rpivotTable package if you haven&#39;t already install.packages(&quot;rpivotTable&quot;) Now that youve installed those packages, you only need to access the rpivotTable package. # Access rpivotTable package library(rpivotTable) The simplest way to create a pivot table is to enter the name of the data frame of interest (demodata) as the sole argument in the function parentheses. The interactive pivot table will likely appear in the Plots window of the RStudio interface. You can expand the size of the window to see the entire pivot table Just like a pivot table in Excel, you can drag variables to the rows and columns areas and select the type of object (e.g., Table, Treemap, Bar Chart) and descriptive statistic (e.g., Count, Average) from the dropdown menus. # Create pivot table rpivotTable(demodata) The pivot table can also be exported to an HTML file, which allows one to open it as an interactive web browser, and even better, you can send the HTML file to colleagues, which allows them to use the pivot table without having access to or changing the source data. In the Viewer window of RStudio, select the Export drop-down menu, followed by Save as Web Page. You may be prompted to downloaded additional required packages; please click Yes to download these packages if you would like to save the pivot table as an HTML file. Once you have done that, you can proceed to save the file in your working directory or elsewhere. When you open (e.g., double-click on) the HTML file in the folder in which you saved it, a web browser will open with your interactive pivot table. To export the pivot table as a HTML web page file, click on the Export dropdown menu in the Viewer window pane and select Save as Web Page. You can also preemptively specify the row variable(s), column variable(s), and descriptive statistic in the rpivotTable function itself. This allows you to keep a record of the different pivots you have applied and avoid having to drag-and-drop and point-and-click in the interactive interface. To add these specifications, we just add additional arguments (separated by commas) to the rpivotTable function. First, lets type rows=\"JobLevel\" (make sure the variable is in quotation marks) to set the JobLevel variable as a row. Second, lets type cols=\"Sex\" to set the Sex variable as a column. # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=&quot;Sex&quot;) Using the c (combine) function within the rpivotTable function, we can specify multiple row or column variables as a vector. For example, lets build upon the previous syntax by adding RaceEthnicity as another column variable by typing cols=c(\"Sex\", \"RaceEthnicity\"). # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=c(&quot;Sex&quot;, &quot;RaceEthnicity&quot;)) Building upon the previous syntax even further, lets add the argument rendererName=\"Heatmap\" to specify that we want the pivot table to be rendered as a heatmap display. # Access rpivotTable package rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=c(&quot;Sex&quot;, &quot;RaceEthnicity&quot;), rendererName=&quot;Heatmap&quot;) We can also render the pivot table as a treemap, where a treemap is type of data visualization in which hierarchical data are displayed as nested rectangles, such that the area of each rectangle correlates with its relative quantitative value. To create the treemap, lets specify rows=c(\"Sex\", \"JobLevel\") to indicate that the row variables are Sex and JobLevel. Next, specify rendererName=\"Treemap\" to indicate that we want the pivot table to be rendered as a treemap. # Create pivot table rpivotTable(demodata, rows=c(&quot;Sex&quot;, &quot;JobLevel&quot;), rendererName=&quot;Treemap&quot;) Now its time to play around with applying a different type of descriptive (aggregate) statistic (as opposed to the default count (i.e., frequency) statistic). First, type rows=\"JobLevel\" to specify JobLevel as the row variable. Second, type cols=\"Sex\" to specify Sex as the column variable. Third, type rendererName=\"Bar Chart\" to request that the pivot table be rendered as a Bar Chart. Fourth, type aggregatorName=\"Average\" to specify that the average (mean) be computed for values (vals) on a specific variable (see next). Finally, type vals=\"AgeYrs_2019\" to specify the variable (AgeYrs_2019) to be used for the calculation of the average. # Create pivot table rpivotTable(demodata, rows=&quot;JobLevel&quot;, cols=&quot;Sex&quot;, rendererName=&quot;Bar Chart&quot;, aggregatorName=&quot;Average&quot;, vals=&quot;AgeYrs_2019&quot;) 26.2.5 Summary In this chapter, we learned how to create interactive HTML pivot tables using the rpivotTable function from the rpivotTable package. "],["employeesurveys.html", "Chapter 27 Introduction to Employee Surveys", " Chapter 27 Introduction to Employee Surveys XXXXX "],["aggregatesegment.html", "Chapter 28 Aggregating &amp; Segmenting Employee Survey Data 28.1 Conceptual Overview 28.2 Tutorial 28.3 Chapter Supplement", " Chapter 28 Aggregating &amp; Segmenting Employee Survey Data In this chapter, we will learn how to aggregate and segment data from an employee survey. Respectively, these two processes allow us to examine data at a higher level of analysis and to examine data by group or cluster. 28.1 Conceptual Overview Aggregation refers to the process of reporting data at a higher level of analysis, where level of analyses might include teams, units, facilities, locations, or organization levels. In many instances, we report aggregate results (e.g., mean, standard deviation, counts) based on an entire sample drawn from a larger population; however, in some instances, we might wish to create a new variable within a data frame that represents descriptive (i.e., summary) statistics for a clusters of cases (e.g., teams of employees), as we might be interested in understanding subsamples. In other instances, we might aggregate data to a higher level of analysis because we are interested in analyzing associations or differences at that higher level. For example, we might aggregate employees engagement scores to the unit level in order to analyze whether significant differences in unit-level employee engagement exist. Sometimes the process of summarizing a variable by groups is referred to as segmentation. For example, after acquiring performance data for employees, we may wish to create a new variable that represents the average (i.e., mean) level of performance for each work teams employees. It is important to note that aggregation does not always imply measures of central tendency like the mean, median, or mode; rather, we can summarize clusters of data in other ways, such as by the minimum or maximum value within a cluster of cases (e.g., performance of worst performing team member), the number of cases within a cluster (e.g., number of members of a team), or the dispersion (i.e., spread) of variable values for each cluster (e.g., standard deviation of performance values for members of a team). In terms of measurement scale, a grouping variable will typically be nominal or ordinal (i.e., categorical). 28.2 Tutorial This chapters tutorial demonstrates how to aggregate and segment employee survey data in R. 28.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/EdfeRpQtF20 28.2.2 Functions &amp; Packages Introduced Function Package group_by dplyr summarize dplyr n dplyr n_distinct dplyr mean base R sd base R median base R var base R min base R max base R mutate dplyr ungroup dplyr str base R as.data.frame base R Histogram lessR BarChart dplyr 28.2.3 Initial Steps If you havent already, save the file called EmployeeSurveyData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called EmployeeSurveyData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object EmpSurvData &lt;- read_csv(&quot;EmployeeSurveyData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## Unit = col_character(), ## Supervisor = col_character(), ## JobSat1 = col_double(), ## JobSat2 = col_double(), ## JobSat3 = col_double(), ## TurnInt1 = col_double(), ## TurnInt2 = col_double(), ## TurnInt3 = col_double(), ## Engage1 = col_double(), ## Engage2 = col_double(), ## Engage3 = col_double(), ## Engage4 = col_double(), ## Engage5 = col_double(), ## ExpIncivil1 = col_double(), ## ExpIncivil2 = col_double(), ## ExpIncivil3 = col_double(), ## ExpIncivil4 = col_double(), ## ExpIncivil5 = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; ## [8] &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; ## [15] &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; # Print number of rows in data frame (tibble) object nrow(EmpSurvData) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(EmpSurvData) ## # A tibble: 6 x 19 ## EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EID294 Mark~ EID373 3 3 3 3 3 3 2 1 2 ## 2 EID295 Mark~ EID373 2 3 2 3 4 2 2 1 3 ## 3 EID296 Mark~ EID373 2 2 3 3 3 3 1 1 2 ## 4 EID301 Mark~ EID367 2 2 3 4 4 4 3 2 3 ## 5 EID306 Mark~ EID367 2 2 2 4 4 4 3 2 3 ## 6 EID213 Huma~ EID370 4 3 4 3 3 3 3 2 4 ## # ... with 7 more variables: Engage4 &lt;dbl&gt;, Engage5 &lt;dbl&gt;, ExpIncivil1 &lt;dbl&gt;, ExpIncivil2 &lt;dbl&gt;, ## # ExpIncivil3 &lt;dbl&gt;, ExpIncivil4 &lt;dbl&gt;, ExpIncivil5 &lt;dbl&gt; The data for this exercise include employees unique identifiers (EmployeeID), the unit they work in (unit), their direct supervisor (Supervisor), and annual employee survey responses to three job satisfaction items (JobSat1, JobSat2_rev, JobSat3), three turnover intentions items (TurnInt1, TurnInt2, TurnInt3), five engagement items (Engage1, Engage2, Engage3, Engage4, Engage5), and five exposure to incivility items (ExpIncivil1, ExpIncivil2, ExpIncivil3, ExpIncivil4, ExpIncivil5_rev). All response scales are 5 points, ranging from strongly disagree (1) to strongly agree (5). 28.2.4 Counts By Group Lets begin by learning how to summarize data in aggregate. When we dont have any grouping variables of interest, we can simply compute descriptive statistics, such as the mean, standard deviation, or count for each variable of interest, and this will compute the descriptive statistics at the sample level. For our purposes, however, we will summarize data in accordance with a grouping (i.e., clustering) variable, which will yield aggregate estimates for each group. Specifically, we will summarize how many employees (who responded to the engagement survey) work in each unit. To do so, we will use the group_by, summarize, n, and n_distinct functions from the dplyr package (Wickham et al. 2021), so if you havent already, be sure to install and access that package before proceeding. # Install dplyr package if not already installed install.packages(&quot;dplyr&quot;) # Access dplyr package library(dplyr) I will demonstrate two approaches for applying the group_by, summarize, n, and n_distinct functions from dplyr. The first option uses pipe(s), which in R is represented by the %&gt;% operator. The pipe operator comes from a package called magrittr (Bache and Wickham 2020), on which the dplyr is partially dependent. In short, a pipe allows one to code more efficiently and to improve the readability of an overall script under certain conditions. Specifically, a pipe forwards the result or value of one object or expression to a subsequent function. In doing so, one can avoid writing functions in which other functions are nested parenthetically. The second option is more traditional and lacks the efficiency and readability of pipes. You can use either approach, and if dont you want to use pipes, skip to the section below called Without Pipes. For more information on the pipe operator, check out this link. 28.2.4.1 With Pipes Well begin by using an approach with the pipe (%&gt;%) operator. Type the name of our data frame object, which we previously named EmpSurvData, followed by the pipe (%&gt;%) operator. This will pipe our data frame into the subsequent function. Either on the same line or on the next line, type the name of the group_by function, and within the parentheses, type Unit as the argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator in order to pipe the results of the group_by function to the subsequent function. Type the summarize function, and within the parentheses, type a new name (of your choosing) for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). The summarize function is most commonly used to describe/summarize data in some way (e.g., counts) that has been aggregated by the group_by function. The n function can be used within the summarize function to count the number of cases per group. Finally, type another pipe (%&gt;%) operator, followed by the function ungroup() (with no arguments in the parentheses); its good to get in the habit of ungrouping the data, particularly if you want to subsequently look at the data without grouping applied. # Counts by groups - n function (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(UnitCount = n()) %&gt;% ungroup() ## # A tibble: 5 x 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 Of note, we could arrive at the same output by using the n_distinct function instead of the n function. The n_distinct function quickly counts the number of unique values (e.g., EID294, EID295) in a variable (e.g., EmployeeID), and when preceded by the group_by function, it counts the number of unique values within each group. The n_distinct function is really handy when unique entities (e.g., employees) each have multiple rows of data, which (depending on how your data are structured) could be the case if you were to have a long-format data frame containing multiple survey administrations for employees over time. Using the function can reveal how many unique employees there are, even when some employees have multiple rows of data. In sum, the n function counts the number of cases (i.e., rows), and the n_distinct function counts the number of unique values. # Counts by groups - n_distinct function (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(UnitCount = n_distinct(EmployeeID)) %&gt;% ungroup() ## # A tibble: 5 x 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 If we wish to summarize by two or more grouping variables, then we just need to add those additional grouping variables to the group_by function. For example, using the same code from above, lets add the Supervisor nominal variable as a second argument to the group_by function. # Counts by two groups - n_distinct function (with pipes) EmpSurvData %&gt;% group_by(Unit, Supervisor) %&gt;% summarize(UnitCount = n_distinct(EmployeeID)) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;Unit&#39;. You can override using the `.groups` argument. ## # A tibble: 16 x 3 ## Unit Supervisor UnitCount ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 HumanResources EID370 10 ## 2 HumanResources EID379 1 ## 3 Manufacturing EID368 17 ## 4 Manufacturing EID369 1 ## 5 Manufacturing EID371 7 ## 6 Manufacturing EID375 6 ## 7 Manufacturing EID380 19 ## 8 Manufacturing EID381 1 ## 9 Manufacturing EID382 21 ## 10 Marketing EID367 10 ## 11 Marketing EID373 9 ## 12 ResearchDevelopment EID372 18 ## 13 Sales EID372 19 ## 14 Sales EID374 3 ## 15 Sales EID376 11 ## 16 Sales EID377 3 28.2.4.2 Without Pipes We can achieve the same output as above without using the pipe (%&gt;%) operator. To begin, type the name of the summarize function. As the first argument within the summarize function, insert the group_by function with the name of our data frame (EmpSurvData) as the first argument and the name of the grouping variable as the second argument (Unit). Doing this informs the summarize function that the cases of interest is grouped by membership in the Unit variable. As the second argument within the summarize function, type a new name for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). # Counts by groups - n function (without pipes) summarize(group_by(EmpSurvData, Unit), UnitCount = n()) ## # A tibble: 5 x 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 In a similar fashion, we can apply the n_distinct function without pipes. # Counts by groups - n_distinct function (without pipes) summarize(group_by(EmpSurvData, Unit), UnitCount = n_distinct(EmployeeID)) ## # A tibble: 5 x 2 ## Unit UnitCount ## &lt;chr&gt; &lt;int&gt; ## 1 HumanResources 11 ## 2 Manufacturing 72 ## 3 Marketing 19 ## 4 ResearchDevelopment 18 ## 5 Sales 36 To summarize by two or more grouping variables, we can simply add additional grouping variables to the group_by function. As shown below, I add the Supervisor nominal variable as a third argument in the group_by function. # Counts by two groups - n_distinct function (without pipes) summarize(group_by(EmpSurvData, Unit, Supervisor), UnitCount = n_distinct(EmployeeID)) ## `summarise()` has grouped output by &#39;Unit&#39;. You can override using the `.groups` argument. ## # A tibble: 16 x 3 ## # Groups: Unit [5] ## Unit Supervisor UnitCount ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 HumanResources EID370 10 ## 2 HumanResources EID379 1 ## 3 Manufacturing EID368 17 ## 4 Manufacturing EID369 1 ## 5 Manufacturing EID371 7 ## 6 Manufacturing EID375 6 ## 7 Manufacturing EID380 19 ## 8 Manufacturing EID381 1 ## 9 Manufacturing EID382 21 ## 10 Marketing EID367 10 ## 11 Marketing EID373 9 ## 12 ResearchDevelopment EID372 18 ## 13 Sales EID372 19 ## 14 Sales EID374 3 ## 15 Sales EID376 11 ## 16 Sales EID377 3 28.2.5 Measures of Central Tendency and Dispersion By Group We can also aggregate data by computing the average of a variable for each group. Because measures of central tendency (e.g., mean, median) and dispersion (e.g., standard deviation, interquartile range) are estimated for continuous variables (i.e., variables with interval or ratio measurement scale), we will choose continuous variables that we wish to summarize in aggregate. For the sake of illustration, we will treat the JobSat1 and JobSat2 variables as having an interval measurement scale even though they would best be described as having an ordinal measurement scale; for a review of measurement scales, please refer to this section from a previous chapter. Further, we will use organizational unit (Unit) as our grouping variable. Well learn two approaches for aggregating data by computing the group average of a variable: with the pipe (%&gt;%) operator and without the pipe (%&gt;%) operator. 28.2.5.1 With Pipes Lets begin by calculating the mean of JobSat1 for each value of the Unit variable using the pipe (%&gt;%) operator. Type the name of the data frame, which we previously named EmpSurvData, followed by the pipe (%&gt;%) operator. Type the name of the group_by function, and within the function parentheses, type Unit as the sole argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator. Type the name of the summarize function, and within the parentheses, type a new name (of your choosing) for a variable that will contain the the mean JobSat1 score for each organizational unit (Mean_JobSat1), and follow that with the = operator. After the = operator, type the name of the mean function from base R, and within it, type the name of our variable of interest (JobSat1) as the first argument; as the second argument, type na.rm=TRUE, which instructs R to exclude missing values for JobSat1 when computing the mean. # Means by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize(Mean_JobSat1 = mean(JobSat1, na.rm=TRUE)) ## # A tibble: 5 x 2 ## Unit Mean_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 ## 2 Manufacturing 3.08 ## 3 Marketing 2.79 ## 4 ResearchDevelopment 3.28 ## 5 Sales 3.22 We can extend our code by estimating the unit-level means and standard deviations for JobSat1 and JobSat2 variables by using the mean and sd base R functions. # Means &amp; SDs by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mean_JobSat2 = mean(JobSat2, na.rm=TRUE), SD_JobSat2 = sd(JobSat2, na.rm=TRUE) ) ## # A tibble: 5 x 5 ## Unit Mean_JobSat1 SD_JobSat1 Mean_JobSat2 SD_JobSat2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.447 ## 2 Manufacturing 3.08 0.818 3.32 0.819 ## 3 Marketing 2.79 0.787 2.89 0.737 ## 4 ResearchDevelopment 3.28 1.02 3.17 0.924 ## 5 Sales 3.22 0.722 3.36 0.723 In addition to the mean and sd functions from base R, we can also apply the base R functions for median (median), variance (var), minimum (min), maximum (max), and interquartile range (IQR). # Multiple descriptive statistics by group (with pipes) EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 x 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopme~ 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 If we wish to save our work, we can assign the data frame (tibble) we generated to an object. Lets call this new object agg_EmpSurvData and assign the results of our operations above to that object using the &lt;- assignment operator. # Multiple descriptive statistics by group (with pipes) # Assign to object agg_EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% summarize( Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) Lets print our new aggregated data frame object called agg_EmpSurvData using the print function. # Print data frame (tibble) object print(agg_EmpSurvData) ## # A tibble: 5 x 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopme~ 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 28.2.5.2 Without Pipes In my opinion, these operations can get a bit harder to read when the pipe (%&gt;%) operator is not used. That being said, some people prefer not to use pipes, and thus, Ill demonstrate how to perform the same operations as above without that operator. Lets begin by computing the JobSat1 means for each Unit value. Type the name of the summarize function. As the first argument in the summarize function, type the name of the group_by function, where the first argument of the group_by function should be the name of the data frame object (EmpSurvData), and the second argument should be the name of the grouping variable (Unit). As the second argument in the summarize function, type a new name (of your choosing) for a variable that will contain the the mean JobSat1 score for each organizational unit (Mean_JobSat1), and follow that with the = operator. After the = operator, type the name of the mean function from base R, and within it, type the name of our variable of interest (JobSat1) as the first argument; as the second argument, type na.rm=TRUE, which instructs R to exclude missing values for JobSat1 when computing the mean. # Means by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 x 2 ## Unit Mean_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 ## 2 Manufacturing 3.08 ## 3 Marketing 2.79 ## 4 ResearchDevelopment 3.28 ## 5 Sales 3.22 We can extend our code by estimating the unit-level means and standard deviations for JobSat1 and JobSat2 variables by using the mean and sd base R functions. # Means &amp; SDs by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mean_JobSat2 = mean(JobSat2, na.rm=TRUE), SD_JobSat2 = sd(JobSat2, na.rm=TRUE) ) ## # A tibble: 5 x 5 ## Unit Mean_JobSat1 SD_JobSat1 Mean_JobSat2 SD_JobSat2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.447 ## 2 Manufacturing 3.08 0.818 3.32 0.819 ## 3 Marketing 2.79 0.787 2.89 0.737 ## 4 ResearchDevelopment 3.28 1.02 3.17 0.924 ## 5 Sales 3.22 0.722 3.36 0.723 In addition to the mean and sd functions from base R, we could also apply the base R functions for median (median), variance (var), minimum (min), maximum (max), and interquartile range (IQR). # Multiple descriptive statistics by group (without pipes) summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) ## # A tibble: 5 x 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopme~ 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 If we wish to save our work, we can assign the data frame (tibble) we generated to an object. Lets call this new object agg_EmpSurvData and assign the results of our operations above to that object using the &lt;- assignment operator. # Multiple descriptive statistics by group (with pipes) # Assign to object agg_EmpSurvData &lt;- summarize(group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE), SD_JobSat1 = sd(JobSat1, na.rm=TRUE), Mdn_JobSat1 = median(JobSat1, na.rm=TRUE), Var_JobSat1 = var(JobSat1, na.rm=TRUE), Min_JobSat1 = min(JobSat1, na.rm=TRUE), Max_JobSat1 = max(JobSat1, na.rm=TRUE), IQR_JobSat1 = IQR(JobSat1, na.rm=TRUE) ) Lets print our new aggregated data frame object called agg_EmpSurvData using the print function. # Print data frame (tibble) object print(agg_EmpSurvData) ## # A tibble: 5 x 8 ## Unit Mean_JobSat1 SD_JobSat1 Mdn_JobSat1 Var_JobSat1 Min_JobSat1 Max_JobSat1 IQR_JobSat1 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HumanResources 3.18 0.751 3 0.564 2 4 1 ## 2 Manufacturing 3.08 0.818 3 0.669 1 5 1 ## 3 Marketing 2.79 0.787 3 0.620 1 4 1 ## 4 ResearchDevelopme~ 3.28 1.02 3 1.04 1 5 1 ## 5 Sales 3.22 0.722 3 0.521 2 5 1 28.2.6 Add Variable to Data Frame Containing Aggregated Values In the previous section, we learned how to create a data frame (tibble) in which the data were aggregated to a higher level of analysis. In this section, we will learn how to add a variable to the existing data frame object that contains aggregated values. This second approach comes in handy when preparing the data to estimate certain types of multilevel models (e.g., multilevel regression, hierarchical linear models, random coefficients models). To add a new variable containing aggregated values, we can swap out the summarize function from the dplyr package (that we used in the previous section) with the mutate function, which is also from the dplyr package. The arguments remain the same, and as before, we can carry out this work with or without the use of the pipe (%&gt;%) operator. We do, however, need to create a new data frame or overwrite the existing data frame to incorporate the new variable. In this example, we will overwrite the existing EmpSurvData data frame by entering the name of that data frame, followed by &lt;- assignment operator and the appropriate code. We will use functions from the dplyr package as we did above, so if you havent already, make sure that you have installed and accessed that package before proceeding. 28.2.6.1 With Pipes Using the pipe (%&gt;%) operator, for the first example, lets add a new variable that we will call UnitCount. The new variable will include the total number of employees within each respective organizational unit (Unit). Type the name of our data frame object, which we previously named EmpSurvData, followed by the &lt;- assignment operator, the name of the data frame object (EmpSurvData), and the pipe (%&gt;%) operator. This will pipe our data frame into the subsequent function. Either on the same line or on the next line, type the group_by function, and within the function parentheses, type Unit as the argument to indicate that we want to group the data by the organizational unit membership variable. Follow this function with another pipe (%&gt;%) operator in order to pipe the results of the group_by function to the subsequent function. Type the name of the mutate function, and within the function parentheses, type a new name (of your choosing) for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). The mutate function is used to add a new variable to a data frame based on some type of operation or analysis. The n function can be used within the mutate function to count the number of cases per value of a grouping variable. Finally, type another pipe (%&gt;%) operator, followed by the function ungroup() (with no arguments in the parentheses); its good to get in the habit of ungrouping the data, particularly if you want to subsequently look at the data without grouping applied. # Add new variable based on counts by group (with pipes) EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% mutate(UnitCount = n()) %&gt;% ungroup() To verify that we added the new UnitCount variable to our EmpSurvData data frame object, lets print the variable names using the names function from base R and then note the addition of the new UnitCount variable. # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; ## [8] &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; ## [15] &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; &quot;UnitCount&quot; Lets work through another example in which we will add a new variable that contains values for the mean level of JobSat1 for each level of the Unit variable. Again, we will use the mutate function; however, we will apply the mean function from base R. Lets call the new aggregated variable Mean_JobSat1. # Add new variable based on means by group (with pipes) EmpSurvData &lt;- EmpSurvData %&gt;% group_by(Unit) %&gt;% mutate(Mean_JobSat1 = mean(JobSat1, na.rm=TRUE)) %&gt;% ungroup() # Print variable names from data frame names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; ## [7] &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; ## [13] &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; 28.2.6.2 Without Pipes Lets repeat the same processes as above, except this time without the the pipe (%&gt;%) operator. Type the name of our data frame, which we previously named EmpSurvData, followed by the &lt;- assignment operator. To the right of the &lt;- operator type the name of the mutate function. As the first argument of the mutate function, type the name of the group_by function. As the first argument of the group_by function, type the name of the data frame object (EmpSurvData). As the second argument of the group_by function, type the name of the grouping variable (Unit). Doing this informs the mutate function that the cases of interest are grouped by membership in the Unit variable. As the second argument in the mutate function, type a new name for a variable that will contain the number (count) of employees within each organizational unit (UnitCount), and follow that with the = operator and an empty n function (i.e., nothing typed in the parentheses). On a new line, apply the ungroup function to the EmpSurvData data frame object; this final step, will ungroup the data frame object. # Add new variable based on counts by group (without pipes) EmpSurvData &lt;- mutate( group_by(EmpSurvData, Unit), UnitCount = n() ) # Ungroup the data frame object EmpSurvData &lt;- ungroup(EmpSurvData) To verify that we added the new UnitCount variable to our EmpSurvData data frame object, lets print the variable names using the names function from base R. # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; ## [7] &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; ## [13] &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; You should see that a new variable called UnitCount is now a part of the EmpSurvData data frame. You can also view your data using the following function and argument: View(EmpSurvData). Lets work through another example. This time we will add a new variable that contains values for the mean level of JobSat1 for each level of the Unit variable. Again, we will use the mutate function; however, we will apply the mean function from base R. Lets call the new aggregated variable Mean_JobSat1. # Add new variable based on means by group (without pipes) EmpSurvData &lt;- mutate( group_by(EmpSurvData, Unit), Mean_JobSat1 = mean(JobSat1, na.rm=TRUE) ) # Ungroup the data frame object EmpSurvData &lt;- ungroup(EmpSurvData) # Print variable names from data frame object names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; ## [7] &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; ## [13] &quot;Engage4&quot; &quot;Engage5&quot; &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; ## [19] &quot;ExpIncivil5&quot; &quot;UnitCount&quot; &quot;Mean_JobSat1&quot; You should see that a new variable called Mean_JobSat1 is now a part of the EmpSurvData data frame. You can view your data using the following function and argument: View(EmpSurvData). 28.2.7 Visualize Data By Group We can visualize a variable by group. For instance, we can use a histogram to visualize the distribution of a variable for each group. Lets visualize the distribution of the JobSat1 variable for each level of the Unit variable. As we did before, were going to treat the JobSat1 variable as a continuous variable (for the sake of demonstration), even though it would be most accurately described as having an ordinal measurement scale. To do so, we will use the Histogram function from the lessR package (Gerbing, Business, and University 2021), which will generate one histogram for each unique value of the grouping variable. Lets begin by installing and accessing the lessR package (if you havent already). # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) When working with readr and dplyr functions (which are part of the tidyverse), as we previously did, we end up working with a tibble, which is a lot like a data frame but with some enhanced features. Sometimes using a tibble (instead of a standard data frame) can result in some issues with certain functions from other packages. To figure out whether the object weve been working with called EmpSurvData is a data frame object or a tibble, We will use the str function from base R. # Print the structure of object to determine whether a tibble or data frame str(EmpSurvData) ## tibble[,21] [156 x 21] (S3: tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:156] &quot;EID294&quot; &quot;EID295&quot; &quot;EID296&quot; &quot;EID301&quot; ... ## $ Unit : chr [1:156] &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; ... ## $ Supervisor : chr [1:156] &quot;EID373&quot; &quot;EID373&quot; &quot;EID373&quot; &quot;EID367&quot; ... ## $ JobSat1 : num [1:156] 3 2 2 2 2 4 3 4 3 3 ... ## $ JobSat2 : num [1:156] 3 3 2 2 2 3 3 4 4 2 ... ## $ JobSat3 : num [1:156] 3 2 3 3 2 4 2 4 4 3 ... ## $ TurnInt1 : num [1:156] 3 3 3 4 4 3 3 2 2 2 ... ## $ TurnInt2 : num [1:156] 3 4 3 4 4 3 2 2 2 2 ... ## $ TurnInt3 : num [1:156] 3 2 3 4 4 3 2 2 2 2 ... ## $ Engage1 : num [1:156] 2 2 1 3 3 3 3 3 3 2 ... ## $ Engage2 : num [1:156] 1 1 1 2 2 2 2 2 2 2 ... ## $ Engage3 : num [1:156] 2 3 2 3 3 4 3 3 2 3 ... ## $ Engage4 : num [1:156] 2 3 1 2 4 3 3 3 1 4 ... ## $ Engage5 : num [1:156] 3 3 2 2 3 3 4 2 2 2 ... ## $ ExpIncivil1 : num [1:156] 2 2 2 3 2 2 2 1 2 2 ... ## $ ExpIncivil2 : num [1:156] 2 2 1 3 3 3 2 2 2 2 ... ## $ ExpIncivil3 : num [1:156] 3 2 2 3 3 3 3 2 1 2 ... ## $ ExpIncivil4 : num [1:156] 2 2 2 4 3 3 3 2 2 2 ... ## $ ExpIncivil5 : num [1:156] 2 1 2 3 3 3 2 2 2 2 ... ## $ UnitCount : int [1:156] 19 19 19 19 19 11 11 72 72 18 ... ## $ Mean_JobSat1: num [1:156] 2.79 2.79 2.79 2.79 2.79 ... We see that the object called EmpSurvData is in fact a tibble. To convert our data frame called EmpSurvData to a regular data frame, we will use the as.data.frame function. First, type EmpSurvData &lt;- to overwrite the existing data frame, and then type the name of the as.data.frame function with the current data frame name (EmpSurvData) as the sole parenthetical argument. # Convert tibble object to standard data frame object EmpSurvData &lt;- as.data.frame(EmpSurvData) Now, lets apply the str function once more to see if the conversion was successful. # Print the structure of object to determine whether conversion successful str(EmpSurvData) ## &#39;data.frame&#39;: 156 obs. of 21 variables: ## $ EmployeeID : chr &quot;EID294&quot; &quot;EID295&quot; &quot;EID296&quot; &quot;EID301&quot; ... ## $ Unit : chr &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; &quot;Marketing&quot; ... ## $ Supervisor : chr &quot;EID373&quot; &quot;EID373&quot; &quot;EID373&quot; &quot;EID367&quot; ... ## $ JobSat1 : num 3 2 2 2 2 4 3 4 3 3 ... ## $ JobSat2 : num 3 3 2 2 2 3 3 4 4 2 ... ## $ JobSat3 : num 3 2 3 3 2 4 2 4 4 3 ... ## $ TurnInt1 : num 3 3 3 4 4 3 3 2 2 2 ... ## $ TurnInt2 : num 3 4 3 4 4 3 2 2 2 2 ... ## $ TurnInt3 : num 3 2 3 4 4 3 2 2 2 2 ... ## $ Engage1 : num 2 2 1 3 3 3 3 3 3 2 ... ## $ Engage2 : num 1 1 1 2 2 2 2 2 2 2 ... ## $ Engage3 : num 2 3 2 3 3 4 3 3 2 3 ... ## $ Engage4 : num 2 3 1 2 4 3 3 3 1 4 ... ## $ Engage5 : num 3 3 2 2 3 3 4 2 2 2 ... ## $ ExpIncivil1 : num 2 2 2 3 2 2 2 1 2 2 ... ## $ ExpIncivil2 : num 2 2 1 3 3 3 2 2 2 2 ... ## $ ExpIncivil3 : num 3 2 2 3 3 3 3 2 1 2 ... ## $ ExpIncivil4 : num 2 2 2 4 3 3 3 2 2 2 ... ## $ ExpIncivil5 : num 2 1 2 3 3 3 2 2 2 2 ... ## $ UnitCount : int 19 19 19 19 19 11 11 72 72 18 ... ## $ Mean_JobSat1: num 2.79 2.79 2.79 2.79 2.79 ... As you can see, we successfully converted EmpSurvData to a conventional data frame object, which often makes it more amenable to non-tidyverse functions. Now we are ready to apply the Histogram from lessR. When specified correctly, this function will display multiple histograms (one per each value of the grouping variable) in a trellis structure. First, type the name of the function Histogram. Second, as the first argument, enter the name of the variable of interest (JobSat1). Third, using the by1= argument, note that the grouping variable is Unit in this instance Fourth, using the data= argument, type the name of the data frame object, which in this instance is EmpSurvData. # Create trellis of histograms corresponding to different values of grouping variable Histogram(JobSat1, by1=Unit, data=EmpSurvData) ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## JobSat1 ## - by levels of - ## Unit ## ## n miss mean sd min mdn max ## HumanResources 11 0 3.18 0.75 2.00 3.00 4.00 ## Manufacturing 72 0 3.08 0.82 1.00 3.00 5.00 ## Marketing 19 0 2.79 0.79 1.00 3.00 4.00 ## ResearchDevelopment 18 0 3.28 1.02 1.00 3.00 5.00 ## Sales 36 0 3.22 0.72 2.00 3.00 5.00 If we want to look at the counts (i.e., frequencies) of cases for each unique value of a grouping variable, we can apply the BarChart from the lessR package. Simply type BarChart as the function. As the first argument, enter the name of the grouping variable (Unit). As the second argument, type data= followed by the name of the data frame object (EmpSurvData). # Create bar chart that shows counts for each level of grouping variable BarChart(Unit, data=EmpSurvData) ## &gt;&gt;&gt; Suggestions ## BarChart(Unit, horiz=TRUE) # horizontal bar chart ## BarChart(Unit, fill=&quot;reds&quot;) # red bars of varying lightness ## PieChart(Unit) # doughnut (ring) chart ## Plot(Unit) # bubble plot ## Plot(Unit, stat=&quot;count&quot;) # lollipop plot ## ## --- Unit --- ## ## Missing Values of Unit: 0 ## ## Unit Count Prop ## --------------------------------- ## HumanResources 11 0.071 ## Manufacturing 72 0.462 ## Marketing 19 0.122 ## ResearchDevelopment 18 0.115 ## Sales 36 0.231 ## --------------------------------- ## Total 156 1.000 ## ## Chi-squared test of null hypothesis of equal probabilities ## Chisq = 77.526, df = 4, p-value = 0.000 Finally, if we wish to visually examine the means for each unique value of a grouping variable, using the the BarChart function, we will begin by specifying the x-axis as the grouping variable using the x= argument and the y-axis as the continuous (interval or ratio) variable using the y= argument. Next, we will apply the stat=\"mean\" argument to request that the mean be computed for the y-axis variable by value of the grouping variable. # Create bar chart that shows counts for each level of grouping variable BarChart(x=Unit, y=JobSat1, stat=&quot;mean&quot;, data=EmpSurvData) ## JobSat1 ## - by levels of - ## Unit ## ## n miss mean sd min mdn max ## HumanResources 11 0 3.18 0.75 2.00 3.00 4.00 ## Manufacturing 72 0 3.08 0.82 1.00 3.00 5.00 ## Marketing 19 0 2.79 0.79 1.00 3.00 4.00 ## ResearchDevelopment 18 0 3.28 1.02 1.00 3.00 5.00 ## Sales 36 0 3.22 0.72 2.00 3.00 5.00 ## &gt;&gt;&gt; Suggestions ## Plot(JobSat1, Unit) # lollipop plot ## ## Data for: JobSat1 ## ------------------ ## HumanResources Manufacturing Marketing ResearchDevelopment Sales ## 3.181818 3.083333 2.789474 3.277778 3.222222 28.2.8 Summary When it comes to describing and summarizing data with multiple levels of analysis, aggregation and segmentation are important processes to consider and implement. In this tutorial, we learned how to summarize data at an aggregate level of analysis, which can allow us to segment the data to look at, perhaps, unique patterns within specific subsamples as opposed to across the entire sample. We also learned how to add new variables that contain aggregate data to an existing data frame object. Finally, we learned how to visualize data across different values of a grouping variable. 28.3 Chapter Supplement In the main portion of this chapter, we learned how to aggregate and segment data using functions from the dplyr package. In this chapter supplement, we will learn additional methods the can be used to aggregate and segment data. 28.3.1 Functions &amp; Packages Introduced Function Package describeBy psych aggregate base R list base R 28.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object EmpSurvData &lt;- read_csv(&quot;EmployeeSurveyData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## Unit = col_character(), ## Supervisor = col_character(), ## JobSat1 = col_double(), ## JobSat2 = col_double(), ## JobSat3 = col_double(), ## TurnInt1 = col_double(), ## TurnInt2 = col_double(), ## TurnInt3 = col_double(), ## Engage1 = col_double(), ## Engage2 = col_double(), ## Engage3 = col_double(), ## Engage4 = col_double(), ## Engage5 = col_double(), ## ExpIncivil1 = col_double(), ## ExpIncivil2 = col_double(), ## ExpIncivil3 = col_double(), ## ExpIncivil4 = col_double(), ## ExpIncivil5 = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(EmpSurvData) ## [1] &quot;EmployeeID&quot; &quot;Unit&quot; &quot;Supervisor&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; ## [8] &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; &quot;Engage4&quot; &quot;Engage5&quot; ## [15] &quot;ExpIncivil1&quot; &quot;ExpIncivil2&quot; &quot;ExpIncivil3&quot; &quot;ExpIncivil4&quot; &quot;ExpIncivil5&quot; 28.3.3 describeBy Function from psych Package The psych package has a useful function called describeBy, which allows us to compute descriptive (i.e., summary) statistics by unique values from a grouping variable. The describeBy is an extension of the describe function, where the latter generates descriptive statistics for the entire sample. Before using the describeBy function, we must install and access the psych package (if we havent already). # Install psych package if not already installed install.packages(&quot;psych&quot;) # Access psych package library(psych) Type the name of the describeBy function, and as the first argument, enter the name of the data frame, which in this example is EmpSurvData. Next, enter the argument group= followed by the name of the name of the data frame (EmpSurvData), followed by $, and the name of the grouping variable (Unit). Remember, the $ operator indicates that a variable belongs to a particular data frame; some functions require the $ operator to be explicitly written as part of the argument, whereas others dont. # Describe/summarize data by grouping variable describeBy(EmpSurvData, group=EmpSurvData$Unit) ## ## Descriptive statistics by group ## group: HumanResources ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 11 6.00 3.32 6 6.00 4.45 1 11 10 0.00 -1.53 1.00 ## Unit* 2 11 1.00 0.00 1 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 11 1.09 0.30 1 1.00 0.00 1 2 1 2.47 4.52 0.09 ## JobSat1 4 11 3.18 0.75 3 3.22 1.48 2 4 2 -0.25 -1.37 0.23 ## JobSat2 5 11 3.00 0.45 3 3.00 0.00 2 4 2 0.00 1.55 0.13 ## JobSat3 6 11 3.09 0.83 3 3.11 1.48 2 4 2 -0.14 -1.67 0.25 ## TurnInt1 7 11 2.91 0.54 3 2.89 0.00 2 4 2 -0.11 -0.01 0.16 ## TurnInt2 8 11 2.73 0.79 3 2.67 1.48 2 4 2 0.43 -1.41 0.24 ## TurnInt3 9 11 2.82 0.60 3 2.78 0.00 2 4 2 0.02 -0.73 0.18 ## Engage1 10 11 3.55 0.52 4 3.56 0.00 3 4 1 -0.16 -2.15 0.16 ## Engage2 11 11 3.18 0.75 3 3.22 1.48 2 4 2 -0.25 -1.37 0.23 ## Engage3 12 11 3.64 0.92 4 3.67 1.48 2 5 3 -0.02 -1.16 0.28 ## Engage4 13 11 3.73 0.65 4 3.67 0.00 3 5 2 0.22 -1.04 0.19 ## Engage5 14 11 3.64 0.67 4 3.56 1.48 3 5 2 0.44 -1.08 0.20 ## ExpIncivil1 15 11 2.09 0.30 2 2.00 0.00 2 3 1 2.47 4.52 0.09 ## ExpIncivil2 16 11 2.36 0.67 2 2.22 0.00 2 4 2 1.34 0.36 0.20 ## ExpIncivil3 17 11 2.45 0.52 2 2.44 0.00 2 3 1 0.16 -2.15 0.16 ## ExpIncivil4 18 11 2.55 0.52 3 2.56 0.00 2 3 1 -0.16 -2.15 0.16 ## ExpIncivil5 19 11 2.55 0.52 3 2.56 0.00 2 3 1 -0.16 -2.15 0.16 ## ------------------------------------------------------------------------------ ## group: Manufacturing ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 72 36.50 20.93 36.5 36.50 26.69 1 72 71 0.00 -1.25 2.47 ## Unit* 2 72 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 72 4.33 2.28 5.0 4.41 2.97 1 7 6 -0.30 -1.33 0.27 ## JobSat1 4 72 3.08 0.82 3.0 3.09 1.48 1 5 4 0.00 -0.46 0.10 ## JobSat2 5 72 3.32 0.82 3.0 3.36 1.48 1 5 4 -0.33 -0.23 0.10 ## JobSat3 6 72 3.36 0.83 3.0 3.36 1.48 2 5 3 0.00 -0.66 0.10 ## TurnInt1 7 72 3.08 0.88 3.0 3.10 1.48 1 5 4 -0.16 -0.63 0.10 ## TurnInt2 8 72 2.92 0.82 3.0 2.95 0.00 1 5 4 -0.31 0.16 0.10 ## TurnInt3 9 72 2.92 0.75 3.0 2.93 0.00 1 4 3 -0.27 -0.32 0.09 ## Engage1 10 72 3.58 0.75 4.0 3.59 1.48 2 5 3 -0.18 -0.32 0.09 ## Engage2 11 72 3.39 0.74 3.0 3.38 0.00 2 5 3 0.28 -0.24 0.09 ## Engage3 12 72 3.28 0.74 3.0 3.31 1.48 2 5 3 -0.06 -0.56 0.09 ## Engage4 13 72 3.51 0.90 4.0 3.53 1.48 1 5 4 -0.27 -0.31 0.11 ## Engage5 14 72 3.40 0.85 3.0 3.40 1.48 1 5 4 -0.04 -0.08 0.10 ## ExpIncivil1 15 72 2.14 0.59 2.0 2.17 0.00 1 3 2 -0.03 -0.30 0.07 ## ExpIncivil2 16 72 2.36 0.51 2.0 2.34 0.00 1 3 2 0.25 -1.25 0.06 ## ExpIncivil3 17 72 2.50 0.75 2.0 2.48 1.48 1 4 3 0.10 -0.40 0.09 ## ExpIncivil4 18 72 2.56 0.65 3.0 2.57 0.00 1 4 3 -0.20 -0.25 0.08 ## ExpIncivil5 19 72 2.57 0.67 3.0 2.57 0.74 1 4 3 -0.11 -0.26 0.08 ## ------------------------------------------------------------------------------ ## group: Marketing ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 19 10.00 5.63 10 10.00 7.41 1 19 18 0.00 -1.39 1.29 ## Unit* 2 19 1.00 0.00 1 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 19 1.47 0.51 1 1.47 0.00 1 2 1 0.10 -2.09 0.12 ## JobSat1 4 19 2.79 0.79 3 2.82 0.00 1 4 3 -0.30 -0.44 0.18 ## JobSat2 5 19 2.89 0.74 3 2.94 0.00 1 4 3 -0.64 0.43 0.17 ## JobSat3 6 19 3.42 1.12 3 3.41 1.48 2 5 3 0.30 -1.41 0.26 ## TurnInt1 7 19 3.05 0.85 3 3.12 1.48 1 4 3 -0.61 -0.33 0.19 ## TurnInt2 8 19 2.84 0.76 3 2.82 1.48 2 4 2 0.24 -1.35 0.18 ## TurnInt3 9 19 2.89 0.81 3 2.88 1.48 2 4 2 0.17 -1.53 0.19 ## Engage1 10 19 3.53 1.17 3 3.59 1.48 1 5 4 -0.26 -0.88 0.27 ## Engage2 11 19 3.26 1.28 4 3.29 1.48 1 5 4 -0.62 -0.97 0.29 ## Engage3 12 19 3.37 0.96 3 3.35 1.48 2 5 3 0.36 -0.94 0.22 ## Engage4 13 19 3.47 1.02 4 3.53 1.48 1 5 4 -0.68 -0.15 0.23 ## Engage5 14 19 3.21 0.71 3 3.18 0.00 2 5 3 0.59 0.28 0.16 ## ExpIncivil1 15 19 2.11 0.46 2 2.12 0.00 1 3 2 0.43 1.06 0.11 ## ExpIncivil2 16 19 2.32 0.58 2 2.35 0.00 1 3 2 -0.10 -0.88 0.13 ## ExpIncivil3 17 19 2.58 0.69 2 2.53 0.00 2 4 2 0.68 -0.83 0.16 ## ExpIncivil4 18 19 2.53 0.70 2 2.47 0.00 2 4 2 0.85 -0.64 0.16 ## ExpIncivil5 19 19 2.58 0.69 3 2.59 0.00 1 4 3 -0.27 -0.39 0.16 ## ------------------------------------------------------------------------------ ## group: ResearchDevelopment ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 18 9.50 5.34 9.5 9.50 6.67 1 18 17 0.00 -1.40 1.26 ## Unit* 2 18 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 18 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN 0.00 ## JobSat1 4 18 3.28 1.02 3.0 3.31 1.48 1 5 4 -0.21 -0.35 0.24 ## JobSat2 5 18 3.17 0.92 3.0 3.12 1.48 2 5 3 0.12 -1.21 0.22 ## JobSat3 6 18 3.50 0.92 3.0 3.50 1.48 2 5 3 0.21 -1.00 0.22 ## TurnInt1 7 18 2.83 0.51 3.0 2.81 0.00 2 4 2 -0.27 0.01 0.12 ## TurnInt2 8 18 2.67 0.59 3.0 2.62 0.00 2 4 2 0.18 -0.92 0.14 ## TurnInt3 9 18 2.61 0.70 3.0 2.62 0.00 1 4 3 -0.37 -0.30 0.16 ## Engage1 10 18 3.67 0.59 4.0 3.75 0.00 2 4 2 -1.41 0.87 0.14 ## Engage2 11 18 3.44 0.70 4.0 3.50 0.00 2 4 2 -0.77 -0.77 0.17 ## Engage3 12 18 3.61 0.61 4.0 3.56 0.74 3 5 2 0.34 -0.95 0.14 ## Engage4 13 18 3.83 0.51 4.0 3.81 0.00 3 5 2 -0.27 0.01 0.12 ## Engage5 14 18 3.50 0.79 4.0 3.50 0.74 2 5 3 -0.34 -0.65 0.19 ## ExpIncivil1 15 18 2.00 0.77 2.0 2.00 1.48 1 3 2 0.00 -1.39 0.18 ## ExpIncivil2 16 18 2.11 0.58 2.0 2.12 0.00 1 3 2 0.01 -0.33 0.14 ## ExpIncivil3 17 18 2.39 0.70 2.0 2.38 0.00 1 4 3 0.37 -0.30 0.16 ## ExpIncivil4 18 18 2.44 0.62 2.5 2.50 0.74 1 3 2 -0.52 -0.86 0.15 ## ExpIncivil5 19 18 2.22 0.65 2.0 2.25 0.00 1 3 2 -0.19 -0.88 0.15 ## ------------------------------------------------------------------------------ ## group: Sales ## vars n mean sd median trimmed mad min max range skew kurtosis se ## EmployeeID* 1 36 18.50 10.54 18.5 18.50 13.34 1 36 35 0.00 -1.30 1.76 ## Unit* 2 36 1.00 0.00 1.0 1.00 0.00 1 1 0 NaN NaN 0.00 ## Supervisor* 3 36 1.94 1.09 1.0 1.83 0.00 1 4 3 0.49 -1.40 0.18 ## JobSat1 4 36 3.22 0.72 3.0 3.23 0.00 2 5 3 0.11 -0.42 0.12 ## JobSat2 5 36 3.36 0.72 3.0 3.37 0.00 2 5 3 0.26 -0.25 0.12 ## JobSat3 6 36 3.58 0.77 4.0 3.60 0.74 2 5 3 -0.27 -0.40 0.13 ## TurnInt1 7 36 2.83 0.65 3.0 2.80 0.00 2 4 2 0.17 -0.79 0.11 ## TurnInt2 8 36 2.72 0.57 3.0 2.70 0.00 2 4 2 0.02 -0.64 0.09 ## TurnInt3 9 36 2.67 0.68 3.0 2.60 1.48 2 4 2 0.48 -0.87 0.11 ## Engage1 10 36 3.75 0.73 4.0 3.73 0.74 2 5 3 -0.03 -0.54 0.12 ## Engage2 11 36 3.44 0.81 3.0 3.43 1.48 2 5 3 0.02 -0.60 0.13 ## Engage3 12 36 3.64 0.64 4.0 3.63 0.00 2 5 3 -0.19 -0.24 0.11 ## Engage4 13 36 3.86 0.83 4.0 3.90 1.48 2 5 3 -0.33 -0.56 0.14 ## Engage5 14 36 3.50 0.91 4.0 3.53 1.48 1 5 4 -0.44 0.09 0.15 ## ExpIncivil1 15 36 2.11 0.52 2.0 2.13 0.00 1 3 2 0.15 0.33 0.09 ## ExpIncivil2 16 36 2.28 0.57 2.0 2.30 0.00 1 3 2 -0.02 -0.64 0.09 ## ExpIncivil3 17 36 2.58 0.65 3.0 2.57 0.74 1 4 3 0.00 -0.40 0.11 ## ExpIncivil4 18 36 2.69 0.71 3.0 2.67 0.74 1 4 3 0.02 -0.48 0.12 ## ExpIncivil5 19 36 2.39 0.64 2.0 2.47 1.48 1 3 2 -0.53 -0.77 0.11 As you can see, the describeBy function generates a table of common descriptive statistics for each unique value of the grouping variable. 28.3.4 aggregate Function from Base R As an alternative to the summarize function from the dplyr package, we can use the aggregate function from base R to aggregate variables to a higher level of analysis. This function is sometimes a nice alternative because (a) it doesnt require installation of an outside package, and (b) it automatically and efficiently aggregates all numeric/integer variables in the data frame object (if thats desired). Type the name of the aggregate function. As the first argument in the aggregate function, specify the name of the data frame object (EmpSurvData). As the second argument in the aggregate function, type by= followed by the list function from base R. As the sole parenthetical argument in the list function, specify what you would like for the group variable to be called in the new aggregated data frame (e.g., Unit), the = operator, and the name of the data frame object (EmpSurvData) followed by the $ operator and the name of the grouping variable (Unit). As the third argument in the aggregate function, type FUN=mean to request that the mean be computed for each value of the grouping variable and for each quantitative (numeric, integer) variable in the data frame object. # Summarize all numeric/integer variables by grouping variable aggregate(EmpSurvData, by=list(Unit=EmpSurvData$Unit), FUN=mean) ## Unit EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 ## 1 HumanResources NA NA NA 3.181818 3.000000 3.090909 2.909091 2.727273 2.818182 ## 2 Manufacturing NA NA NA 3.083333 3.319444 3.361111 3.083333 2.916667 2.916667 ## 3 Marketing NA NA NA 2.789474 2.894737 3.421053 3.052632 2.842105 2.894737 ## 4 ResearchDevelopment NA NA NA 3.277778 3.166667 3.500000 2.833333 2.666667 2.611111 ## 5 Sales NA NA NA 3.222222 3.361111 3.583333 2.833333 2.722222 2.666667 ## Engage1 Engage2 Engage3 Engage4 Engage5 ExpIncivil1 ExpIncivil2 ExpIncivil3 ExpIncivil4 ## 1 3.545455 3.181818 3.636364 3.727273 3.636364 2.090909 2.363636 2.454545 2.545455 ## 2 3.583333 3.388889 3.277778 3.513889 3.402778 2.138889 2.361111 2.500000 2.555556 ## 3 3.526316 3.263158 3.368421 3.473684 3.210526 2.105263 2.315789 2.578947 2.526316 ## 4 3.666667 3.444444 3.611111 3.833333 3.500000 2.000000 2.111111 2.388889 2.444444 ## 5 3.750000 3.444444 3.638889 3.861111 3.500000 2.111111 2.277778 2.583333 2.694444 ## ExpIncivil5 ## 1 2.545455 ## 2 2.569444 ## 3 2.578947 ## 4 2.222222 ## 5 2.388889 Using the aggregate function, we can also summarize data using two or more grouping variables. To do so, we just need to add a second grouping variable within the list function, as shown below. # Summarize all numeric/integer variables by two grouping variables aggregate(EmpSurvData, by=list(Unit=EmpSurvData$Unit, Supervisor=EmpSurvData$Supervisor), FUN=mean) ## Unit Supervisor EmployeeID Unit Supervisor JobSat1 JobSat2 JobSat3 TurnInt1 ## 1 Marketing EID367 NA NA NA 2.800000 2.900000 3.700000 3.200000 ## 2 Manufacturing EID368 NA NA NA 3.294118 3.529412 3.352941 3.058824 ## 3 Manufacturing EID369 NA NA NA 3.000000 4.000000 4.000000 3.000000 ## 4 HumanResources EID370 NA NA NA 3.300000 3.100000 3.200000 2.900000 ## 5 Manufacturing EID371 NA NA NA 3.571429 4.000000 3.428571 2.142857 ## 6 ResearchDevelopment EID372 NA NA NA 3.277778 3.166667 3.500000 2.833333 ## 7 Sales EID372 NA NA NA 3.052632 3.263158 3.368421 2.947368 ## 8 Marketing EID373 NA NA NA 2.777778 2.888889 3.111111 2.888889 ## 9 Sales EID374 NA NA NA 3.666667 3.333333 4.000000 2.333333 ## 10 Manufacturing EID375 NA NA NA 3.166667 3.500000 3.666667 3.000000 ## 11 Sales EID376 NA NA NA 3.454545 3.636364 3.818182 2.818182 ## 12 Sales EID377 NA NA NA 3.000000 3.000000 3.666667 2.666667 ## 13 HumanResources EID379 NA NA NA 2.000000 2.000000 2.000000 3.000000 ## 14 Manufacturing EID380 NA NA NA 2.842105 3.210526 3.263158 3.157895 ## 15 Manufacturing EID381 NA NA NA 4.000000 4.000000 3.000000 3.000000 ## 16 Manufacturing EID382 NA NA NA 2.904762 2.904762 3.333333 3.380952 ## TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 Engage5 ExpIncivil1 ExpIncivil2 ExpIncivil3 ## 1 2.900000 3.000000 3.800000 3.500000 3.400000 3.700000 3.300000 2.300000 2.500000 2.700000 ## 2 2.705882 2.705882 3.529412 3.588235 3.352941 3.705882 3.764706 2.411765 2.529412 2.529412 ## 3 3.000000 3.000000 5.000000 5.000000 4.000000 5.000000 4.000000 2.000000 3.000000 3.000000 ## 4 2.700000 2.800000 3.500000 3.100000 3.500000 3.700000 3.500000 2.100000 2.300000 2.500000 ## 5 2.142857 2.285714 3.714286 3.142857 3.285714 3.285714 3.142857 2.142857 2.285714 2.285714 ## 6 2.666667 2.611111 3.666667 3.444444 3.611111 3.833333 3.500000 2.000000 2.111111 2.388889 ## 7 2.789474 2.684211 3.526316 3.368421 3.631579 3.631579 3.368421 2.157895 2.315789 2.578947 ## 8 2.777778 2.777778 3.222222 3.000000 3.333333 3.222222 3.111111 1.888889 2.111111 2.444444 ## 9 2.333333 2.333333 4.000000 3.333333 4.000000 4.000000 3.666667 2.000000 2.000000 2.666667 ## 10 3.000000 3.166667 3.666667 3.666667 3.333333 3.666667 3.666667 1.666667 2.333333 2.166667 ## 11 2.727273 2.818182 4.000000 3.727273 3.636364 4.181818 3.727273 2.000000 2.272727 2.636364 ## 12 2.666667 2.333333 4.000000 3.000000 3.333333 4.000000 3.333333 2.333333 2.333333 2.333333 ## 13 3.000000 3.000000 4.000000 4.000000 5.000000 4.000000 5.000000 2.000000 3.000000 2.000000 ## 14 2.947368 2.894737 3.473684 3.263158 3.210526 3.421053 3.210526 2.263158 2.315789 2.736842 ## 15 4.000000 3.000000 4.000000 3.000000 4.000000 5.000000 4.000000 2.000000 2.000000 1.000000 ## 16 3.238095 3.238095 3.571429 3.285714 3.190476 3.333333 3.238095 1.952381 2.285714 2.476190 ## ExpIncivil4 ExpIncivil5 ## 1 2.600000 2.800000 ## 2 2.705882 2.411765 ## 3 3.000000 3.000000 ## 4 2.500000 2.600000 ## 5 2.428571 2.142857 ## 6 2.444444 2.222222 ## 7 2.736842 2.421053 ## 8 2.444444 2.333333 ## 9 2.666667 2.333333 ## 10 2.666667 3.000000 ## 11 2.636364 2.363636 ## 12 2.666667 2.333333 ## 13 3.000000 2.000000 ## 14 2.473684 2.526316 ## 15 3.000000 3.000000 ## 16 2.476190 2.714286 "],["cronbachsalpha.html", "Chapter 29 Estimating Internal Consistency Reliability Using Cronbachs alpha 29.1 Conceptual Overview 29.2 Tutorial", " Chapter 29 Estimating Internal Consistency Reliability Using Cronbachs alpha In this chapter, we will learn how to estimate the internal consistency reliability of a multi-item measure (i.e, scale, inventory, test) by using Cronbachs alpha (\\(\\alpha\\)). 29.1 Conceptual Overview We can think of reliability as how consistently or dependably we have measured something in a given sample. Common types of reliability that we encounter in human resource management include inter-rater reliability, test-retest reliability, and internal consistency reliability. Conventionally, a measurement tool demonstrates an acceptable level of reliability in a sample when the reliability estimate is .70 or higher, where .00 indicates very low reliability and 1.00 indicates very high reliability. That being said, we should always strive for reliability estimates that are much closer to 1.00. When we working with multi-item measures multi-item measures, we often estimate internal consistency reliability can be defined as a reliability estimate based on intercorrelation (i.e., homogeneity) among items on a test, with [Cronbachs] alpha being a prime example (Schultz and Whitney 2005). In other words, internal consistency reliability tells us how consistent scores on different items (e.g., questions) are to one another. Homogeneity among items provides some evidence that the items are reliably measuring the same construct (i.e., concept). Of course, just because we are consistently measuring something doesnt necessary mean we are measuring the correct something, which echoes the notion that high reliability is a necessary but not sufficient condition for high validity. Nonetheless, internal consistency reliability is a useful form of reliability when it comes to evaluating multi-item scales and determining whether it is appropriate to create a composite variable (i.e., overall scale score variable) based on the sum or mean of item scores for each case (e.g., observation, person, employee, individual). Cronbachs alpha (\\(\\alpha\\)) is commonly used as an indicator of internal consistency reliability. If our goal is to understand the extent to which the items relate to underlying factor(s), exploratory and/or confirmatory factor analysis would be appropriate. Cronbachs alpha can be used to assess internal consistency reliability when the variables (e.g., survey items, measure items) analyzed are continuous (interval or ratio measurement scale); however, we often relax this assumption to allow the analysis of Likert-type response scale formats (e.g., 1 = Strongly Disagree, 5 = Strongly Disagree) for variables that are technically ordinal in nature. For dichotomous (binary) items, we might use the Kuder-Richardson (K-R) coefficient of equivalence to assess internal consistency reliability. There are different thresholds we might apply to evaluate the internal consistency reliability based on Cronbachs alpha, and the table below shows the thresholds for qualitative descriptors that well apply throughout this book. Cronbachs alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable For additional information on internal consistency reliability, Cronbachs alpha, and reliability in general, please check out this open-source resource (Price et al. 2017). 29.2 Tutorial This chapters tutorial demonstrates how to estimate internal consistency reliability using Cronbachs alpha in R. 29.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/7k35isYrE4Q 29.2.2 Functions &amp; Packages Introduced Function Package alpha psych c base R 29.2.3 Initial Steps If you havent already, save the file called survey.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called survey.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;survey.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## SurveyID = col_double(), ## JobSat1 = col_double(), ## JobSat2 = col_double(), ## JobSat3 = col_double(), ## TurnInt1 = col_double(), ## TurnInt2 = col_double(), ## TurnInt3 = col_double(), ## Engage1 = col_double(), ## Engage2 = col_double(), ## Engage3 = col_double(), ## Engage4 = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; ## [10] &quot;Engage3&quot; &quot;Engage4&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 x 11 ## SurveyID JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 3 3 3 2 1 2 2 ## 2 2 4 4 4 3 3 2 4 4 4 4 ## 3 3 4 4 5 2 1 2 4 4 4 4 ## 4 4 2 3 3 4 4 4 4 4 4 4 ## 5 5 3 3 3 4 3 3 3 3 3 3 ## 6 6 3 3 3 3 2 2 4 4 5 3 The data frame includes annual employee survey responses from 156 employees to three Job Satisfaction items (JobSat1, JobSat2, JobSat3), three Turnover Intentions items (TurnInt1, TurnInt2, TurnInt3), and four Engagement items (Engage1, Engage2, Engage3, Engage4). Employees responded to each item using a 5-point response format, ranging from Strongly Disagree (1) to Strongly Agree (5). Assume that higher scores on an item indicate higher levels of that variable; for example, a higher score on TurnInt1 would indicate that the respondent has higher intentions of quitting the organization. 29.2.4 Compute Cronbachs alpha Prior to creating a composite score (i.e., overall scale score) for each case (e.g., observation, respondent, individual, employee) within our data frame based on their responses to each of the multi-item survey measures, it is common to compute Cronbachs alpha as an estimate of internal consistency reliability. Cronbachs alpha provides us with information we can use to judge whether variables (e.g., items) are internally consistent with each another. If theyre not internally consistent, then it wont make much sense to compute a composite variable (i.e., overall scale score variable) based on the mean or sum of the variables (e.g., items). For the purposes of this book, we will consider a scale with an alpha greater than or equal to .70 to demonstrate acceptable internal consistency for the particular sample, whereas an alpha that falls within the range of .60-.69 would be considered questionable, and an alpha below .60 would be deemed unacceptable. Here is a table of more nuanced qualitative descriptors for Cronbachs alpha: Cronbachs alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable To compute internal consistency reliability, we will use the alpha function from the psych package. To get started, install and access the psych package using the install.packages and library functions, respectively (if you havent already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Presumably, each set of similarly named items are intended to tap into the same underlying concept (e.g., Turnover Intentions: TurnInt1, TurnInt2, TurnInt3), which we are attempting to assess using the items. Lets practice estimating Cronbachs alpha for some of the measures in our data frame, beginning with the three Turnover Intention items (i.e., variables) (TurnInt1, TurnInt2, TurnInt3). Note that you must list the name of the data frame (df) containing these items prior to the first bracket ([. After that and within the c (combine) function, provide the item (variable) names of the scale for which you would like to estimate the internal consistency reliability; because we have included a comma (,) before the c function, we are using matrix/bracket notation to reference columns, which correspond to variables in this data frame; if we had placed the c function before the comma, then we would (,) be referencing rows using matrix/bracket notation (which will almost never be the case when were using a data frame with the alpha function. Finally, the item (variable) names listed as arguments within the c function should be in quotation marks. # Estimate Cronbach&#39;s alpha for Turnover Intentions items alpha(df[,c(&quot;TurnInt1&quot;,&quot;TurnInt2&quot;,&quot;TurnInt3&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;TurnInt1&quot;, &quot;TurnInt2&quot;, &quot;TurnInt3&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.83 0.83 0.78 0.63 5 0.023 2.9 0.64 0.59 ## ## lower alpha upper 95% confidence boundaries ## 0.79 0.83 0.88 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## TurnInt1 0.75 0.75 0.59 0.59 2.9 0.041 NA 0.59 ## TurnInt2 0.73 0.74 0.58 0.58 2.8 0.042 NA 0.58 ## TurnInt3 0.83 0.83 0.70 0.70 4.8 0.028 NA 0.70 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## TurnInt1 154 0.88 0.88 0.80 0.72 3.0 0.77 ## TurnInt2 154 0.88 0.88 0.81 0.73 2.8 0.73 ## TurnInt3 154 0.83 0.84 0.69 0.64 2.8 0.72 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## TurnInt1 0.02 0.23 0.51 0.23 0.01 0.01 ## TurnInt2 0.03 0.29 0.54 0.14 0.01 0.01 ## TurnInt3 0.02 0.31 0.51 0.16 0.00 0.01 Note: If you see the following message at the top of your output, you can often safely ignore it  that is, unless you know that one or more items should have been reverse-coded. If an item needs to be reverse coded, then you would need to take care of that prior to running the alpha function. \\(\\color{red}{\\text{Some items ( [ITEM NAME] ) were negatively correlated with the total scale and probably should be reversed.}}\\) Based on the output from the alpha function, we can conclude that the raw alpha (raw_alpha) of .83 for all three items exceeds our cutoff of .70 for acceptable internal consistency, and enters into the realm of what we would consider to be good internal consistency. Next, take a look at the output table called Reliability if an item is dropped; this table indicates what would happen to Cronbachs alpha if you were to drop the item listed in the row in which the item appears and then re-estimate Cronbachs alpha. For example, if you dropped TurnInt1 and retained all other items, Cronbachs alpha would drop to approximately .75. Similarly, if you dropped TurnInt2 and retained all other items, Cronbachs alpha would drop to .75. Finally, if you dropped TurnInt3 and retained all other items, Cronbachs alpha would remain the same (.83). Thus, given that Cronbachs alpha for all three items exceeds .70 and that dropping any one of the items would not increase Cronbachs alpha for the items from the Turnover Intentions survey measure, from an empirical perspective, we can be reasonably confident that the three Turnover Intentions items are internally consistent with one another, which means it would be acceptable to create an overall scale score based on the sum or mean of these three items. Importantly, before making a final decision on whether to retain all of the items, however, we should review the qualitative content of each item to determine whether it meets our conceptual definition for turnover intentions. Lets imagine our conceptual definition of turnover intentions is a persons thoughts and intentions to leave an organization, and the three turnover intentions items follow. TurnInt1 - I regularly think about leaving this organization. TurnInt2 - I plan on quitting this job within the next year. TurnInt3 - I regularly search for new jobs outside of this organization. In this example, all three of these items appear to tap into our conceptual definition of turnover intentions . Thus, when combined with the acceptable internal consistency reliability for these three items, we can reasonably justify creating a composite variable (i.e., overall scale score variable) based on the mean or sum of these three items; you will learn how to do this in the following chapter. Now, lets apply the alpha function to the three Job Satisfaction items (JobSat1, JobSat2, JobSat3). Given that were working the same data frame object (df), all we need to do is swap out the three turnover intentions item names (i.e., variable names) with the three job satisfaction items names. Please note that if we had fewer or more than three variables, we would simply list fewer variable-name arguments in the c function nested within the alpha function. # Estimate Cronbach&#39;s alpha for Job Satisfaction items alpha(df[,c(&quot;JobSat1&quot;,&quot;JobSat2&quot;,&quot;JobSat3&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;JobSat1&quot;, &quot;JobSat2&quot;, &quot;JobSat3&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.78 0.78 0.72 0.54 3.5 0.032 3.3 0.68 0.47 ## ## lower alpha upper 95% confidence boundaries ## 0.71 0.78 0.84 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## JobSat1 0.62 0.62 0.45 0.45 1.6 0.061 NA 0.45 ## JobSat2 0.63 0.64 0.47 0.47 1.7 0.058 NA 0.47 ## JobSat3 0.82 0.82 0.70 0.70 4.7 0.028 NA 0.70 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## JobSat1 156 0.86 0.87 0.79 0.68 3.1 0.82 ## JobSat2 152 0.85 0.86 0.78 0.67 3.2 0.80 ## JobSat3 156 0.78 0.77 0.55 0.50 3.4 0.86 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## JobSat1 0.02 0.19 0.48 0.28 0.03 0.00 ## JobSat2 0.01 0.14 0.47 0.33 0.04 0.03 ## JobSat3 0.00 0.15 0.39 0.36 0.10 0.00 Based on the output from the alpha function shown above, we can conclude that the raw alpha (raw_alpha) of .78 for all three items exceeds our cutoff of .70 for acceptable internal consistency. Take a look at the output table called Reliability if an item is dropped; this table indicates what would happen to Cronbachs alpha if we were to drop the item listed in the row in which the item appears. For example, if we dropped JobSat1 and retained all other items, Cronbachs alpha would drop to .62. Similarly, if we dropped JobSat2 and retained all other items, Cronbachs alpha would drop to .63. Finally, if we dropped JobSat3 and retained all other items, Cronbachs alpha would increase to .82. Now we are faced with a dilemma: Should we drop JobSat3 to improve Cronbachs alpha by .04? Or should we retain JobSat3 because this increase might be described by some as only marginal? Well, this is a situation where it is especially important to look at the actual qualitative item content  just like we did with the turnover intentions items. Lets imagine our conceptual definition for job satisfaction is a persons evaluation of their work and job, and the three job satisfaction items are as follows. JobSat1 - I enjoy completing my daily work for my job. JobSat2 - I am satisfied with my job. JobSat3 - I am satisfied with my work and with my direct supervisor. In this example, the qualitative item content corroborates the increase in internal consistency reliability should we drop JobSat3. Specifically, the item content for JobSat3 indicates that it is double-barreled (i.e., references two objects), and this likely explains why this item seems to be less consistent with the other two items. Given all that, we would make the decision to drop the JobSat3 and create the composite variable for overall job satisfaction using all items except for JobSat3. I encourage you to practice computing Cronbachs alpha on your own using the alpha function for the four Engagement items (Engage1, Engage2, Engage3, Engage4). Lets say that the conceptual definition for engagement is: The extent to which a person feels enthusiastic, energized, and driven to perform their work. And lets pretend that the actual items are: Engage1 - When Im working, Im full of energy. Engage2 - I complete my work with enthusiasm. Engage3 - I find inspiration in my work. Engage4 - I have no problem working for long periods of time. In the following chapter, we will begin by estimating Cronbachs alpha for these four engagement items and then determine which of the items should be included in a composite variable for engagement. 29.2.5 Summary In this chapter, we learned how to estimate the internal consistency reliability of a multi-item measures by computing Cronbachs alpha (\\(\\alpha\\)). Cronbachs alpha represents one way to estimate the internal consistency reliability of a set of variables (e.g., items). Cronbachs alpha can help us understand whether a set of variables (e.g., items) are homogeneous. The alpha function from the psych package offers an efficient approach to estimating Cronbachs alpha. "],["compositevariable.html", "Chapter 30 Creating a Composite Variable Based on a Multi-Item Measure 30.1 Conceptual Overview 30.2 Tutorial", " Chapter 30 Creating a Composite Variable Based on a Multi-Item Measure In this chapter, we will learn how to create a composite variable based on scores from a multi-item measure. To justify which items should be included or excluded when creating the composite variable, we will compute Cronbachs alpha (\\(\\alpha\\)) as an indicator of internal consistency reliability. 30.1 Conceptual Overview Sometimes it is useful to create a composite variable out of the sum or mean of multiple variables scores. This process results in each case (e.g., observation, employee, individual) receiving a composite score based on the sum or mean of their scores on the variables used to create the composite variable. As an example, imagine that a direct supervisor rates each of their team members on five dimensions from a performance evaluation measure, which results in five separate variables corresponding to the five dimensions. A composite variable could be created for each team member by taking the average of the five ratings each individual received, such that the resulting variable represents each team members overall level of job performance. So why do we create composite variables? Well, psychological constructs (e.g., attitudes, behaviors, feelings) are often multi-faceted, which means that single item will likely not capture the full underlying concept (i.e., construct) space or domain. Continuing with the performance evaluation example, job performance is often multi-faceted, as for a given job, the conceptual performance domain often involves a variety of different behavior types (e.g., customer service, administrative). By creating a composite variable out of variables that are intended to measure the different sub-facets of the conceptual performance domain, we can create an indicator of overall job performance. An overall scale score variable is a specific type of composite variable in which scores on items from a multi-item scale (i.e., measure) are combined (by computing the sum or the mean) into a new variable. This process results in each case (e.g., observation, employee, individual) receiving an overall scale score based on the sum or mean of their scores on the items from a given measure. For example, imagine that employees respond to an annual survey containing a three-item job satisfaction measure. An overall scale score variable could be created by computing each respondents average response scores to the three job satisfaction items and applying these to a new composite variable that represents each respondents overall level of job satisfaction. To justify whether it is appropriate to create a composite variable or which variables should be used to create the composite variable, we can estimate internal consistency reliability (via Cronbachs alpha) for the set of variables (or subsets of those items). Based on data from a given sample, internal consistency reliability provides an indication of how homogeneous a set of variables are in terms of their scores. If you need a refresher on internal consistency reliability, refer to the previous chapter on estimating internal consistency reliability using Cronbachs alpha. Finally, as a reminder, the following table includes the qualitative descriptors that can be used to interpret Cronbachs alpha. Cronbachs alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable 30.2 Tutorial This chapters tutorial demonstrates how to create a composite variable based on scores from a multi-item measure. To justify which items to include in the creation of the composite variable, we will use Cronbachs alpha (\\(\\alpha\\)) as an indicator of internal consistency reliability. 30.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/vdmYv0YnWEE 30.2.2 Functions &amp; Packages Introduced Function Package rowMeans base R rowSums base R c base R names base R 30.2.3 Initial Steps If you havent already, save the file called survey.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called survey.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;survey.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## SurveyID = col_double(), ## JobSat1 = col_double(), ## JobSat2 = col_double(), ## JobSat3 = col_double(), ## TurnInt1 = col_double(), ## TurnInt2 = col_double(), ## TurnInt3 = col_double(), ## Engage1 = col_double(), ## Engage2 = col_double(), ## Engage3 = col_double(), ## Engage4 = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; ## [10] &quot;Engage3&quot; &quot;Engage4&quot; # Print number of rows in data frame (tibble) object nrow(df) ## [1] 156 # Print top 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 x 11 ## SurveyID JobSat1 JobSat2 JobSat3 TurnInt1 TurnInt2 TurnInt3 Engage1 Engage2 Engage3 Engage4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 3 3 3 3 3 2 1 2 2 ## 2 2 4 4 4 3 3 2 4 4 4 4 ## 3 3 4 4 5 2 1 2 4 4 4 4 ## 4 4 2 3 3 4 4 4 4 4 4 4 ## 5 5 3 3 3 4 3 3 3 3 3 3 ## 6 6 3 3 3 3 2 2 4 4 5 3 The data frame includes annual employee survey responses from 156 employees to three Job Satisfaction items (JobSat1, JobSat2, JobSat3), three Turnover Intentions items (TurnInt1, TurnInt2, TurnInt3), and four Engagement items (Engage1, Engage2, Engage3, Engage4). Employees responded to each item using a 5-point response format, ranging from Strongly Disagree (1) to Strongly Agree (5). Assume that higher scores on an item indicate higher levels of that variable; for example, a higher score on TurnInt1 would indicate that the respondent has higher intentions of quitting the organization. 30.2.4 Compute Cronbachs alpha To justify the creation of a composite variable (i.e., overall scale score variable) for one of the multi-item survey measures, well first estimate internal consistency reliability using Cronbachs alpha. To do so, we will use the alpha function from the psych package. To get started, install and access the psych package using the install.packages and library functions, respectively (if you havent already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Now lets compute Cronbachs alpha for the four-item engagement measure. # Estimate Cronbach&#39;s alpha for the four-item Engagement measure alpha(df[,c(&quot;Engage1&quot;,&quot;Engage2&quot;,&quot;Engage3&quot;,&quot;Engage4&quot;)]) ## ## Reliability analysis ## Call: alpha(x = df[, c(&quot;Engage1&quot;, &quot;Engage2&quot;, &quot;Engage3&quot;, &quot;Engage4&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.84 0.84 0.8 0.56 5.1 0.021 3.5 0.66 0.56 ## ## lower alpha upper 95% confidence boundaries ## 0.79 0.84 0.88 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Engage1 0.78 0.78 0.71 0.55 3.6 0.030 0.00048 0.54 ## Engage2 0.78 0.78 0.71 0.54 3.5 0.030 0.00355 0.53 ## Engage3 0.82 0.82 0.75 0.60 4.5 0.025 0.00072 0.60 ## Engage4 0.79 0.79 0.72 0.55 3.7 0.030 0.00524 0.54 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Engage1 156 0.83 0.83 0.75 0.69 3.6 0.77 ## Engage2 156 0.84 0.84 0.76 0.70 3.4 0.83 ## Engage3 156 0.77 0.78 0.66 0.61 3.4 0.76 ## Engage4 156 0.84 0.83 0.74 0.68 3.6 0.86 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Engage1 0.01 0.06 0.35 0.49 0.10 0 ## Engage2 0.02 0.10 0.42 0.39 0.06 0 ## Engage3 0.00 0.10 0.44 0.40 0.06 0 ## Engage4 0.01 0.08 0.30 0.47 0.13 0 Note: If you see the following message at the top of your output, you can often safely ignore it  that is, unless you know that one or more items should have been reverse-coded. If an item needs to be reverse coded, then you would need to take care of that prior to running the alpha function. \\(\\color{red}{\\text{Some items ( [ITEM NAME] ) were negatively correlated with the total scale and probably should be reversed.}}\\) The raw alpha (raw_alpha) based on all four engagement items exceeds the acceptable threshold of .70, and the Reliability if an item is dropped output table indicates that removing an item would result in a lower Cronbachs alpha (i.e., lower internal consistency reliability estimate). Further, lets imagine that the conceptual definition for engagement is the extent to which a person feels enthusiastic, energized, and driven to perform their work., and the items content are as follows: Engage1 - When Im working, Im full of energy. Engage2 - I complete my work with enthusiasm. Engage3 - I find inspiration in my work. Engage4 - I have no problem working for long periods of time. We will retain all four items when computing the composite variable for engagement because: Cronbachs alpha for all four items is above .70 (and thus acceptable); Removing an item would decrease Cronbachs alpha; and The content of all four items seems to fit within the conceptual definition of engagement. As a reminder, for the purposes of this book, we will consider a scale with an alpha greater than or equal to .70 to demonstrate acceptable internal consistency for the particular sample, whereas an alpha that falls within the range of .60-.69 would be considered questionable, and an alpha below .60 would be deemed unacceptable. Here is a table of more nuanced qualitative descriptors for Cronbachs alpha: Cronbachs alpha (\\(\\alpha\\)) Qualitative Descriptor .95-1.00 Excellent .90-.94 Great .80-.89 Good .70-.79 Acceptable .60-.69 Questionable .00-.59 Unacceptable For a more in-depth review of internal consistency reliability and justifying which items (if any) to remove, be sure to check out the previous chapter. 30.2.5 Create a Composite Variable To create a composite variable, we will use the rowMeans function from base R, as it offers a straightforward approach. The function also allows us to decide what to do with cases that have missing data on one or more of the variables (e.g., items). Given that we feel justified to created an composite variable (i.e., overall scale score variable) based on the four engagement items (Engage1, Engage2, Engage3, Engage4), we will include all four of those items in our rowMeans function. First, lets come up with a name for the composite variable were about to create; here, I decided to call the new variable Engage_Overall, as the variable will represent overall engagement. Second, well append the new variable called Engage_Overall to the df data frame using the $ operator to indicate that the new variable will be added to that data frame object. Third, well use the &lt;- operator to indicate that we are assigning the results of the rowMeans function to the new variable. Fourth, we will type the name of the rowMeans function. Fifth, as the first argument, type the name of the data frame object to which the items belong (df). Sixth, following the data frame name, type in brackets ([ ]), and within the brackets, type a comma (,) followed by the c (combine) function; by placing a comma in front of the c function, we are indicating that we will be referencing the names of columns (i.e., variables); within the c function, list the name of each item in quotation marks (\" \"), separated by commas (,). Finally, as the second argument in the rowMeans function, type the na.rm=TRUE argument, which will tell the function to compute the mean for each case that has at least one score for the specified items; in other words, this function allows for the row means to be computed even if there are missing data. [Note: If you wish to create a composite variable based on the sum of item scores, you can use the rowSums function from base R.] # Create composite (overall scale score) variable based on Engagement items df$Engage_Overall &lt;- rowMeans(df[,c(&quot;Engage1&quot;,&quot;Engage2&quot;,&quot;Engage3&quot;,&quot;Engage4&quot;)], na.rm=TRUE) Lets take a look at the variables in our df data frame object by using the names function from base R. # Print variable names to Console names(df) ## [1] &quot;SurveyID&quot; &quot;JobSat1&quot; &quot;JobSat2&quot; &quot;JobSat3&quot; &quot;TurnInt1&quot; ## [6] &quot;TurnInt2&quot; &quot;TurnInt3&quot; &quot;Engage1&quot; &quot;Engage2&quot; &quot;Engage3&quot; ## [11] &quot;Engage4&quot; &quot;Engage_Overall&quot; Note that we now have a variable called Engage_Overall, which is our composite variable meant to represent overall engagement for each survey respondent. You can take a closer look at the composite scores for the new Engage_Overall variable by using the View function from base R. # View variable names View(df) 30.2.6 Summary In this chapter, we learned how to create a composite variable (e.g., overall scale score variable) based on scores from a multi-item measure. To justify which items to include in the composite variable, we computed Cronbachs alpha (\\(\\alpha\\)) as an estimate of internal consistency reliability. The rowMeans function (and rowSums function) from base R is quite useful when it comes to creating composite variables. "],["employeetraining.html", "Chapter 31 Introduction to Employee Training 31.1 Training Evaluation", " Chapter 31 Introduction to Employee Training Link to lecture video: https://youtu.be/nv6N-WqgkKA Link to lecture video: https://youtu.be/PiW0gFxGgbs Link to lecture video: https://youtu.be/uKSOyTm3XFY Link to lecture video: https://youtu.be/uKSOyTm3XFY Link to lecture video: https://youtu.be/b_h3YHBzuzo 31.1 Training Evaluation Training evaluation is an important step of the training process, and we can evaluate the effectiveness of a training program by using different types of training evaluation designs (i.e., research designs). Namely, we can apply various types of pre-experimental, quasi-experimental, or true experimental designs in order to evaluate a training program on selected outcomes (i.e., measures). Depending upon the type of training evaluation design we choose to apply, we will have varying degrees of confidence that and differences or changes we observe were caused by the training itself as opposed to other (confounding) factors. Common examples of training evaluation designs are: Post-test-only without control group; Post-test-only with control group; Post-test-only with two comparison groups; Pre-test/post-test without control group; Pre-test/post-test with control group. Each type of training evaluation design generates data, and thus another important consideration during training evaluation is to determine which type of analysis is most appropriate for analyzing the data from a given training evaluation design. For example, an independent-samples t-test is often appropriate when analyzing data from a post-test-only with control group design, whereas a paired-samples t-test is often appropriate when analyzing data from a pre-test/post-test without control group training evaluation design. More advanced types of training evaluation designs (e.g., mixed-factorial designs) often require other types of statistical analyses from the analysis of variance and regression family of analyses. Link to lecture video: https://youtu.be/XFJCKyAfW7Q "],["pretestposttest.html", "Chapter 32 Evaluating a Pre-Test/Post-Test without Control Group Design Using Paired-Samples t-test 32.1 Conceptual Overview 32.2 Tutorial 32.3 Chapter Supplement", " Chapter 32 Evaluating a Pre-Test/Post-Test without Control Group Design Using Paired-Samples t-test In this chapter, we learn about the post-test/pre-test without control group training evaluation design and how a paired-samples t-test can be used to analyze the data acquired from this design. Well begin with conceptual overviews of this training evaluation design and of the paired-samples t-test, and then well conclude with a tutorial. 32.1 Conceptual Overview In this section, well begin by describing the post-test/pre-test without control group training evaluation design, and well conclude by reviewing the paired-samples t-test, with discussions of statistical assumptions, statistical significance, and practical significance; the section wraps up with a sample-write up of a paired-samples t-test used to evaluate data from a post-test/pre-test without control group training evaluation design. 32.1.1 Review of Pre-Test/Post-Test without Control Group Design In a pre-test/post-test without control group training evaluation design (i.e., research design), all employees participate in the same training program, and there is no random assignment and no control group. This design can best be described as pre-experimental. A paired-samples t-test can be used to analyze the data from a pre-test/post-test without control group design, provided the statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a pre-test/post-test without control group design. As a strength, this design includes a pre-test, which assesses initial performance on the focal outcome measure. The pre-test serves as a baseline and gives us information about where the employees started with respect to outcome measure prior to completing the training. Further, the addition of a pre-test allows us to assess whether employees scores on the outcome measure have changed from before to after training. As a major weakness, this design lacks a control group and, moreover, random assignment to treatment and control groups; for these reasons, this design is not considered (quasi-)experimental. The lack of a control group (i.e., comparison group), means that we are unable to compare if the direction and amount of any observed change from pre-test to post-test differs from a group that did not receive the program training program. Consequently, this design doesnt give us much confidence that any change we observe was due to the training itself  as opposed to natural maturation and developmental processes or other confounding factors. In other words, this design doesnt given us much confidence that the training caused any change we observe from pre-test to post-test. 32.1.2 Review of Paired-Samples t-test The paired-samples t-test is an inferential statistical analysis that can be used to compare the to compare the mean of the differences between two sets of dependent scores to some population mean; in the context of training evaluation, the population mean is typically set to zero. That is, this analysis helps us understand whether one sample of cases shows evidence of change or difference between two dependent sets of scores. Often dependence refers to the fact that the two sets of scores (through which a difference score is calculated) come from the same group of individuals who were measured (assessed) twice over time (e.g., time 1 assessment and time 2 assessment for same employees), twice using the same scale associated with different foci (e.g., ratings of supervisor satisfaction and coworker satisfaction from same employees), or twice using the same scale evaluating a common target but associated with two different raters/sources (e.g., employee self-ratings of performance and supervisor-ratings of employee performance). The paired-samples t-test is sometimes called a dependent-samples t-test, a repeated-measures t-test, or a Students t-test. The formula for a paired-samples t-test can be written as follows: \\(t = \\frac{\\overline{X}_D - \\mu_0}{\\frac{s_D}{\\sqrt{n}}}\\) where \\(\\overline{X}_D\\) is the mean of the differences between the two sets of dependent scores, \\(\\mu_0\\) is the zero or non-zero population mean to which \\(\\overline{X}_D\\) is compared, \\(s_D\\) is the standard deviation of the differences between the dependent scores, and \\(n\\) refers to the number of cases (i.e., sample size). 32.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a paired-samples t-test include: The difference scores based on the two outcome measures (e.g., pre-test, post-test) are independent of each other, suggesting that cases (employees) are randomly sampled from the underlying population; The difference scores have a univariate normal distribution in the underlying population. 32.1.2.2 Statistical Significance If we wish to know whether the mean of the differences differs from the population mean to a statistically significant extent, we can compare our t-value value to a table of critical values of a t-distribution. If our calculated value is larger than the critical value given the number of degrees of freedom (df = n - 2) and the desired alpha level (i.e., significance level, p-value threshold), we would conclude that there is evidence that the mean of the differences differs to a statistically significant extent from the population mean (e.g., zero). Alternatively, we can calculate the exact p-value if we know the t-value and the degrees of freedom. Fortunately, modern statistical software calculates the t-value, degrees of freedom, and p-value for us. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the mean of the differences between the two sets of dependent scores is equal to the population mean; as noted above, in the training evaluation context, the population mean is often set to zero. In other words, if the p-value is less than .05, we conclude that the mean of the differences differs from zero to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the mean of the differences is equal to zero. Put differently, if the p-value is equal to or greater than .05, we conclude that the mean of the differences does not differ from zero to a statistically significant extent, leading us to conclude that there is no change/difference between the two sets of dependent scores in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our paired-samples t-test is estimated using data from a sample drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, the observed mean of the differences is a point estimate of the population parameter and is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether the difference between two means is is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying populations and construct CIs for each of those samples, then the true parameter (i.e., true value of the mean of the differences in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 32.1.2.3 Practical Significance A significant paired-samples t-test and associated p-value only tells us that the mean of the differences is statistically different from zero. It does not, however, tell us about the magnitude of the mean of the differences  or in other words, the practical significance. The standardized mean difference score (Cohens d) is an effect size, which means that it is a standardized metric that can be used to compare d-values compare samples. In essence, the Cohens d indicates the magnitude of the mean of the differences in standard deviation units. A d-value of .00 would indicate that the mean of the differences is equal to zero, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohens d Description .20 Small .50 Medium .80 Large Here is the formula for computing d: \\(d = \\frac{\\overline{X}_D} {s_D}\\) where \\(\\overline{X}_D\\) is the mean of the differences between the two sets of dependent scores, and \\(s_D\\) is the standard deviation of the differences between the dependent scores. 32.1.2.4 Sample Write-Up Example 1: The same employees are assessed on the exact same test before and after completing the same training program; in other words, the employees took part of a pre-test/post-test without control group training evaluation design. Because each participant has two scores on the same test (corresponding to the pre-test and post-test), a paired-samples t-test is an appropriate inferential statistical analysis for determining whether the mean of the differences between the sets of pre-test and post-test scores differs from zero, assuming the aforementioned statistical assumptions have been satisfied. More specifically, a paired-samples t-test was used to determine whether there were significant changes within participants in terms of their average test scores from before participating in the training program to after participating. The mean of the differences (i.e., before-training scores subtracted from after-training scores) for 25 participants was 19.64 (SD = 11.60), and possible test scores could range from 1 to 100 points. Further, the paired-samples t-test indicated that this mean of the differences is significantly greater than zero (t = 8.47, p &lt; .01, 95% CI[14.85, 24.43]) and is large in magnitude (d = 1.69). Example 2: A single sample of 30 employees was asked to rate their pay satisfaction and their supervisor satisfaction. Here, the same employees rated their satisfaction with two different targets: pay and supervisor. Because the same employees rated two different ratings targets, we can classify it as a repeated-measures design, which is amenable to a paired-samples t-test. A paired-samples t-test is used to determine whether there the mean of the differences between employees pay satisfaction and supervisor satisfaction differed from zero. We found that the mean of the differences (M = 2.70, SD = 13.20) did not differ significantly from zero (t = 1.12, p = .27, 95% CI[-2.23, 7.62]) based on this sample of 30 employees. Because we did not find evidence of statistical significance, we assume that, statistically, the mean of the differences does not differ from zero, and thus we will not interpret the level of practical significance (i.e., effect size). 32.2 Tutorial This chapters tutorial demonstrates how to estimate a paired-samples t-test, test the associated statistical assumptions, and present the findings in writing and visually. 32.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/ZMc9IBFdGsw 32.2.2 Functions &amp; Packages Introduced Function Package ttest lessR c base R mean base R data.frame base R remove base R BarChart lessR pivot_longer tidyr factor base R 32.2.3 Initial Steps If you havent already, save the file called TrainingEvaluation_PrePostOnly.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called TrainingEvaluation_PrePostOnly.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PrePostOnly.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## PreTest = col_double(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;PreTest&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [25 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:25] 26 27 28 29 30 31 32 33 34 35 ... ## $ PreTest : num [1:25] 43 58 52 47 43 50 66 48 49 49 ... ## $ PostTest: num [1:25] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. PreTest = col_double(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID PreTest PostTest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 43 66 ## 2 27 58 74 ## 3 28 52 62 ## 4 29 47 84 ## 5 30 43 78 ## 6 31 50 73 There are 25 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), PreTest (pre-training scores on training assessment, ranging from 1-100, where higher scores indicate higher performance), and PostTest (post-training scores on training assessment, ranging from 1-100, where higher scores indicate higher performance). 32.2.4 Estimate Paired-Samples t-test In this chapter, we will review how to estimate a paired-samples t-test using the ttest function from the lessR package. Using this function, we will evaluate whether the mean of the differences between the pre-test (PreTest) and post-test (PostTest) outcome variables differs significantly from zero. In other words, lets find out if assessment scores increased, stayed the same, or decreased from before to after the employees participated in training. Prior to running the paired-samples t-test, well have to assume (because we dont have access to information that would indicate otherwise) that the data meet the statistical assumption that the difference scores based on the two outcome measures (e.g., pre-test, post-test) are independent of each other (i.e., randomly sampled from the underlying population). To access and use the ttest function, we need to install and/or access the lessR package. If you havent already, be sure to install the lessR package; if youve recently installed the package, then you likely dont need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming its been installed), so dont forget that important step. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package and its functions library(lessR) Please note that when you use the library function to access the lessR package in a new R or RStudio session, you will likely receive a message in red font in your Console. Typically, red font in your Console is not a good sign; however, accessing the lessR package is one of the unique situations in which a red-font message is not a warning or error message. You may also receive a warning message that indicates that certain objects are masked. For our purposes, we can ignore that message. The ttest function from the lessR package makes running a paired-samples t-test relatively straightforward, as wrapped up in the function are tests of the second statistical assumption (see Statistical Assumptions section). Now were ready to run a paired-samples t-test. To begin, type the name of the ttest function. As the first argument in the parentheses, specify the name of the first outcome variable (PreTest). As the second argument, specify the name of the second outcome variable (PostTest). Difference scores will be calculated as part of the function by subtracting the variable in the first argument from the variable in the second argument. For the third argument, use data= to specify the name of the data frame (td) where the outcome and predictor variables are located. For the fourth argument, enter paired=TRUE to inform R that the data are paired and, thus, you are requesting a paired-samples t-test. # Paired-samples t-test using ttest function from lessR ttest(PreTest, PostTest, data=td, paired=TRUE) ## ## ## ------ Description ------ ## ## Difference: n.miss = 0, n = 25, mean = 19.640, sd = 11.597 ## ## ## ------ Normality Assumption ------ ## ## Null hypothesis is a normal distribution of Difference. ## Shapiro-Wilk normality test: W = 0.9687, p-value = 0.6134 ## ## ## ------ Inference ------ ## ## t-cutoff for 95% range of variation: tcut = 2.064 ## Standard Error of Mean: SE = 2.319 ## ## Hypothesized Value H0: mu = 0 ## Hypothesis Test of Mean: t-value = 8.468, df = 24, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.787 ## 95% Confidence Interval for Mean: 14.853 to 24.427 ## ## ## ------ Effect Size ------ ## ## Distance of sample mean from hypothesized: 19.640 ## Standardized Distance, Cohen&#39;s d: 1.694 ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for 6.941 ## -------------------------------------------------- As you can see in the output, the ttest function provides descriptive statistics, assumption tests regarding the distribution of the difference scores, a statistical significance test of the mean comparison, and an indicator of practical significance in the form of the standardized mean difference (Cohens d). In addition, the default lollipop data visualization depicts the distance from the PreTest score to the PostTest score for each case, and the default density distribution depicts the shape of the difference-score distribution, and the extent to which the mean of this distribution appears (visually) to differ from zero. Description: The Description section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees (n = 25), and descriptively, the mean of the differences (PostTest minus PreTest) is 19.64 (SD = 11.60), which indicates that, on average, scores increased from before to after training (although we will not know if this difference is statistically significant until we get to the Inference section of the output). Normality Assumption: If the sample size is less than or equal to 30, then the Shapiro-Wilk normality test is used to test the null hypothesis that the distribution of the difference scores demonstrates univariate normality; as such, if the p-values associated with the test statistic (W) is less than the conventional alpha level of .05, then we would reject the null hypothesis and assume that the distribution is not normal. If, however, we fail to reject the null hypothesis, then we dont have statistical evidence that the distribution is anything other than normal; in other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is likely normally distributed. In the output, we can see that the difference is normally distributed (W = .969, p = .613). Inference: In the Inference section of the output, you will find the statistical test of the null hypothesis (e.g., mean of the differences is equal to zero). First, take a look at the line prefaced with Hypothesis Test of Mean, as this line contains the results of the paired-samples t-test (t = 8.468, p &lt; .001). Because the p-value is less than our conventional two-tailed alpha cutoff of .05, we reject the null hypothesis and conclude that the mean of the differences differs from zero to a statistically significant extent. But in what direction? To answer this question, we need to look back to the Description section; in that section, the mean of the differences (PostTest minus PreTest) is 19.64; as such, we can conclude the following: On average, employees performed performance on the assessment increased after completing training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001). Regarding the 95% confidence interval, we can conclude that the true mean of the differences in the population is likely between 14.85 and 24.43 (on a 1-100 point assessment). Effect Size: In the Effect Size section, the standardized mean difference (Cohens d) is provided as an indicator of practical significance. In the output, d is equal to 1.69, which is considered to be a (very) large effect, according to conventional rules-of-thumb (see table below). Please note that typically we only interpret practical significance when the mean of the differences has been found to be statistically significant. Cohens d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program, which was evaluated using a pre-test/post-test without control group training evaluation design. Employees completed an assessment of their knowledge before training and after training, where assessment scores could range from 1-100. On average, employees performance on the assessment increased to a statistically significant extent from before to after completing the training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001, 95% CI[14.85, 24.43]). This improvement can be considered very large (d = 1.69). 32.2.5 Visualize Results Using Bar Chart When we find a statistically significant difference between the mean of the differences, we may decide to create a bar chart in which the mean of the first outcome variable (PreTest) and the mean of the second outcome variable (PostTest) are displayed as separate bars. We will use the BarChart function from lessR to do so. Before doing so, however, we must prep (i.e., restructure, manipulate) the data to meet the needs of the BarChart function. I will show you two approaches for prepping the data. The first approach requires the installation of the tidyr package (from the tidyverse universe of packages), and the second uses only functions from base R. In my opinion, the logic needed to apply the first approach is more straightforward; however, if you dont find that to be the case, then by all means try out the second approach. 32.2.5.1 First Approach For the first approach to prepping the data and generating a bar chart, we will use the pivot_longer function from the tidyr package, which means that we need to install and/or access the tidyr package. If youd like additional information on the restructuring data frames using the pivot_longer function, please check out the chapter on manipulating and restructuring data. If you havent already, be sure to install the lessR package; if youve recently installed the package, then you likely dont need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming its been installed), so dont forget that important step. [Note: If you run into any installation errors, try installing and access the tidyverse grand package, which subsumes the tidyr package, along with many others.] # Install tidyr package install.packages(&quot;tidyr&quot;) # Access tidyr package and its functions library(tidyr) Next, lets use the pivot_longer function from the tidyr package to pivot (i.e., manipulate, restructure) our data into long format, where long format is sometimes referred to as stacked format. As the first step, create a unique name for an object that we can subsequently assign the manipulated data frame object to. Here, I call the new data frame object td_long to indicate that the new data frame object is in long format. Insert the &lt;-operator to the right of the new data frame object name. This operator will assign the manipulated data frame object to the new object weve named. Type the name of the pivot_longer function. As the first argument, type data= followed by the exact name of the original data frame object we were working with (td). This will tell the function which data frame object we wish to manipulate. As the second argument, type the cols= argument to specify a vector of two or more variables we wish to pivot from wide format (i.e., separate columns) to long format (i.e., a single column). Our goal here is to stack the PreTest and PostTest variable names as categories (i.e., levels) within a new variable that we will subsequently create. Thus, we will type the name of the c (combine) function from base R, and as the two arguments, we will list the names of the PreTest and PostTest variables. As the third argument, type the names_to= argument followed by the name of the new stacked variable with categorical labels that wed like to create; make sure the name of the new variable is within quotation marks (\" \"). This is the variable that will contain the categories (i.e., levels) that were the PreTest and PostTest variable names from the original td data frame object. Here, I name this new variable \"Test\". As the fourth argument, type the values_to= argument followed by the name of a new variable containing the test scores that were originally nested within the PreTest and PostTest variable names from the original td data frame object; make sure the name of the new variable is within quotation marks (\" \"). Here, I name this new variable \"Score\". # Manipulate data from wide to long format (i.e., stack) td_long &lt;- pivot_longer(data=td, cols=c(PreTest,PostTest), names_to=&quot;Test&quot;, values_to=&quot;Score&quot;) Lets take a peek at our new td_long data frame object by using the Print function. # Print new data frame object to Console print(td_long) ## # A tibble: 50 x 3 ## EmpID Test Score ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 PreTest 43 ## 2 26 PostTest 66 ## 3 27 PreTest 58 ## 4 27 PostTest 74 ## 5 28 PreTest 52 ## 6 28 PostTest 62 ## 7 29 PreTest 47 ## 8 29 PostTest 84 ## 9 30 PreTest 43 ## 10 30 PostTest 78 ## # ... with 40 more rows As you can see, each observation (i.e., employee) now has two rows of data (as opposed to one), such that they have a row for PreTest scores and PostTest scores. These data are now in long (i.e., stacked) format. By default, the PreTest and PostTest levels of the Test variable we created will be in alphabetical order, such that PostTest will come before PreTest if we created any data visualizations or go to analyze the data from this variable. Intuitively, this is not the order we want, as temporally PreTest should precede (i.e., come before) PostTest. Fortunately, we can use the factor function from base R to convert the Test variable to an ordered factor. Lets overwrite the existing Test variable from the td_long data frame object by typing the name of the data frame object (td_long) followed by the $ operator and the name of the variable (Test). To the right of td_long$Test, type the &lt;- operator so that we can assign the new ordered factored variable to the existing variable called Test from the td_long data frame object. To the right of the &lt;- operator, type the name of the factor function. As the first argument, type the name of the data frame object (td_long) followed by the $ operator and the name of the variable (Test). As the second argument, type ordered=TRUE to signify that this variable will have ordered levels. As the third argument, type levels= followed by a vector of the variable levels (i.e., categories) in ascending order. Note that we use the c (combine) function from base R to construct the vector, and we need to put each level within quotation marks (\" \"). Here, we wish to order the levels such that PreTest comes before PostTest, so we insert the following: c(\"PreTest\",\"PostTest\"). # Re-order the Test variable so that PreTest comes first td_long$Test &lt;- factor(td_long$Test, ordered=TRUE, levels=c(&quot;PreTest&quot;,&quot;PostTest&quot;)) We are now ready to create our bar chart of the average scores on the pre-test and the post-test from our pre-test/post-test without control group evaluation design. Begin by typing the name of the BarChart function from the lessR package; if you havent already, be sure that you have recently installed the lessR package and that you have accessed the package in your current R session. See above for more details on how to install and access the lessR package. As the first argument, type x= followed by the name of the categorical (i.e., nominal, ordinal) variable that contains the names of the different test times. In this example, the name of the variable is Test, and it contains the levels (i.e., categories) PreTest and PostTest. As the second argument, type y= followed by the name of the variable that contains the scores for the different test administration times. In this example, the name of the variable is Score, and it contains the numeric scores on the PreTest and PostTest for each employee (i.e., observation). As the third argument, type data= followed by the exact name of the data frame object to which the variables specified in the two previous arguments belong. In this example, the name of the data frame object is td_long. As the fourth argument, type stat=\"mean\" to indicate that we wish for the BarChart function to compute the average (i.e., mean) score from the Score variable for each level of the Test variable. # Create bar chart BarChart(x=Test, y=Score, data=td_long, stat=&quot;mean&quot;) ## Score ## - by levels of - ## Test ## ## n miss mean sd min mdn max ## PreTest 25 0 52.72 7.05 43.00 51.00 66.00 ## PostTest 25 0 72.36 6.98 60.00 73.00 84.00 ## &gt;&gt;&gt; Suggestions ## Plot(Score, Test) # lollipop plot ## ## Data for: Score ## ---------------- ## PreTest PostTest ## 52.72 72.36 If you wish, you can then add additional arguments like xlab= and ylab= to re-label the x- and y-axes, respectively. # Create bar chart BarChart(x=Test, y=Score, data=td_long, stat=&quot;mean&quot;, xlab=&quot;Average Score&quot;, ylab=&quot;Time&quot;) ## Score ## - by levels of - ## Test ## ## n miss mean sd min mdn max ## PreTest 25 0 52.72 7.05 43.00 51.00 66.00 ## PostTest 25 0 72.36 6.98 60.00 73.00 84.00 ## &gt;&gt;&gt; Suggestions ## Plot(Score, Test) # lollipop plot ## ## Data for: Score ## ---------------- ## PreTest PostTest ## 52.72 72.36 32.2.5.2 Second Approach For the second approach, we will create a data frame of the means and their names so that we can use the data frame as input into the BarChart function. Come up with a unique name for a vector of means that we will create; here, I name the vector prepost_means. Type the &lt;- operator to the right of the vector name (prepost_means) so that we can assign the vector we create to the right of the &lt;- operator to the new object. Second, type the name of the c function from base R. As the first argument within the c function, type the name of the mean function from base R, and within that functions parentheses, type the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). As the second argument within the mean function, type na.rm=TRUE in case there are missing data. As the second argument within the c function, type the name of the mean function from base R, and within that functions parentheses, type the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest); as the second argument within the mean function, type na.rm=TRUE in case there are missing data. # Create vector of means for first and second outcome variables prepost_means &lt;- c(mean(td$PreTest, na.rm=TRUE), mean(td$PostTest, na.rm=TRUE)) Now its time to create a vector of the names (i.e., variable names) that correspond with the means contained within the vector we just created. First, come up with a name for the vector object that will contain the names; here, I name the vector meannames using the &lt;- operator. Second, type the name of the c function from base R. As the first argument within the function, type what you wish to name the first mean (i.e., variable name for the first value), and put it in quotation marks (\"Pre-Test\"). As the second argument within the function, type what you wish to name the second mean (i.e., variable name for the second value), and put it in quotation marks (\"Post-Test\"). # Create vector of names corresponding to the first and second means above meannames &lt;- c(&quot;Pre-Test&quot;,&quot;Post-Test&quot;) To combine these two vectors into a data frame, we will use the data.frame function from base R. First, come up with a name for the data frame object; here, I name the data frame meansdf using the &lt;- naming symbol. Second, type the name of the data.frame function. As the first argument, type the name of meannames vector we created. As the second argument, type the name of the prepost_means vector we created. # Create data frame from the two vectors created above meansdf &lt;- data.frame(meannames, prepost_means) Remove the vectors called meannames and prepost_means from your R Global environment to avoid any confusion when specifying the BarChart function. Use the remove function from base R to accomplish this. # Remove the two vectors created above from R Global Environment remove(prepost_means) remove(meannames) Now have data in a format that can be used as input in the BarChart function from the lessR package. Begin by typing the name of the BarChart function. As the first argument, type the name of the variable that contains the names of the means (meannames). As the second argument, type the name of the variable that contains the actual means (prepost_means). As the third argument, type data= followed by the name of the data frame to which the aforementioned variables belong (meansdf). As the fourth argument, use xlab= to provide the x-axis label (\"Time\"). As the fifth argument, use ylab= to provide the y-axis label (\"Average Score\"). # Create bar chart BarChart(meannames, prepost_means, data=meansdf, xlab=&quot;Time&quot;, ylab=&quot;Average Score&quot;) ## &gt;&gt;&gt; Suggestions ## Plot(prepost_means, meannames) # lollipop plot ## ## Data for: prepost_means ## ------------------------ ## Pre-Test Post-Test ## 52.72 72.36 32.2.6 Summary In this chapter, we learned how to estimate a paired-samples t-test to determine whether the mean of the differences between scores on two dependent variables is significantly different from zero. This analysis is useful when evaluating a pre-test/post-test without control group training design. The ttest function from lessRcan be used to run paired-samples t-tests. We also learned how to visualize the results using the BarChart function from lessR. 32.3 Chapter Supplement In addition to the ttest function from the lessR package covered above, we can use the t.test function from base R to estimate a paired-samples t-test. Because this function comes from base R, we do not need to install and access an additional package. 32.3.1 Functions &amp; Packages Introduced Function Package shapiro.test base R t.test base R cohen.d effsize c base R mean base R 32.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PrePostOnly.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## PreTest = col_double(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;PreTest&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [25 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:25] 26 27 28 29 30 31 32 33 34 35 ... ## $ PreTest : num [1:25] 43 58 52 47 43 50 66 48 49 49 ... ## $ PostTest: num [1:25] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. PreTest = col_double(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID PreTest PostTest ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 43 66 ## 2 27 58 74 ## 3 28 52 62 ## 4 29 47 84 ## 5 30 43 78 ## 6 31 50 73 32.3.3 t.test Function from Base R By itself, the t.test function from base R does not generate statistical assumption tests and and estimates of effect size (practical significance) like the ttest function from lessR does. Thus, we will need to apply additional functions to achieve all of the necessary output. Before running our paired-samples t-test using the t.test function from base R, we should test the second assumption (see Statistical Assumptions section). To do so, we will estimate the Shapiro-Wilk normality test. This test was covered in detail above when we applied the ttest function from lessR, so we will breeze through the interpretation in this section. Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then we can assume that the assumption of univariate normality has been met, which means we wont formally test the assumption for the two independent samples. To compute the Shapiro-Wilk normality test to test the assumption that the difference scores are normally distributed, we will use the shapiro.test function from base R. First, type the name of the shapiro.test function. As the sole argument, type the name of the data frame (td), followed by the $ symbol and the difference between the PostTest scores and PreTest scores: td$PostTest - td$PreTest. # Compute Shapiro-Wilk normality test for normal distribution shapiro.test(td$PostTest - td$PreTest) ## ## Shapiro-Wilk normality test ## ## data: td$PostTest - td$PreTest ## W = 0.96874, p-value = 0.6134 The output of the script/code above indicates that the p-value associated with the test is equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the difference scores are normally distributed. In other words, we have no evidence to suggest that the difference score variable scores are anything other than normally distributed. With confidence that we met the statistical assumptions for a paired-samples t-test, we are ready to apply the t.test function from base R. To begin, type the name of the t.test function. As the first argument, specify the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest). As the second argument, specify the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). Note that the ordering of the variables is opposite of the ttest function from lessR. Difference scores will be calculated as part of the function by subtracting the variable in the second argument from the variable in the first variable argument. For the third argument, enter paired=TRUE to inform R that the data are paired, which means you are requesting a paired-samples t-test. # Paired-samples t-test using t.test function from base R t.test(td$PostTest, td$PreTest, paired=TRUE) ## ## Paired t-test ## ## data: td$PostTest and td$PreTest ## t = 8.4677, df = 24, p-value = 0.00000001138 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 14.853 24.427 ## sample estimates: ## mean of the differences ## 19.64 Note that the t.test output provides you with the results of the paired-samples t-test in terms of the formal statistical test (t = 8.4677, p &lt; .001), the 95% confidence interval (95% CI[14.853, 24.427]), and the mean of the differences (M = 19.64). The output, however, does not include an estimate of practical significance. To compute Cohens d as an estimate of practical significance we will use the cohen.d function from the effsize package. If you havent already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function, specify the name of the data frame (td), followed by the $ symbol and the name of the second outcome variable (PostTest). As the second argument, specify the name of the data frame (td), followed by the $ symbol and the name of the first outcome variable (PreTest). As the third argument, type paired=TRUE to indicate that the data are indeed paired (i.e., dependent). # Compute Cohen&#39;s d cohen.d(td$PostTest, td$PreTest, paired=TRUE) ## ## Cohen&#39;s d ## ## d estimate: 2.800502 (large) ## 95 percent confidence interval: ## lower upper ## 1.325315 4.275689 The output indicates that Cohens d is 1.694, which would be considered large by conventional cutoff standards. Cohens d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program, which was evaluated using a pre-test/post-test without control group training evaluation design. Employees completed an assessment of their knowledge before training and after training, where assessment scores could range from 1-100. On average, employees performance on the assessment increased to a statistically significant extent from before to after completing the training (M = 19.64, SD = 11.60, t = 8.468, p &lt; .001, 95% CI[14.85, 24.43]). This improvement can be considered very large (d = 1.69). "],["posttestonly.html", "Chapter 33 Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test 33.1 Conceptual Overview 33.2 Tutorial 33.3 Chapter Supplement", " Chapter 33 Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples t-test In this chapter, we learn about the post-test-only with control group training evaluation design and how a independent-samples t-test can be used to analyze the data acquired from this design. Well begin with conceptual overviews of this training evaluation design and of the independent-samples t-test, and then well conclude with a tutorial. 33.1 Conceptual Overview In this section, well begin by describing the post-test-only with control group training evaluation design, and well conclude by reviewing the independent-samples t-test, with discussions of statistical assumptions, statistical significance, and practical significance; the section wraps up with a sample-write up of a independent-samples t-test used to evaluate data from a post-test-only with control group training evaluation design. 33.1.1 Review of Post-Test-Only with Control Group Design In a post-test-only with control group training evaluation design (i.e., research design), employees are assigned (randomly or non-randomly) to either a treatment group (e.g., new training program) or a control group (e.g., comparison group, old training program), and every participating employee is assessed on selected training outcomes (i.e., measures) after the training has concluded. If random assignment to groups is used, then a post-test-only with control group design is considered experimental. Conversely, if non-random assignment to groups is used, then the design is considered quasi-experimental. Regardless of whether random or non-random assignment is used, an independent-samples t-test can be used to analyze the data from a post-test-only with control group design, provided key statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a post-test-only with control group design. As a major strength, this design includes a control group, and if coupled with random assignment to groups, then the design qualifies as a true experimental design. With that being said, if we use non-random assignment to the treatment and control groups, then we are less likely to have equivalent groups of individuals who enter each group, which may bias how they engage in the demands of their respective group and how they complete the outcome measures. Further, because this design lacks a pre-test (i.e., assessment of initial performance on the outcome measures), we cannot be confident that employees in the treatment and control groups started in the same place with respect to the outcome(s) we might measure at post-test. Consequently, any differences we observe between the two groups on a post-test outcome measure may reflect pre-existing differences  meaning, the training may not have caused the differences that are apparent at post-test. 33.1.2 Review of Independent-Samples t-test The independent-samples t-test is an inferential statistical analysis that can be used to compare the to compare the means of two independent groups, such as a treatment and control group. That is, this analysis compares differences in means when we have two separate groups of cases drawn from two populations; critically, each case must appear in only one of the two samples, hence the name independent-samples t-test. In the context of a post-test-only training evaluation design, we can conceptualize group membership (e.g., treatment vs. control) as a categorical (nominal, ordinal) predictor variable with just two categories (i.e., levels), and the post-test outcome measure as a continuous (interval, ratio) outcome variable. Importantly, the outcome variable must be continuous for an independent-samples t-test to be an appropriate analysis. The independent-samples t-test is sometimes called a between-subjects t-test or a two-groups t-test. The formula for an independent-samples t-test can be written as follows: \\(t = \\frac{\\overline{X}_1 - \\overline{X}_2}{s_{{X}_{1}{X}_2} \\sqrt{\\frac{2}{n}}}\\) where \\(X_{1}\\) is the mean of the first group and \\(X_{2}\\) is the mean of the second group, \\(s_{{X}_{1}{X}_2}\\) is the pooled standard deviation of \\(X_{1}\\) and \\(X_{2}\\), and \\(n\\) refers to the number of cases (i.e., sample size). 33.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from an independent-samples t-test include: The outcome (dependent, response) variable has a univariate normal distribution in each of the two underlying populations (e.g., samples, groups, conditions), which correspond to the two categories (levels) of the predictor (independent, explanatory) variable; The variances of the outcome (dependent, response) variable are equal across the two populations (e.g., samples, groups, conditions), which is often called the equality of variances or homogeneity of variances assumption. 33.1.2.2 Statistical Significance If we wish to know whether the two means we are comparing differ to a statistically significant extent, we can compare our t-value value to a table of critical values of a t-distribution. If our calculated value is larger than the critical value given the number of degrees of freedom (df = n - 2) and the desired alpha level (i.e., significance level, p-value threshold), we would conclude that there is evidence of a significant difference in means between the two independent samples. Alternatively, we can calculate the exact p-value if we know the t-value and the degrees of freedom. Fortunately, modern statistical software calculates the t-value, degrees of freedom, and p-value for us. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the difference between the two means is equal to zero. In other words, if the p-value is less than .05, we conclude that the two means differ from each other to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the difference between the two means is equal to zero. Put differently, if the p-value is equal to or greater than .05, we conclude that the two means do not differ from zero to a statistically significant extent, leading us to conclude that there is no difference between the two means in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our independent-samples t-test is estimated using data from a sample drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, the observed difference between the two means is a point estimate of the population parameter and is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether the difference between two means is is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying populations and construct CIs for each of those samples, then the true parameter (i.e., true value of the difference in means in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 33.1.2.3 Practical Significance A significant independent-samples t-test and associated p-value only tells us that the two means are statistically different from one another. It does not, however, tell us about the magnitude of the difference between means  or in other words, the practical significance. The standardized mean difference score (Cohens d) is an effect size, which means that it is a standardized metric that can be used to compare d-values compare samples. In essence, the Cohens d indicates the magnitude of the difference between means in standard deviation units. A d-value of .00 indicates that there is no difference between the two means, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohens d Description .20 Small .50 Medium .80 Large Here is the formula for computing d: \\(d = t \\sqrt{\\frac{n_1 + n_2} {n_1n_2}}\\) where \\(t\\) refers to the calculated \\(t\\)-value, \\(n_1\\) refers to the sample size of the first independent sample, and \\(n_2\\) refers to the sample size of the second independent sample. 33.1.2.4 Sample Write-Up One group of 25 participants completes a new safety training program (treatment group), and a separate group of 25 participants completes the old safety training program (control group). Both groups of participants complete a safety knowledge test one week after completing their respective training programs. In other words, these participants were part of a post-test-only with control group training evaluation design. An independent-samples t-test was used to determine whether the average safety knowledge test score for the group that completed the new training program was significantly different than the average safety knowledge test score for the group that completed the old training program. We found a statistically significant difference between the means on the safety knowledge test for the treatment group that participated in the new training program (M = 72.36, SD = 6.98, n = 25) and the control group that participated in the old training program (M = 61.32, SD = 9.15, n = 25), such that those who participated in the new training program performed better on the safety knowledge test (t = 4.80, p &lt; .01, 95% CI[6.41, 15.67]). Further, because we found a statistically significant difference in means, we then interpreted the practical significance of this effect, which was large (d = 1.36). 33.2 Tutorial This chapters tutorial demonstrates how to estimate an independent-samples t-test, test the associated statistical assumptions, and present the findings in writing and visually. 33.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/oATcHuMZtuo 33.2.2 Functions &amp; Packages Introduced Function Package ttest lessR BarChart lessR 33.2.3 Initial Steps If you havent already, save the file called TrainingEvaluation_PostControl.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called TrainingEvaluation_PostControl.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PostControl.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Condition = col_character(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [50 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:50] 26 27 28 29 30 31 32 33 34 35 ... ## $ Condition: chr [1:50] &quot;New&quot; &quot;New&quot; &quot;New&quot; &quot;New&quot; ... ## $ PostTest : num [1:50] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 New 66 ## 2 27 New 74 ## 3 28 New 62 ## 4 29 New 84 ## 5 30 New 78 ## 6 31 New 73 There are 50 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), Condition (training condition: New = new training program, Old = old training program), and PostTest (post-training scores on training assessment, ranging from 1-100, where 100 indicates better performance). Regarding participation in the training conditions, 25 employees participated in the old training program, and 25 employees participated in the new training program. 33.2.4 Estimate Independent-Samples t-test Different functions are available that will allow us to estimate an independent-samples t-test in R. In this chapter, we will review how to run an independent-samples t-test using the ttest function from the lessR package, as it produces a generous amount of output automatically. Using this function, we will evaluate whether the means on the post-test variable (PostTest) differ based on the two levels of the training Condition variable (New, Old) differ from one another to a statistically significant extent; in other words, lets find out if we should treat the means as being different from one another. To access and use the ttest function, we need to install and/or access the lessR package. If you havent already, be sure to install the lessR package; if youve recently installed the package, then you likely dont need to install it in this session, and you can skip that step to save time. You will need to run the library function to access the package (assuming its been installed), so dont forget that important step. # Install lessR package install.packages(&quot;lessR&quot;) # Access lessR package and its functions library(lessR) Please note that when you use the library function to access the lessR package in a new R or RStudio session, you will likely receive a message in red font in your Console. Typically, red font in your Console is not a good sign; however, accessing the lessR package is one of the unique situations in which a red-font message is not a warning or error message. You may also receive a warning message that indicates that certain objects are masked. For our purposes, we can ignore that message. Now were ready to run an independent-samples t-test. To begin, type the name of the ttest function. As the first argument in the parentheses, specify the statistical model that we wish to estimate. To do so, type the name of the continuous outcome (dependent) variable (PostTest) to the left of the ~ operator and the name of the categorical predictor (independent) variable (Condition) to the right of the ~ operator. For the second argument, use data= to specify the name of the data frame where the outcome and predictor variables are located (td). For the third argument, enter paired=FALSE to inform R that the data are not paired (i.e., you are not requesting a paired-samples t-test). # Estimate independent-samples t-test ttest(PostTest ~ Condition, data=td, paired=FALSE) ## ## Compare PostTest across Condition levels New and Old ## ## ------ Describe ------ ## ## PostTest for Condition New: n.miss = 0, n = 25, mean = 72.360, sd = 6.975 ## PostTest for Condition Old: n.miss = 0, n = 25, mean = 61.320, sd = 9.150 ## ## Mean Difference of PostTest: 11.040 ## ## Weighted Average Standard Deviation: 8.136 ## ## ## ------ Assumptions ------ ## ## Note: These hypothesis tests can perform poorly, and the ## t-test is typically robust to violations of assumptions. ## Use as heuristic guides instead of interpreting literally. ## ## Null hypothesis, for each group, is a normal distribution of PostTest. ## Group New Shapiro-Wilk normality test: W = 0.950, p-value = 0.253 ## Group Old Shapiro-Wilk normality test: W = 0.969, p-value = 0.621 ## ## Null hypothesis is equal variances of PostTest, i.e., homogeneous. ## Variance Ratio test: F = 83.727/48.657 = 1.721, df = 24;24, p-value = 0.191 ## Levene&#39;s test, Brown-Forsythe: t = -0.955, df = 48, p-value = 0.344 ## ## ## ------ Infer ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## t-cutoff for 95% range of variation: tcut = 2.011 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t = 4.798, df = 48, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.627 ## 95% Confidence Interval for Mean Difference: 6.413 to 15.667 ## ## ## --- Do not assume equal population variances of PostTest for each Condition ## ## t-cutoff: tcut = 2.014 ## Standard Error of Mean Difference: SE = 2.301 ## ## Hypothesis Test of 0 Mean Diff: t = 4.798, df = 44.852, p-value = 0.000 ## ## Margin of Error for 95% Confidence Level: 4.635 ## 95% Confidence Interval for Mean Difference: 6.405 to 15.675 ## ## ## ------ Effect Size ------ ## ## --- Assume equal population variances of PostTest for each Condition ## ## Standardized Mean Difference of PostTest, Cohen&#39;s d: 1.357 ## ## ## ------ Practical Importance ------ ## ## Minimum Mean Difference of practical importance: mmd ## Minimum Standardized Mean Difference of practical importance: msmd ## Neither value specified, so no analysis ## ## ## ------ Graphics Smoothing Parameter ------ ## ## Density bandwidth for Condition New: 4.175 ## Density bandwidth for Condition Old: 5.464 As you can see in the output, the ttest function provides descriptive statistics, assumption tests regarding the distributions and the variances, a statistical significance test of the mean comparison, and an indicator of practical significance in the form of the standardized mean difference (Cohens d). In addition, the default data visualization depicting the two density distributions is helpful for understanding the difference between the two means. Lets now review each section of the output. Description: The Description section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees in each condition (n = 25), and descriptively, the mean PostTest score for the New condition is 72.36 (SD = 6.98), and the mean PostTest score for the Old condition is 61.32 (SD = 9.15). The difference between the two means is 11.04. Assumptions: As noted in the output: These hypothesis tests can perform poorly, and the t-test is typically robust to violations of assumptions. Use as heuristic guides instead of interpreting literally. Given that, we shouldnt put too much of an emphasis on these statistical assumption tests, but nevertheless than can provide some guidance when it comes to detecting potential statistical-assumption violations that might preclude us from choosing to interpret the results of the independent-samples t-test itself. Lets dive into interpreting these assumption tests. The Shapiro-Wilk normality test is used to test the null hypothesis that a distribution is normal; as such, if the p-value associated with the test statistic (W) is less than the conventional alpha level of .05, then we reject the null hypothesis and assume that the distribution is not normal. If, however, p-value associated with the test statistic (W) is greater than .05, then we fail to reject the null hypothesis, which means that we do not have statistical evidence that the distribution is anything other than normal; in other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is normally distributed, and can feel satisfied that we have met the statistical assumption of normality for that particular distribution. In the output, we can see that the PostTest variable for those in the New training condition is normally distributed (W = .950, p = .253), and for those in the Old training condition, the PostTest variable is also normally distributed (W = .969, p = .621). Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then the ttest function wont report the Shapiro-Wilk test, as univariate normality will be assumed. As for the equal variances assumption, Levenes test (i.e., homogeneity of variances test) is commonly used. The null hypothesis of this test is that the variances are equal. Thus, if the p-value is less than the conventional alpha level of .05, then we reject the null hypothesis and assume the variances are different. If, however, the p-value is equal to or greater than .05, then we fail to reject the null hypothesis and assume that the variances are equal (i.e., variances are homogeneous). In the output, we see that the test is nonsignificant (t = -.955, p = .344), which suggests that, based on this test, we have no reason to believe that the two variances are anything but equal. All in all, we found evidence to support that we met the two statistical assumptions necessary to proceed forward with making inferences. Inference: The Inference section of the output is where you will find the independent-samples t-test itself. This section is called Inference because this is where were making statistical inferences about the underlying population of employees. Specifically, the t-test and its associated p-value represent the statistical test of the null hypothesis (i.e., the two means are equal). If we have evidence that the variances are equal (which we do based on Levenes test), then we should interpret the sub-section titled Assume equal population variances of PostTest for each Condition. If we had instead found evidence that the variances were not equal, then we would want to interpret the sub-section titled Do not assume equal population variances of PostTest for each Condition. First, take a look at the line prefaced with Hypothesis Test of 0 Mean Diff; this line contains the results of the independent-samples t-test (t = 4.798, p &lt; .001). Because the p-value is less than our conventional two-tailed alpha cutoff of .05, we reject the null hypothesis and conclude that the two means are different from one another. How do we know which mean is greater (or less than) the other? Well, we need to look back to the Description section; in that section, we see that the mean PostTest score for the New training condition is greater than the mean for the Old training condition; as such, we can conclude the following: The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001). Regarding the 95% confidence interval, we can conclude that the true mean difference in the population is likely between 6.41 and 15.67 (on a 1-100 point assessment). Effect Size: In the Effect Size section, the standardized mean difference (Cohens d) is provided as an indicator of practical significance. In the output, d is equal to 1.36, which is considered to be a (very) large effect, according to conventional rules-of-thumb (see table below). Please note that typically we only interpret practical significance when a difference has been found to be statistically significant (see Inference section). Cohens d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program and 25 employees participated in the old training program. After completely their respective training programs, employees completed an assessment of their knowledge, where scores could range from 1-100. The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001, 95% CI[6.41, 15.67]). This difference can be considered to be very large (d = 1.36). 33.2.5 Visualize Results Using Bar Chart When we find a statistically significant difference between two means based on an independent-samples t-test, you may decide to present the two means in a bar chart. We will use the BarChart function from lessR to do so. Type the name of the BarChart function. As the first argument, type x= followed by the name of the categorical predictor variable (Condition). As the second argument, type y= followed by the name of the continuous outcome variable (PostTest). As the third argument, specify stat=\"mean\" to request the application of the mean function to the PostTest variable based on the levels of the Condition variable. As the fourth argument, type data= followed by the name of the data frame object to which our predictor and outcome variables belong (td). As the fifth argument, use xlab= to provide the x-axis label (\"Training Condition\"). As the sixth argument, use ylab= to provide the y-axis label (\"Post-Test Score\"). # Create bar chart BarChart(x=Condition, y=PostTest, stat=&quot;mean&quot;, data=td, xlab=&quot;Training Condition&quot;, ylab=&quot;Post-Test Score&quot;) ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## Old 25 0 61.32 9.15 42.00 61.00 79.00 ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, Condition) # lollipop plot ## ## Data for: PostTest ## ------------------- ## New Old ## 72.36 61.32 33.2.6 Summary In this chapter, we learned to use the independent-samples t-test to compare two means from independent groups of cases, which is the case when we evaluating a training program using a post-test-only with control group design. The ttest function from lessR can be used to run an independent-samples t-test, and the BarChart function from lessR can be used to present the results of a significant difference visually. 33.3 Chapter Supplement In addition to the ttest function from the lessR package covered above, we can use the t.test function from base R to estimate an independent-samples t-test. Because this function comes from base R, we do not need to install and access an additional package. 33.3.1 Functions &amp; Packages Introduced Function Package tapply base R shapiro.test base R leveneTest car t.test base R cohen.d effsize mean base R 33.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_PostControl.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Condition = col_character(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [50 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:50] 26 27 28 29 30 31 32 33 34 35 ... ## $ Condition: chr [1:50] &quot;New&quot; &quot;New&quot; &quot;New&quot; &quot;New&quot; ... ## $ PostTest : num [1:50] 66 74 62 84 78 73 60 61 71 83 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 26 New 66 ## 2 27 New 74 ## 3 28 New 62 ## 4 29 New 84 ## 5 30 New 78 ## 6 31 New 73 33.3.3 t.test Function from Base R By itself, the t.test function from base R does not generate statistical assumption tests and and estimates of effect size (practical significance) like the ttest function from lessR does. Thus, we will need to apply additional functions to achieve all of the necessary output. Before running our independent-samples t-test using the t.test function from base R, we should test two assumptions (see Statistical Assumptions section). We will begin by estimating the Shapiro-Wilk normality test and Levenes test of equal variances (i.e., homogeneity of variances), which are two statistical tests of the two statistical assumptions. Both of these tests were covered in detail above when we applied the ttest function from lessR, so we will breeze through the interpretation in this section. To compute the Shapiro-Wilk normality test to test the assumption of normal distributions, we will use the shapiro.test function from base R. Because we need to test the assumption of normality of the outcome variable (PostTest) for both levels of the predictor variable (Condition), we also need to use the tapply function from base R. The tapply function can be quite useful, as it allows us to apply a function to a variable for each level of another categorical (nominal, ordinal) variable. To begin, type the name of the tapply function. As the first argument, type the name of the data frame (td), followed by the $ symbol and the name of the outcome variable (PostTest). As the second argument, type the name of the data frame (td), followed by the $ symbol and the name of the categorical predictor variable (Condition). Finally, as the third argument, type the name of the shapiro.test function. Note: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Thus, if there are more than 30 cases in each independent sample, then we can assume that the assumption of univariate normality has been met, which means we wont formally test the assumption for the two independent samples. # Compute Shapiro-Wilk normality test for normal distributions tapply(td$PostTest, td$Condition, shapiro.test) ## $New ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.95019, p-value = 0.2533 ## ## ## $Old ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.96904, p-value = 0.6208 The output indicates that the p-values associated with both tests are equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed. In other words, we have evidence that the outcome variable is normally distributed for both conditions, which suggests that we have met the first statistical assumption. To test the equality (homogeneity) of variances assumption, we will use the leveneTest function from the car package. More than likely the car package is already installed, as many other packages are dependent on it. That being said, you may still need to install the package prior to accessing it using the library function. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the outcome variable (PostTest) to the left of the ~ symbol and the name of the predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). # Compute Levene&#39;s test for equal variances leveneTest(PostTest ~ Condition, data=td) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.9121 0.3443 ## 48 The output indicates that the p-value (i.e., Pr(&gt;F) = .3443) associated with Levenes test is equal to or greater than an alpha of .05; thus, we fail to reject the null hypothesis that the variances are equal and thus conclude that the variances are equal. We have satisfied the second statistical assumption. Now that we seem to have satisfied the two statistical assumptions, we are ready to apply the t.test function from base R. To begin, type the name of the t.test function. As the first argument, type the name of the outcome (dependent) variable (PostTest) to the left of the ~ symbol and the name of the predictor (independent) variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). As the third argument, type paired=FALSE to indicate that the data are not paired, which means to that are not requesting a paired-samples t-test. As the final argument, type var.equal=TRUE to indicate that we found evidence that the variances were equal; if you had found evidence that the variances were not equal, then you would use the argument var.equal=FALSE. # Independent-samples t-test using t.test function from base R t.test(PostTest ~ Condition, data=td, paired=FALSE, var.equal=TRUE) ## ## Two Sample t-test ## ## data: PostTest by Condition ## t = 4.7976, df = 48, p-value = 0.000016 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 6.413209 15.666791 ## sample estimates: ## mean in group New mean in group Old ## 72.36 61.32 Note that the output provides you with the results of the independent-samples t-test in terms of a formal statistical test (t = 4.798, p &lt; .001), the 95% confidence interval (95% CI[6.413, 15.667]), and the mean of the outcome variable for each level of the categorical predictor variable (New condition: M = 72.36; Old condition: M = 61.32). Thus, we found evidence that the mean PostTest score for the New training condition was statistically significantly higher than the mean of the Old training condition. The output, however, does not include an estimate of practical significance (i.e., effect size). To compute Cohens d as an indicator of practical significance, we will use the cohen.d function from the effsize package. If you havent already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the outcome variable (PostTest) to the left of the ~ symbol and the name of the predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). As the third argument, type paired=FALSE to indicate that the data are not paired (i.e., dependent). # Compute Cohen&#39;s d cohen.d(PostTest ~ Condition, data=td, paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.356961 (large) ## 95 percent confidence interval: ## lower upper ## 0.7262066 1.9877157 The output indicates that Cohens d is 1.357, which would be considered large by conventional cutoff standards. Cohens d Description .20 Small .50 Medium .80 Large Sample Write-Up: In total, 25 employees participated in the new training program and 25 employees participated in the old training program. After completely their respective training programs, employees completed an assessment of their knowledge, where scores could range from 1-100. The average post-training assessment score for employees who participated in the new training program (M = 72.36, SD = 6.98) was significantly higher than the average score for those who participated in the old training program (M = 61.32, SD = 9.15) (t = 4.798, p &lt; .001, 95% CI[6.41, 15.67]). This difference can be considered to be very large (d = 1.36). "],["posttestonly-threegroups.html", "Chapter 34 Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA 34.1 Conceptual Overview 34.2 Tutorial 34.3 Chapter Supplement", " Chapter 34 Evaluating a Post-Test-Only with Two Comparison Groups Design Using One-Way ANOVA In this chapter, we learn about the post-test-only with two comparison groups training evaluation design and how a one-way analysis of variance (ANOVA) can be used to analyze the data acquired from this design. Well begin with conceptual overviews of this training evaluation design and of the one-way ANOVA, and then well conclude with a tutorial. 34.1 Conceptual Overview In this section, we will begin with a description of the post-test-only with two comparison groups training evaluation design. The section concludes with a review of the one-way analysis of variance (ANOVA), including discussions of statistical assumptions, omnibus F-test, post-hoc pairwise mean comparisons, statistical significance, and practical significance; the section wraps up with a sample-write up of a one-way ANOVA used to evaluate data from a post-test-only with two comparison groups training evaluation design. 34.1.1 Review of Post-Test-Only with Two Comparison Groups Design In a post-test-only with two comparison groups training evaluation design (i.e., research design), employees are assigned (randomly or non-randomly) to either a treatment group (e.g., new training program), or one of two comparison groups (e.g., old training program and control group), and every participating employee is assessed on selected training outcomes (i.e., measures) after the training has concluded. If random assignment to groups is used, then a post-test-only with two comparison groups design is considered experimental. Conversely, if non-random assignment to groups is used, then the design is considered quasi-experimental. Regardless of whether random or non-random assignment is used, a one-way analysis of variance (ANOVA) can be used to analyze the data from a post-test-only with two comparison groups design, provided key statistical assumptions are satisfied. Like any evaluation design, there are limitations to the inferences and conclusions we can draw from a post-test-only two comparison groups design. As a strength, this design includes two comparison groups, and if coupled with random assignment to groups, then the design qualifies as a true experimental design. With that being said, if we use non-random assignment to the different groups, then we are less likely to have equivalent groups of individuals who enter each group, which may bias how they engage in the demands of their respective group and how they complete the outcome measures. Further, because this design lacks a pre-test (i.e., assessment of initial performance on the outcome measures), we cannot be confident that employees in the three groups started in the same place with respect to the outcome(s) we might measure at post-test. Consequently, any differences we observe between the three groups on a post-test outcome measure may reflect pre-existing differences  meaning, the training may not have caused the differences that are apparent at post-test. 34.1.2 Review of One-Way ANOVA Analysis of variance (ANOVA) is part of a family of analyses aimed at comparing means, which includes one-way ANOVA, repeated-measures ANOVA, factorial ANOVA, and mixed-factorial ANOVA. In general, an ANOVA is used to compare three or more means; however, it can be used to compare two means on a single factor  but you might as well just use an independent-samples t-test if this is the case. When comparing means, an omnibus F-test is employed to determine whether there are any differences in means across the levels of the categorical (nominal, ordinal) predictor variable. In other words, with an ANOVA, we are attempting to reject the null hypothesis that that there are no mean differences across levels of the categorical predictor variable. It is important to remember, however, that the F-test is an omnibus test, which means that its p-value only indicates whether mean differences exist across two or more means and not where those specific differences in means exist. Typically, post-hoc pairwise comparison tests can be used to uncover which specific pairs of levels (categories) show differences in means. The one-way ANOVA refers to one of the most basic forms of ANOVA  specifically, an ANOVA in which there is only a single categorical predictor variable and a continuous outcome variable. The term one-way indicates that there is just a single factor (i.e., predictor variable, independent variable). If we were to have two categorical predictor variables, then we could call the corresponding analysis a factorial ANOVA or more specifically a two-way ANOVA. A one-way ANOVA is employed to test the equality of two or more means on a continuous (interval, ratio) outcome variable (i.e., dependent variable) all at once by using information about the variances. For a one-way ANOVA the null hypothesis is typically that all means are equal, or rather, there are no differences between the means. More concretely, an F-test is used as an omnibus test for a one-way ANOVA. In essence, the F-test reflects the between-level (between-group, between-category) variance divided by the within-group variance. To calculate the degrees of freedom (df) for the numerator (between-group variance), we subtract 1 from the number of groups (df = k - 1). To calculate the df for the denominator (within-group variance), we subtract the number of groups from the number of people in the overall sample (df = n - k). Note: In this chapter, we will focus on exclusively on applying a one-way ANOVA with balanced groups, where balanced groups means that each group (i.e., category) has the same number of independent cases. The default approach to calculating sum of squares in most R ANOVA functions is to use what is often referred to as Type I sum of squares. Type II and Type III sum of squares refer to the other approaches to calculating sum of squares for an ANOVA. Results are typically similar between Type I, Type II, and Type III approaches when the data are balanced across groups designated by the factors (i.e., predictor variables). To use Type II and Type III sum of squares, I recommend that you use the Anova function from the car package, which we will not cover in this tutorial. Because we are only considering considering a single between-subjects factor (i.e., one-way ANOVA) and have a balanced design, we wont concern ourselves with this distinction. If, however, you wish to extend the one-way ANOVA to a two-way ANOVA, I recommend checking out this link to the R-Bloggers site. To illustrate the computational underpinnings of a one-way ANOVA, we can dive into some formulas for the grand mean, total variation, between-group variation, within-group variation, and the F-test. For the sake of simplicity, I am presenting formulas in which we will assume equal sample sizes for each level of the categorical predictor variable (i.e., equal sample sizes for each independent sample or group); this is often referred to as a balanced design, as noted above. Grand Mean: The formula for the grand mean is as follows: \\(\\overline{Y}_{..} = \\frac{\\sum Y_{ij}}{n}\\) where \\(\\overline{Y}_{..}\\) is the grand mean, \\(Y_{ij}\\) represents a score on the outcome variable, and \\(n\\) is the total sample size. Sum of Squares Between Groups (Between-Group Variation): The formula for the sum of squares between groups is as follows: \\(SS_{between} = \\sum n_{j}(\\overline{Y}_{.j} - \\overline{Y}_{..})^{2}\\) where \\(SS_{between}\\) refers to the sum of squares between groups, \\(n_{j}\\) is the sample size for each group (i.e., level of the categorical predictor variable) assuming equal sample sizes, \\(\\overline{Y}_{.j}\\) represents each groups mean on the continuous outcome variable, and \\(\\overline{Y}_{..}\\) is the grand mean for the outcome variable (i.e., sample mean). In essence, \\(SS_{between}\\) represents variation between the group means. We can compute the variance between groups dividing \\(SS_{between}\\) by the between-groups degrees of freedom (df = k - 1), as shown below. Variance Between Groups: The formula for the variance between groups is as follows: \\(s_{between}^{2} = \\frac{SS_{between}}{k-1}\\) where \\(s_{between}^{2}\\) refers to the variance between groups, \\(SS_{between}\\) refers to the sum of squares between groups, \\(k\\) is the number of levels (categories, groups) for the categorical predictor variable, and \\(k-1\\) is the between-groups df. Sum of Squares Within Groups (Within-Group Variation): The formula for the sum of squares within groups is as follows: \\(SS_{within} = \\sum \\sum (Y_{ij} - Y_{.j})^{2}\\) where \\(SS_{within}\\) refers to the sum of squares within groups (or error variation), \\(Y_{ij}\\) represents a score on the continuous outcome variable, and \\(Y_{.j}\\) represents each groups mean. In essence, \\(SS_{within}\\) represents variation within the groups. We can compute the variance within groups by dividing \\(SS_{within}\\) by the within-groups degrees of freedom (df = n - k), as shown below. Variance Within Groups: The formula for the variance within groups is as follows: \\(s_{within}^{2} = \\frac{SS_{within}}{n-k}\\) where \\(s_{within}^{2}\\) refers to the variance within groups, \\(SS_{within}\\) refers to the sum of squares within groups, \\(n\\) is the total sample size, \\(k\\) is the the number of groups (i.e., levels of the categorical predictor variable), and \\(n-k\\) is the within-groups df. F-value: We can compute the F-value associated with the omnibus tests of means using the following formula: \\(F = \\frac{s_{between}^{2}}{s_{within}^{2}}\\) where \\(F\\) is the F-test value, \\(s_{between}^{2}\\) is the variance between groups, and \\(s_{within}^{2}\\) is the variance within groups. 34.1.2.1 Post-Hoc Pairwise Mean Comparison Tests As a reminder, the F-test indicates whether differences between the group means exist, but it doesnt indicate which specific pairs of groups differ with respect to their means. Thus, we use post-hoc pairwise mean comparison tests to evaluate which groups have (or do not have) significantly different means and the direction of those differences. Post-hoc tests like Tukeys test and Fishers test help us account for what is called family-wise error, where family-wise error refers to the increased likelihood of making Type I errors (i.e., finding something that doesnt really exist in the population; false positive) because we are running multiple pairwise comparisons and may capitalize on chance. Other tests like Dunnetts C are useful when the assumption of equal variances cannot be met. Essentially, the post-hoc pairwise mean comparison tests are independent-samples t-tests that account for the fact that we are making multiple comparisons, resulting in adjustments to the associated p-values. 34.1.2.2 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple linear regression model include: The outcome (dependent, response) variable has a univariate normal distribution in each of the two or more underlying populations (e.g., samples, groups, conditions), which correspond to the two or more categories (levels, groups) of the predictor (independent, explanatory) variable; The variances of the outcome (dependent, response) variable are equal across the two or more populations (e.g., levels, groups, categories), which is often called the equality of variances or homogeneity of variances assumption. 34.1.2.3 Statistical Significance As noted above, for a one-way ANOVA, we use an omnibus F-test and associated p-value to test whether there are statistical significant differences across groups means. Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the differences between the two or means are equal to zero. In other words, if the p-value is less than .05, we conclude that there are statistically significant differences across the group means. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that there are differences across the two or more means. If our omnibus F-test is found to be statistical significant, then we will typically move ahead by performing post-hoc pairwise comparison tests (e.g., Tukeys test, Fishers test, Dunnetts C) and examine the p-value associated with each pairwise comparison test. We interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the difference between the two means is equal to zero. In other words, if the p-value is less than .05, we conclude that the two means differ from each other to a statistically significant extent. In contrast, if the p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the difference between the two means is equal to zero. As noted above, typically, the pairwise comparison tests adjust the p-values for family-wise error to reduce the likelihood of making Type I errors (i.e., false positives). When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. 34.1.2.4 Practical Significance A significant omnibus F-test and associated p-value only tells us that the means in question differ to a statistically significant across groups (e.g., levels, categories). It does not, however, tell us about the magnitude of the difference across means  or in other words, the practical significance. Fortunately, there are multiple model-level effect size indicators, like R2, \\(\\eta^{2}\\), \\(\\omega^{2}\\), and Cohens \\(f\\). All of these provide an indication of the amount of variance explained by the predictor variable in the outcome variable. In the table below, I provide some qualitative descriptors that we can apply when interpreting the magnitude of one of the effect size indicators. Please note that typically we only interpret practical significance when the F-test indicates statistical significance. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohens f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large After finding a statistically significant omnibus F-test, it is customary to then compute post-hoc pairwise comparisons between specific means to determine which pairs of means differ to a statistically significant extent. If a pair of means is found to show a statistically significant difference, then we will proceed forward with interpreting the magnitude of that difference, typically using an effect size indicator like Cohens d, which is the standardized mean difference. In essence, the Cohens d indicates the magnitude of the difference between means in standard deviation units. A d-value of .00 would indicate that there is no difference between the two means, while the following are some generally accepted qualitative-magnitude labels we can attach to the absolute value of d. Cohens d Description .20 Small .50 Medium .80 Large 34.1.2.5 Sample Write-Up As part of a post-test-only with two comparison groups design, 75 employees were randomly assigned to one (and only one) of three following groups associated with our categorical (nominal, ordinal) predictor variable: no noise, some noise, and loud noise. A total of 25 participants were assigned to each group, resulting in a balanced design. For all employees, verbal fluency was assessed while they were exposed to one of the noise conditions, where verbal fluency serves as the continuous (interval, ratio) outcome variable. We applied a one-way ANOVA to determine whether verbal fluency differed across the levels of noise each group of employees experienced as part of our study. We found a significant omnibus F-test, which indicated that there were differences across the means in verbal fluency for the three noise conditions (F = 7.55, p = .03). The R2 associated with the model F-value was .12, which indicates that 12% of the variance in verbal fluency can be explained by the level of noise employees were exposed to. Given the significant omnibus F-test, we computed Tukeys tests to examine the post-hoc pairwise comparisons between each pair of verbal-fluency means. The mean verbal fluency score for the no-noise condition was 77.00 (SD = 3.20), 52.00 (SD = 3.10) for the some-noise condition, and 50.00 (SD = 3.10) for the loud-noise condition. Further, the pairwise comparisons indicated, as expected, that mean verbal fluency was significantly higher for the no-noise condition compared to the some-noise condition (\\(M_{diff}\\) = 15.00, adjusted p = .03) and loud noise condition (\\(M_{diff}\\) = 17.00, adjusted p = .02). We found that the standardized mean difference (Cohens d) for the two significant differences in means was .96 and .91, respectively, which both can considered large. Finally, a significant difference in means was not found when comparing verbal-fluency scores for the some-noise and no-noise conditions (\\(M_{diff}\\) = 2.00, adjusted p = .34). 34.2 Tutorial This chapters tutorial demonstrates how to estimate a one-way ANOVA and post-hoc pairwise mean comparisons, test the associated statistical assumptions, and present the findings in writing and visually. 34.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/e6oVV1ynfWo 34.2.2 Functions &amp; Packages Introduced Function Package Plot lessR tapply base R shapiro.test base R leveneTest car ANOVA lessR cohen.d effsize 34.2.3 Initial Steps If you havent already, save the file called TrainingEvaluation_ThreeGroupPost.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called TrainingEvaluation_ThreeGroupPost.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_ThreeGroupPost.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Condition = col_character(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [75 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:75] 1 2 3 4 5 6 7 8 9 10 ... ## $ Condition: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ PostTest : num [1:75] 74 65 62 68 70 61 79 67 79 59 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 No 74 ## 2 2 No 65 ## 3 3 No 62 ## 4 4 No 68 ## 5 5 No 70 ## 6 6 No 61 There are 75 cases (i.e., employees) and 3 variables in the td data frame: EmpID (unique identifier for employees), Condition (training condition: New = new training program, Old = old training program, No = no training program), and PostTest (post-training scores on training assessment, ranging from 1-100). Regarding participation in the training conditions, 25 employees participated in each condition, with no employee participating in more than one condition; this means that we have a balanced design. Per the output of the str (structure) function above, all of the variables except for Condition are of type integer (continuous: interval/ratio), and Condition is of type character (nominal/categorical). 34.2.4 Test Statistical Assumptions Prior to estimating and interpreting the one-way ANOVA, lets generate a VBS (violin-box-scatter) plot to visualize the statistical assumptions regarding the outcome variable having a univariate normal distribution in each of the three training conditions and the variances of the outcome variable being approximately equal across the three conditions. To do so, well use the Plot function from the lessR package. If you havent already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the Plot function. As the first argument within the function, type the name of the outcome variable (PostTest). As the second argument, type data= followed by the name of the data frame (td). As the third argument, type by1= followed by the name of the grouping variable (Condition), as this will create the trellis (lattice) structure wherein three VBS plots will be created (one for each independent group). # VBS plots of the PostTest distributions by Condition Plot(PostTest, data=td, by1=Condition) ## [Trellis graphics from Deepayan Sarkar&#39;s lattice package] ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(PostTest, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## ANOVA(PostTest ~ Condition) # Add the data parameter if not the d data frame ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## No 25 0 62.36 8.09 47.00 61.00 79.00 ## Old 25 0 69.60 9.11 51.00 70.00 89.00 ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## New 4 73 ## No 3 60 ## Old 3 70 ## ## Parameter values (can be manually set) ## ------------------------------------------------------- ## size: 0.55 size of plotted points ## out_size: 0.80 size of plotted outlier points ## jitter_y: 1.00 random vertical movement of points ## jitter_x: 0.28 random horizontal movement of points ## bw: 3.43 set bandwidth higher for smoother edges Based on the output from the Plot function, note that (at least visually) the three distributions seem to be roughly normally distributed, and the the variances appear to be approximately equal. These are by no means stringent tests of the statistical assumptions, but they provide us with a cursory understanding of the shape of the distributions and the variances. If we were to see evidence of non-normality across the conditions, then we might (a) transform the outcome variable to achieve normality (if possible), or (b) apply a nonparametric analysis like the Kruskal-Wallis rank sum test. We can also go a step further by testing the normal distribution and equal variances statistical assumptions using statistical tests. Assumption of Normally Distributed Outcome Variable Scores for Each Level of Predictor Variable: As a reminder, the first statistical assumption is that the outcome variable has a univariate normal distribution in each of the underlying populations (e.g., groups, conditions), which correspond to the levels of the categorical predictor variable. The Shapiro-Wilk normality test can be used to test the null hypothesis that a distribution is normal; if the p-value associated with the test statistic (W) is less than the conventional alpha level of .05, then we would reject the null hypothesis and assume that the distribution is not normal. If, however, we fail to reject the null hypothesis, then we do not have statistical evidence that the distribution is anything other than normal. In other words, if the p-value is equal to or greater than our alpha level (.05), then we can assume the variable is normally distributed. To compute the Shapiro-Wilk normality test, we will use the shapiro.test function from base R. Because we need to test the assumption of normality of the outcome variable (PostTest) for all three levels of the predictor variable (Condition), we also need to use the tapply function from base R. The tapply function can be quite useful, as it allows us to apply a function to a variable for each level of another categorical variable. To begin, type the name of the tapply function. As the first argument, type the name of the data frame (td), followed by the $ symbol and the name of the outcome variable (PostTest). As the second argument, type the name of the data frame (td), followed by the $ symbol and the name of the categorical predictor variable (Condition). Finally, as the third argument, type the name of the shapiro.test function. # Compute Shapiro-Wilk normality test for normal distributions tapply(td$PostTest, td$Condition, shapiro.test) ## $New ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.95019, p-value = 0.2533 ## ## ## $No ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.97029, p-value = 0.6525 ## ## ## $Old ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.98644, p-value = 0.977 In the output, we can see that the PostTest variable is normally distributed for those in the New training condition (W = .95019, p = .2533), the Old training condition (W = .98644, p = .977), and the No training condition (W = .97029, p = .6525). That is, because the p-values were each equal to or greater than .05, we failed to reject the null hypothesis that the distributions of outcome variable scores were normal. Thus, we have statistical support for having met the first assumption. Assumption of Equal Variances (Homogeneity of Variances): As for the equal variances assumption, Levenes test (i.e., homogeneity of variances test) is commonly used. The null hypothesis of this test is that the variances of the outcome variable are equal across levels of the categorical predictor variable. Thus, if the p-value is less than the conventional alpha level of .05, then we reject the null hypothesis and assume the variances are different. If, however, the p-value is equal to or less than .05, then we fail to reject the null hypothesis and assume that the variances are equal (i.e., variances are homogeneous). To test the equality (homogeneity) of variances assumption, we will use the leveneTest function from the car package. More than likely the car package is already installed on your computer, as many other packages are dependent on it. That being said, you may still need to install the package prior to accessing it using the library function. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the outcome (dependent) variable (PostTest) to the left of the ~ symbol and the name of the predictor (independent) variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame (td). # Compute Levene&#39;s test for equal variances leveneTest(PostTest ~ Condition, data=td) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.6499 0.5251 ## 72 In the output, we see that the test is nonsignificant (F = .6499, p = .5251), which suggests that, based on this test, we have no reason to believe that the three variances are anything but equal. In other words, because the p-value for this test is equal to or greater than .05, we fail to reject the null hypothesis that the variances are equal. All in all, we found evidence to support that we met the two statistical assumptions necessary to proceed forward with estimating our one-way ANOVA. 34.2.5 Estimate One-Way ANOVA There are different functions that can be used to run a one-way ANOVA in R. In this chapter, we will review how to run a one-way ANOVA using the ANOVA function from the lessR package, and if youre interested, in the chapter supplement I demonstrate how to carry out the same processes using the aov function from base R. Using ANOVA function from the lessR package, we will evaluate whether the means on the post-test (PostTest) continuous outcome variable differ between levels of the Condition categorical predictor variable (New, Old, No); in other words, lets find out if we should treat the means as being different from one another. A big advantage of using the ANOVA function from the lessR package to estimate a one-way ANOVA is that the function automatically generates descriptive statistics, the omnibus F-test and associated indicators of effect size (i.e., practical significance), and post-hoc pairwise comparisons. If you havent already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Now were ready to estimate a one-way ANOVA. To begin, type the name of the ANOVA function. As the first argument in the parentheses, specify the statistical model. To do so, type the name of the continuous outcome variable (PostTest) to the left of the ~ symbol and the name of the categorical predictor variable (Condition) to the right of the ~ symbol. For the second argument, use data= to specify the name of the data frame where the outcome and predictor variables are located (td). # One-way ANOVA using ANOVA function from lessR ANOVA(PostTest ~ Condition, data=td) ## BACKGROUND ## ## Response Variable: PostTest ## ## Factor Variable: Condition ## Levels: New No Old ## ## Number of cases (rows) of data: 75 ## Number of cases retained for analysis: 75 ## ## DESCRIPTIVE STATISTICS ## ## n mean sd min max ## New 25 72.36 6.98 60.00 84.00 ## No 25 62.36 8.09 47.00 79.00 ## Old 25 69.60 9.11 51.00 89.00 ## ## Grand Mean: 68.107 ## ## BASIC ANALYSIS ## ## df Sum Sq Mean Sq F-value p-value ## Condition 2 1333.63 666.81 10.16 0.0001 ## Residuals 72 4727.52 65.66 ## ## R Squared: 0.22 ## R Sq Adjusted: 0.20 ## Omega Squared: 0.20 ## ## Cohen&#39;s f: 0.49 ## ## TUKEY MULTIPLE COMPARISONS OF MEANS ## ## Family-wise Confidence Level: ## ----------------------------------- ## diff lwr upr p adj ## No-New -10.00 -15.48 -4.52 0.00 ## Old-New -2.76 -8.24 2.72 0.45 ## Old-No 7.24 1.76 12.72 0.01 ## ## RESIDUALS ## ## Fitted Values, Residuals, Standardized Residuals ## [sorted by Standardized Residuals, ignoring + or - sign] ## [res_rows = 20, out of 75 cases (rows) of data, or res_rows=&quot;all&quot;] ## ----------------------------------------------- ## Condition PostTest fitted residual z-resid ## 75 Old 89.00 69.60 19.40 2.44 ## 57 Old 51.00 69.60 -18.60 -2.34 ## 7 No 79.00 62.36 16.64 2.10 ## 9 No 79.00 62.36 16.64 2.10 ## 64 Old 54.00 69.60 -15.60 -1.96 ## 25 No 47.00 62.36 -15.36 -1.93 ## 73 Old 56.00 69.60 -13.60 -1.71 ## 60 Old 83.00 69.60 13.40 1.69 ## 32 New 60.00 72.36 -12.36 -1.56 ## 29 New 84.00 72.36 11.64 1.47 ## 1 No 74.00 62.36 11.64 1.47 ## 56 Old 81.00 69.60 11.40 1.44 ## 33 New 61.00 72.36 -11.36 -1.43 ## 42 New 61.00 72.36 -11.36 -1.43 ## 20 No 51.00 62.36 -11.36 -1.43 ## 35 New 83.00 72.36 10.64 1.34 ## 43 New 83.00 72.36 10.64 1.34 ## 28 New 62.00 72.36 -10.36 -1.30 ## 41 New 82.00 72.36 9.64 1.21 ## 51 Old 79.00 69.60 9.40 1.18 ## ## ---------------------------------------- ## Plot 1: Scatterplot with Cell Means ## Plot 2: 95% family-wise confidence level ## ---------------------------------------- As you can see in the output, the ANOVA function provides background information about your variables, descriptive statistics, an omnibus statistical significance test of the mean comparison, post-hoc pairwise mean comparisons, and indicators of practical significance. In addition, the default data visualizations include a scatterplot with the cell (group) means and a chart with mean differences between conditions presented. Background: The Background section provides information about the name of the data frame, the name of the response (i.e., outcome) variable, the factor (i.e., categorical predictor) variable and its levels (i.e., conditions, groups), and the number of cases. Descriptive Statistics: The Descriptive Statistics section includes basic descriptive statistics about the sample. In the output, we can see that that there are 25 employees in each condition (n = 25), and descriptively, the mean PostTest score for the New training condition is 72.36 (SD = 6.98), the mean PostTest score for the Old training condition is 69.60 (SD = 9.11), and the mean PostTest score for the No training condition is 62.36 (SD = 8.09). The grand mean (i.e., overall mean for the entire sample) is 68.107. Thus, descriptively we can see that the condition means are not the same, but the question remains whether these means are different from one another to a statistically significant extent. Basic Analysis: In the Basic Analysis section of the output, you will find the statistical test of the null hypothesis (i.e., the means are equal). This is called an omnibus test because we are testing whether or not their is evidence that we should treat all means as equal. First, in the Summary Table, take a look at the line prefaced with Condition; in this line, you will find the degrees of freedom (df), the sum of squares (Sum Sq), and the mean square (Mean Sq) between groups/conditions; in addition, you will find the omnibus F-value and its associated p-value. The F-value and its associated p-value reflect the null hypothesis significance test of the means being equal, and because the p-value is less than the conventional two-tailed alpha of .05, we reject the null hypothesis that means are equal (F = 10.16, p &lt; .001); meaning, we have evidence that at least two of the means differ from one another to a statistically significant extent. But which ones? To answer this question, we will need to look at the pairwise mean comparison tests later in the output. Next, look at the Association and Effect Size table. The (unadjusted) R-squared (R2) value indicates the extent to which the predictor variable explains variance in the outcome variable in this sample; if you multiply the value by 100, you get a percentage. In this case, we find that 22% of the variance in PostTest scores is explained by the different levels of the Condition variable (i.e., New, Old, No) for this example. The adjusted R2 value, however, is an indicator of the magnitude of the association in the underlying population (as opposed to specifically for this sample), and here we see that the adjusted R2 value is .20 (or 20%). We also find information about other effect-size indicators, including omega-squared (\\(\\omega\\)2) and Cohens f. \\(\\omega\\)2 is a population-level indicator of effect size like the adjusted R2 value, and like the adjusted R2 value will tend to be smaller. Some functions compute an effect size indicator called eta-squared (\\(\\eta\\)2), which is equivalent to an unadjusted R2 value in this context. Finally, Cohens f focuses not on the variance explained (i.e., the association) but on the magnitude of the differences in means between groups/conditions. By most standards, these effect sizes would be considered to be a medium-to-large or large in magnitude; remember, these effect-size indicators correspond to the omnibus F-test. In the table below, I provide conventional rules of thumb for qualitatively interpreting the magnitude of R2 (adjusted or unadjusted) and Cohens f; I suggest picking one and using it consistently. Finally, please note that typically we only interpret practical significance when a difference has been found to be statistically significant. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohens f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large Tukey Multiple Comparisons of Means: Recall that based on the omnibus F-test above, we found evidence that the group means were not equal; in other words, the p-value associated with your F-value indicated the means differed significantly across the groups. Because the omnibus F-test indicated statistical significance at the model level, we should proceed forward with post-hoc pairwise mean comparison tests, such as Tukeys test. If the omnibus test had not been statistically significant, then we would not proceed forward with interpreting the post-hoc pairwise mean comparison tests. In the Tukey Multiple Comparisons of Means section, we find the pairwise mean comparisons corrected for family-wise error based on Tukeys approach. Family-wise error refers to instances in which we run multiple statistical tests, which means that we may be more likely to capitalize on chance when searching for statistically significant finds. When tests are adjusted for family-wise error, the p-values (or confidence intervals) are corrected (i.e., penalized) for the fact that multiple statistical tests were run, thereby increasing the threshold for finding a statistically significant result. Each row in the pairwise-comparison table in the output shows the raw difference (i.e., diff) in means between the two specified groups, such that the first group mean is subtracted from the second group mean listed. Next, the lower (i.e., lwr) and upper (i.e., upr) 95% confidence interval limits are presented. Finally, the adjusted p-value is presented (i.e., p adj). In our output, we find that employees who participated in the No training condition scored, on average, 10.00 points lower (-10.00) on their post-test (PostTest) assessment than employees who participated in the New training condition, which is a statistically significant difference (p-adjusted &lt; .01, 95% CI[-15.48, -4.52]). Similarly, employees who participated in the Old training condition scored, on average, 7.24 points higher on their post-test (PostTest) assessment than employees who participated in the No training condition (p-adjusted = .01, 95% CI[1.76, 12.72]). Note that, as evidenced by the 95% confidence intervals, the uncertainty around the mean difference between the Old and No training conditions appears to be notably greater than the uncertainty around the mean difference between the New and No training conditions. Finally, we find that the mean difference of -2.76 between the New and the Old training conditions is not statistically significant (p-adjusted = .45, 95% CI[-8.24, 2.72]). Note that the ANOVA function from lessR does not provide effect size estimates for the post-hoc pairwise mean comparisons, so if you would like those, you can do the following. Effect Sizes of Significant Post-Hoc Pairwise Mean Comparisons: There are various ways that we could go about computing an effect size such as Cohens d for those statistically significant post-hoc pairwise mean comparisons. In the post-hoc pairwise mean comparisons section of the output, we identified that the New and Old training conditions resulted in significantly higher post-training assessment (PostTest) scores compared to the No training condition. The question then becomes: How much better than the No training condition are the New and Old training conditions? To compute Cohens d as an estimate of practical significance we will use the cohen.d function from the effsize package. If you havent already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the continuous outcome variable (PostTest) to the left of the ~ symbol and the name of the categorical predictor variable (Condition) to the right of the ~ symbol. For the second argument, we are going to apply the subset function from base R after data= to indicate that we only want to run a subset of our data frame. The subset function is a simpler version of the filter function from dplyr. Why are we doing this? The cohen.d function will only allow predictor variables with two levels, and our Condition variable has three levels: New, Old, and No. After data=, type subset, and within the subset function parentheses, enter the name of the data frame (td) as the first argument and a conditional statement that removes one of the three predictor variable levels (Condition!=\"Old\"); in this first example, we remove the Old level so that we can compare just the New and Old conditions. Back to the cohen.d function, as the third argument, type paired=FALSE to indicate that the data are not paired (i.e., the data are not dependent). # Compute Cohen&#39;s d for New and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;Old&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.324165 (large) ## 95 percent confidence interval: ## lower upper ## 0.6962342 1.9520949 The output indicates that Cohens d is 1.324, which would be considered large by conventional cutoff standards (see table below). Cohens d Description .20 Small .50 Medium .80 Large Lets repeat the same process as above, except this time we will focus on the Old and No levels of the Condition predictor variable by removing the level called New. # Compute Cohen&#39;s d for Old and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;New&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: -0.8407151 (large) ## 95 percent confidence interval: ## lower upper ## -1.4339989 -0.2474312 The output indicates that Cohens d is .841, which is large but not as large as the Cohens d we saw when comparing the PostTest means for the New and No training conditions. Note: Cohens d was actually negative (-.841), but typically we just report the absolute value in this context, as the negative or positive sign of a Cohens d simply indicates which mean was subtracted from the other mean; and reversing this order would result in the opposite sign. Sample Write-Up: To evaluate the effectiveness of a new training program, we applied a post-test-only with two comparison groups training evaluation design. In total, 25 employees participated in the new training program, 25 employees participated in the old training program, and 25 employees did not participate in a training program. After completing their respective training conditions, employees were assessed on the knowledge they acquired during training, where scores could range from 1-100. We found that post-training assessments differed across training conditions to a statistically significant extent (F = 10.16, p &lt; .001); together, participation in the different training conditions explained 20% of the variability in post-training assessment scores (R2 = .22; R2adjusted = .20). Results of follow-up tests indicated that employees who participated in the new training program performed, on average, 10.00 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[4.52, 15.48]), which was a large difference (d = 1.324). Further, employees who participated in the old training program performed, on average, 7.24 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[1.76, 12.72]), which was a large difference (d = .841). Average post-training assessment scores were not found to differ to a statistically significant extent for those who participated in the new versus old training programs (Mdifference = 2.76, padjusted = .45, 95% CI[-8.24, 2.72]). Note: When interpreting the results, I flipped the sign (+ vs. -) of some of the findings to make the interpretation more consistent. Feel free to do the same. 34.2.6 Visualize Results Using Bar Chart When we find a statistically significant difference between two or more pairs of means based on an one-way ANOVA, we may want to present the means in a bar chart to facilitate storytelling. To do so, we will use the BarChart function from lessR. If you havent already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the BarChart function. As the first argument, type x= followed by the name of the categorical predictor variable (Condition). As the second argument, type y= followed by the name of the continuous outcome variable (PostTest). As the third argument, specify stat=\"mean\" to request the application of the mean function to the PostTest variable based on the levels of the Condition variable. As the fourth argument, type data= followed by the name of the data frame object to which our predictor and outcome variables belong (td). As the fifth argument, use xlab= to provide the x-axis label (\"Training Condition\"). As the sixth argument, use ylab= to provide the y-axis label (\"Post-Test Score\"). # Create bar chart BarChart(x=Condition, y=PostTest, stat=&quot;mean&quot;, data=td, xlab=&quot;Training Condition&quot;, ylab=&quot;Post-Test Score&quot;) ## PostTest ## - by levels of - ## Condition ## ## n miss mean sd min mdn max ## New 25 0 72.36 6.98 60.00 73.00 84.00 ## No 25 0 62.36 8.09 47.00 61.00 79.00 ## Old 25 0 69.60 9.11 51.00 70.00 89.00 ## &gt;&gt;&gt; Suggestions ## Plot(PostTest, Condition) # lollipop plot ## ## Data for: PostTest ## ------------------- ## New No Old ## 72.36 62.36 69.60 34.2.7 Summary In this chapter, we learned how to estimate a one-way ANOVA using the ANOVA function from the lessR package. We also learned how to test statistical assumptions, compute post-hoc pairwise mean comparisons, estimate an effect size for the omnibus test, and estimate effect sizes for the pairwise mean comparisons. 34.3 Chapter Supplement In addition to the ANOVA function from the lessR package covered above, we can use the aov function from base R to estimate an one-way ANOVA. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of the one-way ANOVA results. 34.3.1 Functions &amp; Packages Introduced Function Package aov base R summary base R anova_stats sjstats TukeyHSD base R plot base R mean base R cohen.d effsize apa.aov.table apaTables apa.1way.table apaTables apa.d.table apaTables 34.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;TrainingEvaluation_ThreeGroupPost.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Condition = col_character(), ## PostTest = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;EmpID&quot; &quot;Condition&quot; &quot;PostTest&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,3] [75 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:75] 1 2 3 4 5 6 7 8 9 10 ... ## $ Condition: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ PostTest : num [1:75] 74 65 62 68 70 61 79 67 79 59 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Condition = col_character(), ## .. PostTest = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 3 ## EmpID Condition PostTest ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 No 74 ## 2 2 No 65 ## 3 3 No 62 ## 4 4 No 68 ## 5 5 No 70 ## 6 6 No 61 34.3.3 aov Function from Base R The aov function from base R offers another route for running a one-way ANOVA. Prior to using the aov function, it is advisable to perform statistical tests to give us a better understanding if we have satisfied the two statistical assumptions necessary to estimate and interpret a one-way ANOVA; rather than repeat those same diagnostics tests here, please refer to the Test Statistical Assumptions section to learn how to perform those tests. Assuming we have satisfied the statistical assumptions, we are now ready to estimate the one-way ANOVA. To begin, come up with a name for the one-way ANOVA model that you are specifying; you can call this model object whatever youd like, and here I refer to it as model1. To the right of the model name, type the &lt;- operator to indicate that you are assigning the one-way ANOVA model to the object. Next, to the right of the &lt;- operator, type the name of the aov function from base R. As the first argument, type the name of the continuous outcome variable (PostTest) to the left of the ~ operator and the name of the categorical predictor variable (Condition) to the right of the ~ operator. For the second argument, use data= to specify the name of the data frame (td). On the next line, type the name of the summary function from base R, and as the sole argument, enter the name of the model object you created and named on the previous line. # One-way ANOVA using aov function from base R model1 &lt;- aov(PostTest ~ Condition, data=td) summary(model1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Condition 2 1334 666.8 10.16 0.00013 *** ## Residuals 72 4728 65.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the aov output provides you with the results of the one-way ANOVA. In the output table, take a look at the line prefaced with Condition; in this line, you will find the degrees of freedom (df), the sum of squares (Sum Sq), and the mean square (Mean Sq) between groups/conditions; in addition, you will find the omnibus F-value and its associated p-value. The F-value and its associated p-value reflect the null hypothesis significance test of the means being equal, and because the p-value is less than the conventional two-tailed alpha of .05, we reject the null hypothesis that means are equal (F = 10.16, p = .00013); meaning, we have evidence that at least two of the means differ from one another to a statistically significant extent. But how big is this effect? To assess the practical significance of a statistically significant effect, we will apply the anova_stats function from the sjstats package. If you havent already, install and access the sjstats package. # Install package install.packages(&quot;sjstats&quot;) # Access package library(sjstats) Within the anova_stats function parentheses, enter the name of the object for the one-way ANOVA model you created (model1). # Compute effect sizes for omnibus test anova_stats(model1) ## term | df | sumsq | meansq | statistic | p.value | etasq | partial.etasq | omegasq | partial.omegasq | epsilonsq | cohens.f | power ## -------------------------------------------------------------------------------------------------------------------------------------------- ## Condition | 2 | 1333.627 | 666.813 | 10.156 | &lt; .001 | 0.220 | 0.220 | 0.196 | 0.196 | 0.198 | 0.531 | 0.986 ## Residuals | 72 | 4727.520 | 65.660 | | | | | | | | | In the output, we see the same model information regarding degrees of freed sum of squares, mean of squares, F-value, and p-value, but we also see estimates for effect-size indicators like eta-squared (\\(\\eta^2\\)), omega-squared (\\(\\omega^2\\)), and Cohens f. As a reminder, \\(\\eta^2\\) will be equivalent to an unadjusted R2 value in this context, and \\(\\omega^2\\) will be equivalent to an adjusted R2 value. Finally, unlike the aforementioned effect-size indicators, Cohens f focuses not on the variance explained (i.e., the association) but rather on the magnitude of the differences in means between groups. By most standards, these effect sizes would be considered to be a medium-large or large in magnitude; remember, these effect-size indicators correspond to the omnibus F-test. In the table below, I provide conventional rules of thumb for qualitatively interpreting the magnitude of these effect sizes. Please note that typically we only interpret practical significance when a difference has been found to be statistically significant. R2 \\(\\eta^2\\) \\(\\omega^2\\) Cohens f Description .01 .01 .01 .10 Small .09 .09 .09 .25 Medium .25 .25 .25 .40 Large Based on the omnibus F-test we know that these means are not equivalent to one another, and we know that the effect is medium-large or large in magnitude. What we dont yet know is which pairs of means are significantly from one another and by how much. To answer this question, we will need to run some post-hoc pairwise comparison tests. Tukey Multiple Comparisons of Means: Recall that based on the omnibus F-test above, we found evidence that the group means were not equal; in other words, the p-value associated with your F-value indicated a statistically significant finding. Because the omnibus test was statistically significant, we should proceed forward with post-hoc pairwise mean comparison tests, such as Tukeys test. If the omnibus test had not been statistically significant, then we would not proceed forward to interpret the post-hoc pairwise mean comparison tests. To compute Tukeys test, we will use the TukeyHSD function from base R. First, come up with a name for the object that will contain the results of our Tukeys test. In this example, I call this model TukeyTest. Second, use the &lt;- operator to indicate that youre creating a new object based on the results of the TukeyHSD function that you will write next. Third, type the name of the TukeyHSD function, and within the parentheses, enter the name of the one-way ANOVA model object that you specified above (model1). Finally, on the next line, type the name of the print function from base R, and enter the name of the TukeyTest object you created as the sole argument. # Compute Tukey&#39;s test for pairwise mean comparisons TukeyTest &lt;- TukeyHSD(model1) print(TukeyTest) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = PostTest ~ Condition, data = td) ## ## $Condition ## diff lwr upr p adj ## No-New -10.00 -15.484798 -4.515202 0.0001234 ## Old-New -2.76 -8.244798 2.724798 0.4545618 ## Old-No 7.24 1.755202 12.724798 0.0064718 In the output, we find the pairwise mean comparisons corrected for family-wise error based on Tukeys approach. Family-wise error refers to instances in which we run multiple statistical tests, which means that we may be more likely to capitalize on chance when searching for statistically significant finds. When tests are adjusted for family-wise error, the p-values (or confidence intervals) are corrected (i.e., penalized) for the fact that multiple statistical tests were run, thereby increasing the threshold for finding a statistically significant result. Each row in the pairwise-comparison table in the output shows the raw difference (i.e., diff) in means between the two specified groups, such that the first group mean is subtracted from the second group mean listed. Next, the lower (i.e., lwr) and upper (i.e., upr) 95% confidence interval limits are presented. Finally, the adjusted p-value is presented (i.e., p adj). In our output, we find that employees who participated in the No training condition scored, on average, 10.00 points lower (-10.00) on their post-test (PostTest) assessment than employees who participated in the New training condition, which is a statistically significant difference (p-adjusted &lt; .01, 95% CI[-15.48, -4.52]). Similarly, employees who participated in the Old training condition scored, on average, 7.24 points higher on their post-test (PostTest) assessment than employees who participated in the No training condition (p-adjusted = .01, 95% CI[1.76, 12.72]). Note that, as evidenced by the 95% confidence intervals, the uncertainty around the mean difference between the Old and No training conditions appears to be notably greater than the uncertainty around the mean difference between the New and No training conditions. Finally, we find that the mean difference of -2.76 between the New and the Old training conditions is not statistically significant (p-adjusted = .45, 95% CI[-8.24, 2.72]). We can also plot the pairwise mean comparisons by entering the name of the TukeyTest object we created as the sole argument in the plot function from base R. # Plot pairwise mean comparisons from Tukey&#39;s test plot(TukeyTest) The TukeyHSD function from base R does not provide effect size estimates for the post-hoc pairwise mean comparisons, so if you would like those, you can do the following. Effect Sizes of Significant Post-Hoc Pairwise Comparisons: There are various ways that we could go about computing an effect size such as Cohens d for those post-hoc pairwise mean comparisons that were statistically significant. In the post-hoc pairwise mean comparisons, we identified that the New and Old training conditions resulted in significantly higher post-training assessment (PostTest) scores compared to the No training condition. The question then becomes: How much better than the No training condition are the New and Old training conditions? To compute Cohens d as an estimate of practical significance we will use the cohen.d function from the effsize package. If you havent already, install the effsize package. Make sure to access the package using the library function. # Install package install.packages(&quot;effsize&quot;) # Access package library(effsize) As the first argument in the cohen.d function parentheses, type the name of the continuous outcome variable (PostTest) to the left of the ~ operator and the name of the categorical predictor variable (Condition) to the right of the ~ operator. For the second argument, we are going to apply the subset function from base R after data= to indicate that we only want to run a subset of our data frame. The subset function is a simpler version of the filter function from dplyr. Why are we doing this? The cohen.d function will only allow predictor variables with two levels/categories, and our Condition variable has three levels: New, Old, and No. After data=, type subset, and within the subset function parentheses, enter the name of the data frame (td) as the first argument and a conditional statement that removes one of the three predictor variable levels (Condition!=\"Old\"); in this first example, we remove the Old level so that we can compare just the New and Old conditions. Back to the cohen.d function, as the third argument, type paired=FALSE to indicate that the data are not paired (i.e., the data are not dependent). # Compute Cohen&#39;s d for New and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;Old&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: 1.324165 (large) ## 95 percent confidence interval: ## lower upper ## 0.6962342 1.9520949 The output indicates that Cohens d is 1.324, which would be considered large by conventional cutoff standards (see table below). Cohens d Description .20 Small .50 Medium .80 Large Lets repeat the same process as above, except this time we will focus on the Old and No levels of the Condition predictor variable by removing the level called New. # Compute Cohen&#39;s d for Old and No condition means cohen.d(PostTest ~ Condition, data=subset(td, Condition!=&quot;New&quot;), paired=FALSE) ## ## Cohen&#39;s d ## ## d estimate: -0.8407151 (large) ## 95 percent confidence interval: ## lower upper ## -1.4339989 -0.2474312 The output indicates that Cohens d is .841, which is large but not as large as the Cohens d we saw when comparing the PostTest means for the New and No training conditions. Note: Cohens d was actually negative (-.841), but typically we just report the absolute value in this context, as the negative or positive sign of a Cohens d simply indicates which mean was subtracted from the other mean; and reversing this order would result in the opposite sign. Sample Write-Up: To evaluate the effectiveness of a new training program, we applied a post-test-only with two comparison groups training evaluation design. In total, 25 employees participated in the new training program, 25 employees participated in the old training program, and 25 employees did not participate in a training program. After completing their respective training conditions, employees were assessed on the knowledge they acquired during training, where scores could range from 1-100. We found that post-training assessments differed across training conditions to a statistically significant extent (F = 10.16, p &lt; .001); together, participation in the different training conditions explained 20% of the variability in post-training assessment scores (R2 = .22; R2adjusted = .20). Results of follow-up tests indicated that employees who participated in the new training program performed, on average, 10.00 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[4.52, 15.48]), which was a large difference (d = 1.324). Further, employees who participated in the old training program performed, on average, 7.24 points better on their post-training assessment than those who did not participate in a training program (padjusted &lt; .01, 95% CI[1.76, 12.72]), which was a large difference (d = .841). Average post-training assessment scores were not found to differ to a statistically significant extent for those who participated in the new versus old training programs (Mdifference = 2.76, padjusted = .45, 95% CI[-8.24, 2.72]). Note: When interpreting the results, I flipped the sign (+ vs. -) of some of the findings to make the interpretation more consistent. Feel free to do the same. 34.3.4 APA-Style Table of Results If you want to present the results of your one-way ANOVA to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. Using the aov function from base R, as we did above, lets begin by specifying a one-way ANOVA model and naming the model object (model1). # One-way ANOVA using aov function from base R model1 &lt;- aov(PostTest ~ Condition, data=td) If you havent already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) As a precaution, consider installing the MBESS package, as the function we are about to use is dependent on that package. If you dont have the MBESS package installed, youll get an error message when you run the apa.aov.table from apaTables. You may need to re-access the apaTables package using the library function after installing the MBESS package. # Install package install.packages(&quot;MBESS&quot;) To create an APA-style table that contains model summary information like the sum of squares, degrees of freedom, F-value, and p-value, we will use the apa.aov.table function. As the sole argument in the function, type the name of the one-way ANOVA model object you specified above (model1). # Create APA-style one-way ANOVA model summary table apa.aov.table(model1) ## ## ## ANOVA results using PostTest as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 CI_90_partial_eta2 ## (Intercept) 130899.24 1 130899.24 1993.59 .000 ## Condition 1333.63 2 666.82 10.16 .000 .22 [.08, .34] ## Error 4727.52 72 65.66 ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared If you would like to write (export) the table to a Word document (.doc), as a second argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style one-way ANOVA model summary table (write to working directory) apa.aov.table(model1, filename=&quot;One-Way ANOVA Summary Table.doc&quot;) ## ## ## ANOVA results using PostTest as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 CI_90_partial_eta2 ## (Intercept) 130899.24 1 130899.24 1993.59 .000 ## Condition 1333.63 2 666.82 10.16 .000 .22 [.08, .34] ## Error 4727.52 72 65.66 ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared The apa.reg.table function from the apaTables package can table the model-level results of a one-way ANOVA in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. To create a summary table that contains the mean and standard deviation (SD) of the outcome variable for each level of the categorical predictor variable, use the apa.1way.table function. As the first argument, type iv= followed by the name of the categorical predictor (independent) variable. As the second argument, type dv= followed by the name of the categorical outcome (dependent) variable. As the third argument, data= followed by the name of the data frame object to which the predictor and outcome variables belong. # Create APA-style means/SDs table apa.1way.table(iv=Condition, dv=PostTest, data=td) ## ## ## Descriptive statistics for PostTest as a function of Condition. ## ## Condition M SD ## New 72.36 6.98 ## No 62.36 8.09 ## Old 69.60 9.11 ## ## Note. M and SD represent mean and standard deviation, respectively. ## If you would like to write (export) the table to a Word document (.doc), as a fourth argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style means/SDs table (write to working directory) apa.1way.table(iv=Condition, dv=PostTest, data=td, filename=&quot;Means-SDs Table.doc&quot;) ## ## ## Descriptive statistics for PostTest as a function of Condition. ## ## Condition M SD ## New 72.36 6.98 ## No 62.36 8.09 ## Old 69.60 9.11 ## ## Note. M and SD represent mean and standard deviation, respectively. ## The apa.reg.table function from the apaTables package can table the group means and standard deviations in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. To create a summary table that contains the mean and standard deviation (SD) of the outcome variable for each level of the categorical predictor variable and Cohens d values for pair-wise comparisons, use the apa.d.table function. As the first argument, type iv= followed by the name of the categorical predictor (independent) variable. As the second argument, type dv= followed by the name of the categorical outcome (dependent) variable. As the third argument, data= followed by the name of the data frame object to which the predictor and outcome variables belong. # Create APA-style means/SDs &amp; Cohen&#39;s ds table apa.d.table(iv=Condition, dv=PostTest, data=td) ## ## ## Means, standard deviations, and d-values with confidence intervals ## ## ## Variable M SD 1 2 ## 1. New 72.36 6.98 ## ## 2. No 62.36 8.09 1.32 ## [0.70, 1.93] ## ## 3. Old 69.60 9.11 0.34 0.84 ## [-0.22, 0.90] [0.26, 1.42] ## ## ## Note. M indicates mean. SD indicates standard deviation. d-values are estimates calculated using formulas 4.18 and 4.19 ## from Borenstein, Hedges, Higgins, &amp; Rothstein (2009). d-values not calculated if unequal variances prevented pooling. ## Values in square brackets indicate the 95% confidence interval for each d-value. ## The confidence interval is a plausible range of population d-values ## that could have caused the sample d-value (Cumming, 2014). ## If you would like to write (export) the table to a Word document (.doc), as a fourth argument, add filename= followed by whatever you would like to name the file in quotation marks (\" \"). Make sure you include the .doc file type at the end. # Create APA-style means/SDs &amp; Cohen&#39;s ds table (write to working directory) apa.d.table(iv=Condition, dv=PostTest, data=td, filename=&quot;Means-SDs &amp; Cohen&#39;s ds Table.doc&quot;) ## ## ## Means, standard deviations, and d-values with confidence intervals ## ## ## Variable M SD 1 2 ## 1. New 72.36 6.98 ## ## 2. No 62.36 8.09 1.32 ## [0.70, 1.93] ## ## 3. Old 69.60 9.11 0.34 0.84 ## [-0.22, 0.90] [0.26, 1.42] ## ## ## Note. M indicates mean. SD indicates standard deviation. d-values are estimates calculated using formulas 4.18 and 4.19 ## from Borenstein, Hedges, Higgins, &amp; Rothstein (2009). d-values not calculated if unequal variances prevented pooling. ## Values in square brackets indicate the 95% confidence interval for each d-value. ## The confidence interval is a plausible range of population d-values ## that could have caused the sample d-value (Cumming, 2014). ## The apa.reg.table function from the apaTables package can table the group means, standard deviations, and Cohens d-values in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. "],["selection.html", "Chapter 35 Introduction to Employee Selection", " Chapter 35 Introduction to Employee Selection Link to lecture video: https://youtu.be/eKtUYHHXVjg Link to lecture video: https://youtu.be/SpDP-z6XhE0 "],["disparateimpact.html", "Chapter 36 Investigating Disparate Impact 36.1 Conceptual Overview 36.2 Tutorial", " Chapter 36 Investigating Disparate Impact In this chapter, we learn about how to detect evidence of disparate impact by using tests like the 4/5ths Rule (80% Rule), \\(Z_{D}\\) test, \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence (i.e., chi-square test of independence), and Fisher exact test. 36.1 Conceptual Overview Disparate impact, which is also referred to as adverse impact, refers to situations in which organizations use seemingly neutral criteria that have a discriminatory effect on a protected group (Bauer et al. 2019). There are various tests  both statistical and nonstatistical  that may be used to evaluate whether there is evidence of disparate impact, such as the 4/5ths Rule (80% Rule), \\(Z_{Difference}\\) test (i.e., \\(Z_{D}\\) test), \\(Z_{Impact Ratio}\\) test (i.e., \\(Z_{IR}\\) test), \\(\\chi^2\\) test of independence, and Fisher exact test. In the United States, it is legally advisable to begin with testing the 4/5ths Rule followed by a statistical test like the \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence, or Fisher exact test. If you would like to learn more about how to evaluate disparate impact along with empirical evidence regarding under which conditions various tests perform best, I recommend checking out the following resources: Collins, M. W., &amp; Morris, S. B. (2008). Testing for adverse impact when sample size is small. Journal of Applied Psychology, 93(2), 463-471. Dunleavy, E., Morris, S., &amp; Howard, E. (2015). Measuring adverse impact in employee selection decisions. In C. Hanvey &amp; K. Sady (Eds.), Practitioners guide to legal issues in organizations (pp. 1-26). Switzerland: Springer, Cham. Finch, D. M., Edwards, B. D., &amp; Wallace, J. C. (2009). Multistage selection strategies: Simulating the effects on adverse impact and expected performance for various predictor combinations. Journal of Applied Psychology, 94(2), 318-340. Morris, S. B. (2001). Sample size required for adverse impact analysis. Applied HRM Research, 6(1-2), 13-32. Morris, S. B., &amp; Lobsenz, R. E. (2000). Significance tests and confidence intervals for the adverse impact ratio. Personnel Psychology, 53(1), 89-111. Office of Federal Contract Compliance Programs. (1993). Federal contract compliance manual (SUDOC L 36.8: C 76/993). Washington, DC: U. S. Department of Labor, Employment Standards Administration. Roth, P. L., Bobko, P., &amp; Switzer, F. S. III. (2006). Modeling the behavior of the 4/5ths rule for determining adverse impact: Reasons for caution. Journal of Applied Psychology, 91(3), 507-522. 36.2 Tutorial This chapters tutorial demonstrates how to compute an (adverse) impact ratio to test the 4/5ths Rule (80% Rule) and how to estimate the \\(Z_{D}\\) test, \\(Z_{IR}\\) test, \\(\\chi^2\\) test of independence, and Fisher exact test. 36.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorials below. Link to video tutorial: https://youtu.be/1e-ZW-AC_6o Link to video tutorial: https://youtu.be/oddQdDH73oQ 36.2.2 Functions &amp; Packages Introduced Function Package xtabs base R print base R addmargins base R proportions base R chisq.test base R phi psych fisher.test base R sum base R prop.table base R sqrt base R abs base R pnorm base R log base R exp base R 36.2.3 Initial Steps If you havent already, save the file called DisparateImpact.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called DisparateImpact.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;DisparateImpact.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## ID = col_double(), ## Cognitive_Test = col_character(), ## Gender = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;ID&quot; &quot;Cognitive_Test&quot; &quot;Gender&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,3] [274 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:274] 1 2 3 4 5 6 7 8 9 10 ... ## $ Cognitive_Test: chr [1:274] &quot;Pass&quot; &quot;Pass&quot; &quot;Pass&quot; &quot;Pass&quot; ... ## $ Gender : chr [1:274] &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_double(), ## .. Cognitive_Test = col_character(), ## .. Gender = col_character() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 3 ## ID Cognitive_Test Gender ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Pass Man ## 2 2 Pass Man ## 3 3 Pass Man ## 4 4 Pass Man ## 5 5 Pass Man ## 6 6 Pass Man There are 3 variables and 274 cases (i.e., applicants) in the df data frame: ID, Cognitive_Test, and Gender. ID is the unique applicant identifier variable. Imagine that these data were collected as part of selection process, and this data frame contains information about the applicants. The Cognitive_Test variable is a categorical (nominal) variable that indicates whether an applicant passed (Pass) or failed (Fail) a cognitive ability selection test. Gender is also a categorical (nominal) variable that provides each applicants self-reported gender identify, which in this sample happens to include Man and Woman. 36.2.4 4/5ths Rule Evaluating the 4/5ths Rule involves simple arithmetic. As an initial step, we will create a cross-tabulation (i.e., cross-tabs, contingency) table using the xtabs function from base R. As the first argument, in the xtabs function, type the tilde (~) operator followed by the projected class variable (Gender), followed by the addition (+) operator, followed by the selection tool variable containing the pass/fail information (Cognitive_Test). As the second argument, type data= followed by the name of the data frame object (df) to which the two variables mentioned in the first argument live. Next, to the left of the xtabs function, type any name youd like for the new cross-tabs table object were creating using the xtabs function, followed by the left arrow (&lt;-) operator; I call this cross-tabs table object observed because it will contain our observed frequency/count data for the number of people from each gender who passed and failed this cognitive ability test. # Create cross-tabs table of observed data observed &lt;- xtabs(~ Gender + Cognitive_Test, data=df) Lets take a look at the table object we created called observed by entering its name as the sole parenthetical argument in the print function from base R. # Print cross-tabs table of observed data print(observed) ## Cognitive_Test ## Gender Fail Pass ## Man 90 70 ## Woman 72 42 As you can see, the observed object contains the raw counts/frequencies of those men and women who passed and failed the cognitive ability test. Now were ready to compute the selection ratios (i.e., selection rates, pass rates) for the two groups we wish to compare. In this example, we will focus on the protected class variable Gender and the two Gender categories available in these data: Man and Woman. The approach well use is not the most efficient or elegant way to compute selection ratios, but it is perhaps more transparent than other approaches  and thus good for learning. Lets begin by calculating the total number of men who participated in this cognitive ability test. To do so, we will use matrix/bracket notation to pull specific values from our observed table object that correspond to the number of men who passed and the number of men who failed; if we add those two values together, we will get the total sample size for men who participated in this cognitive ability test. To illustrate how matrix/bracket notation works, lets practice by just pulling the number of men who passed. Simply, type the name of the table object (observed), followed by brackets [ ]. Within the brackets, a number that comes before the comma represents the row number in the table or matrix object, and the number that comes after the comma represents the column number. To pull the number of men who passed the test, we would reference the cell from our table that corresponds to row number 1 and column number 2 (as shown below). # Practice matrix notation observed[1,2] ## [1] 70 Now were ready to pull the pass and fail counts for men and assign them to object names of our choosing. For clarity, Im labeling these objects as pass_men and fail_men, and we use the left arrow (&lt;-) operator to assign the values to these new objects. # Create pass and fail objects for men containing the raw frequencies/counts pass_men &lt;- observed[1,2] fail_men &lt;- observed[1,1] If youd like you can print the pass_men and fail_men objects to verify that the correct table values were assigned to each object. By adding our pass_men and fail_men values together, we can compute the total number of men who participated in this cognitive ability test. Lets call this new object N_men. # Create an object that contains the total number of men who participated in the test N_men &lt;- pass_men + fail_men Lets now proceed with doing the same for the number of women who passed and failed, as well as the total number of women. # Create object containing number of women who passed, number of women who failed, # and the total number of women who participated in the test pass_women &lt;- observed[2,2] fail_women &lt;- observed[2,1] N_women &lt;- pass_women + fail_women We now have the ingredients necessary to compute the selection ratios for men and women. To calculate the selection ratio for men, simply divide the number of men who passed (pass_men) by the total number of men (N_men). Using the left arrow (&lt;-) operator, we will assign this quotient to an object that Im arbitrarily calling SR_men (for selection ratio for men). # Create object containing selection ratio for men SR_men &lt;- pass_men / N_men Lets do create an object called SR_women that contains the selection ratio for women. # Create object containing selection ratio for women SR_women &lt;- pass_women / N_women As the final computational step, well create an object called IR containing the impact ratio value. Simply divide our SR_women object by the SR_men object, and assign that quotient to an object called IR using the left arrow (&lt;-) operator. In most cases, it is customary to set the group with the lower selection ratio as the numerator and the group with the higher selection ratio as the denominator. There can be exceptions, though, such as in instances in which a law is directional, such as the Age Discrimination in Employment Act of 1967, which at the federal level stipulates that workers older than 40 years of age are projected; in this particular case, we would typically always set the selection ratio for workers older 40 as the numerator and the selection ratio for workers younger than 40 as the denominator. # Create impact ratio object IR &lt;- SR_women / SR_men To view the IR object, use the print function. # Print impact ratio object print(IR) ## [1] 0.8421053 Because the impact ratio (IR) value is .84 and thus greater than .80 (80%, 4/5ths), then we would conclude based on this test that there is not evidence of disparate impact on this cognitive ability test when comparing the selection ratios of men and women. If the impact ratio had been less than .80, then we would have concluded that based on the 4/5ths Rule, there was evidence of disparate impact. It is sometimes recommended that we apply the flip-flop rule, which is essentially a sensitivity test for the 4/5ths Rule test. This is often called the impact ratio adjusted (IR_adj). To compute the impact ratio adjusted, we re-apply the impact ratio formula; however, this time we add in a hypothetical women who passed and subtract a hypothetical man who passed. # Apply &quot;flip-flop rule&quot; (impact ratio adjusted) IR_adj &lt;- ((pass_women + 1) / N_women) / ((pass_men - 1) / N_men) print(IR_adj) ## [1] 0.8746504 If the impact ratio adjusted (IR_adj) value is less than 1.0, then the original interpretation of the impact ratio stands; in this example, the impact ratio adjusted value is less than 1.0, and thus we continue on with our original interpretation that there is no violation of the 4/5ths Rule and thus no evidence of disparate impact. Finally, its important to note that the impact ratio associated with the 4/5ths Rule is an effect size and, thus, can be compared across samples and tests. With that being said, on its own, the 4/5ths Rule doesnt produce a test of statistical significance. For a test of adverse impact that yield a statistical significance estimate, we should should tern to the \\(\\chi^2\\) test of independence, Fisher Exact test, \\(Z\\)-test, or \\(Z_{ImpactRatio}\\). 36.2.4.1 Optional: Alternative Approach to Computing 4/5ths Rule If youd like to practice your R skills and learn some additional functions, youre welcome to apply this mathematically and operationally equivalent approach to the 4/5ths Rule. As a side note, its worth noting that the addmargins function from base R can be used to automatically compute the row and column marginals for a table object. This can be a handy function for simplifying some operations, and matrix/bracket notation can still be used on a new table object that is created using the addmargins function. # View row and column marginals for table addmargins(observed) ## Cognitive_Test ## Gender Fail Pass Sum ## Man 90 70 160 ## Woman 72 42 114 ## Sum 162 112 274 Now, check out the following steps to see an alternative approach to computing an impact ratio. Note that the proportions function from base R is introduced and that were referencing the same table object (observed) as as above. # Convert table object values to proportions by Gender (columns) prop_observed &lt;- proportions(observed, &quot;Gender&quot;) # Compute selection ratios for men SR_men &lt;- prop_observed[1,2] # Compute selection ratios for women SR_women &lt;- prop_observed[2,2] # Compute impact ratio (IR) IR &lt;- SR_women / SR_men # Print impact ratio (IR) print(IR) ## [1] 0.8421053 36.2.5 chi-square (\\(\\chi^2\\)) Test of Independence The chi-square (\\(\\chi^2\\)) test of independence is a statistical test than can be applied to evaluating whether this evidence of disparate impact. Its relatively simple to compute. For background information on the \\(\\chi^2\\) test of independence, please refer to this chapter Using the chisq.test function from base R, as the first argument, type the name of the table object containing the raw counts/frequencies (observed). As the second argument, enter the argument correct=FALSE, which stipulates that we wish for our \\(\\chi^2\\) test to be computed the traditional way without a continuity correction. # Compute chi-square test of independence chisq.test(observed, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: observed ## X-squared = 1.3144, df = 1, p-value = 0.2516 In the output, we should focus our attention on the p-value and whether it is equal to or greater than the conventional two-tailed alpha level of .05. In this case, p-value is greater than .05, so we would conclude that there appears to be no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, gender does not appear have an effect on whether someone passes or fails this particular test, and thus there is no evidence of disparate impact based on this statistical test. This corroborates what we found when test the 4/5ths Rule above. If the p-value had been less than .05, then we would have concluded there is statistical evidence of disparate impact. For fun, we can append $observed and $expected to our function to see the observed and expected counts/frequencies that were used to compute the \\(\\chi^2\\) test of independence behind the scenes. # View observed values chisq.test(observed, correct=FALSE)$observed ## Cognitive_Test ## Gender Fail Pass ## Man 90 70 ## Woman 72 42 # View expected values chisq.test(observed, correct=FALSE)$expected ## Cognitive_Test ## Gender Fail Pass ## Man 94.59854 65.40146 ## Woman 67.40146 46.59854 As an aside, to apply the Yates continuity correction, we would simply flip correct=FALSE to correct=TRUE. This continuity correction is available when we are evaluating data from a 2x2 table, which is the case in this example. There is a bit of a debate regarding whether to apply this continuity correction. Briefly, this continuity correction was introduced to account for the fact that in the specific case of 2x2 contingency tables (like ours), \\(\\chi^2\\) values tend to be upwardly biased. Others, however, counter that this test is too strict. For a more conservative test, apply the continuity correction. # Compute chi-square test of independence (with Yates continuity correction) chisq.test(observed, correct=TRUE) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: observed ## X-squared = 1.0441, df = 1, p-value = 0.3069 36.2.5.1 Optional: Compute Phi (\\(\\phi\\)) Coefficient (Effect Size) for chi-square Test of Independence The p-value we computed for the 2x2 chi-square (\\(\\chi^2\\)) test of independence is a test of statistical significance, which informs whether we should treat the association between two categorical variables is statistically significant. The p-value, however, does not give us any indication of practical significance or effect size, where an effect size is a standardized metric that can be compared across tests and samples. As an indicator of effect size, the phi (\\(\\phi\\)) coefficient can be computed for a 2x2 \\(\\chi^2\\) test of independence, where both variables have two levels (i.e., categories). Important note: Typically, we would only compute the phi (\\(\\phi\\)) coefficient in the event we observed a statistically significant \\(\\chi^2\\) test of independence. As shown in the previous section, we did not find evidence that the \\(\\chi^2\\) test of independence was statistically significant, so would not usually go ahead and compute an effect size. That being said, we will compute phi (\\(\\phi\\)) coefficient just for demonstration purposes. To compute the phi (\\(\\phi\\)) coefficient, we will use the phi function from the psych package, which means we first need to install (if not already installed) the psych package using the install.packages function from base R and then access the package using the library function from base R. # Install psych package (if not already installed) install.packages(&quot;psych&quot;) # Access the psych package library(psych) Now were ready to deploy the phi function. # Compute phi coefficient phi(observed) ## [1] 0.06926145 The phi (\\(\\phi\\)) coefficient is -.07. Because this is as an effect size, we can describe it qualitatively (e.g., small, medium, large). For a 2x2 contingency table, the phi (\\(\\phi\\)) coefficient is equivalent to a Pearson product-moment correlation (r). Like a correlation (r) coefficient, the (\\(\\phi\\)) coefficient can range from -1.00 (perfect negative association) to 1.00 (perfect positive association), with .00 indicating no association. Further, we can use the conventional correlation thresholds for describing the magnitude of the effect, which I display in the table below. \\(\\phi\\) Description .10 Small .30 Medium .50 Large Because the absolute value of the phi (\\(\\phi\\)) coefficient is .07 and thus falls below the .10 threshold for small, we might describe the effect as negligible or very small. Better yet, because the p-value associated with the \\(\\chi^2\\) test of independence indicated that the association was nonsignificant, we should just conclude that there is no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, we treat the effect size as though it were zero. Finally, one limitation of the phi (\\(\\phi\\)) coefficient is that the upper limit of the observed coefficient will be attenuated to the extent that the two categorical variables dont have a 50/50 distribution (Dunleavy, Morris, and Howard 2015), which is often the case in the specific context of selection ratios and adverse impact. For example, if the proportion of applicants who passed the selection test is anything but .50, then the estimated \\(\\phi\\) value will not have the potential to reach the upper limits of -1.00 or 1.00. Similarly, if the proportion of applicants from one group relative to another group is anything but .50, then the estimated \\(\\phi\\) value will not have the potential to reach the upper limits of -1.00 or 1.00. 36.2.6 Fisher Exact Test In instances in which we have a small sample size (e.g., N &lt; 30) and/or one of the 2x2 cells has an expected frequency that is less than 5, the Fisher exact test is a more appropriate test than other tests like the chi-square (\\(\\chi^2\\)) test of independence or the \\(Z\\)-test (Federal Contract Compliance Programs 1993). The Fisher exact test is very simple to compute when using the fisher.test function from base R. Simply, enter the name of the table object (observed) as the sole parenthetical argument in the function. # Compute fisher test fisher.test(observed) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: observed ## p-value = 0.2643 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.4441286 1.2622356 ## sample estimates: ## odds ratio ## 0.7507933 In the output, we should focus our attention on the p-value and whether it is equal to or greater than the conventional two-tailed alpha level of .05. In this case, p-value is greater than .05, so we would conclude that there appears to be no statistical association between whether someone passes or fails the cognitive ability test and whether they identify as a man or women. In other words, gender does not appear have an effect on whether someone passes or fails this particular test, and thus there is no evidence of disparate impact based on this statistical test. This corroborates what we found when test the 4/5ths Rule above. If the p-value had been less than .05, then we would have concluded there is statistical evidence of disparate impact based on this statistical test. If wed like to compute an effect size as an indicator of practical significance for a significant Fisher exact test, then we can compute the phi (\\(\\phi\\)) coefficient (see previous section)  or another effect size like an odds ratio. 36.2.7 \\(Z_{D}\\) Test The \\(Z_{D}\\) test  or just \\(Z\\) test  offers another method for evaluating whether evidence of disparate impact exists, and more specifically, this test tests whether there is a significant difference between two selection ratios. The \\(Z_{D}\\) test is also referred to as the Two Standard Deviation Test. Of note, the \\(Z_{D}\\) test is statistically equivalent to a 2x2 chi-square (\\(\\chi^2\\)) test of independence, which means that they will yield the same p-value. To compute the \\(Z_{D}\\) test, well use the contingency table (cross-tabulation) object called observed that we created previously. To begin, well create objects called N_men and N_woman that represent the total number of men and women in our sample, respectively. First, well use matrix notation to reference row 1 of our contingency table (observed[1,]), which contains the number of men who passed and failed the cognitive ability test. Second, well wrap row 1 from the matrix in the sum function from base R. # Create object for the total number of men N_men &lt;- sum(observed[1,]) Well do the same thing to compute the total number of women in our sample, and their counts reside row 2 of our contingency table. # Create object for the total number of women N_women &lt;- sum(observed[2,]) Now its time to compute the total selection ratio for the sample, irrespective of gender identity, and well assign the total selection ratio to an object that well name SR_total by using the &lt;- operator. To the right of the &lt;-, we will specify the ratio. First, well specify the numerator (i.e., total number of individuals who passed the cognitive ability test) by computing the sum of column 2 in the observed contingency table (sum(observed[,2])). Second, well divide the numerator (using the / operator) by the denominator (i.e., total number of individuals in the sample) by computing the sum of the all counts in our contingency table (sum(observed)). Well set aside the resulting SR_total object until were ready to plug it into the \\(Z_{D}\\) test formula. # Calculate marginal (overall, total) selection ratio/rate SR_total &lt;- sum(observed[,2]) / sum(observed) Our next objective is to convert the counts in the observed contingency table to proportions by row. Using the prop.table function from base R, we will type the name of the contingency object as the first argument, and type 1 as the second argument, where the latter argument requests proportions by row. Using the &lt;- operator, well assign the table with row proportions to an object that well name prop_observed. # Convert contingency table to proportions by row prop_observed &lt;- prop.table(observed, 1) Now were ready to assign the selection ratios for men and women, respectively, to objects that well call SR_men and SR_women, and well accomplish this by using the &lt;- operator. Our selection ratios for each gender identity have already been computed in the row proportions contingency table object that we created above (prop_observed). The selection ratio for men appears in the cell corresponding to row 1 and column 2 (i.e., the proportion of men who passed the test), and the selection ratio for women appears in the cell corresponding to row 2 and column 2 (i.e., the proportion of women who passed the test). Well grab these values from the prop_observed object using matrix notation and then assign them to their respective objects. # Compute selection ratios/rates (SR) SR_men &lt;- prop_observed[1,2] SR_women &lt;- prop_observed[2,2] The time has come to apply the objects we created above to the \\(Z_{D}\\) test formula, which is shown below. \\(Z_{D} = \\frac{SR_{women} - SR_{men}} {\\sqrt{SR_{total}(1-SR_{total})(\\frac{1}{N_{women}} + \\frac{1}{N_{men}})}}\\) Lets apply the objects we created using the mathematical operations detailed in the formula, and lets assign the resulting \\(Z\\)-value to an object well call Z. # Compute Z-value Z &lt;- (SR_women - SR_men) / sqrt( (SR_total*(1 - SR_total) * ((1/N_women) + (1/N_men))) ) Using the abs function from base R, lets compute the absolute value of the Z object so that we can compare it to the critical \\(Z\\)-values for statistical significance. # Compute absolute value of Z-value abs(Z) ## [1] 1.146481 The absolute value of the \\(Z\\)-value is 1.15, which falls below the critical value for significance of 1.96 for two-tailed test with an alpha of .05, where alpha is our p-value cutoff for statistical significance. Based on this critical value, we can conclude that there is not a statistical difference between the selection ratios for men and women for the cognitive ability test based on this sample. With that being said, some argue that a more liberal one-tailed test with alpha equal to .05 might be more appropriate, so as to avoid false negatives (Type II Error). The critical value for a one-tailed test with an alpha of .05 is 1.645. We can see that our observed \\(Z\\)-value of 1.15 is less than that critical value as well. Thus, at least with respect to the \\(Z_{D}\\) test, we can conclude that the two selection ratios are not statistically different from one another. More specifically, failure to exceed the critical values indicates that the difference between the two selection ratios for men and women is less than two standard deviations, which is where the other name for this test comes from (i.e., Two Standard Deviations Test) In many cases, we might also want want to report the exact p-value for the \\(Z_{D}\\) test. To calculate the exact p-value for a one-tailed test, we will plug the absolute value of our \\(Z\\)-value into the first argument of the pnorm function from base R; and as the second argument, we will specify lower.tail=FALSE, to specify that only want the upper tail of the test, thereby requesting a one-tailed test. # Calculate exact p-value (one-tailed) pnorm(abs(Z), lower.tail=FALSE) ## [1] 0.1257981 To compute a two-tailed p-value, we will take the pnorm function from above, and multiple the resulting p-value by 2 using the * (multiplication) operator. # Calculate exact p-value (two-tailed) 2*pnorm(abs(Z), lower.tail=FALSE) ## [1] 0.2515962 Both the one- and two-tailed p-values fall below the conventional alpha of .05, and thus both indicate that we should fail to reject the null hypothesis that there is zero difference between the population selection ratios for these two gender identities. 36.2.8 \\(Z_{IR}\\) Test The \\(Z_{IR}\\) test is more a direct test of the 4/5ths Rule and associated impact ratio than, say, the chi-square (\\(\\chi^2\\)) test of independence or the \\(Z_{D}\\) test, which if you recall is the selection ratio of one group divided by the selection ratio of another group. The \\(Z_{IR}\\) also tends to be a more appropriate test when sample sizes are small, when the selection ratios for specific groups are low, and when the proportion of the focal group (e.g., women) relative to the overall sample is small (Finch, Edwards, and Wallace 2009; Morris 2001; Morris and Lobsenz 2000). The formula for the \\(Z_{IR}\\) test is as follows. \\(Z_{IR} = \\frac{ln(\\frac{SR_{women}}{SR_{men}})} {\\sqrt{(\\frac{ 1-SR_{total}}{SR_{total}})(\\frac{1}{N_{women}} + \\frac{1}{N_{men}})}}\\) Fortunately, when we prepared to compute the \\(Z_{D}\\) test above, we created all of the necessary components (i.e., objects) needed to compute the \\(Z_{IR}\\) test. So lets plug them into our formula and assign the resulting \\(Z_{IR}\\) to an object called Z_IR. Note that the log function from base R computes the natural logarithm. # Compute Z-value Z_IR &lt;- log(SR_women/SR_men) / sqrt(((1 - SR_total)/SR_total) * (1/N_women + 1/N_men)) Using the abs function from base R, lets compute the absolute value of the Z_IR object so that we can compare it to the critical \\(Z\\)-values for statistical significance. # Compute absolute value of Z-value abs(Z_IR) ## [1] 1.16584 The absolute value of the \\(Z\\)-value is 1.17, which falls below the critical value for significance of 1.96 for two-tailed test with an alpha of .05, where alpha is our p-value cutoff for statistical significance. Based on this critical value, we can conclude that the ratio of the selection ratios for men and women (i.e., the impact ratio) does not differ significantly from an impact ratio of 1.0, where the latter would indicate that the selection ratios are equal. Like we did with the \\(Z_{D}\\) test, we can apply the more liberal one-tailed test with alpha equal to .05, where he critical value for a one-tailed test with an alpha of .05 is 1.645. We can see that our observed \\(Z\\)-value of 1.17 is also less than that critical value. Thus, with respect to the \\(Z_{IR}\\) test, we can conclude that the there is no relative difference between the selection ratios for men and women. We might also want want to report the exact p-value for the \\(Z_{IR}\\) test. To calculate the exact p-value for a one-tailed test, we will plug the absolute value of our \\(Z\\)-value into the first argument of the pnorm function from base R; and as the second argument, we will specify lower.tail=FALSE, to specify that only want the upper tail of the test, thereby requesting a one-tailed test. # Calculate exact p-value (one-tailed) pnorm(abs(Z_IR), lower.tail=FALSE) ## [1] 0.1218396 To compute a two-tailed p-value, we will take the pnorm function from above, and multiple the resulting p-value by 2 using the * (multiplication) operator. # Calculate exact p-value (two-tailed) 2*pnorm(abs(Z_IR), lower.tail=FALSE) ## [1] 0.2436793 Both the one- and two-tailed p-values are above the conventional alpha of .05, and thus both indicate that we should fail to reject the null hypothesis that there is zero relative difference between the population selection ratios for men and women with respect to the cognitive ability test. 36.2.8.1 Optional: Compute Confidence Intervals for \\(Z_{IR}\\) Test Given that tests of disparate impact often involve small sample sizes and thus are susceptible to low statistical power, some have recommended that effect sizes and confidence intervals be used instead of interpreting statistical significance using a p-value (see Morris and Lobsenz 2000). Confidence intervals around an impact ratio estimate reflect sampling error and provide a range of possible values in which the population parameter (i.e., population impact ratio) may fall  or put differently, how the impact ratio will likely vary across different samples drawn from the same population. To compute the confidence intervals for the \\(Z_{IR}\\) test, well first need to compute the natural log of the impact ratio using the log function from base R. Well assign the natural log of the impact ratio to an object that well call IR_log. # Compute natural log of impact ratio (IR) IR_log &lt;- log(SR_women/SR_men) Next, we will compute the standard error of the impact ratio using the formula shown in the code chunk below, and we will assign it to an object that well call SE_IR. # Compute standard error of IR (SE_IR) SE_IR &lt;- sqrt( ((1 - SR_women) / (N_women * SR_women) + (1 - SR_men)/(N_men * SR_men)) ) Using the standard error of the impact ratio object (SE_IR) and the natural log of the impact ratio object (IR_log), we can compute the lower and upper limits of the a 95% confidence interval by adding and subtracting, respectively, the product of the standard error of the impact ratio and 1.96 to/from the natural log of the impact ratio. # Compute bounds of 95% confidence interval for natural log # of impact ratio LCL_log &lt;- IR_log - 1.96 * SE_IR # lower UCL_log &lt;- IR_log + 1.96 * SE_IR # upper Finally, we can convert the lower and upper limits of the 95% confidence interval to the original scale of the impact ratio metric by exponentiating the lower and upper limits of the natural log of the impact ratio. # Convert to scale of original IR metric LCL &lt;- exp(LCL_log) # lower UCL &lt;- exp(UCL_log) # upper # Print the 95% confidence intervals print(LCL) ## [1] 0.6252696 print(UCL) ## [1] 1.134137 Thus, we can be 95% confident that the true population impact ratio falls somewhere between .63 and 1.13 (95% CI[.63, 1.13]), and we already know that the effect size (i.e., impact ratio) for this cognitive ability test with respect to gender is .84 based on our test of the 4/5ths Rule (see above). Note that if the population parameter were to fall at the lower limit of the confidence interval (.63), then we would conclude that there was disparate impact based on the 4/5ths Rule; however, if the the population parameter were to fall at the upper limit of the confidence interval (1.13), we would conclude that there is no evidence of disparate impact based on the 4/5ths Rule. Thus, the confidence interval indicates that if we were to sample from the same population many more times, we would sometimes find a violation of the 4/5ths Rule  but sometimes not. If we want, we can also compute the 90% confidence intervals for the impact ratio by swapping the 1.96 critical \\(Z\\)-value for a two-tailed test (alpha = .05) with the 1.645 critical \\(Z\\)-value for a one-tailed test (alpha = .05). Everything else will remain the same as when we computed the 95% confidence interval. # Compute bounds of 90% confidence interval for natural log # of impact ratio LCL_log &lt;- IR_log - 1.645 * SE_IR # lower UCL_log &lt;- IR_log + 1.645 * SE_IR # upper # Convert to scale of original IR metric LCL &lt;- exp(LCL_log) # lower UCL &lt;- exp(UCL_log) # upper # Print the 90% confidence intervals print(LCL) ## [1] 0.655915 print(UCL) ## [1] 1.081148 Based on these calculations, we can be 90% confident that the true population impact ratio falls somewhere between .66 and 1.08 (90% CI[.66, 1.08]). 36.2.9 Summary In this chapter, we learned how to evaluate whether there is evidence of disparate impact using the 4/5ths Rule, chi-square (\\(\\chi^2\\)) test of independence, Fisher exact test, \\(Z_{D}\\) test, and \\(Z_{IR}\\) test. "],["criterionrelatedvalidity.html", "Chapter 37 Estimating Criterion-Related Validity of a Selection Tool Using Correlation 37.1 Conceptual Overview 37.2 Tutorial 37.3 Chapter Supplement", " Chapter 37 Estimating Criterion-Related Validity of a Selection Tool Using Correlation In this chapter, we will learn how to estimate the criterion-related validity of an employee selection tool by using a correlation. Well begin with a conceptual overview of criterion-related validity and correlation, and well conclude with a tutorial. 37.1 Conceptual Overview In this section, well begin by reviewing the concept of criterion-related validity and then conclude by reviewing the Pearson product-moment correlation. With respect to the latter, well discuss the statistical assumptions that should be satisfied prior to estimating and interpreting a correlation as well as statistical significance and and practical significance in the context of correlation. The section will wrap up with a sample-write up of a correlation when used to estimate the criterion-related validity of a selection tool. 37.1.1 Review of Criterion-Related Validity Criterion-related validity (criterion validity) has to do with the association between a variable and some outcome variable. The term criterion can be thought of as some outcome or correlate of practical or theoretical interest. In the employee selection context, the criterion of interest is often some indicator of job performance (e.g., performance evaluations). Where possible, the U.S. court system generally favors evidence of criterion-related validity when demonstrating that a selection tool is job-relevant and job-related. The data necessary to estimate criterion-related validity can be acquired from concurrent validation studies/designs or predictive validation studies/designs, both of which can be classified beneath the umbrella term of criterion-related validation studies/designs. For additional information on criterion-related validity and other types of validity (e.g., content, construct, convergent, discriminant), I recommend reading the following free resources: Pages 10-15 from the Principles for the Validiation and Use of Personnel Selection Procedures (Society for Industrial &amp; Organizational Psychology 2018); Chapter 4.2 from Research Methods in Psychology (Price et al. 2017). 37.1.2 Review of Correlation A correlation represents the sign (i.e., direction) and magnitude (i.e., strength) of an association between two variables. There are different types of correlations we can estimate, and their appropriateness will depend on the measurement scales of the two variables. For instance, the Pearson product-moment correlation is used when both variables are continuous (i.e., have interval or ratio measurement scale), whereas the biserial correlation is used when one variable is continuous and the other is dichotomous (e.g., has a nominal measurement scale with just two levels or categories). In this chapter, we will focus on estimating Pearson product-moment correlations, which I will henceforth refer to as just correlation. Correlation coefficients can range from -1.00 to +1.00, where zero (.00) represents no association, -1.00 represents a perfect negative (inverse) association, and +1.00 represents a perfect positive association. A Pearson product-moment correlation coefficient (r) for a sample can be computed using the following formula: \\(r = \\frac{\\sum XY - \\frac{(\\sum X)(\\sum Y)}{n}} {\\sqrt{(\\sum X^{2} - \\frac{\\sum X^{2}}{n}) (\\sum Y^{2} - \\frac{\\sum Y^{2}}{n})}}\\) where \\(X\\) refers to scores from one variable and \\(Y\\) refers to scores from the other variable, \\(n\\) refers to the sample size (i.e., the number of pairs of data corresponding to the number of cases  with complete data). When estimated using data acquired from a criterion-related validation study (e.g, concurrent or predictive validation study/design), a correlation coefficient can be referred to as a validity coefficient. 37.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Each variable shows a univariate normal distribution; Each variable is free of univariate outliers, and together the variables are free of bivariate outliers; Variables demonstrate a bivariate normal distribution - meaning, each variable is normally distributed at each level/value of the other variable. Often, this roughly takes the form of an ellipse shape, if you were to superimpose an oval that would fit around most cases in a bivariate scatter plot; The association between the two variables is linear. 37.1.2.2 Statistical Significance The significance level of a correlation coefficient is determined by the sample size and the magnitude of the correlation coefficient. Specifically, a t-statistic with N-2 degrees of freedom (df) is calculated and compared to a Students t-distribution with N-2 df and a given alpha level (usually two-tailed, alpha = .05). If the calculated t-statistic is greater in magnitude than the chosen t-distribution, we conclude that the population correlation coefficient is significantly greater than zero. We use the following formula to calculate the t-statistic: \\(t = \\frac{r \\sqrt{N-2}}{1-r^{2}}\\) where \\(r\\) refers to the estimated correlation coefficient and \\(N\\) refers to the sample size. Alternatively, the exact p-value can be computed using a statistical software program like R if we know the df and t-value. In practice, however, we dont always report the associated t-value; instead, we almost always report the exact p-value associated with the t-value when reporting information about statistical significance. When using null hypothesis significance testing, we interpret a p-value that is less than our chosen alpha level (which is conventionally .05, two-tailed) to meet the standard for statistical significance. This means that we reject the null hypothesis that the correlation is equal to zero. By rejecting this null hypothesis, we conclude that the correlation is significantly different from zero. If the p-value is equal to or greater than our chosen alpha level (e.g., .05, two-tailed), then we fail to reject the null hypothesis that the correlation is equal to zero; meaning, we conclude that there is no evidence of linear association between the two variables. 37.1.2.3 Practical Significance The size of a correlation coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large Some people like to also report the coefficient of determination as an indicator of effect size. The coefficient of determination is calculated by squaring the correlation coefficient to create r2. When multiplied by 100, the coefficient of determination (r2) can be interpreted as the percentage of variance/variability shared between the two variables, which is sometimes stated as follows: Variable \\(X\\) explains \\(X\\)% of the variance in Variable \\(Y\\) (or vice versa). Please note that we use the lower-case r in r2 to indicate that we are reporting the variance overlap between only two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. A bivariate scatter plot comes in handy when visually depicting the direction and strength of an association between two continuous (interval or ratio measurement scale) variables. Further, the scatter plot can be applied to understand whether statistical assumptions related to linearity and a bivariate normal distribution have been met. To create examples of scatter plots based on simulated data, check out this free tool. The tool also does a nice job of depicting the concept of shared variance in the context of correlation (i.e., coefficient of determination) using a Venn diagram. 37.1.2.4 Sample Write-Up Our organization implemented a concurrent validation study to collect the data necessary to estimate the criterion-related validity of a new structured interview selection tool for customer service representatives. The concurrent validation study included a sample of 67 job incumbents (N = 67) and their scores on the structured interview and on the criterion of job performance. Using a Pearson product-moment correlation, we found a statistically significant, medium-sized correlation between structured interview scores and job performance scores (r = .29, p = .02), such that those who scored higher on the structured interview showed moderately higher levels of job performance. The 95% confidence interval indicated that the true correlation value for the entire population of employees likely falls somewhere between .05 and .50 (95% CI[.05, .50]). In this context, the correlation coefficient can be conceptualized as a validity coefficient and, thus, an indicator of criterion-related validity. Given that the correlation is statistically significant, we can conclude that the structured interview shows evidence of criterion-related validity for the customer service representative job, thereby demonstrating that this selection tool is job-related. Note: If the p-value were equal to or greater than our alpha level (e.g., .05, two-tailed), then we would typically state that the association between the two variables is not statistically significant, and we would not proceed forward with interpreting the effect size (i.e., level of practical significance) because the test of statistical significance indicates that it is very unlikely based on our sample that a true association between these two variables exists in the population. 37.2 Tutorial This chapters tutorial demonstrates how to estimate criterion-related validity using a correlation, and how to present the results in writing. 37.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/yPAfqmISQ3U 37.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR Correlation lessR 37.2.3 Initial Steps If you havent already, save the file called SelectionData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called SelectionData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## Conscientiousness = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,4] [163 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 The data frame contains 4 variables and 163 cases (i.e., employees): EmployeeID, Conscientiousness, Interview, and Performance. Lets assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., Conscientiousness, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The Conscientiousness variable contains the scores on a personality test designed to tap into the psychological concept of conscientiousness; potential scores on this variable could range from 1 (low conscientiousness) to 5 (high conscientiousness). The Interview variable contains the scores for a structured interview designed to assess interviewees level customer-service knowledge and skills; potential scores on this variable could range from 1 (poor customer service) to 5 (strong customer service). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 5 (exceeds performance standards). 37.2.4 Visualize Association Using a Scatter Plot Two of the key statistical assumptions that should be satisfied prior to estimating a Pearson product-moment correlation are that (a) the association between the two variables is approximately linear and that (b) a bivariate normal distribution exists between the two variables. A data visualization called a scatter plot can be used to test both of these assumptions. The ScatterPlot function from the lessR package does a nice job generate generating scatter plots  and it even provides an estimate of the correlation by default. If you havent already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To begin, type the name of the ScatterPlot function. As the first two arguments of the function, type the names of the two variables you wish to visualize; lets start by visualizing the association between the Conscientiousness selection tool and the criterion (Performance). The variable name that we type after the x= argument will set the x-axis, and the variable name that we type after the y= argument will set the y-axis. Conventionally, we place the criterion variable on the y-axis, as it is the outcome. As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (df). # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Conscientiousness, y=Performance, data=df) ## &gt;&gt;&gt; Suggestions ## Plot(Conscientiousness, Performance, enhance=TRUE) # many options ## Plot(Conscientiousness, Performance, fill=&quot;skyblue&quot;) # interior fill color of points ## Plot(Conscientiousness, Performance, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Conscientiousness, Performance, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 In our plot window, we can see a fairly clear positive linear trend between Conscientiousness and Performance, which provides us with some evidence that the assumption of a linear association has been satisfied. Furthermore, the distribution is ellipse-shaped, which gives us some evidence that the underlying distribution between the two variables is likely bivariate normal  thereby satisfying the second assumption mentioned above. Note that the ScatterPlot function automatically provides an estimate of a (Pearson product-moment) correlation in the output (r = .509), along with the associated p-value (p &lt; .001). 37.2.4.1 Optional: Stylizing the ScatterPlot Function from lessR If you would like to optionally stylize your scatter plot, we can use the xlab= and ylab= arguments to change the default names of the x-axis and y-axis, respectively. # Optional: Styling the scatter plot ScatterPlot(x=Conscientiousness, y=Performance, data=df, xlab=&quot;Conscientiousness Test Score&quot;, ylab=&quot;Job Performance Score&quot;) ## &gt;&gt;&gt; Suggestions ## Plot(Conscientiousness, Performance, enhance=TRUE) # many options ## Plot(Conscientiousness, Performance, color=&quot;red&quot;) # exterior edge color of points ## Plot(Conscientiousness, Performance, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Conscientiousness, Performance, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 We can also superimpose an ellipse by adding the argument ellipse=TRUE, which can visually aid our judgument on whether the distribution is bivariate normal. # Scatterplot using ScatterPlot function from lessR ScatterPlot(x=Conscientiousness, y=Performance, data=df, xlab=&quot;Conscientiousness Test Score&quot;, ylab=&quot;Job Performance Score&quot;, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## &gt;&gt;&gt; Suggestions ## Plot(Conscientiousness, Performance, enhance=TRUE) # many options ## Plot(Conscientiousness, Performance, fill=&quot;skyblue&quot;) # interior fill color of points ## Plot(Conscientiousness, Performance, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Conscientiousness, Performance, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 37.2.5 Estimate Correlation Important note: For space considerations, we will assume that the variables we wish to correlate have met the necessary statistical assumptions (see Statistical Assumptions section). For instance, we will assume that employees in this sample were randomly drawn from the underlying population of employees, that there is evidence of univariate and bivariate normal distributions for the variables in question, that the data are free of outliers, and that the association between variables is linear. To learn how to create a histogram or VBS plot (violin-box-scatter plot) to estimate the shape of the univariate distribution for each variable and to flag any potential univariate outliers, which can be used to assess the assumptions of univariate normal distribution and absence of univariate outliers, check out the chapter called Descriptive Statistics. There are different functions we could use to estimate a Pearson product-moment correlation between two variables (or a biserial correlation). In this chapter, I will demonstrate how to use the Correlation function from the lessR package. If you havent already, install and access the lessR package using the install.packages and library functions, respectively (see above). To begin, type the name of the Correlation function from the lessR package. As the first two arguments of the function, type the names of the two variables you wish to visualize; lets start by visualizing the association between the Conscientiousness selection tool and the criterion (Performance). Conventionally, the variable name that appears after the x= argument is our predictor (e.g., selection tool), and the variable name that appears after the y= argument is our criterion (e.g., job performance). As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (df). # Estimate correlation using Correlation function from lessR Correlation(x=Conscientiousness, y=Performance, data=df) ## Correlation Analysis for Variables Conscientiousness and Performance ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 163 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = 0.324 ## ## Sample Correlation: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 As you can see, we find the following: r = .469, p &lt; .001, 95% CI[.339, .581]. We can interpret the finding as follows: Scores on the conscientiousness test are positively associated with job performance scores to a statistically significant extent (r = .469, p &lt; .001, 95% CI[.339, .581]), such that individuals with higher conscientiousness tend to have higher job performance. How do we know the correlation coefficient is statistically significant? Well, the p-value is less than the conventional alpha cutoff of .05. Given the conventional rules of thumb for interpreting a correlation coefficient as an effect size (see Practical Significance section and table below), we can describe this correlation coefficient of .469 as approximately medium or medium-to-large in magnitude. The point estimate of the correlation coefficient (r = .469) does not directly reflect the sampling error that inevitably affects the estimated correlation coefficient derived from our sample; this is where the confidence interval can augment our interpretation by expressing a range, within which we can be reasonably confident that the true (population) correlation coefficient likely falls. With regard to the 95% confidence interval, it is likely that the range from .339 to .581 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. r Description .10 Small .30 Medium .50 Large In the context of selection tool validation, the correlation coefficient between a selection tool and a criterion can be used as an indicator of criterion-related validity. Because the correlation above is statistically significant, this provides initial evidence that the conscientiousness test has sufficiently high criterion-related validity, as it appears to be significantly associated with the criterion of job performance. In this context, we can refer to the correlation coefficient as a validity coefficient. Moreover, the practical significance (as indicated by the effect size) is approximately medium or medium-to-large, given the thresholds mentioned above. Thus, as a selection tool, the conscientiousness test appears to have relatively good criterion-related validity for this population. 37.2.6 Summary In this chapter, we learned how to create a scatter plot using the ScatterPlot function from the lessR package, and how to estimate a correlation using the Correlation function from the lessR package. 37.3 Chapter Supplement In addition to the Correlation function from the lessR package covered above, we can use the cor and cor.test functions from base R to estimate a correlation. Because this function comes from base R, we do not need to install and access an additional package. 37.3.1 Functions &amp; Packages Introduced Function Package cor base R cor.test base R 37.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## Conscientiousness = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,4] [163 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 37.3.3 cor Function from Base R The cor function from base R is a quick-and-easy approach to estimating a correlation coefficient if youre only interested in the sign and magnitude (and not the significance level). To begin, type the name of the cor function. As the first argument, type the name of your data frame (df) followed by the $ symbol and the name of one of your continuous (interval/ratio) variables (Conscientiousness). As the second argument, type the name of your data frame (df) followed by the $ symbol and the name of one of your continuous (interval/ratio) variables (Performance). Finally, as the third argument, specify method=\"pearson\" to estimate a Pearson product-moment correlation. If you were estimating the association between two rank-order variables, you could replace pearson with spearman to estimate a Spearman correlation. # Estimate correlation using cor function cor(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4686663 37.3.4 cor.test Function from Base R To estimate the correlation coefficient and associated p-value (along with the confidence interval), we can use the cor.test function from base R. By default, the alpha level is set to .05 (two-tailed), and thus a 95% confidence interval is used. As another default, the alternative/research hypothesis is that the true (population) correlation is not equal to zero. If wish to use these defaults, which would be most consistent with common practice. We can use the exact same three arguments as we used for the cor function above. # Estimate correlation using cor.test function cor.test(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: df$Conscientiousness and df$Performance ## t = 6.7318, df = 161, p-value = 0.0000000002801 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3393973 0.5805611 ## sample estimates: ## cor ## 0.4686663 If we know that we have missing values (i.e., missing data) on at least one of the variables used to estimate the correlation, we can add the na.action=na.omit argument, which excludes cases with missing values on one or both of the variables. We dont have any missing values in this data frame, so the estimate remains the same. # Estimate correlation using cor.test function cor.test(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;, na.action=na.omit) ## ## Pearson&#39;s product-moment correlation ## ## data: df$Conscientiousness and df$Performance ## t = 6.7318, df = 161, p-value = 0.0000000002801 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.3393973 0.5805611 ## sample estimates: ## cor ## 0.4686663 "],["predictingcriterionscores.html", "Chapter 38 Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression 38.1 Conceptual Overview 38.2 Tutorial 38.3 Chapter Supplement", " Chapter 38 Predicting Criterion Scores Based on Selection Tool Scores Using Simple Linear Regression In this chapter, we will learn how to estimate a simple linear regression model and apply the models equation to predict future scores on the criterion variable. Well begin with conceptual overviews of simple linear regression and predicting criterion scores, and well conclude with a tutorial. 38.1 Conceptual Overview Like correlation, regression can provide us with information about the strength and sign of the relationship between a predictor variable and an outcome variable. Regression expands upon basic correlation by providing additional information about the nature of a linear relation, such that we can predict changes in a criterion (i.e., outcome) variable based on changes in a predictor variable. More specifically, regression provides us with the basic equation for predicting scores on the outcome variable based on one or more predictor variable scores. Regression equations include information about the Y-intercept and the estimated regression coefficients (i.e., weights, slopes) associated with each predictor in the model. There are many different types of regression models, and in this chapter, were going to review simple linear regression and how we can apply the associated equation to predict scores on the criterion. 38.1.1 Review of Simple Linear Regression A simple linear regression model has one predictor (i.e., independent) variable and one outcome (i.e., dependent) variable. In this tutorial, we will learn how to estimate an ordinary least squares (OLS) simple linear regression model, where OLS refers to the process of estimating the unknown components (i.e., parameters) of the regression model by attempting to minimize the sum of squared residuals. The sum of the squared residuals are the result of a process in which the differences between the observed outcome variable values and the predicted outcome variable values are calculated, squared, and then summed in order to identify a model with the least amount of error (residuals). This is the where the line of best fit comes into play, as the line of best fit represents the regression model (equation) in which the error (residuals) between the predicted and observed outcome variable values are minimized. In other words, the goal is to find the linear model that best fits the data at hand. The equation for a simple linear regression with unstandardized regression coefficients (\\(b\\)) is as follows: \\(\\hat{Y} = b_{0} + b_{1}X + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variable \\(X\\) is equal to zero, \\(b_{1}\\) represents the unstandardized coefficient (i.e., weight, slope) of the association between the predictor variable \\(X\\) and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. Importantly, the unstandardized regression coefficient \\(b_{1}\\) represents the raw slope (i.e., weight, coefficient)  or rather, how many unstandardized units of \\(\\hat{Y}\\) increase or decrease as a result of a single unit increase in \\(X\\). That is, unstandardized regression coefficients reflect the nature of the association between two variables when the variables retain their original scaling. Often this is why we choose to use the unstandardized regression coefficients when making predictions about \\(\\hat{Y}\\), as the predicted scores will be have the same scaling as the outcome variable in its original form. As shown below, we can conceptualize the our simple linear regression model as depicting a line of best fit, wherein the estimate linear equation attempts to minimize the errors in prediction in \\(\\hat{Y}\\) based on observed scores of \\(X\\). A simple linear regression equation can be thought of as a line of best fit. We can map our regression equation estimates and concepts onto the line of best fit as shown below. The intercept and regression coefficient values signal how the line of best fit is constructed. We can also obtain standardized regression coefficients. To do so, the predictor variable (\\(X\\)) and outcome variable (\\(Y\\)) scores must be standardized. To standardize variables, we convert the predictor and outcome variables to z-scores, such that their respective means are standardized to 0 and their variances and standard deviations are standardized to 1. When standardized, our simple linear regression model equation will have a \\(\\hat{Y}\\)-intercept value equal to zero (and thus is not typically reported) and the standardized regression coefficient is commonly signified using the Greek letter \\(\\beta\\): \\(\\hat{Y} = \\beta_{1}X + e\\) where \\(\\hat{Y}\\) represents the predicted standardized score on the outcome variable (\\(Y\\)), \\(\\beta_{1}\\) represents the standardized coefficient (i.e., weight, slope) of the association between the predictor variable \\(X\\) and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. When we have obtain standardized regression coefficients in a simple linear regression model, the \\(\\beta_{1}\\) value will be equal to the correlation (r) between \\(X\\) and \\(\\hat{Y}\\). A standardized regression coefficient (\\(\\beta\\)) also allows us to also compare the relative magnitude of one \\(\\beta\\) to another \\(\\beta\\); with that being said, in the case of a multiple linear regression model (in which there are 2+ more predictor variables), comparing \\(\\beta\\) coefficients is only appropriate when the predictor variables in the model share little to no intercorrelation (i.e., have low collinearity). Given that, I recommend that you proceed with caution should you choose to make such comparisons. In terms of interpretation, in a simple linear regression model, the standardized regression coefficient (\\(\\beta_{1}\\)) indicates the standardized slope or, rather, how many standard units of \\(\\hat{Y}\\) increase or decrease as a result of a single standard unit increase in \\(X\\). 38.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple linear regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of bivariate outliers; The association between predictor and outcome variables is linear; Variables demonstrate a bivariate normal distribution; Average residual error value is zero for each level of the predictor variable; Variances of residual errors are equal for all levels of the predictor variable, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for each level of the predictor variable. Note: Regarding the first statistical assumption (i.e., cases randomly sampled from population), we will assume in this chapters data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 38.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficients p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficients p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficients p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 38.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a simple linear regression model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. As noted above, a standardized regression coefficient (\\(\\beta\\)) will be equal to a correlation coefficient (r) in the specific case of a simple linear regression model in which there is just one predictor variable and a single outcome variable. Thus, a standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) using the same interpretation thresholds as a correlation coefficient. Accordingly, in the case of a simple linear regression model, we could just estimate a correlation coefficient to get an idea of the level of practical significance. The size of a correlation coefficient (r)  or a standardized regression coefficient (\\(\\beta\\)) in the case of a simple linear regression model  can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large In a simple linear regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variable (i.e., R2). In fact, in the case of a simple linear regression, the R2 estimate will be equal to the coefficient of determination (r2), as described in the chapter on estimating criterion-related validity using correlations. Conceptually, we can think of the overlap between the variability in the predictor and outcome variables as the variance explained (R2). Ive found that the R2 is often more readily interpretable by non-analytics audiences. For example, an R2 of .10 in a simple linear regression model can be interpreted as: 10% of the variability in scores on the outcome variable can be explained by scores on the predictor variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large As an effect size, R2 indicates the proportion of variance explained by the predictor variable in the outcome variable  or in other words, the shared variance between the two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. 38.1.1.4 Sample Write-Up A team of researchers is interested in whether a basketball players height relates to the number of points scored during a 10-game season, and they use a sample of 100 basketball players to estimate a simple linear regression model. In this case, the predictor variable is basketball players heights (in inches), and the outcome variable is the number of points scored by the basketball players. Our hypothesis for such a situation might be: Basketball players heights will be positively associated with the number of points players score in a 10-game season, such that taller players will tend to score more points in a season. If we find that the unstandardized regression coefficient associated with player height in relation to points scored is statistically significant and positive (b = 2.44, p = .01) and the R2 value is .05 (p = .01), we could summarize the findings as follows: Based on a sample of 100 basketball players, basketball player height was found to predict points scored in a 10-game season, such that taller players tended to score more points (b = 2.44, p = .01). Specifically, for every 1 inch increase in height, players tended to score 2.44 additional points during the season. Further, this reflects a small-to-medium effect, as approximately 5% of the variability in points scored was explained by players heights (R2 = .05, p = .01). 38.1.2 Predicting Future Criterion Scores Using Simple Linear Regression As noted above, the intercept and coefficients estimated from a simple linear regression model can be used to construct a linear equation. This equation can be estimated based on one sample of data and then applied to a second sample of data. In doing so, new data for the predictor variable from the second sample can be plugged into the model that we estimated from the first sample, thereby allowing us to predict future criterion (i.e., outcome variable) scores. This two-sample process moves us a step towards true predictive analytics. [Note: In the employee selection context, we often refer to the outcome variable as the criterion and multiple outcome variables as criteria.] In the context of selection tool validation, the process of predicting criterion scores can be quite useful. It allows us to estimate a model for a given selection tool (e.g., assessment, procedure, test) based on data from a criterion-related validity study (e.g., concurrent validation design/study, predictive validation design/study), and then apply that model to data from actual job applicants who have taken the same selection tool. In doing so, we can make predictions about applicants future criterion scores (e.g., job performance scores). Often, we can improve the accuracy of predicted criterion scores by adding additional predictor variables to our model, which transitions the model from a simple linear regression model to a multiple linear regression model, where the latter is covered in the next chapter. 38.2 Tutorial This chapters tutorial demonstrates how to estimate a simple linear regression model and then apply the models equation to future applicants selection tool scores to predict their future criterion scores. We also learn how to present the results in writing. 38.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/TxUaAAR55bs Additionally, in the following video tutorial, I go into greater depth on how to test the statistical assumptions of a simple linear regression model  just as I do in the written tutorial below. Link to video tutorial: https://youtu.be/Qe6LLJAmJ6c 38.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR Regression lessR order base R 38.2.3 Initial Steps If you havent already, save the file called SelectionData.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called SelectionData.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## Conscientiousness = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,4] [163 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 The data frame contains 4 variables and 163 cases (i.e., employees): EmployeeID, Conscientiousness, Interview, and Performance. Lets assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., Conscientiousness, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The Conscientiousness variable contains the scores on a personality test designed to tap into the psychological concept of conscientiousness; potential scores on this variable could range from 1 (low conscientiousness) to 5 (high conscientiousness). The Interview variable contains the scores for a structured interview designed to assess interviewees level customer-service knowledge and skills; potential scores on this variable could range from 1 (poor customer service) to 5 (strong customer service). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 5 (exceeds performance standards). 38.2.4 Estimate Simple Linear Regression Model When we estimate the criterion-related validity of a selection tool, we typically use a correlation coefficient, and in this context, the correlation coefficient can be referred to as a validity coefficient. That being said, if we would like to make predictions based on selection tool scores, simple linear regression (or multiple linear regression) is the way to go. Thus, when it comes to selection tool validation, my recommendation is to run a correlation when estimating criterion-related validity and to estimate a simple linear regression model when attempting to predict criterion scores based on selection tool scores. As the name implies, a simple linear regression model assumes a linear association between the predictor and outcome scores. Check out the chapter on estimating criterion-related validity using a correlation if you need a refresher on how to interpret a correlation coefficient as an indicator of criterion-related validity. In this chapter, well specify a simple linear regression model in which the selection tool called Conscientiousness is the predictor variable and the job performance variable called Performance is our criterion. 38.2.4.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a simple linear regression model, we need to first test the statistical assumptions. To begin, lets generate a scatter plot to visualize the association between the two variables and get a rough idea of whether a few of the statistical assumptions have been met. To do so, well use the ScatterPlot function from the lessR package. If you havent already, install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Type the name of the ScatterPlot function. As the first argument within the function, type x= followed by the name of the predictor variable (Conscientiousness). As the second argument, type y= followed by the name of the outcome variable (Performance). As the third argument, type data= followed by the name of the data frame (df). As the fourth argument, type ellipse=TRUE to generate an ellipse around the data points. # Scatterplot of Conscientiousness &amp; Performance variables ScatterPlot(x=Conscientiousness, y=Performance, data=df, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## &gt;&gt;&gt; Suggestions ## Plot(Conscientiousness, Performance, enhance=TRUE) # many options ## Plot(Conscientiousness, Performance, color=&quot;red&quot;) # exterior edge color of points ## Plot(Conscientiousness, Performance, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Conscientiousness, Performance, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 163 ## Sample Correlation of Conscientiousness and Performance: r = 0.469 ## ## Hypothesis Test of 0 Correlation: t = 6.732, df = 161, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.339 to 0.581 Based on the output from the ScatterPlot function, first, note that the association between the two variables appears to be linear, which means that a simple linear regression model might be appropriate. Second, note that the association between the two variables seems to have a bivariate normal distribution, as we can see a general ellipse (or American football) shape of the data points. Third, it does look like we might have one or more bivariate outliers, which are those points that seem to notably deviate from the rest. As you can see, we can eyeball the data to give us an initial (yet rough) idea of whether three of the statistical assumptions may have been met: (a) data are free of bivariate outliers, (b) association between variables is linear, and (c) variables display a bivariate normal distribution. Fortunately, we have some other diagnostic tools that we will use to further test some of these assumptions (as well as the others) when we estimate our simple linear regression model. To generate these additional statistical assumption tests, we need to estimate our simple linear regression model. There are different functions we could use to estimate a simple linear regression model, but well begin by focusing on the Regression function from the lessR package. In the chapter supplement, you can learn how to carry how the same tests using the lm function from base R. To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome (i.e., criterion) variable (Performance) to the left of the tilde (~) operator and the name of the predictor (e.g., selection tool) variable (Conscientiousness) to the right of the ~ operator. We are telling the function to regress Performance on Conscientiousness. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate simple linear regression model Regression(Performance ~ Conscientiousness, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ Conscientiousness, data=df, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable: Conscientiousness ## ## Number of cases (rows) of data: 163 ## Number of cases retained for analysis: 163 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 0.780 0.333 2.343 0.020 0.123 1.437 ## Conscientiousness 0.631 0.094 6.732 0.000 0.446 0.817 ## ## Standard deviation of Performance: 0.966 ## ## Standard deviation of residuals: 0.856 for 161 degrees of freedom ## 95% range of residual variation: 3.379 = 2 * (1.975 * 0.856) ## ## R-squared: 0.220 Adjusted R-squared: 0.215 PRESS R-squared: 0.199 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 45.317 df: 1 and 161 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 33.176 33.176 45.317 0.000 ## Residuals 161 117.866 0.732 ## Performance 162 151.043 0.932 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance Conscientiousness ## Performance 1.00 0.47 ## Conscientiousness 0.47 1.00 ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 163 rows of data, or do res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## Conscientiousness Performance fitted resid rstdnt dffits cooks ## 134 1.667 3.350 1.832 1.518 1.829 0.399 0.078 ## 161 4.667 1.820 3.726 -1.906 -2.283 -0.352 0.060 ## 130 4.833 2.200 3.831 -1.631 -1.951 -0.333 0.054 ## 116 4.333 1.880 3.516 -1.636 -1.943 -0.240 0.028 ## 64 5.000 5.000 3.936 1.064 1.267 0.238 0.028 ## 110 3.833 1.000 3.200 -2.200 -2.628 -0.231 0.026 ## 39 2.000 1.000 2.042 -1.042 -1.241 -0.227 0.026 ## 106 2.167 1.000 2.148 -1.148 -1.363 -0.226 0.025 ## 43 2.333 1.000 2.253 -1.253 -1.486 -0.222 0.024 ## 99 2.500 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 127 2.500 1.000 2.358 -1.358 -1.609 -0.215 0.023 ## 34 3.000 4.560 2.674 1.886 2.242 0.212 0.022 ## 143 4.167 5.000 3.410 1.590 1.884 0.207 0.021 ## 126 2.333 3.390 2.253 1.137 1.347 0.201 0.020 ## 55 3.000 4.450 2.674 1.776 2.107 0.199 0.019 ## 132 2.833 1.000 2.569 -1.569 -1.858 -0.197 0.019 ## 92 4.000 5.000 3.305 1.695 2.009 0.196 0.019 ## 95 4.333 4.800 3.516 1.284 1.519 0.187 0.017 ## 28 3.000 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## 142 3.000 1.000 2.674 -1.674 -1.983 -0.187 0.017 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## Conscientiousness Performance pred sf pi.lwr pi.upr width ## 134 1.667 3.350 1.832 0.875 0.104 3.560 3.455 ## 144 1.667 1.880 1.832 0.875 0.104 3.560 3.455 ## ... ## 150 3.333 2.600 2.884 0.858 1.189 4.579 3.390 ## 27 3.500 2.500 2.989 0.858 1.295 4.684 3.390 ## 32 3.500 2.220 2.989 0.858 1.295 4.684 3.390 ## ... ## 36 5.000 3.420 3.936 0.870 2.218 5.655 3.436 ## 64 5.000 5.000 3.936 0.870 2.218 5.655 3.436 ## 107 5.000 3.220 3.936 0.870 2.218 5.655 3.436 ## ## --------------------------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: Reg Line, Confidence &amp; Prediction Intervals ## --------------------------------------------------- By default, the output for the Regression function produces three plots that are useful for assessing statistical assumptions and for interpreting the results, as well as text output. Lets begin by reviewing the first two plots depicting the residuals and the section of the text called Residuals and Influence. We previously saw using the ScatterPlot function from lessR that the association between the two variables appeared to be linear and showed evidence of a bivariate normal distribution. We did note, however, that there might be some bivariate outliers that could influence the estimated regression model. Lets take a look at the second plot in your Plot window; you may need to hit the back arrow button to review the three plots. Fitted Values &amp; Residuals Plot: The second plot is scatterplot that shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimates  or in other words, how much our predicted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term fitted values is another way of saying predicted values for the outcome variable. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential bivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be only one case that is flagged as a potential bivariate outlier (case associated with row number 134). This case was flagged based on an outlier/influence statistic called Cooks distance (D). Residuals &amp; Influence Output: Moving to the text output section called Residuals and Influence, we see a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: (a) studentized residual (rstdnt), (b) number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and (c) Cooks distance (cooks). Corroborating what we saw in the plot, the case associated with row number 134 has the highest Cooks distance value (.078) and stands somewhat apart from the next highest values (i.e., .060, .054). There are many different rules of thumbs for what constitutes an extreme Cooks distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cooks distance values exceed 1, where values in excess of 1 would indicate a problematic case. Regardless, as a sensitivity analysis, we would perhaps want to estimate our model once more after removing the case associated with row number 134 from our data frame. Overall, we may have found what seems to be one potentially influential bivariate outlier case, but we can feel reasonably good about satisfying the assumptions of average residuals being approximately zero and homoscedasticity of residuals. Next, lets consider the only statistical assumption we havent yet tested: Residual errors are normally distributed for each level of the predictor variable. Distribution of Residuals Plot: Moving on to the first plot, which displays the distribution of the residuals, we can use the display to determine whether or not we have satisfied that final statistical assumption. We see a histogram and density distribution of our residuals with the shape of a normal distribution superimposed. As you can see, our residuals show a mostly normal distribution, which is great and in line with the assumption. Because we have satisfied this and other assumptions to a reasonable extent, we should feel confident that we can interpret our statistical tests, confidence intervals, and prediction intervals in a meaningful way, beginning with the Background section of the output. 38.2.4.2 Interpret Simple Linear Regression Model Results Background: The Background section of the text output section shows which data frame object was used to estimate the model, the name of the response (outcome, criterion) variable, and the name of the predictor variable. In addition, it shows the number of cases in the data frame as well as how many were used in the estimation of the model; by default, the Regression function uses listwise deletion when one or more of the variables in the model has a missing value, which means that a case with any missing value on one of the focal variables is removed as part of the analysis. Here we can see that all 163 cases in the data frame were retained for the analysis, which means that neither Performance or Conscientiousness had missing values. Basic Analysis: The Basic Analysis section of the output first displays a table containing the estimated regression model (Estimated Model for [INSERT OUTCOME VARIABLE NAME]), including the regression coefficients (slopes, weights) and their standard errors, t-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (more on that later). The regression coefficient associated with the predictor variable (Conscientiousness) in relation to the outcome variable (Performance) is often of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .682, and its associated p-value is less than .001 (b = .631, p &lt; .001). [NOTE: Because the regression coefficient is unstandardized, its practical significance cannot be directly interpreted, and it is not a standardized effect size like a correlation coefficient.] Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. Further, the 95% confidence interval ranges from .446 to .817 (i.e., 95% CI[.446, .817]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (Conscientiousness), the outcome variable (Performance) increases by .631 (unstandardized) units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = .780 + (.631 * Conscientiousness_{observed})\\) Note that the equation above is simply the equation for a line: \\(Y_{predicted} = .780 + (.631 * X_{observed})\\). If we plug in, for instance, the value 3 as an observed value of Conscientiousness, then we get a predicted criterion score of 2.673, as shown below: \\(2.673 = .780 + (.631 * 3)\\) Thus, based on our estimate model (i.e., equation), we are able to predict values of Performance. Well bring this process to life in the following section. The Model Fit section of the output appears below the table containing the regression coefficient estimates. In this section, you will find the (unadjusted) R-squared (R2) estimate, which is an indicator of the models fit to the data as well as the extent to which the predictor variable explains variance (i.e., variability) in the outcome variable. The R-squared (R2) value of .220 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 22.0% of the variance in Performance is explained by Conscientiousness. You can also think of the R2 values as effect sizes (i.e., indicators of practical significance) at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the R2 effect size, which is another way of saying determining the level of practical significance: R2 Description .01 Small .09 Medium .25 Large The raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .215 (or 21.5%). If space permits, its a good idea to report both values, but given how close the unadjusted and adjusted R2 estimates tend to be, reporting and interpreting just the unadjusted R2 is usually fine. The Model Fit section also contains the sum of squares, F-value, and p-value includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS).. In this table, we are mostly interested in the F-value and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variable(s). In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.317 and that its associated p-value is less than .001  the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). Again, you can think of the R2 value as an indicator of effect size at the model level, and in the table above, you will find the conventional thresholds for qualitatively describing a small, medium, or large R2 value. Relations Among the Variables: The section called Relations Among the Variables displays the zero-order (Pearson product-moment) correlation between the predictor variable and outcome variable. This correlation can be used to gauge the effect size of the predictor variable in relation to the outcome variable, as a correlation coefficient is a standardized metric that can be compared across samples - unlike an unstandardized regression coefficient. In the specific case of a simple linear regression model, the square-root of the unadjusted R2 value for the model and the correlation coefficient (r) between the predictor variable and the outcome variable will be equivalent. For example: \\(R_{unadjusted}^{2} = .220\\) \\(r = \\sqrt{R_{unadjusted}^{2}}\\) \\(r = \\sqrt{.220}\\) \\(r = .469\\) As you can see in the Correlation Matrix section, the correlation coefficient between Conscientiousness and Performance is indeed approximately .47 (after rounding to two digits after the decimal). By conventional standards, in terms of practical significance, this association can be described qualitatively as medium or medium-large. Below, I provide a table containing conventional rules of thumb for interpreting a correlation coefficient as an effect size (r); you can find a more detailed overview of interpreting correlation coefficients in the chapter on estimating criterion-related validity using correlation. r Description .10 Small .30 Medium .50 Large Prediction Error: In the output section called Prediction Error, information about the forecasting error and prediction intervals. This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, were not performing true predictive analytics. As such, we wont pay much attention to interpreting this section of the output in this tutorial. With that said, if youre curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to train or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after weve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. What lessR is doing in the Prediction Error section is taking the model you estimated using the focal dataset, which we could call our training dataset, and then it takes the values for our predictor and outcome variables from our sample and plugs them into the model and accounts for forecasting error for each set of values. Specifically, the standard error of forecast (sf) for each set of values is base on a combination of the standard deviation of the residuals for the entire model (modeling error) and the sampling error for the value on the regression line. Consequently, each set of values is assigned a lower and upper bound of a prediction interval for the outcome variable. The width of the prediction interval is specific to the values used to test the model, so the widths vary across the values. In fact, the further one gets from the mean of the outcome variable (in either direction), the wider the prediction intervals become. The 95% prediction intervals, along with the 95% confidence intervals and regression line of best fit, are plotted on the third and final plot of the function output. As you can, see the prediction intervals are the outermost lines as they include both sampling error and the modeling error, whereas the confidence intervals are the inner lines, as they reflect just the sampling error. Sample Technical Write-Up of Results: A concurrent validation study was conducted to assess the criterion-related validity of a conscientiousness personality test in relation to the criterion of job performance. A simple linear regression model was estimated based on a sample of 163 job incumbents scores on the conscientiousness test and job performance. A statistically significant association was found between scores on the conscientiousness test and job performance, such that for every one point increase in conscientiousness, job performance tended to increase by .631 points (b = .631, p &lt; .001, 95% CI[.446, .817]. The unadjusted R2 value of .220 indicated that the model fit the data reasonably well, as evidenced by what can be described as a medium or medium-large effect by conventional standards. Specifically, the adjusted R2 value of .220 indicates that scores on the conscientiousness test explained 22.0% of the variability in job performance scores. In corroboration, the zero-order correlation coefficient for conscientiousness test in relation to the criterion of job performance was medium or medium-large in magnitude (r = .47), where the correlation coefficient can be referred to as a validity coefficient in this context. 38.2.4.3 Optional: Obtaining Standardized Coefficients As an optional detour, if you would like to estimate the same simple linear regression model but view the standardized regression coefficients, simply add the argument new_scale=\"z\" to your previous Regression function; that argument rescales your outcome and predictor variables to z-scores prior to estimating the model, which in effect produces standardized coefficients. # Estimate multiple linear regression model with standardized coefficients Regression(Performance ~ Conscientiousness, data=df, new_scale=&quot;z&quot;) ## ## Rescaled Data, First Six Rows ## Performance Conscientiousness ## 134 0.389 -2.523 ## 144 -1.133 -2.523 ## 11 -0.180 -2.058 ## 39 -2.044 -2.058 ## 49 -0.822 -1.826 ## 106 -2.044 -1.826 ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ Conscientiousness, data=df, new_scale=&quot;z&quot;, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable: Conscientiousness ## ## Number of cases (rows) of data: 163 ## Number of cases retained for analysis: 163 ## ## Data are Standardized ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 0.000 0.069 0.000 1.000 -0.137 0.137 ## Conscientiousness 0.469 0.070 6.732 0.000 0.331 0.606 ## ## Standard deviation of Performance: 1.000 ## ## Standard deviation of residuals: 0.886 for 161 degrees of freedom ## 95% range of residual variation: 3.500 = 2 * (1.975 * 0.886) ## ## R-squared: 0.220 Adjusted R-squared: 0.215 PRESS R-squared: 0.199 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 45.326 df: 1 and 161 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 35.584 35.584 45.326 0.000 ## Residuals 161 126.396 0.785 ## Performance 162 161.981 1.000 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance Conscientiousness ## Performance 1.00 0.47 ## Conscientiousness 0.47 1.00 ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 163 rows of data, or do res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## Conscientiousness Performance fitted resid rstdnt dffits cooks ## 134 -2.523 0.389 -1.183 1.572 1.829 0.399 0.078 ## 161 1.662 -1.195 0.779 -1.974 -2.283 -0.352 0.060 ## 130 1.894 -0.802 0.888 -1.690 -1.951 -0.333 0.054 ## 116 1.197 -1.133 0.561 -1.694 -1.943 -0.240 0.028 ## 64 2.127 2.098 0.997 1.101 1.267 0.238 0.028 ## 110 0.499 -2.044 0.234 -2.278 -2.628 -0.231 0.026 ## 39 -2.058 -2.044 -0.965 -1.079 -1.240 -0.227 0.026 ## 106 -1.826 -2.044 -0.856 -1.188 -1.363 -0.226 0.025 ## 43 -1.593 -2.044 -0.747 -1.297 -1.486 -0.222 0.024 ## 99 -1.361 -2.044 -0.638 -1.406 -1.609 -0.215 0.023 ## 127 -1.361 -2.044 -0.638 -1.406 -1.609 -0.215 0.023 ## 34 -0.663 1.643 -0.311 1.954 2.242 0.212 0.022 ## 143 0.964 2.098 0.452 1.646 1.884 0.206 0.021 ## 126 -1.593 0.431 -0.747 1.178 1.347 0.201 0.020 ## 55 -0.663 1.529 -0.311 1.840 2.108 0.199 0.019 ## 132 -0.896 -2.044 -0.420 -1.624 -1.857 -0.197 0.019 ## 92 0.732 2.098 0.343 1.755 2.009 0.196 0.019 ## 95 1.197 1.891 0.561 1.330 1.518 0.187 0.017 ## 28 -0.663 -2.044 -0.311 -1.733 -1.983 -0.187 0.017 ## 142 -0.663 -2.044 -0.311 -1.733 -1.983 -0.187 0.017 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## Conscientiousness Performance pred sf pi.lwr pi.upr width ## 134 -2.523 0.389 -1.183 0.906 -2.972 0.607 3.578 ## 144 -2.523 -1.133 -1.183 0.906 -2.972 0.607 3.578 ## ... ## 150 -0.198 -0.387 -0.093 0.889 -1.848 1.663 3.511 ## 27 0.034 -0.491 0.016 0.889 -1.739 1.771 3.510 ## 32 0.034 -0.781 0.016 0.889 -1.739 1.771 3.510 ## ... ## 36 2.127 0.462 0.997 0.901 -0.782 2.776 3.559 ## 64 2.127 2.098 0.997 0.901 -0.782 2.776 3.559 ## 107 2.127 0.255 0.997 0.901 -0.782 2.776 3.559 ## ## --------------------------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: Reg Line, Confidence &amp; Prediction Intervals ## --------------------------------------------------- In the Estimated Model section of the output, note that the intercept value is zeroed out due to the standardization, and the regression coefficient associated with Conscientiousness is now in standardized units (\\(\\beta\\) = .469, p &lt; .001). Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: For every one standardized unit increase in Conscientiousness, Performance increases by .469 standardized units. In the special case of a simple linear regression model, the standardized coefficient associated with the single predictor variable will be equal to the correlation coefficient between the predictor and outcome variables. 38.2.5 Predict Criterion Scores Using the simple linear regression model we estimated above, we can write code that generates predicted criterion scores based on new data we feed into the model. In fact, we can even add these predictions to the new data frame to which the new data belong. As we did above, lets practice using the conscientiousness selection tool (Conscientiousness) and the job performance criterion (Performance). Well begin by specifying the same simple linear regression model as above  except this time we will assign the estimated model to an object that we can subsequently reference. To do so, well use the &lt;- operator. In this example, Im naming the model reg_mod. # Assign simple linear regression model to object reg_mod &lt;- Regression(Performance ~ Conscientiousness, data=df) Now that we have assigned the simple linear regression model to the object reg_mod, we can reference specific elements from that model, such as the regression coefficients. Were going to do so to build our regression model equation as an R formula. Lets begin by referencing the model intercept value and assigning it to an object called b0. Im calling this object b0 because conventionally the b or B notation is used to signify regression coefficients and because the 0 (zero) is often used as a subscript to signify our intercept value in a regression model equation. To pull or reference the intercept value from our regression model, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients. If we were to run this by itself, it would print all of the regression coefficient values estimated for the model; we, however, want just the intercept value. Given that, immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact text: \"(Intercept)\". This will call up the intercept coefficient for any model estimated using the Regression function from lessR. # Reference estimated model intercept and assign to object b0 &lt;- reg_mod$coefficients[&quot;(Intercept)&quot;] Next, lets pull the regression coefficient associated with the slope, which in a simple linear regression is the coefficient associated with the sole predictor variable (Conscientiousness). This time, lets assign this regression coefficient to an object called b1, which adheres to conventional notation. To reference this coefficient, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients  just as we did above with the model intercept. Immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact name of the predictor variable associated with the coefficient you wish to reference: \"Conscientiousness\". # Reference estimated model slope coefficient and assign to object b1 &lt;- reg_mod$coefficients[&quot;Conscientiousness&quot;] Now that weve pulled the coefficients and assigned them to objects (b0, b1) that we can reference in our regression model equation, lets read in new data so that we can reference new applicants scores on the same conscientiousness selection tool. As we did above in the Initial Steps section, lets read in the data file called ApplicantData.csv and assign it to a new object. Here I call this new object new_df. # Read in data new_df &lt;- read.csv(&quot;ApplicantData.csv&quot;) Lets take a peek at first six rows of the new_df data frame object. # Print first six rows head(new_df) ## ApplicantID Conscientiousness Interview ## 1 AA1 4.3 4.1 ## 2 AA2 3.1 4.9 ## 3 AA3 1.1 2.5 ## 4 AA4 1.0 3.3 ## 5 AA5 2.4 4.4 ## 6 AA6 2.9 3.3 Note that the data frame object includes an ApplicantID unique identifier variable as well as variables associated with two selection tools: Conscientiousness and Interview. Note that we dont have any criterion (Performance) scores here because these folks are still applicants. Our goal then is to predict their future criterion scores using our regression model from above. To make criterion-score predictions based on the new conscientiousness test data for applicants, well need to create a regression equation based on estimates from the simple linear regression model. Fortunately, weve already created objects corresponding to our model intercept (b0) and slope coefficient (b1). Using these values, well specify the equation for a line (e.g., Y = b0 + b1 * X). In our model equation, well start by specifying a new variable containing what will be our vector of criterion-score predictions based on the Conscientiousness selection tool variable. Im going to call this new variable Perf_Predict_Consc, as hopefully that name signals that the variable contains performance predictions based on the conscientiousness test scores. Using the $ operator, we can attach this new variable to the new data frame object we read in called new_df. To the right of the &lt;- operator, well specify the rest of the equation; in this context, you can think of the &lt;- operator as being the equal sign in our equation; in fact, you could replace the &lt;- operator with = if you wanted to. First, type in the object associated with model intercept (b0) followed by the + operator. Second, type in the object associated with the slope coefficient associated with the predictor variable (b1), and follow that with the multiplication operator (*) so that we can multiple the slope coefficient by values of the predictor variable (Conscientiousness). Finally, after the * operator, type in the name of the predictor variable from the new data frame object: new_df$Conscientiousness. # Assemble regression equation and assign to new variable in second data frame new_df$Perf_Predict_Consc &lt;- b0 + b1 * new_df$Conscientiousness Lets take a look at the new_df data frame object to verify that the new Perf_Predict_Consc variable (containing the predicted criterion scores) was added successfully. # View data frame object View(new_df) In the viewer tab, we can use the up and down arrows to sort by variable scores. Alternatively and optionally, we can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 4 AA4 1.0 3.3 1.411143 ## 3 AA3 1.1 2.5 1.474276 ## 5 AA5 2.4 4.4 2.295012 ## 6 AA6 2.9 3.3 2.610680 ## 2 AA2 3.1 4.9 2.736947 ## 9 AA9 3.9 4.2 3.242015 ## 1 AA1 4.3 4.1 3.494549 ## 7 AA7 4.4 5.0 3.557682 ## 10 AA10 4.7 4.6 3.747083 ## 8 AA8 4.9 4.5 3.873350 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. # Sort data frame by new variable in descending order new_df[order(-new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 8 AA8 4.9 4.5 3.873350 ## 10 AA10 4.7 4.6 3.747083 ## 7 AA7 4.4 5.0 3.557682 ## 1 AA1 4.3 4.1 3.494549 ## 9 AA9 3.9 4.2 3.242015 ## 2 AA2 3.1 4.9 2.736947 ## 6 AA6 2.9 3.3 2.610680 ## 5 AA5 2.4 4.4 2.295012 ## 3 AA3 1.1 2.5 1.474276 ## 4 AA4 1.0 3.3 1.411143 As you can see, applicant AA8 has the highest predicted criterion score, where the criterion in this context is future job performance. Importantly, this process of applying a model to make predictions using new data can be expanded to multiple linear regression models. We would simply expand our regression equation to include the coefficients associated with the other predictor variables in such a model. 38.2.6 Summary In this chapter, we learned how to estimate a simple linear regression model using the Regression function from the lessR package, and how to apply the model to subsequent data to predict criterion scores based on employee selection tool scores. 38.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a simple linear regression model and the predict function from base R to predict criterion scores. Because these functions come from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 38.3.1 Functions &amp; Packages Introduced Function Package lm base R print base R plot base R cooks.distance base R sort base R head base R summary base R confint base R cor base R scale base R predict base R apa.reg.table apaTables 38.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;SelectionData.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_double(), ## Conscientiousness = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;Conscientiousness&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,4] [163 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : num [1:163] 1 2 3 4 5 6 7 8 9 10 ... ## $ Conscientiousness: num [1:163] 3.33 3.83 3.33 4 3.67 ... ## $ Interview : num [1:163] 3.5 3.61 3.42 4.48 4.82 3.54 3.87 3.54 4.08 5 ... ## $ Performance : num [1:163] 2.35 1.54 3.99 3.78 3.77 3.51 3.65 1.55 2.51 3.63 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_double(), ## .. Conscientiousness = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 4 ## EmployeeID Conscientiousness Interview Performance ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3.33 3.5 2.35 ## 2 2 3.83 3.61 1.54 ## 3 3 3.33 3.42 3.99 ## 4 4 4 4.48 3.78 ## 5 5 3.67 4.82 3.77 ## 6 6 4 3.54 3.51 38.3.3 lm Function from Base R As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variable (Conscientiousness) to the right of the ~ operator. We are telling the function to regress Performance on Conscientiousness. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify simple linear regression model and assign to object reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) Note: You wont see any output in your console by specifying the regression model above. If you print the model (reg.mod1) to your console using the print function from base R, you only get the regression coefficients (but no statistical tests or model fit information). Later on, well apply a different function to obtain the full model results. # Print very brief model information (NOT NECESSARY) print(reg.mod1) ## ## Call: ## lm(formula = Performance ~ Conscientiousness, data = df) ## ## Coefficients: ## (Intercept) Conscientiousness ## 0.7798 0.6313 Statistical Assumptions: Recall, towards the beginning of this tutorial, we used the ScatterPlot function from lessR to generate a bivariate scatterplot between the Conscientiousness and Performance variables to assess the assumption of linearity, look for bivariate outliers, and assess whether the assumption of bivariate normality has been satisfied. Now we will generate additional plots and other output to inform our conclusions whether we have satisfied certain statistical assumptions. We will begin by generating a scatterplot displaying the association between the fitted (predicted) values and residuals. To do so, we will use the plot function from base R. As the first argument, enter the name of the regression model object you created above (reg.mod1). As the second argument, type the numeral 1, which will request the first of four possible diagnostic plots, of which we will review three. # Diagnostics plot: fitted values &amp; residuals plot(reg.mod1, 1) We previously saw using the ScatterPlot function from lessR that the association between the two variables appeared to be linear and evidenced a bivariate normal distribution. We did, however, note that there might be some bivariate outliers that could influence the estimated regression model. The current plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term fitted values is another way of saying predicted values for the outcome variable, but the language fitted is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential bivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be three cases that are flagged as a potential bivariate outlier (i.e., row numbers 34, 110, and 161). As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the plot script from above, but this time, enter the numeral 2 (instead of 1) to request the second diagnostic plot. # Diagnostics plot: normal Q-Q plot plot(reg.mod1, 2) Normally distributed residuals will fall along the dotted diagonal line. As you can see, the residuals fall, for the most part, on or near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers 34, 110, and 161. As the last diagnostic plot, lets look at Cooks distance (D) across cases. Once again, adapt the plot script from above, but this time, enter the numeral 4 to request the fourth diagnostic plot. Were skipping the third plot. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.mod1, 4) Here we see flagged cases associated with row numbers 130, 134, and 161 based on Cooks distance. We can also grab the cases with the highest Cooks distances. Lets create an object called cooksD that we will assign a vector of Cooks distance values to using the cooks.distance function from base R. Just enter the name of the regression model object (reg.mod1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cooks distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.mod1) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 134 161 130 116 64 110 39 106 43 ## 0.07848460 0.06028472 0.05444552 0.02820208 0.02819805 0.02575822 0.02559242 0.02537055 0.02443426 ## 99 127 34 143 126 55 132 92 95 ## 0.02292889 0.02292889 0.02189203 0.02098965 0.02012473 0.01941305 0.01905427 0.01887586 0.01739145 ## 28 142 ## 0.01723973 0.01723973 Here we get the numeric values associated with Cooks distances, which corroborates what we saw in the plot of Cooks distances above. Again, the case associated with row 134 is by far the largest, and the next largest values are clustered closer together. There are many different rules of thumbs for what constitutes an extreme Cooks distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cooks distance values exceed 1. Across the three diagnostic plots and the vector of Cooks distances, 134 appears to be consistently the most extreme outlier, but it still may not be extreme enough to consider removing. One could perform a sensitivity analysis by estimating the model with and without case 134 to see if the pattern of results remains the same. Obtaining the Model Results: Type the name of the summary function from base R and include whatever you named your regression model (reg.mod1) as the sole parenthetical argument; we specified the regression model object called reg.mod1) earlier in the tutorial. The summary function simply returns a summary of your estimated regression model results. # Get summary of simple linear regression model results summary(reg.mod1) ## ## Call: ## lm(formula = Performance ~ Conscientiousness, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.19993 -0.49687 -0.00948 0.66074 1.88619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.77981 0.33276 2.343 0.0203 * ## Conscientiousness 0.63134 0.09378 6.732 0.00000000028 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8556 on 161 degrees of freedom ## Multiple R-squared: 0.2196, Adjusted R-squared: 0.2148 ## F-statistic: 45.32 on 1 and 161 DF, p-value: 0.0000000002801 The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called Coefficients contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, t-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficient for the predictor variable (Conscientiousness) in relation to the outcome variable (Performance) is often of substantive interest. Here, we see that the unstandardized regression coefficient for Conscientiousness is .631, and its associated p-value is less than .001 (b = .631, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (Conscientiousness), the outcome variable (Performance) increases by .631 (unstandardized) units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = .780 + (.631 * Conscientiousness_{observed})\\) Note that the equation above is simply the equation for a line: \\(Y_{predicted} = .780 + (.631 * X_{observed})\\). If we plug in, for instance, the value 3 as an observed value of Conscientiousness, then we get a predicted criterion score of 2.673, as shown below: \\(2.673 = .780 + (.631 * 3)\\) Thus, we are able to predict future values of Performance based on our estimated regression model. Below the table containing the regression coefficient estimates, the (unadjusted, multiple) R-squared (R2) and adjusted R2 values appear, which are indicators of the models fit to the data as well as the extent to which the predictor variable explains variability in the outcome variable. First, the R-squared (R2) value of .220 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 22.0% of the variance in Performance is explained by Conscientiousness. This raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .215 (or 21.5%). Below the R-squared (R2) and adjusted R2 values, the F-value and p-value is a statitical test of overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. The F-value and its associated p-value indicate whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variable(s). In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.32 and its associated p-value is very small (less than .001), the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variable(s). You can think of the R2 values as indicators of effect size at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the effect size, which is another way of saying determining the level of practical signficance: R2 Description .01 Small .09 Medium .25 Large To estimate the 95% confidence intervals, we can apply the confint function from base R, and enter the name of the regression model (reg.mod1) as the sole argument. # Estimate 95% confidence intervals confint(reg.mod1) ## 2.5 % 97.5 % ## (Intercept) 0.1226692 1.4369466 ## Conscientiousness 0.4461298 0.8165403 The 95% confidence interval ranges from .446 to .817 (i.e., 95% CI[.446, .817]), which indicates that the true population parameter for the association likely falls somewhere between those two values. As a direct indicator of the effect size (practical significance), we can calculate the zero-order (Pearson product-moment) correlation between Conscientiousness and Performance. Keep the method=\"pearson\" as is to request a Pearson product-moment correlation, and be sure to include the name of the data frame object (Selection) followed by the $ operator in front of each variable. # Estimate zero-order correlation (effect size) cor(df$Conscientiousness, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4686663 The correlation coefficient is medium-large in magnitude (r = .47), which indicates a moderate-strong, positive, linear association between Conscientiousness and Performance. r Description .10 Small .30 Medium .50 Large Sample Technical Write-Up of Results: A concurrent validation study was conducted to assess the criterion-related validity of a conscientiousness personality test in relation to the criterion of job performance. A simple linear regression model was estimated based on a sample of 163 job incumbents scores on the conscientiousness test and job performance. A statistically significant association was found between scores on the conscientiousness test and job performance, such that for every one point increase in conscientiousness, job performance tended to increase by .631 points (b = .631, p &lt; .001, 95% CI[.446, .817]. The unadjusted R2 value of .220 indicated that the model fit the data reasonably well, as evidenced by what can be described as a medium or medium-large effect by conventional standards. Specifically, the adjusted R2 value of .220 indicates that scores on the conscientiousness test explained 22.0% of the variability in job performance scores. In corroboration, the zero-order correlation coefficient for conscientiousness test in relation to the criterion of job performance was medium or medium-large in magnitude (r = .47), where the correlation coefficient can be referred to as a validity coefficient in this context. Obtaining Standardized Coefficients: If you would like to estimate the same simple linear regression model but view the standardized regression coefficients, just do small tweaks to the lm code/script that we specified above. First, lets name the model something different given that it will include standardized coefficients; here, I decided to name the model st_reg.mod1. Next, within the lm function, apply the scale function from base R to the predictor and outcome variables as shown; doing so will standardize our variables and center them around zero. # Estimate simple linear regression model with standardized coefficient estimates st_reg.mod1 &lt;- lm(scale(Performance) ~ scale(Conscientiousness), data=df) summary(st_reg.mod1) ## ## Call: ## lm(formula = scale(Performance) ~ scale(Conscientiousness), data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27833 -0.51458 -0.00982 0.68429 1.95341 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000000000000000661 0.06940583793586863059 0.000 1 ## scale(Conscientiousness) 0.46866634251609151640 0.06961972392189001713 6.732 0.00000000028 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8861 on 161 degrees of freedom ## Multiple R-squared: 0.2196, Adjusted R-squared: 0.2148 ## F-statistic: 45.32 on 1 and 161 DF, p-value: 0.0000000002801 In the Coefficients portion of the output, note that the intercept value is virtually zeroed out due to the standardization, and the regression coefficient associated with Conscientiousness is now in standardized units (\\(\\beta\\) = .47, p &lt; .001). The reason that the intercept value is not exactly zero is rounding error. Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: For every one standardized unit increase in Conscientiousness, Performance increases by .47 standardized units. Note that in the case of a simple linear regression model, the standardized regression coefficient is equal to the corresponding correlation coefficient between the same two variables. 38.3.4 predict Function from Base R The predict function from base R pairs nicely with the lm function, and it makes it easier to take an estimated regression model  and associated regression equation  and apply that model to new (fresh) predictor variable data. Prior to applying the predict function, we must specify a regression model. Using the lm (linear model) function from base R, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variable (Conscientiousness) to the right of the ~ operator. We are telling the function to regress Performance on Conscientiousness. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify simple linear regression model and assign to object reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) Next, as we did above in the Initial Steps section, lets read in the data file called ApplicantData.csv and assign it to a new object. Here I call this new object new_df. Lets pretend this new data frame object contains data from future applicants who have completed the Conscientiousness personality measure as part of the organizations selection process. We will ultimately plug these applicants Conscientiousness scores into our regression model equation to predict the applicants scores on the criterion variable called Performance. # Read in data new_df &lt;- read.csv(&quot;ApplicantData.csv&quot;) Next, as the first argument in the predict function, type object= followed by the estimated model object that we named reg.mod1. As the second argument, type newdata= followed by the name of the data frame object that contains new data on the predictor variable. Its important to make sure that the predictor variables name (Conscientiousness) in our new data is exactly the same as the predictor variables name (Conscientiousness) in our original data that we used to estimate the model. If that were not the case, we would want to rename the predictor variable in the new data frame object to match the corresponding name in the original data frame object. # Predict scores on the criterion (outcome) variable predict(object=reg.mod1, newdata=new_df) ## 1 2 3 4 5 6 7 8 9 10 ## 3.494549 2.736947 1.474276 1.411143 2.295012 2.610680 3.557682 3.873350 3.242015 3.747083 In your console, you should see a vector of scores  these are the predicted criterion scores. In many cases, we might want to append this vector as a variable to our new data frame object. To do so, we just need to apply the &lt;- operator and, to the left of it, specify the name of the new data frame object (new_df) followed by the $ operator and a name for the new variable that will contain the predicted criterion scores (Perf_Predict_Consc). # Predict scores on the criterion (outcome) variable # and append as new variable in data frame new_df$Perf_Predict_Consc &lt;- predict(object=reg.mod1, newdata=new_df) We can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict_Consc), ] ## ApplicantID Conscientiousness Interview Perf_Predict_Consc ## 4 AA4 1.0 3.3 1.411143 ## 3 AA3 1.1 2.5 1.474276 ## 5 AA5 2.4 4.4 2.295012 ## 6 AA6 2.9 3.3 2.610680 ## 2 AA2 3.1 4.9 2.736947 ## 9 AA9 3.9 4.2 3.242015 ## 1 AA1 4.3 4.1 3.494549 ## 7 AA7 4.4 5.0 3.557682 ## 10 AA10 4.7 4.6 3.747083 ## 8 AA8 4.9 4.5 3.873350 38.3.5 APA-Style Results Table If you want to present the results of your simple linear regression to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. Using the lm function from base R, as we did above, lets begin by estimating a simple linear regression model and naming the model object (reg.mod1). # Estimate simple linear regression model reg.mod1 &lt;- lm(Performance ~ Conscientiousness, data=df) If you havent already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (reg.mod1) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(reg.mod1) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 0.78* [0.12, 1.44] ## Conscientiousness 0.63** [0.45, 0.82] 0.47 [0.33, 0.61] .22 [.12, .32] .47** ## R2 = .220** ## 95% CI[.12,.32] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file APA Simple Linear Regression Table.doc. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(reg.mod1, filename=&quot;APA Simple Linear Regression Table.doc&quot;) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 0.78* [0.12, 1.44] ## Conscientiousness 0.63** [0.45, 0.82] 0.47 [0.33, 0.61] .22 [.12, .32] .47** ## R2 = .220** ## 95% CI[.12,.32] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table simple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. "],["incrementalvalidity.html", "Chapter 39 Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression 39.1 Conceptual Overview 39.2 Tutorial 39.3 Chapter Supplement", " Chapter 39 Estimating Incremental Validity of a Selection Tool Using Multiple Linear Regression In this chapter, we will learn how to estimate a multiple linear regression model in order to investigate whether a selection tool shows evidence of incremental validity with respect to other selection tools in the model. Well begin with a conceptual overview of multiple linear regression, and well conclude with a tutorial. 39.1 Conceptual Overview Multiple linear regression models allow us to apply statistical control and to examine whether a predictor variable shows incremental validity relative to other predictor variables with respect to an outcome variable. In this section, I will begin by reviewing fundamental concepts related to multiple linear regression and incremental validity. 39.1.1 Review of Multiple Linear Regression Like simple linear regression, multiple linear regression can provide us with information about the strength and sign of a linear association between a predictor variable and an outcome variable; however, unlike a simple linear regression model, a multiple linear regression model allows us to assess the strength and sign of the associations between two or more predictor variables and a single outcome variable. In doing so, using multiple linear regression, we can infer the association between a predictor variable and and outcome variable while statistically controlling for the associations between other predictor variables and the same outcome variable. When we statistically control for the effects of other predictor variables in a model, we are able to evaluate whether evidence of incremental validity exists for each predictor variable. Incremental validity refers to instances in which a predictor variable explains significant amounts of variance in the outcome variable even when statistically controlling for the effects of other predictor variables in the model. When a predictor variable (or a block of predictor variables) shows evidence of incremental validity, sometimes we use language like: Over and beyond the variance explained by Predictors W and Z, Predictor X explained significant variance in Outcome Y. In the context of employee selection, evaluating whether a selection tool shows evidence of incremental validity can be of value, as evidence of incremental validity can signify that a selection tool explains unique variance in the criterion (i.e., outcome) when accounting for the effects of other selection tools. In other words, if a selection tool shows evidence of incremental validity, we can be more confident that it contributes uniquely to the prediction of criterion scores and thus is not overly redundant with the other selection tools. In this chapter, we will learn how to estimate an ordinary least squares (OLS) multiple linear regression model, where OLS refers to the process of estimating the unknown components (i.e., parameters) of the regression model by attempting to minimize the sum of squared residuals. The sum of the squared residuals are the result of a process in which the differences between the observed outcome variable values and the predicted outcome variable values are calculated, squared, and then summed in order to identify a model with the least amount of error (residuals). This is where the concept of best fit comes into play, as the model is estimated to most closely fit the available data such that the error (residuals) between the predicted and observed outcome variable values are minimized. In other words, the goal is to find the linear model that best fits the data at hand; with that said, a specific type of regression called polynomial regression can be used to test nonlinear associations. Lets consider a scenario in which we estimate a multiple linear regression model with two predictor variables and a single outcome variable. The equation for such a model with unstandardized regression coefficients (\\(b\\)) would be as follows: \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables \\(X_1\\) and \\(X_2\\) are equal to zero, \\(b_{1}\\) and \\(b_{2}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_1\\) and \\(X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. Importantly, the unstandardized regression coefficients \\(b_{1}\\) and \\(b_{2}\\) represent the raw slopes (i.e., weights, coefficients)  or rather, how many unstandardized units of \\(\\hat{Y}\\) increase or decrease as a result of a single unit increase in either \\(X_1\\) or \\(X_2\\) when controlling for the effect of the other predictor variable. That is, unstandardized regression coefficients reflect the nature of the association between two variables when the variables retain their original scaling. Often this is why we choose to use the unstandardized regression coefficients when making predictions about \\(\\hat{Y}\\), as the predicted scores will be have the same scaling as the outcome variable in its original form. If we wanted to estimate a multiple linear regression model with three predictor variables, our equation would change as follows. \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + b_{3}X_3 + e\\) We can also obtain standardized regression coefficients. To do so, the predictor variables (e.g., \\(X_1\\), \\(X_2\\)) and outcome variable (\\(Y\\)) scores must be standardized. To standardize variables, we convert the predictor and outcome variables to z-scores, such that their respective means are standardized to 0 and their variances and standard deviations are standardized to 1. When standardized, our multiple linear regression model equation will have a \\(\\hat{Y}\\)-intercept value equal to zero (and thus is not typically reported) and the standardized regression coefficient is commonly signified using the Greek letter \\(\\beta\\): \\(\\hat{Y} = \\beta_{1}X_1 + \\beta_{2}X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted standardized score on the outcome variable (\\(Y\\)), \\(\\beta_{1}\\) and \\(\\beta_{2}\\) represent the standardized coefficients (i.e., weights, slopes) of the association between the predictor variables \\(X_1\\) and \\(X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. A standardized regression coefficient (\\(\\beta\\)) allows us to also compare the relative magnitude of one \\(\\beta\\) to another \\(\\beta\\); with that being said, in the case of a multiple linear regression model, comparing \\(\\beta\\) coefficients is only appropriate when the predictor variables in the model share little to no intercorrelation (i.e., have low collinearity). Given that, I recommend that you proceed with caution should you choose to make such comparisons. In terms of interpretation, in a multiple linear regression model, the standardized regression coefficient (\\(\\beta_{1}\\)) indicates the standardized slope when controlling for the effects of other predictor variables in the model  or rather, how many standard units of \\(\\hat{Y}\\) increase or decrease as a result of a single standard unit increase in \\(X_1\\) when accounting for the effects of other predictor variables in the model. 39.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a multiple linear regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of multivariate outliers; The association between the predictor and outcome variables is linear; There is no (multi)collinearity between predictor variables; Average residual error value is zero for all levels of the predictor variables; Variances of residual errors are equal for all levels of the predictor variables, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for all levels of the predictor variables. The fourth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so lets take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple linear regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights  and even the signs  of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapters data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 39.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of the other predictor variables in the model. In other words, if a regression coefficients p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent when controlling for the effects of other predictor variables in the model. In contrast, if the regression coefficients p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of other predictor variables in the model. Put differently, if a regression coefficients p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population  when controlling for the effects of other predictor variables in the model. I should note that it is entirely possible for a predictor variable and outcome variable to show a statistically significant association in a simple linear regression but for that same association to be not statistically significant when one or more predictor variables are added to the model. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 39.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a multiple linear regression model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. A standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) given that it is standardized. With that being said, I suggest doing so with caution as collinearity (i.e., correlation) between predictor variables in the model can bias our interpretation of \\(\\beta\\) as an effect size. Thus, if your goal is just to understand the bivariate association between a predictor variable and an outcome variable (without introducing statistical control), then I recommend to just estimate a correlation coefficient as an indicator of practical significance, which I discuss in the chapter on estimating criterion-related validity using correlations. In a multiple linear regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variables (i.e., R2). That is, in a multiple linear regression model, R2 represents the proportion of collective variance explained in the outcome variable by all of the predictor variables. Conceptually, we can think of the overlap between the variability in the predictor variables and and outcome variable as the variance explained (R2), and R2 is a way to evaluate how well a model fits the data (i.e., model fit). Ive found that the R2 is often readily interpretable by non-analytics audiences. For example, an R2 of .25 in a multiple linear regression model can be interpreted as: the predictor variable scores explain 25% of the variability in scores on the outcome variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large As an effect size, R2 indicates the proportion of variance explained by the predictor variables in relation to the outcome variable  or in other words, the shared variance between the variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. 39.1.1.4 Sample Write-Up A team of researchers is interested in whether a basketball players height and intelligence predict the number of points scored during a 10-game season. In this case, the predictor variables are the basketball players heights (in inches) and levels of intelligence, and the outcome variable is the number of points scored by the basketball players. Lets imagine that the researchers collected data on these variables from a sample of 100 basketball players. Our hypothesis for such a situation might be: Basketball players heights and intelligence scores will both be positively related to the number of points players score in a 10-game season, such that taller and more intelligent players will tend to score more points in a season. Imagine that we find that the unstandardized regression coefficients associated with player height (b = 2.31, p = .02) and player intelligence (b = .59, p = .01) in relation to points scored are statistically significant and positive, and that the R2 value is .24 (p &lt; .01). We could summarize the findings as follows: Based on a sample of 100 basketball players, basketball player height was found to predict points scored in a 10-game season, after controlling for player intelligence, such that taller players tended to score more points (b = 2.31, p = .02). Specifically, for every 1-inch increase in height, players tended to score 2.31 additional points during the season, when controlling for player intelligence. In addition, basketball player intelligence was found to predict points scored, when controlling for player height, such that more intelligent players tended to score more points (b = .59, p = .01). Specifically, for every 1 additional point scored on the intelligence test, players tended to score .59 additional points during the season. Further, this reflects a large collective effect, as approximately 24% of the variability in points scored was explained by players heights and intelligence, collectively (R2 = .24, p &lt; .01). 39.2 Tutorial This chapters tutorial demonstrates how to estimate a multiple linear regression model and interpret incremental validity. We also learn how to present the results in writing. 39.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/AOeJ2byUJdw Additionally, in the following video tutorial, I go into greater depth on how to test the statistical assumptions of a multiple linear regression model  just as I do in the written tutorial below. Link to video tutorial: https://youtu.be/zyEZop-5K9Q 39.2.2 Functions &amp; Packages Introduced Function Package Regression lessR 39.2.3 Initial Steps If you havent already, save the file called ConcurrentValidation.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called ConcurrentValidation.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## SJT = col_double(), ## EI = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 The data frame contains 5 variables and 300 cases (i.e., employees): EmployeeID, SJT, EI, Interview, and Performance. Lets assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., SJT, EI, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The SJT variable contains the scores on a situational judgment test designed to tap into the psychological concepts of emotional intelligence and empathy; potential scores on this variable could range from 1 (low emotional intelligence &amp; empathy) to 10 (emotional intelligence &amp; empathy). The EI variable contains scores on an emotional intelligence assessment; potential scores on this variable could range from 1 (low emotional intelligence) to 10 (emotional intelligence). The Interview variable contains the scores for a structured interview designed to assess interviewees level of interpersonal skills; potential scores on this variable could range from 1 (poor interpersonal skills) to 15 (interpersonal skills). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 30 (exceeds performance standards). 39.2.4 Estimate Multiple Linear Regression Model Lets assume that evidence of criterion-related validity was already found for the three selection tools (SJT, EI, Interview) using correlations; that is, the validity coefficient associated with each selection tool and the criterion (Performance) was statistically significant. If you want, you can run the correlations to verify this  or you can trust me on this. For a review of correlation in this context, check out the chapter on estimating criterion-related validity using a correlation. Given that evidence of criterion-related validity was already found for the three selection tools, our next step is to include all three selection tools as predictors in a multiple linear regression model. The criterion (Performance) variable will serve as our sole outcome variable in the model. In doing so, we can evaluate which selection tools show evidence of incremental validity. 39.2.4.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a multiple linear regression model, we need to first test the statistical assumptions. Fortunately, the Regression function from the lessR package automatically produces common tests of statistical assumptions. So to get started, lets install and access the lessR package using the install.packages and library functions, respectively. In the chapter supplement, you can learn how to carry how the same tests using the lm function from base R. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the names of the predictor variables (SJT, EI, Interview) to the right of the ~ operator; note that we use the + to add additional predictor variables to the model. We are telling the function to regress Performance on SJT, EI, and Interview. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate multiple linear regression model Regression(Performance ~ SJT + EI + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + EI + Interview, data=df, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: EI ## Predictor Variable 3: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.602 0.574 9.756 0.000 4.472 6.732 ## SJT 0.485 0.231 2.099 0.037 0.030 0.939 ## EI 0.091 0.237 0.385 0.700 -0.376 0.559 ## Interview 0.378 0.067 5.686 0.000 0.247 0.509 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.996 for 296 degrees of freedom ## 95% range of residual variation: 11.794 = 2 * (1.968 * 2.996) ## ## R-squared: 0.265 Adjusted R-squared: 0.257 PRESS R-squared: 0.233 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 35.507 df: 3 and 296 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.101 0.000 ## EI 1 36.702 36.702 4.088 0.044 ## Interview 1 290.301 290.301 32.331 0.000 ## ## Model 3 956.435 318.812 35.507 0.000 ## Residuals 296 2657.762 8.979 ## Performance 299 3614.197 12.088 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT EI Interview ## Performance 1.00 0.42 0.43 0.39 ## SJT 0.42 1.00 0.93 0.24 ## EI 0.43 0.93 1.00 0.32 ## Interview 0.39 0.24 0.32 1.00 ## ## Tolerance VIF ## SJT 0.127 7.887 ## EI 0.121 8.278 ## Interview 0.873 1.146 ## ## SJT EI Interview R2adj X&#39;s ## 1 0 1 0.259 2 ## 1 1 1 0.257 3 ## 0 1 1 0.249 2 ## 1 1 0 0.179 2 ## 0 1 0 0.178 1 ## 1 0 0 0.171 1 ## 0 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] ## --------------------------------------------------------------------------- ## SJT EI Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 8.000 1.000 24.000 11.074 12.926 4.516 0.765 0.137 ## 70 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 170 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 270 10.000 8.000 12.000 5.000 15.720 -10.720 -3.728 -0.753 0.136 ## 233 10.000 9.000 6.000 27.000 13.542 13.458 4.693 0.654 0.100 ## 1 9.000 8.000 2.000 22.000 11.452 10.548 3.634 0.556 0.074 ## 101 9.000 8.000 2.000 22.000 11.452 10.548 3.634 0.556 0.074 ## 69 2.000 1.000 1.000 18.000 7.041 10.959 3.772 0.505 0.061 ## 169 2.000 1.000 1.000 18.000 7.041 10.959 3.772 0.505 0.061 ## 92 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 192 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 292 8.000 7.000 15.000 10.000 15.794 -5.794 -1.989 -0.437 0.047 ## 20 2.000 1.000 8.000 18.000 9.689 8.311 2.837 0.422 0.044 ## 120 2.000 1.000 8.000 18.000 9.689 8.311 2.837 0.422 0.044 ## 283 9.000 7.000 5.000 22.000 12.496 9.504 3.249 0.418 0.042 ## 269 2.000 1.000 5.000 18.000 8.554 9.446 3.225 0.385 0.036 ## 97 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 197 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 297 2.000 1.000 3.000 14.000 7.797 6.203 2.096 0.248 0.015 ## 82 2.000 1.000 4.000 14.000 8.176 5.824 1.966 0.230 0.013 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT EI Interview Performance pred sf pi.lwr pi.upr width ## 69 2.000 1.000 1.000 18.000 7.041 3.023 1.092 12.990 11.898 ## 169 2.000 1.000 1.000 18.000 7.041 3.023 1.092 12.990 11.898 ## 41 1.000 2.000 3.000 8.000 7.404 3.052 1.398 13.410 12.012 ## ... ## 210 8.000 7.000 2.000 10.000 10.876 3.021 4.930 16.822 11.892 ## 4 6.000 5.000 5.000 11.000 10.859 3.002 4.951 16.766 11.815 ## 47 6.000 5.000 5.000 11.000 10.859 3.002 4.951 16.766 11.815 ## ... ## 225 7.000 6.000 9.000 16.000 12.948 3.011 7.022 18.874 11.852 ## 28 4.000 7.000 14.000 12.000 13.477 3.149 7.278 19.675 12.396 ## 128 4.000 7.000 14.000 12.000 13.477 3.149 7.278 19.675 12.396 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: ScatterPlot Matrix ## ---------------------------------- By default, the output for the Regression function produces three plots that are useful for assessing statistical assumptions and for interpreting the results, as well as text output. Lets begin by reviewing the first two plots depicting the residuals and the section of the text called Residuals and Influence. Lets take a look at the second plot in your Plot window; you may need to hit the back arrow button to review the three plots. Fitted Values &amp; Residuals Plot: The second plot is scatterplot that shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimates  or in other words, how much our predicted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term fitted values is another way of saying predicted values for the outcome variable. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for across levels of the predictor variables (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for all levels of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to deviate slightly from zero, which may indicate some degree of heteroscedasticity, and one case that is flagged as a potential multivariate outlier (case associated with row number 201). This case was flagged based on an outlier/influence statistic called Cooks distance (D). So we may have a slight violation of the homscedasticity of residuals assumption. Residuals &amp; Influence Output: Moving to the text output section called Residuals and Influence, we see a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: (a) studentized residual (rstdnt), (b) number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and (c) Cooks distance (cooks). Corroborating what we saw in the plot, the case associated with row number 201 has the highest Cooks distance value (.137), followed closely by the cases associated with row numbers 70, 170, and 270. There are several different rules of thumbs for what constitutes an extreme Cooks distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). None of our Cooks distance values exceed 1, where values in excess of 1 would indicate a problematic case. Regardless, as a sensitivity analysis, we would perhaps want to estimate our model once more after removing the cases associated with row number 201, 70, 170, and 270 from our data frame. Overall, we may have found what seems to be four potentially influential multivariate outlier cases, and we should be a bit concerned about satisfying the homoscedasticity of residuals, as the average residuals deviated slightly from zero. Its possible that removing the four aforementioned cases might also help to address the slight violation of the homoscedasticity of residuals assumption. Next, lets consider the following statistical assumption: Residual errors are normally distributed for each level of the predictor variable. Distribution of Residuals Plot: Moving on to the first plot, which displays the distribution of the residuals, we can use the display to determine whether or not we have satisfied the statistical assumption that the residuals are normally distributed. We see a histogram and density distribution of our residuals with the shape of a normal distribution superimposed. As you can see, our residuals show a mostly normal distribution, which is great and in line with the assumption. Collinearity: Moving onto the third diagnostic plot containing the scatterplot matrix, lets see if we might have any collinearity concerns. Specifically, lets focus in on the correlations between the predictor variables (i.e., selection tool variables), as these can be indicators of whether collinearity might be of concern in this model. The correlation between SJT and Interview is .24, and the correlation between Interview and EI is .32  both of which are acceptable from a collinearity perspective. In contrast, the correlation between SJT and EI is HUGE at .93. Generally speaking, a correlation that is .85 or higher can indicate that two variables are practically indistinguishable from one another, and here the correlation between SJT and EI is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error. To further explore potential collinearity in a different way, lets check out the table in our text output called Collinearity. The corresponding table shows two indices of collinearity: tolerance and valence inflation factor (VIF). Because the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), lets focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned about collinearity when the tolerance statistics falls below .20, with a tolerance of .00 being the worst and thus the strongest level of collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. In the table, we can see that SJT has a tolerance statistic of .127, which is lower than .20 and indicates that SJT overlaps considerably with the other predictor variables (EI, Interview) in the model in terms of shared variance. Similarly, EI has a troublesome tolerance statistic of .121, which suggests a similar problem. The tolerance for Interview is .873, which suggests low levels of collinearity (which is a good thing). Because SJT and EI both have low tolerance statistics, in this situation, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the correlation matrix (see above). As such, in the current model, we definitely have a collinearity problem. Summary of Statistical Assumptions Tests: Given the apparent slight violation of the assumption of homoscedasticity, the four potential multivariate outlier cases (i.e., 201, 70, 170, and 270, which correspond to EmployeeID values of EE223, EE92, EE292, EE192, respectively), and the very concerning levels of collinearity involving our SJT and EI variables, we should avoid interpreting the model as currently specified. In fact, we should at the very least remove either the SJT or EI predictor variable from the model to address the most concerning statistical assumption violation. So how do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource-intensive tool. In addition, we might consider applicants reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, lets assume that we have a compelling reason for dropping EI and retaining SJT. Re-Specify Model to Address Collinearity Issue: Given our decision to drop EI as a selection tool due to a severe violation of the collinearity assumption (see above), lets re-specify and re-estimate our multiple linear regression model with just SJT and Interview as predictor variables. # Re-estimate multiple linear regression model # (with EI variable dropped due to collinearity) Regression(Performance ~ SJT + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + Interview, data=df, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.527 0.539 10.249 0.000 4.465 6.588 ## SJT 0.567 0.085 6.714 0.000 0.401 0.734 ## Interview 0.385 0.064 6.031 0.000 0.260 0.511 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.992 for 297 degrees of freedom ## 95% range of residual variation: 11.777 = 2 * (1.968 * 2.992) ## ## R-squared: 0.264 Adjusted R-squared: 0.259 PRESS R-squared: 0.236 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 53.339 df: 2 and 297 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.303 0.000 ## Interview 1 325.670 325.670 36.375 0.000 ## ## Model 2 955.102 477.551 53.339 0.000 ## Residuals 297 2659.095 8.953 ## Performance 299 3614.197 12.088 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT Interview ## Performance 1.00 0.42 0.39 ## SJT 0.42 1.00 0.24 ## Interview 0.39 0.24 1.00 ## ## Tolerance VIF ## SJT 0.944 1.060 ## Interview 0.944 1.060 ## ## SJT Interview R2adj X&#39;s ## 1 1 0.259 2 ## 1 0 0.171 1 ## 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## SJT Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 1.000 24.000 11.019 12.981 4.537 0.736 0.169 ## 70 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 170 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 270 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 233 10.000 6.000 27.000 13.513 13.487 4.709 0.645 0.129 ## 1 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 101 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 69 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 169 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 92 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 192 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 292 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 20 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 120 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 283 9.000 5.000 22.000 12.560 9.440 3.226 0.372 0.045 ## 269 2.000 5.000 18.000 8.588 9.412 3.216 0.371 0.045 ## 97 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 197 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 297 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 82 2.000 4.000 14.000 8.203 5.797 1.959 0.224 0.017 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT Interview Performance pred sf pi.lwr pi.upr width ## 69 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 169 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 41 1.000 3.000 8.000 7.250 3.021 1.305 13.195 11.891 ## ... ## 210 8.000 2.000 10.000 10.837 3.015 4.903 16.771 11.868 ## 4 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## 47 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## ... ## 222 9.000 9.000 11.000 14.102 3.015 8.168 20.035 11.867 ## 92 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## 192 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: ScatterPlot Matrix ## ---------------------------------- In a real-world situation, we would once again work through the statistical assumption tests that we did above; however, for sake of brevity, we will assume that the statistical assumptions have been reasonably satisfied in this re-specified model in which only the SJT and Interview variables are included as predictors. Thus, we will feel confident that we can interpret our statistical tests, confidence intervals, and prediction intervals in a meaningful way, beginning with the Background section of the output. 39.2.4.2 Interpret Multiple Linear Regression Model Results Background: The Background section of the text output section shows which data frame object was used to estimate the model, the name of the response (outcome, criterion) variable, and the name of the predictor variable. In addition, it shows the number of cases in the data frame as well as how many were used in the estimation of the model; by default, the Regression function uses listwise deletion when one or more of the variables in the model has a missing value, which means that a case with any missing value on one of the focal variables is removed as part of the analysis. Here we can see that all 300 cases in the data frame were retained for the analysis, which means that none of the variables in the model had any missing values. Basic Analysis: The Basic Analysis section of the output first displays a table containing the estimated regression model (Estimated Model for [INSERT OUTCOME VARIABLE NAME]), including the regression coefficients (slopes, weights) and their standard errors, t-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (more on that later). The regression coefficients associated with the predictor variables (SJT and Interview) in relation to the outcome variable (Performance) are, however, of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .567, and its associated p-value is less than .001 (b = .567, p &lt; .001). [NOTE: Because the regression coefficient is unstandardized, its practical significance cannot be directly interpreted, and it is not a standardized effect size like a correlation coefficient.] Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero when statistically controlling for the effects of Interview. Further, the 95% confidence interval ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one point increase in situational judgment test (SJT) scores, job performance (Performance) scores increase by .567 points when controlling for the effect of structured interview (Interview) scores. Next, the unstandardized regression coefficient for Interview is .385, and its associated p-value is less than .001 (b = .385, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero when statistically controlling for the effects of SJT. Further, the 95% confidence interval ranges from .260 to .511 (i.e., 95% CI[.260, .511]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: For every one point increase in structured interview (Interview) scores, job performance (Performance) scores increase by .385 points when controlling for the effect of situational judgment test (SJT) scores. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Lets assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance  something well cover in greater depth in the next chapter. The Model Fit section of the output appears below the table containing the regression coefficient estimates. In this section, you will find the (unadjusted) R-squared (R2) estimate, which is an indicator of the models fit to the data as well as the extent to which the predictor variable explains variance (i.e., variability) in the outcome variable. The R-squared (R2) value of .264 indicates the extent to which the predictor variables collectively explain variance in the outcome variable in this sample  or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in Performance is explained by SJT and Interview collectively. You can also think of the R2 values as effect sizes (i.e., indicators of practical significance) at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the R2 effect size, which is another way of saying determining the level of practical significance: R2 Description .01 Small .09 Medium .25 Large The raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .259 (or 25.9%). If space permits, its a good idea to report both values, but given how close the unadjusted and adjusted R2 estimates tend to be, reporting and interpreting just the unadjusted R2 is usually fine  and is typically customary. The Model Fit section also contains the sum of squares, F-value, and p-value includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS).. In this table, we are mostly interested in the F-value and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variables. In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 45.317 and that its associated p-value is less than .001  the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. Again, you can think of the R2 value as an indicator of effect size at the model level, and in the table above, you will find the conventional thresholds for qualitatively describing a small, medium, or large R2 value. Relations Among the Variables: The section called Relations Among the Variables displays the zero-order (Pearson product-moment) correlation between the predictor variable and outcome variable. This correlation can be used to gauge the effect size of a predictor variable in relation to the outcome variable, as a correlation coefficient is a standardized metric that can be compared across samples - unlike an unstandardized regression coefficient; however, its important to note that such a correlation represents a bivariate (i.e., two-variable) association  and thus doesnt involve statistical control like our multiple linear regression model. Collinearity: The Colinearity section displays the tolerance statistics. As you can see, both tolerance statistics are .944 and thus close to 1.00, which indicates very low levels of collinearity. We would be concerned if a tolerance statistic fell below .20, which is not the case for this model when applied to this sample of employees. Prediction Error: In the output section called Prediction Error, information about the forecasting error and prediction intervals. This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, were not performing true predictive analytics. As such, we wont pay much attention to interpreting this section of the output in this tutorial. With that said, if youre curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to train or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after weve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. What lessR is doing in the Prediction Error section is taking the model you estimated using the focal dataset, which we could call our training dataset, and then it takes the values for our predictor and outcome variables from our sample and plugs them into the model and accounts for forecasting error for each set of values. Specifically, the standard error of forecast (sf) for each set of values is base on a combination of the standard deviation of the residuals for the entire model (modeling error) and the sampling error for the value on the regression line. Consequently, each set of values is assigned a lower and upper bound of a prediction interval for the outcome variable. The width of the prediction interval is specific to the values used to test the model, so the widths vary across the values. In fact, the further one gets from the mean of the outcome variable (in either direction), the wider the prediction intervals become. The 95% prediction intervals, along with the 95% confidence intervals and regression line of best fit, are plotted on the third and final plot of the function output. As you can, see the prediction intervals are the outermost lines as they include both sampling error and the modeling error, whereas the confidence intervals are the inner lines, as they reflect just the sampling error. Sample Technical Write-Up of Results: A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (SJT), an emotional intelligence assessment (EI), and a structured interview (Interview) in relation to job performance (Performance). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (b = .567, p &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (b = .385, p &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (R2 = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion. 39.2.4.3 Optional: Obtaining Standardized Coefficients As an optional detour, if you would like to estimate the same simple linear regression model but view the standardized regression coefficients, simply add the argument new_scale=\"z\" to your previous Regression function; that argument rescales your outcome and predictor variables to z-scores prior to estimating the model, which in effect produces standardized coefficients. # Estimate multiple linear regression model with standardized coefficients Regression(Performance ~ SJT + EI, data=df, new_scale=&quot;z&quot;) ## ## Rescaled Data, First Six Rows ## Performance SJT EI ## 1 3.240 1.632 1.609 ## 2 0.076 1.158 0.657 ## 3 -1.650 0.683 0.657 ## 4 0.076 0.209 0.181 ## 5 0.363 0.209 0.181 ## 6 0.363 -0.266 0.181 ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + EI, data=df, new_scale=&quot;z&quot;, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: EI ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## Data are Standardized ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) -0.000 0.052 -0.000 1.000 -0.103 0.103 ## SJT 0.158 0.145 1.089 0.277 -0.127 0.443 ## EI 0.278 0.145 1.919 0.056 -0.007 0.564 ## ## Standard deviation of Performance: 1.000 ## ## Standard deviation of residuals: 0.906 for 297 degrees of freedom ## 95% range of residual variation: 3.567 = 2 * (1.968 * 0.906) ## ## R-squared: 0.184 Adjusted R-squared: 0.179 PRESS R-squared: 0.165 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 33.551 df: 2 and 297 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 52.079 52.079 63.418 0.000 ## EI 1 3.024 3.024 3.683 0.056 ## ## Model 2 55.104 27.552 33.551 0.000 ## Residuals 297 243.898 0.821 ## Performance 299 299.002 1.000 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT EI ## Performance 1.00 0.42 0.43 ## SJT 0.42 1.00 0.93 ## EI 0.43 0.93 1.00 ## ## Tolerance VIF ## SJT 0.131 7.655 ## EI 0.131 7.655 ## ## SJT EI R2adj X&#39;s ## 1 1 0.179 2 ## 0 1 0.178 1 ## 1 0 0.171 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] ## ----------------------------------------------------------------- ## SJT EI Performance fitted resid rstdnt dffits cooks ## 233 2.107 2.086 4.678 0.913 3.765 4.316 0.594 0.111 ## 70 2.107 1.609 -1.650 0.780 -2.430 -2.741 -0.405 0.054 ## 170 2.107 1.609 -1.650 0.780 -2.430 -2.741 -0.405 0.054 ## 270 2.107 1.609 -1.650 0.780 -2.430 -2.741 -0.405 0.054 ## 201 1.632 1.609 3.815 0.705 3.110 3.519 0.395 0.050 ## 283 1.632 1.133 3.240 0.573 2.667 3.007 0.385 0.048 ## 20 -1.689 -1.724 2.089 -0.746 2.835 3.199 0.373 0.045 ## 69 -1.689 -1.724 2.089 -0.746 2.835 3.199 0.373 0.045 ## 120 -1.689 -1.724 2.089 -0.746 2.835 3.199 0.373 0.045 ## 169 -1.689 -1.724 2.089 -0.746 2.835 3.199 0.373 0.045 ## 269 -1.689 -1.724 2.089 -0.746 2.835 3.199 0.373 0.045 ## 1 1.632 1.609 3.240 0.705 2.535 2.848 0.320 0.033 ## 101 1.632 1.609 3.240 0.705 2.535 2.848 0.320 0.033 ## 31 -2.164 -0.771 -1.362 -0.556 -0.806 -0.916 -0.229 0.017 ## 131 -2.164 -0.771 -1.362 -0.556 -0.806 -0.916 -0.229 0.017 ## 231 -2.164 -0.771 -1.362 -0.556 -0.806 -0.916 -0.229 0.017 ## 82 -1.689 -1.724 0.939 -0.746 1.685 1.881 0.220 0.016 ## 97 -1.689 -1.724 0.939 -0.746 1.685 1.881 0.220 0.016 ## 182 -1.689 -1.724 0.939 -0.746 1.685 1.881 0.220 0.016 ## 197 -1.689 -1.724 0.939 -0.746 1.685 1.881 0.220 0.016 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT EI Performance pred sf pi.lwr pi.upr width ## 20 -1.689 -1.724 2.089 -0.746 0.912 -2.542 1.049 3.591 ## 40 -1.689 -1.724 -1.650 -0.746 0.912 -2.542 1.049 3.591 ## 55 -1.689 -1.724 0.076 -0.746 0.912 -2.542 1.049 3.591 ## ... ## 211 0.683 -0.295 -0.212 0.026 0.918 -1.782 1.833 3.615 ## 4 0.209 0.181 0.076 0.083 0.908 -1.703 1.870 3.573 ## 5 0.209 0.181 0.363 0.083 0.908 -1.703 1.870 3.573 ## ... ## 294 0.209 0.181 -0.212 0.083 0.908 -1.703 1.870 3.573 ## 28 -0.740 1.133 0.363 0.198 0.946 -1.664 2.061 3.724 ## 128 -0.740 1.133 0.363 0.198 0.946 -1.664 2.061 3.724 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: ScatterPlot Matrix ## ---------------------------------- In the Estimated Model section of the output, note that the intercept value is zeroed out due to the standardization, and the regression coefficients associated with SJT and Interview are now in standardized units (\\(\\beta\\) = .344, p &lt; .001 and \\(\\beta\\) = .309, p &lt; .001, respectively). Note that the p-values are the same as the unstandardized regression coefficients we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When statistically controlling for the effect of Interview, for every one standardized unit increase in SJT, Performance increases by .344 standardized units, and when statistically controlling for the effect of SJT, for every one standardized unit increase in Interview, Performance increases by .309 standardized units. 39.2.5 Summary In this chapter, we learned how to estimate a multiple linear regression model using the Regression function from the lessR package in order to estimate whether evidence of incremental validity exists. 39.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a multiple linear regression model. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 39.3.1 Functions &amp; Packages Introduced Function Package lm base R print base R vif car plot base R cooks.distance base R sort base R head base R summary base R confint base R cor base R scale base R apa.reg.table apaTables 39.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## SJT = col_double(), ## EI = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 39.3.3 lm Function from Base R In the following section, we will learn how to apply the lm function from base R to estimate a multiple linear regression model. Please note that we are doing some of the operations in a different order than what we did with the Regression function from lessR. Why? Well, the Regression function from lessR generates a number of diagnostics automatically (by default), and thus we took advantage of that in the previous section. With the lm function from base R, we have to piece together the diagnostics the old-fashioned way, which also happens to mean that we have more control over the order in which we do things. As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variables (SJT, EI, Interview) to the right of the ~ operator. We are telling the function to regress Performance on SJT, EI, and Interview. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify multiple linear regression model reg.mod1 &lt;- lm(Performance ~ SJT + EI + Interview, data=df) Note: You wont see any output in your console by specifying the regression model above. If you print the model (reg.mod1) to your console using the print function from base R, you only get the regression coefficients (but no statistical tests or model fit information). Later on, well apply a different function to obtain the full model results. # Print very brief model information (NOT NECESSARY) print(reg.mod1) ## ## Call: ## lm(formula = Performance ~ SJT + EI + Interview, data = df) ## ## Coefficients: ## (Intercept) SJT EI Interview ## 5.60170 0.48470 0.09147 0.37827 Now that we have specified our model, lets see if we might have any worrisome collinearity among our predictors, which if you recall, a key statistical assumption is that the model is free of collinearity. Identifying problematic collinearity typically means we need to re-specify our model by perhaps dropping a predictor variable or two. Thus, lets start with testing the assumption related to collinearity. Creating a correlation matrix is one great way to identify possible collinearity issues, so lets start with a correlation matrix of the variables in our data frame. Lets use the cor function from base R. Before doing, so lets create a temporary data frame object (temp) in which we drop the EmployeeID variable from the original df data frame object; we need to do this because the cor function only accepts numeric variables, and the EmployeeID variable is non-numeric. , and enter the name of our data frame as the sole argument. To learn more about how to remove variables, feel free to check out the chapter on filtering # Create temporary data frame object # with EmployeeID variable removed temp &lt;- subset(df, select=-EmployeeID) # Create basic correlation matrix cor(temp) ## SJT EI Interview Performance ## SJT 1.0000000 0.9324020 0.2373394 0.4173192 ## EI 0.9324020 1.0000000 0.3175592 0.4255306 ## Interview 0.2373394 0.3175592 1.0000000 0.3906502 ## Performance 0.4173192 0.4255306 0.3906502 1.0000000 High correlations between pairs of predictor variables are indicative of high collinearity. The correlation between SJT and Interview is approximately .24, and the correlation between EI and Interview is .32, both of which are in the acceptable range. The correlation between SJT and EI is HUGE at .93. Generally speaking, a correlation that is .85 or higher can indicate that two predictors are practically distinguishable from one another, and here the correlation between SJT and EmotionalIntelligence is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error. To further explore this collinearity issue, lets estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you havent already, install the car package which contains the vif function we will use. # Install car package if you haven&#39;t already install.packages(&quot;car&quot;) Now, access the car package using the library function. # Access car package library(car) # Compute VIF statistic vif(reg.mod1) ## SJT EI Interview ## 7.887216 8.277683 1.145830 Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide 1 by the VIF. # Compute tolerance statistic 1 / vif(reg.mod1) ## SJT EI Interview ## 0.1267875 0.1208068 0.8727299 In the table, we can see that SJT has a tolerance statistic of .127, which is lower than .20 and indicates that SJT overlaps considerable with the other predictor variables (EI, Interview) in terms of shared variance. Similarly, the EI has a tolerance statistic of .121, which suggests a similar problem. Because SJT and EI both have very low tolerance statistics that fall below .20, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the scatterplot matrix plot. As such, in the current model, we definitely have a collinearity problem involving SJT and EI, which means interpreting the rest of the output would not be appropriate. When we face severe collinearity between scores on two selection tools, as we do in this scenario, we need to make a thoughtful decision. Given that the correlation between SJT and EI is extremely high at .93, these variables are from a statistical perspective essentially redundant. How do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource intensive tool. In addition, we might consider applicants reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, lets assume that we have a compelling reason for dropping EI and retaining SJT. Given our decision to drop EI as a selection tool due to a violation of the collinearity assumption, lets re-specify and re-estimate our multiple linear regression model with just SJT and Interview as predictor variables. This time, lets name our model object reg.mod2. # Re-specify multiple linear regression model # with just SJT &amp; Interview as predictor variables reg.mod2 &lt;- lm(Performance ~ SJT + Interview, data=df) Lets quickly run the VIF and tolerance statistics to see if collinearity is an issue with the two predictors in this new model. # Compute VIF statistic vif(reg.mod2) ## SJT Interview ## 1.059692 1.059692 # Compute tolerance statistic 1 / vif(reg.mod2) ## SJT Interview ## 0.94367 0.94367 Both tolerance statistics are identical (because we only have two predictors in the model), and both values are .943, which is nearly 1.00. Accordingly, we dont have any concerns with collinearity in this new model. Were ready to move on to evaluating other important statistical assumptions. Statistical Assumptions: Lets look at some diagnostics to determine whether we have reason to believe we have met the other statistical assumptions described at the beginning of this tutorial. We will generate plots and other output to inform our conclusions whether we have satisfied certain statistical assumptions. We will begin by generating a scatterplot displaying the association between the fitted (predicted) values and residuals. To do so, we will use the plot function from base R. As the first argument, enter the name of the regression model object you created above (reg.mod2). As the second argument, type the numeral 1, which will request the first of four possible diagnostic plots, of which we will review three. # Diagnostics plot: fitted values &amp; residuals plot(reg.mod2, 1) The resulting plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term fitted values is another way of saying predicted values for the outcome variable, but the language fitted is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may not be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may not be zero for each level of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be three cases that are flagged as a potential multivariate outlier (i.e., row numbers 69, 201, and 233). As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the plot script from above, but this time, enter the numeral 2 (instead of 1) to request the second diagnostic plot. # Diagnostics plot: normal Q-Q plot plot(reg.mod2, 2) Normally distributed residuals will fall along the dotted diagonal line. As you can see, many of the residuals fall on or near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers 69, 201, and 233. We also see some deviations from the dotted line at the low and high ends of the theoretical quantiles, which shows some departure from normality in the residuals. As the last diagnostic plot, lets look at Cooks distance (D) across cases. Once again, adapt the plot script from above, but this time, enter the numeral 4 to request the fourth diagnostic plot. Were skipping the third plot. # Diagnostics plot: Cook&#39;s Distance plot plot(reg.mod2, 4) Here we see case associated with row number 201 again, but we also see cases associate with row numbers 70, 170, and 201 based on Cooks distance. We can also grab the cases with the highest Cooks distances. Lets create an object called cooksD that we will assign a vector of Cooks distance values to using the cooks.distance function from base R. Just enter the name of the regression model object (reg.mod1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cooks distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(reg.mod2) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top-20 Cook&#39;s distance values head(cooksD, n=20) ## 201 70 170 270 233 101 1 69 169 ## 0.16917874 0.14329599 0.14329599 0.14329599 0.12946844 0.09214623 0.09214623 0.08140835 0.08140835 ## 92 192 292 20 120 283 269 97 197 ## 0.06114519 0.06114519 0.06114519 0.05121764 0.05121764 0.04481510 0.04456115 0.01981777 0.01981777 ## 297 82 ## 0.01981777 0.01656404 Here we get the numeric values associated with Cooks distances, which corroborates what we saw in the plot of Cooks distances above. Again, the case associated with row 201 is by far the largest, and the next largest values are clustered closer together, which include the cases associated with row numbers 70, 170, and 270. There are many different rules of thumbs for what constitutes an extreme Cooks distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply 1 (Bollen and Jackman 1985). Across the three diagnostic plots and the vector of Cooks distances, 201 appears to be consistently the most concerning case, and cases associated with row numbers 69, 70, 170, 270, and 233 may also be problematic. For the most part, the distribution of the residuals look mostly normal. As such, as a sensitivity analysis and for the sake of demonstration, we will estimate our model once more after removing the cases associated with row numbers 201, 69, 70, 170, 270, and 233 from our data frame. If youve completed the entire tutorial thus far, you may have noted that some of the plots we are using with the lm function are different than those that come with the Regression function from lessR (see above), and further, you may have noticed that we have flagged more potential problematic cases in using these plots than the ones for the Regression function. This goes to show, again, how thoughtful we must be with the tools that we use, and again, I recommend erring on the side of caution when it comes to removing cases for subsequent analyses. And, remember, no model will ever be perfect. For now, lets go ahead and interpret the results of the multiple linear regression model. Obtaining the Model Results: Type the name of the summary function from base R and include whatever you named your regression model (reg.mod1) as the sole parenthetical argument; we specified the regression model object called reg.mod2) earlier in the tutorial. The summary function simply returns a summary of your estimated regression model results. # Get summary of multiple linear regression model results summary(reg.mod2) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8249 -1.5198 -0.0401 1.0396 13.4868 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.52654 0.53925 10.249 &lt; 0.0000000000000002 *** ## SJT 0.56748 0.08453 6.714 0.0000000000964 *** ## Interview 0.38530 0.06388 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.992 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called Coefficients contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, t-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficients for the predictor variables in relation to the outcome variable are often of substantive interest. Here, we see that the unstandardized regression coefficient for SJT is .567, and its associated p-value is less than .001 (b = .567, p &lt; .001). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: Controlling for the effects of Interview, for every one point increase in SJT, Performance increases by .567 points Lets move on to the regression coefficient for Interview, which is statistically significant and positive too (b = .385, p &lt; .001). Because these two regression coefficients are unstandardized, it would not be appropriate to compare their magnitudes. We can interpret the significant regression coefficient as follows: Controlling for the effects of SJT, for every one point increase in Interview, Performance increases by .385 points. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) If we plug in, for example, the value 6 as an observed value of SJT and the value 7 as an observed value of Interview, then we get 11.624, as shown below: \\(11.624 = 5.527 + (.567 * 6) + (.385 * 7)\\) Thus, we are able to predict future values of Performance based on our estimated regression model. Below the table containing the regression coefficient estimates, the (unadjusted multiple) R-squared (R2) and adjusted R2 values appear, which are indicators of the models fit to the data as well as the extent to which the predictor variables collectively explain variability in the outcome variable. First, the (multiple) R-squared (R2) value of .264 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in Performance is explained by SJT. This raw, unadjusted R-squared (R2) value, however, is sensitive to the sample size and the number of predictor variables in the model. The adjusted R2 value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The adjusted R2 is a better indicator of the magnitude of the association in the underlying population and thus tends to be more accurate. Here we see that the adjusted R2 value is slightly smaller at .259 (or 25.9%). Typically, its a good idea to report both values. The table containing the sum of squares, F-values, and p-values includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. In this table, we are mostly interested in the F-value associated with the overall model and its associated p-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the null model does not include any predictor variables. In other words, the F-value and p-value are associated with the R2 value and whether the unadjusted and adjusted R2 values are significantly different from zero. In this example, we see that the F-value is 53.733 and its associated p-value is less than .001, the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the R2 values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. You can think of the R2 values as indicators of effect size at the model level. I provide some rules of thumb for qualitatively interpreting the magnitude of the effect size, which is another way of saying determining the level of practical significance (see table below). As you can see, our statistically significant unadjusted and adjusted R2 value can both be described as large by conventional standards. Thus, we seem to have a good model here. R2 Description .01 Small .09 Medium .25 Large To estimate the 95% confidence intervals, we can apply the confint function from base R, and enter the name of the regression model (reg.mod1) as the sole argument. # Estimate 95% confidence intervals confint(reg.mod2) ## 2.5 % 97.5 % ## (Intercept) 4.4652963 6.5877787 ## SJT 0.4011348 0.7338281 ## Interview 0.2595753 0.5110242 The 95% confidence interval for the SJT regression coefficient ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for association likely falls somewhere between those two values. The 95% confidence interval for the Interview regression coefficient ranges from .260 to .511. (i.e., 95% CI[.260, .511.]). As a direct indicator of the effect size (practical significance), we can calculate the zero-order (Pearson product-moment) correlations between SJT and Performance and between Interview and Performance. Keep the method=\"pearson\" as is to request a Pearson product-moment correlation, and be sure to include the name of the data frame object (df) followed by the $ operator in front of each variable. # Estimate zero-order correlations (effect sizes) cor(df$SJT, df$Performance, method=&quot;pearson&quot;) ## [1] 0.4173192 cor(df$Interview, df$Performance, method=&quot;pearson&quot;) ## [1] 0.3906502 The correlation coefficients between SJT and Performance and between Interview and Performance are medium-large in magnitude (r = .42 and r = .39, respectively), which indicates a moderate-strong, positive, effects for each of these selection tools with respect to Performance. r Description .10 Small .30 Medium .50 Large Sample Technical Write-Up of Results: A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (SJT), an emotional intelligence assessment (EI), and a structured interview (Interview) in relation to job performance (Performance). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (b = .567, p &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (b = .385, p &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (R2 = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion. Obtaining Standardized Coefficients: If you would like to estimate the same multiple linear regression model but view the standardized regression coefficients, just do some small tweaks to the lm code/script that we specified above. First, lets name the model something different given that it will include standardized coefficients; here, I decided to name the model st_reg.mod2. Next, within the lm function, apply the scale function from base R to the predictor and outcome variables as shown; doing so will standardize our variables and center them around zero. # Estimate multiple linear regression model # with standardized coefficient estimates st_reg.mod2 &lt;- lm(scale(Performance) ~ scale(SJT) + scale(Interview), data=df) summary(st_reg.mod2) ## ## Call: ## lm(formula = scale(Performance) ~ scale(SJT) + scale(Interview), ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1135 -0.4371 -0.0115 0.2990 3.8792 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.000000000000000002772 0.049688712929418260567 0.000 1 ## scale(SJT) 0.343978856791380960267 0.051235702957663491197 6.714 0.0000000000964 *** ## scale(Interview) 0.309010486383703486535 0.051235702957663539769 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8606 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 In the Coefficients portion of the output, note that the intercept value is virtually zeroed out due to the standardization, and the regression coefficients associated with SJT and Interview are now in standardized units (\\(\\beta\\) = .340, p &lt; .001 and \\(\\beta\\) = .309, p &lt; .001, respectively). The reason that the intercept value is not exactly zero (in scientific notation) is rounding error. Note that the p-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When controlling for Interview, for every one standardized unit increase in SJT, Performance increases by .344 standardized units, and when controlling for SJT, for every one standardized unit increase in Interview, Performance increases by .309 standardized units. Dealing with Multivariate Outliers: If you recall above, we found that the case associated with row numbers 201, 70, 270, and 170 in this sample may be candidates for removal. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison cases, unless the cases appear to have a dramatic influence on the estimated regression line. Some might argue that we should retain cases with row numbers 201, 70, 270, and 170, and others might argue that we should remove them; really, it comes down to your own logic, rationale, and justification, and I recommend pulling in other information you might have about these cases (even beyond data contained in this dataset) to inform your decision. If you were to decide to remove these cases, heres what you would do. First, look at the data frame (using the View function) and determine which cases row number 201, 70, 270, and 170 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that row numbers 201, 70, 270, and 170 in our data frame are associated with EmployeeIDs EE223, EE92, EE292, and EE192, respectively. Next, with respect to estimating the regression model, I suggest naming the unstandardized regression model something different, and here I name it reg.mod3. The model should be specified just as it was earlier in the tutorial, but now lets add an additional argument: subset=(!EmployeeID %in% c(\"EE223\", \"EE92\", \"EE292\", \"EE192\")); the subset argument subsets the data frame within the lm function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which EmployeeID is not equal to EE223, EE92, EE292, and EE192. Remember, the logical operator ! means not and the %in% operator means within. Revisit the chapter on filtering data if you want to see the full list of logical operators and more information on removing multiple cases. # Estimate multiple linear regression model # but remove row numbers 201, 70, 270, and 170 # which correspond to EmployeeID numbers of # EE223, EE92, EE292, and EE192 reg.mod3 &lt;- lm(Performance ~ SJT + Interview, data=df, subset=(!EmployeeID %in% c(&quot;EE223&quot;, &quot;EE92&quot;, &quot;EE292&quot;, &quot;EE192&quot;))) summary(reg.mod3) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df, subset = (!EmployeeID %in% ## c(&quot;EE223&quot;, &quot;EE92&quot;, &quot;EE292&quot;, &quot;EE192&quot;))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1832 -1.2913 -0.0509 0.8278 13.1206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70328 0.49639 9.475 &lt; 0.0000000000000002 *** ## SJT 0.61531 0.07723 7.968 0.000000000000036184 *** ## Interview 0.50383 0.05873 8.579 0.000000000000000567 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.677 on 293 degrees of freedom ## Multiple R-squared: 0.371, Adjusted R-squared: 0.3667 ## F-statistic: 86.41 on 2 and 293 DF, p-value: &lt; 0.00000000000000022 The pattern of results remains mostly the same; however, the model fit (as evidenced R2) improved after removing these outliers. Even with an improved model fit, We should remain hesitant to use the model based on the reduced sample (without these multivariate outliers), as this could potentially hurt our ability to generalize our findings to future samples of applicants. Then again, if the outlier cases really to do seem to be atypical for the population of interest, then you might make a case for removing them. Basically, its up to you to justify your decision and document it. If your data frame does not have a unique identifier variable, you could remove the outlier by referencing the outlier by row number. Instead of using the subset= argument, you would follow up your data=df argument with [-c(201, 70, 270, 170),] to indicate that you wish to remove row numbers 201, 70, 270, and 170. The minus sign (-) signals not. # Estimate multiple linear regression model # but remove row numbers 201, 70, 270, and 170 reg.mod3 &lt;- lm(Performance ~ SJT + Interview, data=df[-c(201, 70, 270, 170),]) summary(reg.mod3) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df[-c(201, ## 70, 270, 170), ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1832 -1.2913 -0.0509 0.8278 13.1206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70328 0.49639 9.475 &lt; 0.0000000000000002 *** ## SJT 0.61531 0.07723 7.968 0.000000000000036184 *** ## Interview 0.50383 0.05873 8.579 0.000000000000000567 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.677 on 293 degrees of freedom ## Multiple R-squared: 0.371, Adjusted R-squared: 0.3667 ## F-statistic: 86.41 on 2 and 293 DF, p-value: &lt; 0.00000000000000022 39.3.4 APA-Style Results Table If you want to present the results of your multiple linear regression to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. Using the lm function from base R, as we did above, lets begin by estimating a multiple linear regression model and naming the model object (reg.mod1). # Estimate multiple linear regression model reg.mod1 &lt;- lm(Performance ~ SJT + Interview, data=df) If you havent already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (reg.mod1) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(reg.mod1) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 5.53** [4.47, 6.59] ## SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42** ## Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39** ## R2 = .264** ## 95% CI[.18,.34] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file APA Multiple Linear Regression Table.doc. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(reg.mod1, filename=&quot;APA Multiple Linear Regression Table.doc&quot;) ## ## ## Regression results using Performance as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r Fit ## (Intercept) 5.53** [4.47, 6.59] ## SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42** ## Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39** ## R2 = .264** ## 95% CI[.18,.34] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table multiple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. "],["compensatory.html", "Chapter 40 Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression 40.1 Conceptual Overview 40.2 Tutorial 40.3 Chapter Supplement", " Chapter 40 Applying a Compensatory Approach to Selection Decisions Using Multiple Linear Regression In this chapter, we will learn how to apply a compensatory approach to making selection decisions by using multiple linear regression. Well begin with a conceptual overview of the compensatory approach, and well conclude with a tutorial. 40.1 Conceptual Overview Just as we can use multiple linear regression to evaluate whether evidence of incremental validity exists for a selection tool, we can also use multiple linear regression to apply a compensatory approach to making selection decisions. In general, there are three overarching (and mutually non-exclusive) approaches to making selection decisions: (a) compensatory (e.g., multiple linear regression), (b) noncompensatory (e.g., multiple-cutoff), and (c) multiple-hurdle. These three approaches can be mixed and matched to fit the selection-decision needs of an organization; however, in this chapter, well focus specifically on the compensatory approach, whereas in the next chapter, we will focus on the multiple-cutoff noncompensatory approach. 40.1.1 Review of Multiple Linear Regression This chapter is an extension of the chapter on estimating the incremental validity of a selection tool using multiple linear regression. Thus, in this chapter, I will forgo a review of ordinary least squares (OLS) multiple linear regression, as that is already covered in the previous chapter, along with a discussion of statistical assumptions, statistical significance, and a sample write-up. 40.1.2 Review of Compensatory Approach In the context of employee selection, a compensatory approach refers to the process of using applicants scores on multiple selection tools to predict who will have the highest scores on the criterion. In doing so, applicants can compensate for lower scores on one selection tool with higher scores on one or more other selection tools  hence the name compensatory approach. One of the simpler ways to apply a compensatory approach is to apply equal weights to scores from the various selection tools when predicting future criterion scores; however, such an approach does not acknowledge that some selection tools may explain more variance in the criterion (e.g., job performance) than others. Using regression coefficients from a multiple linear regression model, we can weight scores on selection tools differentially when predicting criterion scores. Notably, these weights are empirically driven (e.g., data informed) as opposed to judgment informed (e.g., subject matter expert ratings). In essence, applying a compensatory approach using multiple linear regression allows us to apply our estimated regression model equation to predict criterion scores for applicants. Subsequently, we can sort the predicted criterion scores to determine which applicants are most promising and to inform selection decisions. To understand how a compensatory approach works, lets consider a hypothetical scenario in which we administer three selection tools to applicants: cognitive ability test (CA), work simulation (WS), and structured interview (SI). Lets assume that a predictive validation study was conducted previously to estimate the criterion-related validities of these three selection tools and whether they each showed evidence of incremental validity with respect to each other  and lets imagine that our criterion of interest is job performance. Finally lets assume that the following equation represents the general framework for the multiple linear regression model we estimated for investigating incremental validity: \\(\\hat{Y}_{Performance} = b_{Intercept} + b_{CA}X_{CA} + b_{WS}X_{WS} + b_{SI}X_{SI}\\) where \\(\\hat{Y}_{Performance}\\) represents the predicted score on the performance criterion variable, \\(b_{Intercept}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables are equal to zero, \\(b_{CA}\\), \\(b_{WS}\\), and \\(b_{SI}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_{CA}\\) (cognitive ability), \\(X_{WS}\\) (work simulation), and \\(X_{SI} (structured interview)\\), respectively, and the outcome variable \\(\\hat{Y}_{Performance}\\). Now, lets plug some hypothetical intercept and regression coefficient estimates into the equation, and assume that these estimates were pulled from our unstandardized multiple linear regression output. \\(\\hat{Y}_{Performance} = 1.32 + .42_{CA}X_{CA} + .50_{WS}X_{WS} + .22_{SI}X_{SI}\\) Finally, lets imagine that we administered the cognitive ability test, work simulation, and structured interview to two applicants with the goal of predicting their future job performance scores. The first applicant received the following scores on the selection tools, where all tests happened to be out of a possible 10 points: cognitive ability (CA) = 5, work simulation (WS) = 7, and structured interview (SI) = 9. If we plug those values into the regression equation, we get a predicted job performance score of 8.90. \\(8.90_{Performance} = 1.32 + .42_{CA}*5_{CA} + .50_{WS}*7_{WS} + .22_{SI}*9_{SI}\\) In contrast, the second applicant received the following scores on the selection tools: cognitive ability (CA) = 7, work simulation (WS) = 5, and structured interview (SI) = 9. Notice that the structured interview score was the same for both applicants but that the cognitive ability and work simulation scores were flipped-flopped. In this hypothetical example, if you were to add up each applicants total score across the three selection tools, the sum would be the same: 21 points. Because each test in this example has maximum potential score of 10, the total possible points is 30 points. Thus, if we were to simply add up both applicants scores, these applicants would be tied. Fortunately, just like we did for the first applicant, we can apply our regression coefficients (i.e., weights) to these three scores to generated a score that reflects a weighted compensatory approach. For the second applicant, the predicted performance score is 8.74. \\(8.74_{Performance} = 1.32 + .42_{CA}*7_{CA} + .50_{WS}*5_{WS} + .22_{SI}*9_{SI}\\) As you can see, the first applicant has a higher predicted performance score (8.90) than the second applicant (8.74), so based purely off of this information, we would rank the first applicant higher. Hopefully, you can see how the selection tool scores are differentially weighted based on their corresponding regression coefficient estimates. 40.2 Tutorial This chapters tutorial demonstrates how to apply a compensatory approach to making selection decisions by estimate a multiple linear regression model. 40.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/vtnYvcs1yUQ 40.2.2 Functions &amp; Packages Introduced Function Package Regression lessR order base R 40.2.3 Initial Steps If you havent already, save the file called ConcurrentValidation.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called ConcurrentValidation.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## SJT = col_double(), ## EI = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 The data frame contains 5 variables and 300 cases (i.e., employees): EmployeeID, SJT, EI, Interview, and Performance. Lets assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., SJT, EI, Interview) were administered to job incumbents and the criterion measure (Performance) was administered at about the same time. To begin, EmployeeID is the unique identifier variable. The SJT variable contains the scores on a situational judgment test designed to tap into the psychological concepts of emotional intelligence and empathy; potential scores on this variable could range from 1 (low emotional intelligence &amp; empathy) to 10 (emotional intelligence &amp; empathy). The EI variable contains scores on an emotional intelligence assessment; potential scores on this variable could range from 1 (low emotional intelligence) to 10 (emotional intelligence). The Interview variable contains the scores for a structured interview designed to assess interviewees level of interpersonal skills; potential scores on this variable could range from 1 (poor interpersonal skills) to 15 (interpersonal skills). Finally, the criterion for this concurrent validation study is the Performance variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (does not meet performance standards) to 30 (exceeds performance standards). 40.2.4 Estimate Multiple Linear Regression Model Well begin by first assuming that that evidence of criterion-related validity was already found for the three selection tools (SJT, EI, Interview) using correlations. Second, well assume that the EI selection tool was not found to have incremental validity when all three selection tools were included in a multiple linear regression model, thereby leaving us to estimate a second multiple linear regression model in which only SJT and Interview were included as predictors. For reviews of the aforementioned assumed prior steps, please refer to the previous chapters on estimating criterion-related validity using a correlation and estimating the incremental validity of a selection tool using multiple linear regression. In this chapter, well begin by estimating the multiple linear regression model with SJT and Interview specified as predictor variables and Performance specified as the outcome variable. There are different functions we could use to estimate a multiple linear regression model, and in this tutorial well focus on just one: the Regression function from lessR. Note that this is the same function that we can use to estimate a simple linear regression model. If you havent already install and access the lessR package using the install.packages and library functions, respectively. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) To use the Regression function from the lessR package, type the name of the Regression function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome (i.e., criterion) variable (Performance) to the left of the tilde (~) operator and the names of the predictor (e.g., selection tool) variables (SJT, Interview) separated by the + operator to the right of the ~ operator. We are telling the function to regress Performance on SJT and Interview. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). # Estimate multiple linear regression model Regression(Performance ~ SJT + Interview, data=df) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## Regression(my_formula=Performance ~ SJT + Interview, data=df, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: df ## ## Response Variable: Performance ## Predictor Variable 1: SJT ## Predictor Variable 2: Interview ## ## Number of cases (rows) of data: 300 ## Number of cases retained for analysis: 300 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.527 0.539 10.249 0.000 4.465 6.588 ## SJT 0.567 0.085 6.714 0.000 0.401 0.734 ## Interview 0.385 0.064 6.031 0.000 0.260 0.511 ## ## Standard deviation of Performance: 3.477 ## ## Standard deviation of residuals: 2.992 for 297 degrees of freedom ## 95% range of residual variation: 11.777 = 2 * (1.968 * 2.992) ## ## R-squared: 0.264 Adjusted R-squared: 0.259 PRESS R-squared: 0.236 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 53.339 df: 2 and 297 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## SJT 1 629.432 629.432 70.303 0.000 ## Interview 1 325.670 325.670 36.375 0.000 ## ## Model 2 955.102 477.551 53.339 0.000 ## Residuals 297 2659.095 8.953 ## Performance 299 3614.197 12.088 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## Performance SJT Interview ## Performance 1.00 0.42 0.39 ## SJT 0.42 1.00 0.24 ## Interview 0.39 0.24 1.00 ## ## Tolerance VIF ## SJT 0.944 1.060 ## Interview 0.944 1.060 ## ## SJT Interview R2adj X&#39;s ## 1 1 0.259 2 ## 1 0 0.171 1 ## 0 1 0.150 1 ## ## [based on Thomas Lumley&#39;s leaps function from the leaps package] ## ## ## RESIDUALS AND INFLUENCE ## ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] ## --------------------------------------------------------------------- ## SJT Interview Performance fitted resid rstdnt dffits cooks ## 201 9.000 1.000 24.000 11.019 12.981 4.537 0.736 0.169 ## 70 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 170 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 270 10.000 12.000 5.000 15.825 -10.825 -3.755 -0.670 0.143 ## 233 10.000 6.000 27.000 13.513 13.487 4.709 0.645 0.129 ## 1 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 101 9.000 2.000 22.000 11.404 10.596 3.653 0.537 0.092 ## 69 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 169 2.000 1.000 18.000 7.047 10.953 3.775 0.505 0.081 ## 92 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 192 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 292 8.000 15.000 10.000 15.846 -5.846 -2.008 -0.430 0.061 ## 20 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 120 2.000 8.000 18.000 9.744 8.256 2.819 0.397 0.051 ## 283 9.000 5.000 22.000 12.560 9.440 3.226 0.372 0.045 ## 269 2.000 5.000 18.000 8.588 9.412 3.216 0.371 0.045 ## 97 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 197 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 297 2.000 3.000 14.000 7.817 6.183 2.092 0.245 0.020 ## 82 2.000 4.000 14.000 8.203 5.797 1.959 0.224 0.017 ## ## PREDICTION ERROR ## ## Data, Predicted, Standard Error of Forecast, ## 95% Prediction Intervals ## [sorted by lower bound of prediction interval] ## [to see all intervals do pred_rows=&quot;all&quot;] ## ---------------------------------------------- ## ## SJT Interview Performance pred sf pi.lwr pi.upr width ## 69 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 169 2.000 1.000 18.000 7.047 3.018 1.107 12.987 11.880 ## 41 1.000 3.000 8.000 7.250 3.021 1.305 13.195 11.891 ## ... ## 210 8.000 2.000 10.000 10.837 3.015 4.903 16.771 11.868 ## 4 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## 47 6.000 5.000 11.000 10.858 2.998 4.959 16.757 11.798 ## ... ## 222 9.000 9.000 11.000 14.102 3.015 8.168 20.035 11.867 ## 92 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## 192 8.000 15.000 10.000 15.846 3.057 9.829 21.862 12.033 ## ## ---------------------------------- ## Plot 1: Distribution of Residuals ## Plot 2: Residuals vs Fitted Values ## Plot 3: ScatterPlot Matrix ## ---------------------------------- Given that the output for this model has already been reviewed in detail in the chapter on estimating the incremental validity of a selection tool using multiple linear regression, well focus only on the information that is relevant for applying a compensatory model  namely the regression coefficients. For the estimated model, we find that both selection tools (i.e., predictor variables) show evidence of incremental validity; that is, both selection tools are significantly associated with the criterion of job performance when statistically controlling for the effects of each other. Specifically, the association between SJT and Performance (when controlling for Interview) is positive and statistically significant (b = .567, p &lt; .001), and the association between Interview and Performance (when controlling for SJT) is also positive and statistically significant (b = .385, p &lt; .001). If you recall from the chapter on predicting criterion scores on using simple linear regression, the intercept and predictor variable regression coefficients can be used to construct an equation, and this equation serves as the basis for making criterion-score predictions. In fact, when an equation comes from estimated multiple linear regression coefficients, the equation can serve as the basis for applying a compensatory approach to selection decisions. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Lets assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance. Well bring this process to life in the following section in order to apply a compensatory approach to selection decisions. 40.2.5 Predict Criterion Scores Using the multiple linear regression model we estimated above, we can write code that generates predicted criterion scores based on new data we feed into the model. In fact, we can even add these predictions to the new data frame to which the new data belong. Well begin by specifying the same multiple linear regression model as above  except this time we will assign the estimated model to an object that we can subsequently reference. To do so, well use the &lt;- operator. In this example, Im naming the model reg_mod. # Assign multiple linear regression model to object reg_mod &lt;- Regression(Performance ~ SJT + Interview, data=df) Now that because we have assigned the multiple linear regression model to the object reg_mod, we can reference specific elements from that model, such as the regression coefficients. Were going to do so to build our regression model equation as an R formula. Lets begin by referencing the model intercept value and assigning it to an object called b0. Im calling this object b0 because conventionally the b or B notation is used to signify regression coefficients and because the 0 (zero) is often used as a subscript to signify our intercept value in a regression model equation. To pull or reference the intercept value from our regression model, we can specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients. If we were to run this by itself, it would print all of the regression coefficient values estimated for the model; we, however, want just the intercept value. Given that, immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact text: \"(Intercept)\". This will call up the intercept coefficient for any model estimated using the Regression function from lessR. # Reference estimated model intercept and assign to object b0 &lt;- reg_mod$coefficients[&quot;(Intercept)&quot;] Next, lets pull the regression coefficients associated with the two predictor variables: SJT and Interview. Lets assign these regression coefficients to objects called b1 and b2, respectively, which adhere to conventional notation. To reference the coefficient associated with SJT, we will specify the name of our regression model object (reg_mod) followed by the $ operator and coefficients  just as we did above with the model intercept. Immediately following coefficients, include brackets ([ ]). Within the brackets, include the exact name of the predictor variable associated with the coefficient you wish to reference (within quotation marks): \"SJT\". # Reference estimated model regression coefficient for SJT and assign to object b1 &lt;- reg_mod$coefficients[&quot;SJT&quot;] Lets repeat the process for the Interview variable, except name this object b2. # Reference estimated model regression coefficient for Interview and assign to object b2 &lt;- reg_mod$coefficients[&quot;Interview&quot;] Now that weve pulled the coefficients and assigned them to objects (b0, b1, b2) that we can reference in our regression model equation, lets read in new data so that we can reference new applicants scores on the same conscientiousness selection tool. As we did above in the Initial Steps section, lets read in the data file called NewApplicants.csv and assign it to a new object. Here I call this new object new_df. # Read in data new_df &lt;- read.csv(&quot;NewApplicants.csv&quot;) Lets take a peek at first six rows of the new_df data frame object. # Print first six rows head(new_df) ## ApplicantID SJT Interview ## 1 AA1 6 9 ## 2 AA2 2 7 ## 3 AA3 6 3 ## 4 AA4 10 5 ## 5 AA5 7 13 ## 6 AA6 4 10 Note that the data frame object includes an ApplicantID unique identifier variable as well as variables associated with two selection tools: SJT and Interview. Note that we dont have any criterion (Performance) scores here because these folks are still applicants. Our goal then is to predict their future criterion scores using our regression model from above. To make criterion-score predictions based on the new applicant data for the SJT and Interview selection tools, well need to create a regression equation based on estimates from the multiple linear regression model. Fortunately, weve already created objects corresponding to our model intercept (b0), SJT coefficient (b1), and Interview coefficient (b2). Using these values, well specify a linear equation (e.g., Y = b0 + b1 * X + b2 * W). In our model equation, well start by specifying a new variable containing what will be our vector of criterion-score predictions based on the SJT and Interview selection tool variables. Im going to call this new variable Perf_Predict, as hopefully that name signals that the variable contains performance predictions. Using the $ operator, we can attach this new variable to the new data frame object we read in called new_df. To the right of the &lt;- operator, well specify the rest of the equation; in this context, you can think of the &lt;- operator as being the equal sign in our equation; in fact, you could replace the &lt;- operator with = if you wanted to. First, type in the object associated with model intercept (b0) followed by the + operator. Second, type in the object associated with the SJT coefficient (b1), and follow that with the multiplication operator (*) so that we can multiple the coefficient by the new values for the selection tool called SJT; after the * operator, type in the name of the corresponding selection tool variable from the new data frame object: new_df$SJT; follow this with the + operator. Third, type in the object associated with the Interview coefficient (b2), and follow that with the multiplication operator (*) so that we can multiple the coefficient by the new values for the selection tool called Interview; after the * operator, type in the name of the corresponding selection tool variable from the new data frame object: new_df$Interview. # Assemble regression equation and assign to new variable in second data frame new_df$Perf_Predict &lt;- b0 + b1 * new_df$SJT + b2 * new_df$Interview Lets take a look at the new_df data frame object to verify that the new Perf_Predict variable (containing the predicted criterion scores) was added successfully. # View data frame object View(new_df) In the viewer tab, we can use the up and down arrows to sort by variable scores. Alternatively and optionally, we can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict_Consc from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 2 AA2 2 7 9.358599 ## 7 AA7 2 8 9.743899 ## 3 AA3 6 3 10.087326 ## 10 AA10 8 2 10.836989 ## 8 AA8 2 11 10.899798 ## 15 AA15 6 6 11.243225 ## 6 AA6 4 10 11.649461 ## 11 AA11 5 10 12.216943 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 4 AA4 10 5 13.127851 ## 5 AA5 7 13 14.507805 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 14 AA14 6 15 14.710923 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. when making selection decisions using a compensatory approach (like we are doing in this tutorial), it often makes most sense to sort in descending order. # Sort data frame by new variable in default descending order new_df[order(-new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 14 AA14 6 15 14.710923 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 5 AA5 7 13 14.507805 ## 4 AA4 10 5 13.127851 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 11 AA11 5 10 12.216943 ## 6 AA6 4 10 11.649461 ## 15 AA15 6 6 11.243225 ## 8 AA8 2 11 10.899798 ## 10 AA10 8 2 10.836989 ## 3 AA3 6 3 10.087326 ## 7 AA7 2 8 9.743899 ## 2 AA2 2 7 9.358599 As you can see, applicants AA14, AA9, and AA12 have the highest predicted criterion scores, where the criterion in this context is future job performance. If we had three open positions in need of filling, using a compensatory approach, we could extend offers first to these three applicants. If any one of them were to decline, we could start working our way down the list. In some instances, we might set a floor for the lowest acceptable predicted score and not consider any applicants with scores below that threshold. Alternatively, we could use a banding approach to identify groups of applicants for which we treat their predicted criterion scores as essentially equivalent (Mueller et al. 2007). One potential drawback to using a compensatory approach is that all applicants needs need to participate in all selection tools used in the multiple linear regression model, which can add expense and time. 40.2.6 Summary In this tutorial, we learned how to estimate a multiple linear regression model using the Regression function from the lessR package in order to apply a compensatory approach to selection decisions. 40.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm and predict functions from base R to estimate a multiple linear regression model and predict future criterion scores. Because this function comes from base R, we do not need to install and access an additional package. 40.3.1 Functions &amp; Packages Introduced Function Package lm base R summary base R predict base R order car 40.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;ConcurrentValidation.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## SJT = col_double(), ## EI = col_double(), ## Interview = col_double(), ## Performance = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df) ## [1] &quot;EmployeeID&quot; &quot;SJT&quot; &quot;EI&quot; &quot;Interview&quot; &quot;Performance&quot; # View variable type for each variable in data frame str(df) ## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ... ## $ SJT : num [1:300] 9 8 7 6 6 5 5 4 3 8 ... ## $ EI : num [1:300] 8 6 6 5 5 5 4 2 2 7 ... ## $ Interview : num [1:300] 2 3 4 5 6 7 7 8 9 2 ... ## $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. SJT = col_double(), ## .. EI = col_double(), ## .. Interview = col_double(), ## .. Performance = col_double() ## .. ) # View first 6 rows of data frame head(df) ## # A tibble: 6 x 5 ## EmployeeID SJT EI Interview Performance ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EE23 9 8 2 22 ## 2 EE24 8 6 3 11 ## 3 EE25 7 6 4 5 ## 4 EE26 6 5 5 11 ## 5 EE27 6 5 6 12 ## 6 EE28 5 5 7 12 40.3.3 lm &amp; predict Functions from Base R In the following section, we will learn how to apply the lm and predict functions from base R to estimate a multiple linear regression model and predict future criterion scores. As a critical first step, we must specify the regression model using the lm function. To use the lm (linear model) function, create a name for your regression model (reg.mod1) using the &lt;- symbol. Next, type the name of the lm function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (Performance) to the left of the ~ operator and the name of the predictor variables (SJT, Interview) to the right of the ~ operator. We are telling the function to regress Performance on SJT and Interview. As the second argument, type data= followed by the name of the data frame object to which the variables in your model belong (df). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results. # Specify multiple linear regression model reg.mod &lt;- lm(Performance ~ SJT + Interview, data=df) To view the model estimation results, well use the summary function and specify the name of our regression model object as the sole parenthetical argument. # Print summary of multiple linear regression model results summary(reg.mod) ## ## Call: ## lm(formula = Performance ~ SJT + Interview, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8249 -1.5198 -0.0401 1.0396 13.4868 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.52654 0.53925 10.249 &lt; 0.0000000000000002 *** ## SJT 0.56748 0.08453 6.714 0.0000000000964 *** ## Interview 0.38530 0.06388 6.031 0.0000000048298 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.992 on 297 degrees of freedom ## Multiple R-squared: 0.2643, Adjusted R-squared: 0.2593 ## F-statistic: 53.34 on 2 and 297 DF, p-value: &lt; 0.00000000000000022 Given that the output for this model has already been reviewed in detail in the chapter on estimating the incremental validity of a selection tool using multiple linear regression, well focus only on the information that is relevant for applying a compensatory model  namely the regression coefficients. For the estimated model, we find that both selection tools (i.e., predictor variables) show evidence of incremental validity; that is, both selection tools are significantly associated with the criterion of job performance when statistically controlling for the effects of each other. Specifically, the association between SJT and Performance (when controlling for Interview) is positive and statistically significant (b = .567, p &lt; .001), and the association between Interview and Performance (when controlling for SJT) is also positive and statistically significant (b = .385, p &lt; .001). If you recall from the chapter on predicting criterion scores on using simple linear regression, the intercept and predictor variable regression coefficients can be used to construct an equation, and this equation serves as the basis for making criterion-score predictions. In fact, when an equation comes from estimated multiple linear regression coefficients, the equation can serve as the basis for applying a compensatory approach to selection decisions. Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows: \\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\\) Lets assume, for example, that a future applicant scores 5 points on the SJT and 7 points on the Interview. If we plug those values into our equation, we get a predictor criterion score (i.e., Performance score) of 11.057: \\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\\) Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable Performance. Next, well bring this process to life in order to apply a compensatory approach to selection decisions. The predict function from base R pairs nicely with the lm function, and it makes it easier to take an estimated regression model  and associated regression equation  and apply that model to new (fresh) predictor variable data. Before applying the predict function, lets read in the data file called NewApplicants.csv and assign it to a new object. Here I call this new object new_df. Lets pretend this new data frame object contains data from future applicants who have completed the SJT and Interview tools as part of the organizations selection process. We will ultimately plug these applicants SJT and Interview scores into our multiple linear regression model equation to predict the applicants scores on the criterion variable called Performance. # Read in data new_df &lt;- read.csv(&quot;NewApplicants.csv&quot;) Next, as the first argument in the predict function, type object= followed by the estimated model object that we named reg.mod above. As the second argument, type newdata= followed by the name of the data frame object that contains new data on the predictor variable. Its important to make sure that the predictor variables names (SJT, Interview) in our new data are exactly the same as the predictor variables names (SJT, Interview) in our original data that we used to estimate the model. If that were not the case, we would want to rename the predictor variable in the new data frame object to match the corresponding name in the original data frame object, which is covered in the chapter on adding/removing variable names and renaming specific variables. # Predict scores on the criterion (outcome) variable predict(object=reg.mod, newdata=new_df) ## 1 2 3 4 5 6 7 8 9 10 ## 12.399124 9.358599 10.087326 13.127851 14.507805 11.649461 9.743899 10.899798 14.689987 10.836989 ## 11 12 13 14 15 ## 12.216943 14.689987 12.399124 14.710923 11.243225 In your console, you should see a vector of scores  these are the predicted criterion scores. In many cases, we might want to append this vector as a variable to our new data frame object. To do so, we just need to apply the &lt;- operator and, to the left of it, specify the name of the new data frame object (new_df) followed by the $ operator and a name for the new variable that will contain the predicted criterion scores (Perf_Predict). # Predict scores on the criterion (outcome) variable # and append as new variable in data frame new_df$Perf_Predict &lt;- predict(object=reg.mod, newdata=new_df) We can use the order function from base R and bracket ([ ]) notation to sort in ascending order (by default). Simply type the name of the data frame object (new_df), followed by brackets ([ ]). Within the brackets type the name of the order function, and within the function parentheses by the name of the variable(s) we wish to sort by, which is Perf_Predict from the new_df data frame in this example. After the function parentheses (( )) and still within the brackets, type a comma (,), as anything within brackets that comes to the left of the comma indicates that columns (not rows) are being referenced. # Sort data frame by new variable in default ascending order new_df[order(new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 2 AA2 2 7 9.358599 ## 7 AA7 2 8 9.743899 ## 3 AA3 6 3 10.087326 ## 10 AA10 8 2 10.836989 ## 8 AA8 2 11 10.899798 ## 15 AA15 6 6 11.243225 ## 6 AA6 4 10 11.649461 ## 11 AA11 5 10 12.216943 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 4 AA4 10 5 13.127851 ## 5 AA5 7 13 14.507805 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 14 AA14 6 15 14.710923 Conversely, we can sort in descending order by typing a minus (-) before the variable(s) we wish to sort by. when making selection decisions using a compensatory approach (like we are doing in this tutorial), it often makes most sense to sort in descending order. # Sort data frame by new variable in default descending order new_df[order(-new_df$Perf_Predict), ] ## ApplicantID SJT Interview Perf_Predict ## 14 AA14 6 15 14.710923 ## 9 AA9 8 12 14.689987 ## 12 AA12 8 12 14.689987 ## 5 AA5 7 13 14.507805 ## 4 AA4 10 5 13.127851 ## 1 AA1 6 9 12.399124 ## 13 AA13 6 9 12.399124 ## 11 AA11 5 10 12.216943 ## 6 AA6 4 10 11.649461 ## 15 AA15 6 6 11.243225 ## 8 AA8 2 11 10.899798 ## 10 AA10 8 2 10.836989 ## 3 AA3 6 3 10.087326 ## 7 AA7 2 8 9.743899 ## 2 AA2 2 7 9.358599 As you can see, applicants AA14, AA9, and AA12 have the highest predicted criterion scores, where the criterion in this context is future job performance. If we had three open positions in need of filling, using a compensatory approach, we could extend offers first to these three applicants. If any one of them were to decline, we could start working our way down the list. In some instances, we might set a floor for the lowest acceptable predicted score and not consider any applicants with scores below that threshold. Alternatively, we could use a banding approach to identify groups of applicants for which we treat their predicted criterion scores as essentially equivalent (Mueller et al. 2007). One potential drawback to using a compensatory approach is that all applicants needs need to participate in all selection tools used in the multiple linear regression model, which can add expense and time. "],["multiplecutoff.html", "Chapter 41 Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method 41.1 Conceptual Overview 41.2 Tutorial", " Chapter 41 Applying a Noncompensatory Approach to Selection Decisions Using Angoff Method In this chapter, we will learn how to apply a noncompensatory approach to making selection decisions by using the Angoff Method. Well begin with a conceptual overview of the noncompensatory approach and Angoff Method, and well conclude with a tutorial. 41.1 Conceptual Overview In general, there are three overarching (and mutually non-exclusive) approaches to making selection decisions: (a) compensatory (e.g., multiple linear regression), (b) noncompensatory (e.g., multiple-cutoff), and (c) multiple-hurdle. These three approaches can be mixed and matched to fit the selection-decision needs of an organization. In the previous chapter, we focused a compensatory approach using multiple linear regression, and in this chapter, we will focus on the multiple-cutoff noncompensatory approach using the Angoff Method. 41.1.1 Review of Noncompensatory Approach A noncompensatory approach to making selection decisions involves the application of multiple cutoff scores, where cutoff scores are sometimes referred to as critical scores or just cut scores. A noncompensatory approach signals that an applicants score on one selection tool cannot compensate for their score on another selection tool. A common noncompensatory approach involves setting a separate cutoff score for each selection tool. Naive, judgmental, and empirical methods exist for setting cutoff scores (Mueller et al. 2007). Of the judgmental methods, the Angoff Method (Angoff 1971) is one of the more well-known. Briefly, this method requires that a set of subject matter experts (SMEs) estimate the probability that a minimally qualified applicant would respond correctly to an item (e.g., question) from a selection tool (e.g., assessment, test). Assuming a multi-item selection tool, the SMEs probability estimates are then averaged for each item, and the cutoff score is then computed by calculating the sum of the average SME probability estimates across items. Conceptually, the resulting cutoff score is thought to represent the mean of the distribution of selection tool overall scores. Only relatively simple arithmetic (e.g., mean, sum) is needed when applying the Angoff Method for two or more selection tools as part of a multiple-cutoff approach to making selection decisions. That being said, sometimes it helps to practice prepping the data and applying such arithmetic along with logical expressions to construct a simple algorithm. 41.2 Tutorial This chapters tutorial demonstrates how to apply a noncompensatory approach to making selection decisions by using the Angoff Method. 41.2.1 Functions &amp; Packages Introduced Function Package colMeans base R sum base R c base R rowSums base R ifelse base R is.na base R 41.2.2 Initial Steps If you havent already, save the file called angoff_sme.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called angoff_sme.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df1 &lt;- read_csv(&quot;angoff_sme.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## SME = col_double(), ## CogAb_i1 = col_double(), ## CogAb_i2 = col_double(), ## CogAb_i3 = col_double(), ## CogAb_i4 = col_double(), ## CogAb_i5 = col_double(), ## KnowTest_i1 = col_double(), ## KnowTest_i2 = col_double(), ## KnowTest_i3 = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(df1) ## [1] &quot;SME&quot; &quot;CogAb_i1&quot; &quot;CogAb_i2&quot; &quot;CogAb_i3&quot; &quot;CogAb_i4&quot; &quot;CogAb_i5&quot; &quot;KnowTest_i1&quot; ## [8] &quot;KnowTest_i2&quot; &quot;KnowTest_i3&quot; # View variable type for each variable in data frame str(df1) ## spec_tbl_df[,9] [5 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ SME : num [1:5] 1 2 3 4 5 ## $ CogAb_i1 : num [1:5] 0.78 0.85 0.76 0.75 0.81 ## $ CogAb_i2 : num [1:5] 0.72 0.72 0.7 0.69 0.69 ## $ CogAb_i3 : num [1:5] 0.67 0.6 0.59 0.59 0.63 ## $ CogAb_i4 : num [1:5] 0.53 0.61 0.52 0.56 0.59 ## $ CogAb_i5 : num [1:5] 0.44 0.46 0.49 0.47 0.45 ## $ KnowTest_i1: num [1:5] 0.69 0.8 0.69 0.67 0.75 ## $ KnowTest_i2: num [1:5] 0.7 0.72 0.69 0.69 0.79 ## $ KnowTest_i3: num [1:5] 0.72 0.76 0.78 0.77 0.73 ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. SME = col_double(), ## .. CogAb_i1 = col_double(), ## .. CogAb_i2 = col_double(), ## .. CogAb_i3 = col_double(), ## .. CogAb_i4 = col_double(), ## .. CogAb_i5 = col_double(), ## .. KnowTest_i1 = col_double(), ## .. KnowTest_i2 = col_double(), ## .. KnowTest_i3 = col_double() ## .. ) # View first 6 rows of data frame head(df1) ## # A tibble: 5 x 9 ## SME CogAb_i1 CogAb_i2 CogAb_i3 CogAb_i4 CogAb_i5 KnowTest_i1 KnowTest_i2 KnowTest_i3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.78 0.72 0.67 0.53 0.44 0.69 0.7 0.72 ## 2 2 0.85 0.72 0.6 0.61 0.46 0.8 0.72 0.76 ## 3 3 0.76 0.7 0.59 0.52 0.49 0.69 0.69 0.78 ## 4 4 0.75 0.69 0.59 0.56 0.47 0.67 0.69 0.77 ## 5 5 0.81 0.69 0.63 0.59 0.45 0.75 0.79 0.73 The data frame contains 9 variables and 5 observations (i.e., SMEs). The SME variable contains unique identifiers for the subject matter experts (SMEs) who estimated the probability (for each item) that a minimally qualified applicant would answer correctly (i.e., Angoff Method). The variables labeled CogAb_i1 through CogAb_i5 correspond to five cognitive ability test items that were designed to have increasing levels of difficulty, with the first item (CogAb_i1) being the easiest and the fifth item (CogAb_i5) being the most difficult. The variables labeled KnowTest_i1 through KnowTest_i3 correspond to three knowledge test items. 41.2.3 Create Cutoff Scores To create cutoff scores for the cognitive ability and knowledge tests, well nest the colMeans function within the sum function, where both functions are from base R. Lets begin by creating a cutoff score for the cognitive ability test based on its five items: CogAb_i1, CogAb_i2, CogAb_i3, CogAb_i4, and CogAb_i5. Specify a unique name for an object that we can subsequently assign the cognitive ability test cutoff score to. Below, I name this object CogAb_cutoff, but you could name it whatever makes sense to you. To the right of the object name you specified, type the &lt;- operator, which will allow us to assign to the object the value that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the sum function. The sum function computes the sum for a vector of scores. As the sole argument within the sum function specify the colMeans function. The colMeans function computes the means for all columns within a data frame  or a subset of columns if we reference specific variables. As the sole argument in the colMeans function, well use bracket (i.e., matrix) notation to specify the data frame name and the specific variables we wish to estimate the column means for. First, type the name of the data frame object (df1), and follow that object name up with brackets ([ ]). Using bracket notation, we can reference columns by typing a comma (,), followed by a vector containing the columns (i.e., variables) we wish to reference. To specify that vector of column (variable) names, we can use the c function from base R, and within that function, specify each variables name in quotation marks (\" \"), and separate variable names using commas. # Create cutoff score for cognitive ability test CogAb_cutoff &lt;- sum( colMeans( df1[, c(&quot;CogAb_i1&quot;,&quot;CogAb_i2&quot;,&quot;CogAb_i3&quot;,&quot;CogAb_i4&quot;,&quot;CogAb_i5&quot;)] ) ) Next, lets apply the same process as above to the three knowledge test items in order to create a cutoff score for the knowledge test: KnowTest_i1, KnowTest_i2, and KnowTest_i3. # Create cutoff score for knowledge test KnowTest_cutoff &lt;- sum( colMeans( df1[, c(&quot;KnowTest_i1&quot;,&quot;KnowTest_i2&quot;,&quot;KnowTest_i3&quot;)] ) ) 41.2.4 Apply Cutoff Scores to Make Selection Decisions After creating the cutoff scores for our two selection tools, were ready to apply these cutoff scores to data collected from applicants, which will result in an algorithm of sorts. The name of the data file containing the applicant data on the cognitive ability and knowledge tests is angoff_applicant.csv. Below, I read in the data as a data frame and assign the data frame to an object that Im calling df2. # Read in data df2 &lt;- read.csv(&quot;angoff_applicant.csv&quot;) Lets print this data frame to our Console window by using the print function from base R. # Print data frame object print(df2) ## Applicant_ID CogAb_i1 CogAb_i2 CogAb_i3 CogAb_i4 CogAb_i5 KnowTest_i1 KnowTest_i2 KnowTest_i3 ## 1 AA1 0 0 1 0 0 0 0 1 ## 2 AA2 1 1 1 1 0 1 1 1 ## 3 AA3 1 1 0 0 0 1 0 1 ## 4 AA4 1 0 0 1 0 0 0 0 ## 5 AA5 1 0 1 0 1 1 1 0 ## 6 AA6 1 1 1 0 0 0 1 1 ## 7 AA7 0 0 0 0 0 0 1 1 ## 8 AA8 1 1 0 1 0 0 0 1 ## 9 AA9 0 1 1 0 0 1 1 1 ## 10 AA10 1 1 1 1 1 1 1 1 ## 11 AA11 1 0 0 0 0 1 0 1 ## 12 AA12 1 0 1 0 0 1 0 0 ## 13 AA13 1 1 1 0 1 1 1 1 ## 14 AA14 1 1 0 0 0 0 0 0 ## 15 AA15 1 0 1 0 0 0 1 1 ## 16 AA16 1 1 0 0 1 1 0 1 ## 17 AA17 1 1 0 1 0 1 1 1 ## 18 AA18 1 1 0 0 1 1 0 1 ## 19 AA19 1 0 0 0 0 1 0 1 ## 20 AA20 0 1 0 1 0 1 1 0 The data frame contains 9 variables and 20 observations (i.e., applicants). The Applicant_ID variable contains unique identifiers for the applicants who completed each of the items for the cognitive ability and knowledge tests. The variables labeled CogAb_i1 through CogAb_i5 correspond to five cognitive ability test items that were designed to have increasing levels of difficulty, with the first item (CogAb_i1) being the easiest and the fifth item (CogAb_i5) being the most difficult. The variables labeled KnowTest_i1 through KnowTest_i3 correspond to three knowledge test items. Each of the items consist of scores of 1 and 0, where 1 indicates that an applicant answered the item correctly and 0 indicates that an applicant answered the item incorrectly. Using the applicant data (df2), we need to compute the overall score for each applicant on each of the two selection tools. Lets begin with the cognitive ability test items: CogAb_i1, CogAb_i2, CogAb_i3, CogAb_i4, and CogAb_i5. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the cognitive ability test overall scores. Because were going to compute an overall score based on the sum of the number of items each applicant answered correctly, I decided to name this variable CogAb_sum. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the rowSums function from base R. The rowSums function computes the sum for each row. As the first argument in the rowSums function, well use bracket (i.e., matrix) notation to specify the data frame name and the specific variables we wish to estimate the column means for. First, type the name of the data frame object (df2), and follow that object name up with brackets ([ ]). Using bracket notation, we can reference columns by typing a comma (,), followed by a vector containing the columns (i.e., variables) we wish to reference. To specify that vector of column (variable) names, we can use the c function from base R, and within that function, specify each variables name in quotation marks (\" \"), and separate variable names using commas. As the second argument in the rowSums function, type the argument na.rm=TRUE, which will tell the function to ignore missing data when computing the sum for each row. # Compute the overall (sum) cognitive ability test score for each applicant df2$CogAb_sum &lt;- rowSums( df2[,c(&quot;CogAb_i1&quot;,&quot;CogAb_i2&quot;,&quot;CogAb_i3&quot;,&quot;CogAb_i4&quot;,&quot;CogAb_i5&quot;)], na.rm=TRUE) Next, repeat the same process for the three knowledge test items: KnowTest_i1, KnowTest_i2, and KnowTest_i3. # Compute the overall (sum) knowledge test score for each applicant df2$KnowTest_sum &lt;- rowSums( df2[,c(&quot;KnowTest_i1&quot;,&quot;KnowTest_i2&quot;,&quot;KnowTest_i3&quot;)], na.rm=TRUE) Were now ready to apply the cutoff scores to applicants scores on the two selection tools. Lets begin by applying the cutoff score for the cognitive ability test to the applicants overall scores on the cognitive ability test. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the cognitive ability test pass vs. fail scores (CogAb_pass). To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the ifelse function from base R. The ifelse function allows us to apply if/else logical arguments to a vector of values (such as a variable). As the first argument in the ifelse function, specify a logical argument that applies the cutoff score to the applicants overall scores for the cognitive ability test. Specifically, we want to make a logical expression in which the cognitive ability overall scale variable scores are greater than or equal to the cutoff score object we created previously: df2$CogAb_sum &gt;= CogAb_cutoff. As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicants overall cognitive ability test score is greater than or equal to the cutoff score, then lets type \"Pass\" as the resulting value. As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be false for a particular applicant. If an applicants overall cognitive ability test score is not greater than or equal to the cutoff score, then lets type \"Fail\" as the resulting value. # Create variable containing cognitive ability test pass/fail scores df2$CogAb_pass &lt;- ifelse( df2$CogAb_sum &gt;= CogAb_cutoff, &quot;Pass&quot;, &quot;Fail&quot; ) Next, lets apply the same process to the knowledge test overall scores. # Create variable containing knowledge test pass/fail scores df2$KnowTest_pass &lt;- ifelse( df2$KnowTest_sum &gt;= KnowTest_cutoff, &quot;Pass&quot;, &quot;Fail&quot; ) We are now ready to start making some overall selection decisions using whats referred to as a multiple-cutoff approach. Our goal is to create a new variable that indicates which applicants passed both selection tools based on the cutoff scores we previously applied. Type the name of the second data frame object containing the applicant data (df2). Next, insert the $ operator and then specify a unique name for a variable to which we can subsequently assign the overall pass vs. fail scores (Overall_pass) based on our multiple-cutoff approach. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the operations we will type to the right of the &lt;- operator (in the subsequent steps). Type the name of the ifelse function from base R. As the first argument in the ifelse function, specify a logical argument with a logical &amp; (AND) operator to specify that in order for an applicant to pass both selection tools, they need to earn passing scores on both. Specifically, we want to make a logical expression in which applicants scores on pass/fail variables we created above are used to flag those individuals who earned a score of Pass on both: df2$CogAb_pass == \"Pass\" &amp; df2$KnowTest_pass == \"Pass\". As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicant satisfies both of those logical expressions in the previous argument, lets assign them a \"Pass\" score. As the second argument in the ifelse function, provide a value that will be generated should one or both of the logical expressions in the first argument be false for a particular applicant. If an applicant does not satisfy both of those logical expressions in the first argument, lets assign them a \"Fail\" score. # Create variable containing overall pass/fail scores based on both tools df2$Overall_pass &lt;- ifelse( df2$CogAb_pass == &quot;Pass&quot; &amp; df2$KnowTest_pass == &quot;Pass&quot;, &quot;Pass&quot;, &quot;Fail&quot; ) Using the ifelse function once more, lets create a vector containing the applicant unique identifier numbers for those who passed the multiple-cutoff selection process and NA (missing) for everyone else who failed. Come up with a unique name for an object to which we can assign the vector of applicant unique ID numbers and NAs so that we can reference it later. Here, I name the object Applicant_ID_pass. To the right of the new variable name, type the &lt;- operator, which will allow us to assign to the object the vector that results from the ifelse function. Type the name of the ifelse function from base R. As the first argument in the ifelse function, specify a logical argument in which scores on the Overall_pass variable we created above is equal to the value Pass: df2$Overall_pass == \"Pass\". As the second argument in the ifelse function, provide a value that will be generated should the logical expression in the first argument be true for a particular applicant. If an applicant satisfies both of those logical expressions in the previous argument, lets reference their applicant unique identifier number (Applicant_ID) from the applicant data frame object. As the second argument in the ifelse function, simply enter NA. Should the logical expression in the first argument be false, then the applicant will receive a missing value. # Create a vector of the applicant unique identifiers for those who passed # and NAs for those who failed the multiple-cutoff selection process Applicant_ID_pass &lt;- ifelse( df2$Overall_pass == &quot;Pass&quot;, df2$Applicant_ID, NA) Finally, from the Applicant_ID_pass vector object we created above, lets drop all NA values. Type name of the vector we created above that contains the applicant unique identifier numbers for those who passed the multiple-cutoff selection process and NAs for those who failed (Applicant_ID_pass). Following the name of the Applicant_ID_pass vector object, insert brackets ([ ]). Within the brackets ([ ]), type the not (!) operator followed by the is.na function from base R. The is.na function returns a TRUE if a value is NA and a FALSE if the value is not NA. By preceding that function with the not (!) operator, we can flip that logic, such that those with an NA will effectively receive a FALSE value. As the sole parenthetical argument within the is.na function, type the exact name of the same vector object from the first step (Applicant_ID_pass). # Retain only the applicant unique identifer numbers for those who passed # the multiple-cutoff selection process Applicant_ID_pass[!is.na(Applicant_ID_pass)] ## [1] &quot;AA2&quot; &quot;AA10&quot; &quot;AA13&quot; In your Console, you should see three applicant unique identifier numbers: AA2, AA10, and AA13. These are the applicants who passed the multiple-cutoff selection process. Depending on the overall design of the selection system, these are the three applicants who should be given either job offers or passed along to the next phase of the selection system (e.g., interview). 41.2.5 Summary In this chapter, we learned how to apply common functions from base R to make selection decisions from a multiple-cutoff selection process that uses the Angoff Method, where the application of multiple cutoffs is a noncompensatory approach. "],["differentialprediction.html", "Chapter 42 Testing for Differential Prediction Using Moderated Multiple Linear Regression 42.1 Conceptual Overview 42.2 Tutorial 42.3 Chapter Supplement", " Chapter 42 Testing for Differential Prediction Using Moderated Multiple Linear Regression In this chapter, we will learn how to apply test whether a selection tool shows evidence of differential prediction by using moderating multiple linear regression. Well begin with conceptual overviews of the moderated multiple linear regression and differential prediction, and well conclude with a tutorial. 42.1 Conceptual Overview In this chapter, we will learn how to use an ordinary least squares (OLS) moderated multiple linear regression model to test for differential prediction of an employee selection tool due to protected class membership. Thus, the way in which we build and interpret our moderated multiple regression model will be couched in the context of differential prediction. Nonetheless, this tutorial should still provide you with the technical and interpretation skills to understand how to apply moderated multiple linear regression in other non-selection contexts. 42.1.1 Review of Moderated Multiple Linear Regression Moderated multiple linear regression (MMLR) is a specific type of multiple linear regression, which means that MMLR involves multiple predictor variables and a single outcome variable. For more information on multiple linear regression, check out the chapter on estimating incremental validity. In MMLR, we are in effect determining whether the association between a predictor variable and the outcome variable is conditional upon the level/value of another predictor variable, the latter of which we often refer to as a moderator or moderator variable. Moderators: A moderator (i.e., moderator variable) is a predictor variable upon which an association between two other variables is conditional. A moderator variable can be continuous (e.g., age: measured in years) or categorical (e.g., location: Portland vs. Beaverton). When we find that one variable moderates the association between two other variables, we often refer to this as an interaction or interaction effect. For example, lets imagine that we are interested in the association between job satisfaction and job performance in our organization. By introducing the categorical moderator variable of employee location (Portland vs. Beaverton), we can determine whether the magnitude and/or sign of the association between job satisfaction and job performance varies by the location at which employees work. Perhaps we find that the association between job satisfaction and job performance is significantly stronger for employees who work in Beaverton as compared to employees who work in Portland (see figure below), and thus we would conclude that there is a significant interaction between job satisfaction and location in relation to job performance. This line and scatter plot illustrates pictorially how a variable like employee location might moderate the association between job satisfaction and job performance. When investigating whether an interaction exists between two variables relative to the outcome variable, we need to create a new variable called an interaction term (i.e., product term). An interaction term is created by multiplying the scores of two predictor variables  or in other words, the computing the product of two variables respective scores. In fact, the presence of an in interaction term  and associated multiplication  signals why MMLR is referred to as a multiplicative model. In terms of the language we use to describe an interaction and the associated interaction term, we often refer to one of the predictor variables involved in an interaction as the moderator. For an illustration of how an interaction term is created, please refer to the table below. To create an interaction term (i.e., product term), we simply multiply the associated predictor variables scores (e.g., predictor variable and moderator variable) for each case (e.g., employee). Centering Predictor &amp; Moderator Variables: To improve the interpretability of the main effects (but not the interaction term) and to reduce collinearity between the predictor variables involved in the interaction term, we typically grand-mean center any continuous (interval, ratio) predictor variables; we do not need to center the outcome variable or a predictor or moderator variable that is categorical (e.g., dichotomous). As discussed in the chapter on centering and standardizing variables, grand-mean centering refers to the process of subtracting the overall sample mean for a variable from each cases score on that variable. We center the predictor variables (i.e., predictor and moderator variables) before computing the interaction term based on those variables. To illustrate how grand-mean centering works, take a look at the table below, and note that we are just subtracting the (grand) mean for the sample from the specific value on the predictor variable for each case. To grand-mean center a variable, we subtract the overall sample mean of the variable from each cases score on that variable. When we grand-mean center the predictor variables in a regression model, it also changes our interpretation of the \\(\\hat{Y}\\)-intercept value. If you recall from prior reviews of simple linear regression and multiple linear regression, the \\(\\hat{Y}\\)-intercept value is the mean of the outcome variable when each predictor variable(s) in the model is/are set to zero. Thus, when we center each predictor variable, we shift the mean of each variable to zero, which means that after grand-mean centering, the \\(\\hat{Y}\\)-intercept value is the mean of the outcome variable when each predictor variable is set to its mean. Beyond moderated multiple linear regression, grand-mean centering can be useful in simple and multiple linear regression models when interpreting the regression coefficients associated with the intercept and the slope(s), as in some cases, a score of zero on the predictor variable is theoretically meaningless, implausible, or both. For example, if one of our predictor variables has an interval measurement scale that ranges from 10-100, then a score of zero is not even part of the scale range, which would make shifting the zero value to the grand-mean a more meaningful value. Regardless of your decision to grand-mean center or to not grand-mean center the predictor variable and moderator variables, the interaction terms significance level (p-value) and model fit (i.e., R2) will remain the same. In fact, sometimes it makes more sense to plot the findings of your model based on the uncentered version of the variables. That choice is really up to you and should be based on the context at hand. Moderated Multiple Linear Regression: In moderated multiple linear regression (MMLR), we have three or more predictor variables and a single outcome variable, where one of the predictor variables is the interaction term (i.e., product term) between two of the predictor variables (e.g., predictor and moderator variables). Just like multiple linear regression, MMLR allows for a series of covariates (control variables) to be included in the model; accordingly, each regression coefficient (i.e., slope, weight) represents the relationship between the associated predictor and the outcome variable when statistically controlling for all other predictors in the model. To better understand what MMLR is and how it works, I find that its useful to consider a MMLR model in equation form. Specifically, lets consider a scenario in which we estimate a MMLR with two predictor variables, their interaction term (i.e., product term), and a single outcome variable. The equation for such a model with unstandardized regression coefficients (\\(b\\)) would be as follows: \\(\\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + b_{3}X_1*X_2 + e\\) where \\(\\hat{Y}\\) represents the predicted score on the outcome variable (\\(Y\\)), \\(b_{0}\\) represents the \\(\\hat{Y}\\)-intercept value (i.e., model constant) when the predictor variables \\(X_1\\) and \\(X_2\\) and interaction term \\(X_1*X_2\\) are equal to zero, \\(b_{1}\\), \\(b_{2}\\), and \\(b_{3}\\) represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables \\(X_1\\) and \\(X_2\\) and interaction term \\(X_1*X_2\\), respectively, and the outcome variable \\(\\hat{Y}\\), and \\(e\\) represents the (residual) error in prediction. If the regression coefficient associated with the interaction term (\\(b_{3}\\)) is found to be statistically significant, then we would conclude that there is evidence of a significant interaction effect. If we conceptualize one of the predictor variables as a moderator variable, then we might re-write the equation from above by swapping out \\(X_2\\) with \\(W\\) (or \\(Z\\)). The meaning of the equation remains the same, but changing the notation in this way may signal more clearly which variable we are labeling a moderator. \\(\\hat{Y} = b_{0} + b_{1}X + b_{2}W + b_{3}X*W + e\\) Unstandardized regression coefficient indicates the raw regression coefficient (i.e., weight, slope) when controlling for the associations between all other predictors in the model and the outcome variable; accordingly, each unstandardized regression coefficient represents how many unstandardized units of \\(\\hat{Y}\\) increase/decrease as a result of a single unit increase in a predictor variable, when statistically controlling for the effects of other predictors in the regression model. Typically, we are most interested in whether slope differences exist; with that being said, in the context of testing for differential prediction (described below), we are also interested in whether intercept differences exist. In essence, a significant interaction term indicates the presence of slope differences, where slopes differences refer to significant variation in the association (i.e., slope) between the predictor variable and outcome variable by levels/values of the moderator variable. In the context of our first MMLR equation (see above), if the regression coefficient (\\(b_{3}\\)) associated with the interaction term (\\(X_1*X_2\\)) is statistically significant, we conclude that there is significant interaction and thus significant slope differences. In fact, we might even us language like, \\(X_2\\) moderates the association between \\(X_1\\) and \\(Y\\). When we find a statistically significant interaction, it is customary to plot the simple slopes and estimate whether each of the simple slopes is significantly different from zero. Simple slopes allow us to probe the form of the significant interaction so that we can more accurately describe it. Plus, the follow-up simple slopes analysis helps us understand at which levels of the moderator variable the association between the predictor and outcome variables is statistically significant. For a continuous moderator variable, we typically plot and estimate the simple slopes at 1 standard deviation (SD) above and below the mean for the variable  and sometimes at the mean of the variable as well; with that being said, if we want, we can choose more theoretically or empirically meaningful values for the moderator variable. For a categorical variable, we plot and estimate the simple slopes based on the different discrete levels (categories) of the variable. For example, if our moderator variable captures gender identity operationalized as man (0) and woman (1), then we would plot the simple slopes when sex is equal to man (0) and when sex is equal to woman (1). 42.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a moderated multiple linear regression model are the same as those for conventional multiple linear regression include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of multivariate outliers; The association between the predictor and outcome variables is linear; There is no (multi)collinearity between predictor variables; Average residual error value is zero for all levels of the predictor variables; Variances of residual errors are equal for all levels of the predictor variables, which is referred to as the assumption of homoscedasticity; Residual errors are normally distributed for all levels of the predictor variables. The fourth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so lets take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple linear regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights  and even the signs  of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapters data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 42.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of the other predictor variable(s) and interaction term variable in the model. In other words, if a regression coefficients p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent when controlling for the effects of other predictor variables in the model. In contrast, if the regression coefficients p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of other predictor variable(s) and interaction term variable in the model. Put differently, if a regression coefficients p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable (or interaction term variable) and the outcome variable in the population  when controlling for the effects of other predictor variables and interaction term variable in the model. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. 42.1.1.3 Practical Significance As a reminder, an effect size is a standardized metric that can be compared across samples. In a moderated multiple linear regression (MMLR) model, an unstandardized regression coefficient (\\(b\\)) is not an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples. A standardized regression coefficient (\\(\\beta\\)) can be interpreted as an effect size (and thus an indicator of practical significance) given that it is standardized. With that being said, I suggest doing so with caution as collinearity (i.e., correlation) between predictor variables in the model can bias our interpretation of \\(\\beta\\) as an effect size. Thus, if your goal is just to understand the bivariate association between a predictor variable and an outcome variable (without introducing statistical control), then I recommend to just estimate a correlation coefficient as an indicator of practical significance, which I discuss in the chapter on estimating criterion-related validity using correlations. In a MMLR model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variables (i.e., R2). That is, in a multiple linear regression model, R2 represents the proportion of collective variance explained in the outcome variable by all of the predictor variables. Conceptually, we can think of the overlap between the variability in the predictor variables and and outcome variable as the variance explained (R2), and R2 is a way to evaluate how well a model fits the data (i.e., model fit). Ive found that the R2 is often readily interpretable by non-analytics audiences. For example, an R2 of .25 in a MMLR model can be interpreted as: the predictor variable and interaction term variable scores explain 25% of the variability in scores on the outcome variable. That is, to convert an R2 from a proportion to a percent, we just multiply by 100. R2 Description .01 Small .09 Medium .25 Large 42.1.2 Review of Differential Prediction Differential prediction refers to differences in intercepts, slopes, or both based on subgroup (projected class) membership (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968), and sometimes differential prediction is referred to as differential predictability or predictive bias. Assuming a single regression line is used to predict criterion (e.g., job performance) scores of applicants, differential prediction is concerned with overpredicting or underpredicting criterion scores for one group relative to another (Cascio and Aguinis 2005). For example, imagine a situation in which scores on a selection tool differentially predict scores on job performance for men and women, such that the slopes are the same, but the intercepts of each slope are different (see figure below). The dashed black line indicates what a single regression line would like if it were used. As you can see, if we used a single common regression line when intercept differences exist between men and women, then we would tend to underpredict job performance scores for men and overpredict for women. Thus, a common regression line would be inappropriate when evidence of intercept differences exists. Plus, in the U.S., the federal government requires that a common regression line is used as opposed to separate regression lines for each subgroup. This simple slopes plot illustrates intercept differences by group but not slope differences. The dashed line indicates what a single regression line would like if it were used. In the figure below, an example is provided in which the slopes (e.g., criterion-related validities) are different when the selection tool is used for men compared to women. Again, the dashed black line represents what would happen if a common regression line were used, which would clearly be inappropriate . Note that, in this particular example, the job performance scores would be generally underpredicted for men and overpredicted for women when using the sample regression. Further, the selection tool appears to show criterion-related validity (i.e., a significant slope) for men but not for women; accordingly, based on this sample, the selection tool would only be considered a valid predictor of job performance for men. Clearly this is a problem. This simple slopes plot illustrates slope differences by group (i.e., different criterion-related validities) but not intercept differences. The dashed line indicates what a single regression line would like if it were used. The figure below displays a situation in which there are intercept and slope differences. Again, use of a common regression line would pose problems in terms of predictive bias. In this example, for men, use of a common regression line would overpredict job performance scores for a certain range of selection tool scores and underpredict job performance scores for another range of selection tool scores; the opposite would be true for women. This simple slopes plot illustrates both intercept and slope differences by group. The dashed line indicates what a single regression line would like if it were used. Words of Caution: In this tutorial, we will learn how to apply MMLR in the service of detecting differential predication using an approach that is generally supported by the U.S. court system and researchers. With that said, using MMLR to detect differential prediction should be used cautiously with small samples, as failure to detect differential prediction that actually exists (i.e., Type II Error, false negative) could be the result of low statistical power, and relatedly, unreliability or bias in the selection tool or criterion and violation of certain statistical assumptions (e.g., homoscedasticity of residual error variances) can affect our ability to find differential prediction that actually exists (Aguinis and Pierce 1998). Further, when one subgroup is less than 30% of the total sample size, statistical power is also diminished (Stone-Romero, Alliger, and Aguinis 1994). The take-home message is to not necessarily assume that the absence of evidence is evidence of absence when it comes to differential prediction. Multistep MMLR Process for Differential Prediction: We will use a three-step step-up process for investigating differential prediction using MMLR (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). This process uses what is referred to as hierarchical regression, as additional predictor variables are added at each subsequent step, and their incremental validity above and beyond the other predictors in the model is assessed. In the first step, we will regress the criterion variable on the selection tool variable, and in this step, we will verify whether the selection tool shows criterion-related validity by itself. In the second step, we include the selection tool variable as before but add the dichotomous (binary) protected class/group variable to the model. If the protected class variable is statistically significant when controlling for the selection tool variable, then there is evidence of intercept differences. You might be asking yourself: What happens when a protected class variable is not inherently categorical or, specifically, dichotomous? When it comes to the protected class of race, many organizations have more than two races represented among their employees. In this situation, it is customary (at least within the U.S.) to compare two subgroups at time (e.g., White and Black) or to create a dichotomy consisting of the majority race and members of other non-majority races (e.g., White and Non-White). For a inherently continuous variable like age, classically, the standard procedure was to dichotomize the age variable by those who are 40 years and older and those who are younger than 40 years, as this is consistent with the U.S. Age Discrimination in Employment Act (ADEA) of 1967. There are two potential issues with this: (a) Some states have further age protections in place that protect workers under 40 as well, and (b) Some jobs in organizations have very young workforces, leaving the question of whether people in their 30s are being favored over people in their 20s, or vice versa (for example). In addition, when we look at organizations with workers who are well into their 50s, 60s, and 70s, then there might be evidence of age discrimination and differential prediction for those who are in their 50s compared to those who are in their 60s. Dichotomizing the age variable using a 40 and above vs. under 40 split might miss these issues and generally nuance. [For a review of this topic, see Fisher, Finkelstein, Truxillo, and Wallace, 2017.] As such, with a variable like age, there might be some advantages to keeping it as a continuous variable, as MMLR can handle this. In the third step, we include the selection tool and protected class variables as before but add the interaction term between the selection tool and protected class variables. If the interaction term is statistically significant, then there is evidence of differential prediction in the form of slope differences. 42.1.2.1 Sample Write-Up Our HR analytic team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction of the structured interview based on the U.S. protected class of gender. Based on a sample of 377 individuals, we applied a three-step process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of both intercept and slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .307, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, and found that the model intercept for women was significantly lower than the intercept for men (b = -.602, p &lt; .001). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between interview scores and job performance scores to a statistically significant extent (b = .284, p = .007). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant for women (b = .26, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .26 units; in contrast, the associated between structured interview scores and job performance scores for men was nonsignificant (b = -.02, p = .77). In sum, we found evidence of both intercept and slope differences, which means that this interview tool shows predictive bias with respect to gender, making the application of a common regression line (i.e., equation) inappropriate according to U.S. guidelines. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we recommend that the structured interview not be used for employee selection purposes. 42.2 Tutorial This chapters tutorial demonstrates how to use moderated multiple linear regression in the service of detecting differential prediction in a selection tool. 42.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/3n2vvMc4yHs 42.2.2 Functions &amp; Packages Introduced Function Package mean base R Regression lessR reg_brief lessR probe_interaction interactions 42.2.3 Initial Steps If you havent already, save the file called DiffPred.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called DiffPred.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object DiffPred &lt;- read_csv(&quot;DiffPred.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## emp_id = col_character(), ## perf_eval = col_double(), ## interview = col_double(), ## age = col_double(), ## gender = col_character(), ## race = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(DiffPred) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;age&quot; &quot;gender&quot; &quot;race&quot; # View variable type for each variable in data frame str(DiffPred) ## spec_tbl_df[,6] [377 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:377] &quot;MA322&quot; &quot;MA323&quot; &quot;MA324&quot; &quot;MA325&quot; ... ## $ perf_eval: num [1:377] 4.2 4.9 4.2 5.3 4.2 6.9 3.4 5.8 4.4 5.6 ... ## $ interview: num [1:377] 7.5 9.3 7.5 8 9.3 6.8 6.7 7 8.2 6.4 ... ## $ age : num [1:377] 29.7 31.7 29.4 37.9 30.9 46.2 43.9 47.8 31.7 44.6 ... ## $ gender : chr [1:377] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; ... ## $ race : chr [1:377] &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. age = col_double(), ## .. gender = col_character(), ## .. race = col_character() ## .. ) # View first 6 rows of data frame head(DiffPred) ## # A tibble: 6 x 6 ## emp_id perf_eval interview age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 MA322 4.2 7.5 29.7 woman asian ## 2 MA323 4.9 9.3 31.7 man asian ## 3 MA324 4.2 7.5 29.4 woman asian ## 4 MA325 5.3 8 37.9 woman asian ## 5 MA326 4.2 9.3 30.9 man black ## 6 MA327 6.9 6.8 46.2 woman asian There are 6 variables and 377 cases (i.e., employees) in the DiffPred data frame: emp_id, perf_eval, interview, age, gender, and race. Per the output of the str (structure) function above, the variables perf_eval, interview, and age are of type numeric (continuous: interval/ratio), and the variables emp_id, gender, and race are of type character (nominal/categorical). The variable emp_id is the unique employee identifier. Imagine that these data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered a rated structured interview (interview) 90 days after entering the organization. The structured interview (interview) variable was designed to assess individuals interpersonal skills, and ratings can range from 1 (very weak interpersonal skills) to 10 (very strong interpersonal skills). The interviews were scored by untrained raters who were often the hiring managers but not always. The perf_eval variable is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, with possible ratings ranging from 1-7, with 7 indicating high performance. The age variable represents the job incumbents ages (in years). The gender variable represents the job incumbents tender identify and is defined by two levels/categories/values: man and woman. Finally, the race variable represents the job incumbents race/ethnicity and is defined by three levels/categories/values: asian, black, and white. 42.2.4 Grand-Mean Center Continuous Predictor Variables To improve the interpretability of our model and to reduce the collinearity between the predictor and moderator variables with their interaction term, we will grand-mean center continuous predictor and moderator variables. [Note: A moderator variable is just a predictor variable that we are framing as a moderator.] With regard to interpretability, grand-mean centering aids in our interpretation of the regression coefficients associated with our predictor and moderator variables in our MMLR by reducing collinearity between these variables and their interaction term and by estimating the y-intercept when all predictors are at their respective means (averages), as opposed to when they are at zero (which is not always a plausible or meaningful value for some variables). Further, when interpreting the association between one predictor variable and the outcome variable while statistically controlling for other variables in the model, grand-mean centering allows us to interpret the effect of the focal predictor variable when the other predictor variables are at their respective means (as opposed to zero). For more detailed information on centering, check out the chapter on centering and standardizing variables. We only center predictor and moderator variables that are of type numeric and that we conceptualize as having a continuous (interval or ratio) measurement scale. In our current data frame, we will grand-mean center just the interview and age variables. We will use what is perhaps the most intuitive approach for grand-mean centering variables. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variables names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (DiffPred), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each cases value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction symbol (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (DiffPred), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. Now repeat those same steps for the age variable. # Grand-mean centering: Using basic arithmetic and the mean function from base R DiffPred$c_interview &lt;- DiffPred$interview - mean(DiffPred$interview, na.rm=TRUE) DiffPred$c_age &lt;- DiffPred$age - mean(DiffPred$age, na.rm=TRUE) 42.2.5 Estimate Moderated Multiple Linear Regression Model As described above, we will use the standard three-step differential prediction assessment using MMLR. We will repeat this three step process for all three protected class variables in our data frame: gender, c_age, and race. [Note that we are using the grand-mean centered age variable we created above.]. Our selection tool of interest is our c_interview (which is the grand-mean centered version of the variable we created above), and our criterion of interest is perf_eval. I will show you how to test differential prediction for each of these protected class variables using the Regression (or reg_brief) function from lessR. Note: To reduce the overall length of this tutorial, I have omitted the diagnostic tests of the statistical assumptions, and thus for the purposes of this tutorial, we will assume that the statistical assumptions have been met. For your own work, be sure to run diagnostic tests to evaluate whether your model meets these statistical assumptions. For more information on statistical assumption testing in this context, check out the chapter on estimating increment validity using multiple linear regression. If you havent already install and access the lessR package using the install.packages and library functions, respectively (see above). This will allow us to access the Regression and reg_brief functions. # Install lessR package if you haven&#39;t already install.packages(&quot;lessR&quot;) # Access lessR package library(lessR) 42.2.5.1 Test Differential Prediction Based on gender Lets start with investigating whether differential prediction exists for our interview selection tool in relation to perf_eval based gender subgroup membership (i.e., man, woman). Step One: Lets being by regressing the criterion variable perf_eval on the selection tool variable c_interview. Here we are just establishing whether the selection tool shows evidence of criterion-related validity. See the chapters on predicting criterion scores using simple linear regression if you would like more information on regression-based approaches to assessing criterion-related validity, including determining whether statistical assumptions have been met. You can use either the Regression or reg_brief function from lessR, where the latter provides less output. To keep the amount of output in this tutorial to a minimum, I will use the reg_brief function, but when you evaluate whether statistical assumptions have been met, be sure to use the full Regression function. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.705 0.049 95.145 0.000 4.608 4.802 ## c_interview 0.307 0.041 7.465 0.000 0.226 0.387 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.960 for 375 degrees of freedom ## 95% range of residual variation: 3.776 = 2 * (1.966 * 0.960) ## ## R-squared: 0.129 Adjusted R-squared: 0.127 PRESS R-squared: 0.120 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 55.732 df: 1 and 375 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 51.380 51.380 55.732 0.000 ## Residuals 375 345.720 0.922 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient for c_interview is statistically significant and positive (b = .307, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .129, adjusted R2 = .127, p &lt; .001). See table below for common rules of thumbs for qualitatively describing R2 values. R2 Description .01 Small .09 Medium .25 Large Step Two: Next, lets add the protected class variable gender to our model to investigate whether intercept differences exist. For more information on how to interpret and test the statistical assumptions of a multiple linear regression model, check out the chapter on estimating incremental validity using multiple linear regression. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (gender) reg_brief(perf_eval ~ c_interview + gender, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + gender, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: gender ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.083 0.091 55.833 0.000 4.904 5.262 ## c_interview 0.164 0.049 3.325 0.001 0.067 0.261 ## genderwoman -0.602 0.123 -4.889 0.000 -0.844 -0.360 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.932 for 374 degrees of freedom ## 95% range of residual variation: 3.666 = 2 * (1.966 * 0.932) ## ## R-squared: 0.182 Adjusted R-squared: 0.177 PRESS R-squared: 0.169 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 41.521 df: 2 and 374 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 51.380 51.380 59.136 0.000 ## gender 1 20.770 20.770 23.906 0.000 ## ## Model 2 72.150 36.075 41.521 0.000 ## Residuals 374 324.950 0.869 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient associated with gender (or more specifically genderwoman) is negative and statistically significant (b = -.602, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Note that the name of the regression coefficient for gender has woman appended to it. This means that the intercept represents the perf_eval scores of women minus those for men; because the coefficient is negative, it implies that the intercept value is significantly lower for women relative to men. The amount of variance explained by c_interview and gender in perf_eval is medium-large in magnitude (R2 = .182, adjusted R2 = .177, p &lt; .001), which is larger than when just c_interview was included in the model. We can visualize the nature of the intercept differences by creating a plot with the simple slopes. To do so, we will use a package called interactions, so if you havent already, install and access the package. # Install interactions package if you haven&#39;t already install.packages(&quot;interactions&quot;) # Access interactions package library(interactions) As input to the probe_interaction function from the interactions package, we will need to use the lm function base R to specify our regression model from above. We can just copy and paste the same arguments from the reg_brief function above into the lm function parentheses. We need to name this model something so that we can reference it in the next function, so lets name it Int_Model (for intercept differences model) using the &lt;- operator. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (gender) Int_Model &lt;- lm(perf_eval ~ c_interview + gender, data=DiffPred) Now were ready to use the probe_interaction to plot this additive model. As the first argument, enter the name of the regression model we just created above (Int_Model). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we arent requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Visualize intercept differences probe_interaction(Int_Model, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.16 0.05 3.32 0.00 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.16 0.05 3.32 0.00 [If you got an error message when running the probe_interaction function, you may need to install the sandwich package and then try again.] Because we were only testing an additive model, the figure shows the slopes for each subgroup (men, women) as equal with just the intercepts differing. In the following step, we will formally test whether a multiplicative interaction effect exists, which would suggest slope differences. Step Three: As the final step, its time to add the interaction term between c_interview and gender to the model. Fortunately, this is very easy to do in R. All we need to do is change the + symbol from our previous regression model script to the *, where the * implies an interaction in the context of a model. Conceptually, the * is appropriate because we are estimating a multiplicative model when we have an interaction term, which is commonly known as a product term. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * gender, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * gender, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: gender ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 5.258 0.111 47.559 0.000 5.040 5.475 ## c_interview -0.025 0.085 -0.291 0.771 -0.191 0.142 ## genderwoman -0.724 0.130 -5.572 0.000 -0.980 -0.469 ## c_interview:genderwoman 0.284 0.104 2.734 0.007 0.080 0.488 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.924 for 373 degrees of freedom ## 95% range of residual variation: 3.634 = 2 * (1.966 * 0.924) ## ## R-squared: 0.198 Adjusted R-squared: 0.191 PRESS R-squared: 0.181 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 30.652 df: 3 and 373 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 51.380 51.380 60.160 0.000 ## gender 1 20.770 20.770 24.319 0.000 ## c_interview:gender 1 6.385 6.385 7.476 0.007 ## ## Model 3 78.536 26.179 30.652 0.000 ## Residuals 373 318.565 0.854 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR In our output, we now have a third regression term (c_interview:gender) that is our interaction term. You may find that the because the label for the interaction term is long-ish, the columns associated with regression coefficients, standard errors, etc. are shifted over a bit, so make sure that you are referencing the values that you think you are. The regression coefficient associated with the interaction term (c_interview:gender) is positive and statistically signficant (b = .284, p = .007), but dont pay too much attention to the sign of the interaction term as it is difficult to interpret it without graphing the entire regression equation. What we do care about is that the interaction term is statistically significant, which indicates that there is evidence of slope differences; in other words, there seems to be multiplicative between c_interview and gender. We already know that there is evidence of intercept differences, where men have a higher intercept, but now we have evidence that the criterion-related validities for the selection tool (c_interview) in relation to the criterion (perf_eval) are significantly different from one another. The amount of variance explained by c_interview, gender, and their interaction in relation to perf_eval is medium-large in magnitude (R2 = .198, adjusted R2 = .191, p &lt; .001), which is larger than when just c_interview and gender were included in the model. Given the statistically significant interaction term, it is appropriate (and helpful) to probe the interaction by plotting it. This will help us understand the form of the interaction. Just like we did for visualizing intercept differences, we will use the probe_interaction function from the interactions package. As input to the probe_interaction function from the interactions package, we will need to use the lm function base R to specify our regression model from above. We can just copy and paste the same arguments from the reg_brief function above into the lm function parentheses. We need to name this model something so that we can reference it in the next function, so lets name it Slope_Model (for slope differences model) using the &lt;- operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class Slope_Model &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) Now were ready to use the probe_interaction to plot this multiplicative model. As the first argument, enter the name of the regression model we just created above (Slope_Model). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we arent requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Probe the significant interaction (slope differences) probe_interaction(Slope_Model, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------- ------ -------- ------ ## -0.02 0.08 -0.29 0.77 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.26 0.06 4.32 0.00 The plot output shows that the two slopes are indeed different, such that there doesnt seem to be much of an association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) for men, but there seems to be a positive association for women. Now take a look at the text output in your console. The simple slopes analysis automatically estimates the regression coefficient (slope) for both levels of the moderator variable (gender: man, woman). Corroborating what we see in the plot, we find that the simple slope for women is indeed statistically significant and positive (b = .26, p &lt; .01), such that for every one unit increase in interview scores, job performance scores tend to go up by .26 units. In contrast, we find that the simple slope for men is nonsignificant (b = -.02, p = .77). In sum, we found evidence of intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the levels/subgroups of the protected class variable (gender). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. Sample Write-Up: Our HR analytic team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction of the structured interview based on the U.S. protected class of gender. Based on a sample of 377 individuals, we applied a three-step process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of both intercept and slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .307, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, and found that the model intercept for women was significantly lower than the intercept for men (b = -.602, p &lt; .001). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between interview scores and job performance scores to a statistically significant extent (b = .284, p = .007). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant for women (b = .26, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .26 units; in contrast, the associated between structured interview scores and job performance scores for men was nonsignificant (b = -.02, p = .77). In sum, we found evidence of both intercept and slope differences, which means that this interview tool shows predictive bias with respect to gender, making the application of a common regression line (i.e., equation) inappropriate according to U.S. guidelines. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we recommend that the structured interview not be used for employee selection purposes. 42.2.5.2 Test Differential Prediction Based on c_age Investigating whether differential prediction exists for c_interview in relation to perf_eval based on the continuous moderator variable c_age will unfold very similarly to the process we used for the dichotomous gender variable from above. The only differences will occur when it comes to estimating and visualizing the intercept differences and simple slopes (assuming we find statistically significant effects). Given this, we will breeze through the steps, which means I will provide less explanation. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.705 0.049 95.145 0.000 4.608 4.802 ## c_interview 0.307 0.041 7.465 0.000 0.226 0.387 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.960 for 375 degrees of freedom ## 95% range of residual variation: 3.776 = 2 * (1.966 * 0.960) ## ## R-squared: 0.129 Adjusted R-squared: 0.127 PRESS R-squared: 0.120 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 55.732 df: 1 and 375 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 51.380 51.380 55.732 0.000 ## Residuals 375 345.720 0.922 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient for c_interview is statistically significant and positive (b = .307, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .129, adjusted R2 = .127, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable c_age. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) reg_brief(perf_eval ~ c_interview + c_age, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + c_age, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: c_age ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.705 0.046 102.806 0.000 4.615 4.795 ## c_interview 0.558 0.049 11.307 0.000 0.461 0.655 ## c_age 0.059 0.007 7.988 0.000 0.045 0.074 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.889 for 374 degrees of freedom ## 95% range of residual variation: 3.495 = 2 * (1.966 * 0.889) ## ## R-squared: 0.256 Adjusted R-squared: 0.252 PRESS R-squared: 0.244 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 64.442 df: 2 and 374 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 51.380 51.380 65.067 0.000 ## c_age 1 50.392 50.392 63.816 0.000 ## ## Model 2 101.772 50.886 64.442 0.000 ## Residuals 374 295.328 0.790 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient associated with c_age is positive and statistically significant (b = .059, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is positive, it implies that the intercept value is significantly higher for older workers relative to younger workers. The amount of variance explained by c_interview and c_age in perf_eval is large in magnitude (R2 = .256, adjusted R2 = .252, p &lt; .001), which is notably larger than when just c_interview was included in the model. Using the probe_interaction function from the interactions package, we will visualize the intercept differences. As input to the probe_interaction function, use the lm function base R to specify our regression model from above. Name this model something so that we can reference it in the next function (Int_Model). # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) Int_Model &lt;- lm(perf_eval ~ c_interview + c_age, data=DiffPred) Now specify the arguments in the probe_interaction function. Note that the function automatically categorizes the continuous moderator variable by its mean and 1 standard deviation (SD) below and above the mean. As you can see, individuals whose age is 1 SD above the mean have the highest intercept value. # Visualize intercept differences probe_interaction(Int_Model, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.029929095154546558888 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 ## ## Slope of c_interview when c_age = 0.000000000000001243921 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 ## ## Slope of c_interview when c_age = 8.029929095154550111602 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 Step Three: Now add the interaction term between c_interview and c_age to the model using the * symbol. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (c_age), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * c_age, data=DiffPred) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * c_age, data=DiffPred, Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: c_age ## ## Number of cases (rows) of data: 377 ## Number of cases retained for analysis: 377 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.795 0.051 93.404 0.000 4.694 4.896 ## c_interview 0.572 0.049 11.753 0.000 0.476 0.668 ## c_age 0.069 0.008 8.893 0.000 0.054 0.085 ## c_interview:c_age 0.015 0.004 3.658 0.000 0.007 0.023 ## ## Standard deviation of perf_eval: 1.028 ## ## Standard deviation of residuals: 0.874 for 373 degrees of freedom ## 95% range of residual variation: 3.438 = 2 * (1.966 * 0.874) ## ## R-squared: 0.282 Adjusted R-squared: 0.276 PRESS R-squared: 0.267 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 48.845 df: 3 and 373 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 51.380 51.380 67.222 0.000 ## c_age 1 50.392 50.392 65.929 0.000 ## c_interview:c_age 1 10.230 10.230 13.384 0.000 ## ## Model 3 112.002 37.334 48.845 0.000 ## Residuals 373 285.098 0.764 ## perf_eval 376 397.100 1.056 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient associated with the interaction term (c_interview:c_age) is positive and statistically signficant (b = .015, p &lt; .001), which indicates that there is evidence of slope differences. The amount of variance explained by c_interview, c_age, and their interaction in relation to perf_eval is large in magnitude (R2 = .282, adjusted R2 = .276, p &lt; .001), which is larger than when just c_interview and c_age were included in the model. Given the statistically significant interaction term, we will use the probe_interaction function from the interactions package to probe the interaction. As input to the probe_interaction function from the interactions package, we will need to use the lm function base R to specify our regression model from above. We need to name this model something so that we can reference it in the next function, so lets name it Slope_Model (for slope differences model) using the &lt;- operator. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (c_age), # and interaction between selection tool and protected class Slope_Model &lt;- lm(perf_eval ~ c_interview * c_age, data=DiffPred) Now were ready to use the probe_interaction to plot this multiplicative model. # Probe the significant interaction (slope differences) probe_interaction(Slope_Model, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.029929095154546558888 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.45 0.06 8.09 0.00 ## ## Slope of c_interview when c_age = 0.000000000000001243921 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.57 0.05 11.75 0.00 ## ## Slope of c_interview when c_age = 8.029929095154550111602 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.69 0.06 11.40 0.00 The plot and the simple slopes analyses output show that the two slopes are indeed different, such that the association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) is stronger for older workers (b = .69, p &lt; .01), than for average-aged workers (b = .57, p &lt; .01) and younger workers (b = .45, p &lt; .01). Notably, all three simple slopes are statistically significant and positive, but the slope is stronger for older workers. In sum, we found evidence of intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the protected class variable (c_age). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.2.5.3 Test Differential Prediction Based on race Investigating whether differential prediction exists for c_interview in relation to perf_eval based on race variable will unfold in mostly the same manner as the dichotomous gender variable from above. However, we will filter our data within the regression function to compare just Asian individuals with White individuals, as it is customer to compare just two races/ethnicities at a time. The decision to focus on only Asian and white individuals is arbitrary and just for demonstration purposes. Note that in the DiffPred data frame object that both asian and white are lowercase category labels in the race variable. Step one: Regress the criterion variable perf_eval on the selection tool variable c_interview. To subset the data such that only individuals who are Asian or white are retained, we will add the following argument: rows=(race==\"asian\" | race==\"white\"). This argument uses conditional statements to indicate that we want to retain those cases for which race is equal to Asian or for which race is equal to White. We have effectively dichotomized the race variable using this approach. If you need to review conditional/logical statements when filtering data, check out the chapter on filtering data. # Regress perf_eval (criterion) on c_interview (selection tool) reg_brief(perf_eval ~ c_interview, data=DiffPred, rows=(race==&quot;asian&quot; | race==&quot;white&quot;)) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview, data=DiffPred, rows=(race == &quot;asian&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable: c_interview ## ## Number of cases (rows) of data: 346 ## Number of cases retained for analysis: 346 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.751 0.051 93.696 0.000 4.651 4.851 ## c_interview 0.348 0.043 8.128 0.000 0.264 0.433 ## ## Standard deviation of perf_eval: 1.024 ## ## Standard deviation of residuals: 0.940 for 344 degrees of freedom ## 95% range of residual variation: 3.696 = 2 * (1.967 * 0.940) ## ## R-squared: 0.161 Adjusted R-squared: 0.159 PRESS R-squared: 0.152 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 66.064 df: 1 and 344 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## Model 1 58.317 58.317 66.064 0.000 ## Residuals 344 303.658 0.883 ## perf_eval 345 361.975 1.049 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient for c_interview is statistically significant and positive (b = .348, p &lt; .001) based on this reduced sample of 346 individuals, which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .161, adjusted R2 = .159, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable race. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) reg_brief(perf_eval ~ c_interview + race, data=DiffPred, rows=(race==&quot;asian&quot; | race==&quot;white&quot;)) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview + race, data=DiffPred, rows=(race == &quot;asian&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: race ## ## Number of cases (rows) of data: 346 ## Number of cases retained for analysis: 346 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.918 0.070 70.754 0.000 4.781 5.054 ## c_interview 0.407 0.045 8.945 0.000 0.317 0.496 ## racewhite -0.373 0.108 -3.445 0.001 -0.586 -0.160 ## ## Standard deviation of perf_eval: 1.024 ## ## Standard deviation of residuals: 0.925 for 343 degrees of freedom ## 95% range of residual variation: 3.639 = 2 * (1.967 * 0.925) ## ## R-squared: 0.189 Adjusted R-squared: 0.184 PRESS R-squared: 0.175 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 40.010 df: 2 and 343 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 58.317 58.317 68.152 0.000 ## race 1 10.156 10.156 11.869 0.001 ## ## Model 2 68.473 34.236 40.010 0.000 ## Residuals 343 293.502 0.856 ## perf_eval 345 361.975 1.049 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient associated with race (or more specifically racewhite) is negative and statistically signficant (b = -.373, p = .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is negative and because white is appended to the race variable name, it implies that the intercept value is significantly lower for White individuals compared to Asian individuals. The amount of variance explained by c_interview and race in perf_eval is medium-large in magnitude (R2 = .189, adjusted R2 = .184, p &lt; .001). Using the probe_interaction function from the interactions package, we will visualize the intercept differences. As input to the probe_interaction function, use the lm function base R to specify our regression model from above. The subset= argument functions the same way as the rows= argument from the reg_brief and Regression functions from lessR, so well use that instead for the lm function. Name this model something so that we can reference it in the next function (Int_Model). # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) Int_Model &lt;- lm(perf_eval ~ c_interview + race, data=DiffPred, subset=(race==&quot;asian&quot; | race==&quot;white&quot;)) Now specify the arguments in the probe_interaction function. # Visualize intercept differences probe_interaction(Int_Model, pred=c_interview, modx=race, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when race = white: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.41 0.05 8.94 0.00 ## ## Slope of c_interview when race = asian: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.41 0.05 8.94 0.00 Step Three: Now add the interaction term between c_interview and race to the model using the * symbol. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (race), # and interaction between selection tool and protected class reg_brief(perf_eval ~ c_interview * race, data=DiffPred, rows=(race==&quot;asian&quot; | race==&quot;white&quot;)) ## &gt;&gt;&gt; Suggestion ## # Create an R markdown file for interpretative output with Rmd = &quot;file_name&quot; ## reg(perf_eval ~ c_interview * race, data=DiffPred, rows=(race == &quot;asian&quot; | race == &quot;white&quot;), Rmd=&quot;eg&quot;) ## ## BACKGROUND ## ## Data Frame: DiffPred ## ## Response Variable: perf_eval ## Predictor Variable 1: c_interview ## Predictor Variable 2: race ## ## Number of cases (rows) of data: 346 ## Number of cases retained for analysis: 346 ## ## BASIC ANALYSIS ## ## Estimate Std Err t-value p-value Lower 95% Upper 95% ## (Intercept) 4.896 0.072 67.863 0.000 4.754 5.038 ## c_interview 0.362 0.061 5.974 0.000 0.243 0.481 ## racewhite -0.374 0.108 -3.459 0.001 -0.587 -0.161 ## c_interview:racewhite 0.103 0.092 1.124 0.262 -0.077 0.283 ## ## Standard deviation of perf_eval: 1.024 ## ## Standard deviation of residuals: 0.925 for 342 degrees of freedom ## 95% range of residual variation: 3.638 = 2 * (1.967 * 0.925) ## ## R-squared: 0.192 Adjusted R-squared: 0.185 PRESS R-squared: 0.174 ## ## Null hypothesis of all 0 population slope coefficients: ## F-statistic: 27.115 df: 3 and 342 p-value: 0.000 ## ## df Sum Sq Mean Sq F-value p-value ## c_interview 1 58.317 58.317 68.204 0.000 ## race 1 10.156 10.156 11.878 0.001 ## c_interview:race 1 1.081 1.081 1.264 0.262 ## ## Model 3 69.554 23.185 27.115 0.000 ## Residuals 342 292.421 0.855 ## perf_eval 345 361.975 1.049 ## ## K-FOLD CROSS-VALIDATION ## ## RELATIONS AMONG THE VARIABLES ## ## RESIDUALS AND INFLUENCE ## ## PREDICTION ERROR The regression coefficient associated with the interaction term (c_interview:racewhite) is not statistically significant (b = .103, p = .262), which indicates that there is no evidence of slope differences. Given this, we wont probe the interaction because, well, there is not an interaction to probe. In sum, we found evidence of intercept differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the protected class variable (race). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.2.6 Summary In this chapter, we learned how to use moderated multiple linear regression (MMLR) to test whether evidence of differential prediction exists. To do so, we learned how to use the Regression function (well, technically we used the reg_brief function) from lessR. In addition, we used the probe_interaction function from the interactions package to probe the intercept differences and slope differences visually and using simple slopes analyses. 42.3 Chapter Supplement In addition to the Regression function from the lessR package covered above, we can use the lm function from base R to estimate a moderated multiple linear regression (MMLR) model. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results. 42.3.1 Functions &amp; Packages Introduced Function Package mean base R lm base R summary base R anova base R probe_interaction interactions apa.reg.table apaTables 42.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object DiffPred &lt;- read_csv(&quot;DiffPred.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## emp_id = col_character(), ## perf_eval = col_double(), ## interview = col_double(), ## age = col_double(), ## gender = col_character(), ## race = col_character() ## ) # Print the names of the variables in the data frame (tibble) objects names(DiffPred) ## [1] &quot;emp_id&quot; &quot;perf_eval&quot; &quot;interview&quot; &quot;age&quot; &quot;gender&quot; &quot;race&quot; # View variable type for each variable in data frame str(DiffPred) ## spec_tbl_df[,6] [377 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ emp_id : chr [1:377] &quot;MA322&quot; &quot;MA323&quot; &quot;MA324&quot; &quot;MA325&quot; ... ## $ perf_eval: num [1:377] 4.2 4.9 4.2 5.3 4.2 6.9 3.4 5.8 4.4 5.6 ... ## $ interview: num [1:377] 7.5 9.3 7.5 8 9.3 6.8 6.7 7 8.2 6.4 ... ## $ age : num [1:377] 29.7 31.7 29.4 37.9 30.9 46.2 43.9 47.8 31.7 44.6 ... ## $ gender : chr [1:377] &quot;woman&quot; &quot;man&quot; &quot;woman&quot; &quot;woman&quot; ... ## $ race : chr [1:377] &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; &quot;asian&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. emp_id = col_character(), ## .. perf_eval = col_double(), ## .. interview = col_double(), ## .. age = col_double(), ## .. gender = col_character(), ## .. race = col_character() ## .. ) # View first 6 rows of data frame head(DiffPred) ## # A tibble: 6 x 6 ## emp_id perf_eval interview age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 MA322 4.2 7.5 29.7 woman asian ## 2 MA323 4.9 9.3 31.7 man asian ## 3 MA324 4.2 7.5 29.4 woman asian ## 4 MA325 5.3 8 37.9 woman asian ## 5 MA326 4.2 9.3 30.9 man black ## 6 MA327 6.9 6.8 46.2 woman asian 42.3.3 lm Function from Base R We will use the lm function from base R to specify and estimate our regression functions. As I did above, I will show you how to apply moderated multiple linear regression (MMLR) in the service of detecting differential prediction of a selection tool based on protected class membership. Grand-Mean Center Variables: Before we estimate the regression models, however, we need to grand-mean center certain variables. We only center predictor and moderator variables that are of type numeric and that we conceptualize as having a continuous (interval or ratio) measurement scale. In our current data frame, we will grand-mean center just the interview and age variables. We will use what is perhaps the most intuitive approach for grand-mean centering variables. We must begin by coming up with a new name for one of our soon-to-be grand-mean centered variable, and in this example, we will center the interview variable. I typically like to name the centered variable by simply adding the c_ prefix to the existing variables names (e.g., c_interview). Type the name of the data frame object to which the new centered variable will be attached (DiffPred), followed by the $ operator and the name of the new variable we are creating (c_interview). Next, add the &lt;- operator to indicate what values you will assign to this new variable. To create a vector of values to assign to this new c_interview variable, we will subtract the mean (average) score for the original variable (interview) from each cases value on the variable. Specifically, enter the name of the data frame object, followed by the $ operator and the name of the original variable (interview). After that, enter the subtraction symbol (-). And finally, type the name of the mean function from base R. As the first argument in the mean function, enter the name of the data frame object (DiffPred), followed by the $ operator and the name of the original variable (interview). As the second argument, enter na.rm=TRUE to indicate that you wish to drop missing values when calculating the grand mean for the sample. Now repeat those same steps for the age variable. # Grand-mean centering: Using basic arithmetic and the mean function from base R DiffPred$c_interview &lt;- DiffPred$interview - mean(DiffPred$interview, na.rm=TRUE) DiffPred$c_age &lt;- DiffPred$age - mean(DiffPred$age, na.rm=TRUE) 42.3.3.1 Test Differential Prediction Based on gender Lets start with investigating whether differential prediction exists for our interview selection tool in relation to perf_eval based gender subgroup membership (i.e., man, woman). Step One: Lets being by regressing the criterion variable perf_eval on the selection tool variable c_interview. Here we are just establishing whether the selection tool shows evidence of criterion-related validity. See the chapters on criterion-related validity using correlation and predicting criterion scores using simple linear regression if you would like more information on regression-based approaches to assessing criterion-related validity, including determining whether statistical assumptions have been met. Lets name our regression model for this first step model.1. Remember, we need to use the summary function from base R to view our model output. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.86274 -0.58012 0.01324 0.58922 2.74118 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70504 0.04945 95.145 &lt; 0.0000000000000002 *** ## c_interview 0.30665 0.04108 7.465 0.000000000000587 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9602 on 375 degrees of freedom ## Multiple R-squared: 0.1294, Adjusted R-squared: 0.1271 ## F-statistic: 55.73 on 1 and 375 DF, p-value: 0.0000000000005866 The regression coefficient for c_interview is statistically significant and positive (b = .307, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .129, adjusted R2 = .127, p &lt; .001). See table below for common rules of thumbs for qualitatively describing R2 values. R2 Description .01 Small .09 Medium .25 Large Step Two: Next, lets add the protected class variable gender to our model to investigate whether intercept differences exist. Well name this model model.2. For more information on how to interpret and test the statistical assumptions of a multiple linear regression model, check out the chapter on estimating incremental validity. In addition, lets use the anova function from base R to do a nested model comparison between model.1 and model.2. This will tell us whether adding gender to the model significantly improved model fit. Finally, lets reference the summary/output for each model and specifically the r.squared values to determine what the change in R2 was from one model to the next. The change in R2 gives as an idea of how much (in terms of practical significance) the model fit improved when adding the additional variable. Note that we use the $ symbol to indicate that we want the r.squared value from the summary(model.2) and summary(model.2) output, and then we do a simple substraction to get the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (gender) model.2 &lt;- lm(perf_eval ~ c_interview + gender, data=DiffPred) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + gender, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.93543 -0.63543 -0.00836 0.59461 2.75731 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.08327 0.09104 55.833 &lt; 0.0000000000000002 *** ## c_interview 0.16419 0.04939 3.325 0.000973 *** ## genderwoman -0.60166 0.12306 -4.889 0.0000015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9321 on 374 degrees of freedom ## Multiple R-squared: 0.1817, Adjusted R-squared: 0.1773 ## F-statistic: 41.52 on 2 and 374 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 375 345.72 ## 2 374 324.95 1 20.77 23.905 0.000001505 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.052305 The regression coefficient associated with gender (or more specifically genderwoman) is negative and statistically signficant (b = -.602, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Note that the name of the regression coefficient for gender has woman appended to it. This means that the intercept represents the perf_eval scores of women minus those for men; because the coefficient is negative, it implies that the intercept value is significantly lower for women relative to men. The amount of variance explained by c_interview and gender in perf_eval is medium-large in magnitude (R2 = .182, adjusted R2 = .177, p &lt; .001). The significant F-value (23.905, p &lt; .001) in from the nested model comparison indicates that adding gender significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .052), which indicates that adding gender to the model explained an additional 5.2% of the variance in perf_eval. We can visualize the nature of the intercept differences by creating a plot with the simple slopes. To do so, we will use a package called interactions, so if you havent already, install and access the package. # Install interactions package if you haven&#39;t already install.packages(&quot;interactions&quot;) # Access interactions package library(interactions) Now were ready to use the probe_interaction to plot this additive model. As the first argument, enter the name of the regression model we just created above (model.2). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we arent requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Visualize intercept differences probe_interaction(model.2, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.16 0.05 3.32 0.00 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.16 0.05 3.32 0.00 Because we were only testing an additive model, the figure shows the slopes for each subgroup (men, women) as equal with just the intercepts differing. In the following step, we will formally test whether a multiplicative interaction effect exists, which would suggest slope differences. Step Three: As the final step, its time to add the interaction term between c_interview and gender to the model. Fortunately, this is very easy to do in R. All we need to do is change the + symbol from our previous regression model script to the *, where the * implies an interaction in the context of a model. Conceptually, the * is appropriate because we are estimating a multiplicative model when we have an interaction term, which is commonly known as a product term. Lets name this model model.3. As before, well do a nested model comparison with the previous model and this one to determine whether there is a significant improvement in fit. In addition, well estimate change in R2. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (gender), # and interaction between selection tool and protected class model.3 &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * gender, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.70489 -0.55275 0.02541 0.58815 2.84361 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.25779 0.11055 47.559 &lt; 0.0000000000000002 *** ## c_interview -0.02466 0.08466 -0.291 0.77099 ## genderwoman -0.72437 0.13000 -5.572 0.0000000483 *** ## c_interview:genderwoman 0.28376 0.10378 2.734 0.00655 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9242 on 373 degrees of freedom ## Multiple R-squared: 0.1978, Adjusted R-squared: 0.1913 ## F-statistic: 30.65 on 3 and 373 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + gender ## Model 2: perf_eval ~ c_interview * gender ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 374 324.95 ## 2 373 318.56 1 6.3852 7.4762 0.00655 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.01607944 In our output, we now have a third regression term (c_interview:genderwoman) that is our interaction term. You may find that the because the label for the interaction term is long-ish, the columns associated with regression coefficients, standard errors, etc. are shifted over a bit, so make sure that you are referencing the values that you think you are. The regression coefficient associated with the interaction term (c_interview:genderwoman) is positive and statistically significant (b = .284, p = .007), but dont pay too much attention to the sign of the interaction term as it is difficult to interpret it without graphing the entire regression equation. What we do care about is that the interaction term is statistically significant, which indicates that there is evidence of slope differences; in other words, there seems to be multiplicative between c_interview and gender. We already know that there is evidence of intercept differences, where men have a higher intercept, but now we have evidence that the criterion-related validities for the selection tool (c_interview) in relation to the criterion (perf_eval) are significantly different from one another. The amount of variance explained by c_interview, gender, and their interaction in relation to perf_eval is medium-large in magnitude (R2 = .198, adjusted R2 = .191, p &lt; .001). The significant F-value (7.476, p = .006) in from the nested model comparison indicates that adding the interaction term significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .016), which indicates that adding the interaction term to the model explained an additional 1.6% of the variance in perf_eval. Given the statistically significant interaction term, it is appropriate (and helpful) to probe the interaction by plotting it. This will help us understand the form of the interaction. Just like we did for visualizing intercept differences, we will use the probe_interaction function from the interactions package to plot this model - except this one is multiplicative instead of additive. As the first argument, enter the name of the regression model we just created above (model.3). As the second argument, type the name of the variable you are framing as the predictor after pred=, which in our case is the selection tool variable (c_interview). As the third argument, type the name of the variable you are framing as the moderator after modx=, which in our case is the protected class variable (gender). As the fourth argument, enter johnson_neyman=FALSE, as we arent requesting the Johnson-Neyman test in this tutorial. Finally, if you choose to, you can use x.label= and y.label= to create more informative names for the predictor and outcome variables, respectively. You could also add, if you should choose to do so, the legend.main= argument to provide a more informative name for the moderator variable, and/or the modx.labels= to provide more informative names for the moderator variable labels. # Probe the significant interaction (slope differences) probe_interaction(model.3, pred=c_interview, modx=gender, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when gender = man: ## ## Est. S.E. t val. p ## ------- ------ -------- ------ ## -0.02 0.08 -0.29 0.77 ## ## Slope of c_interview when gender = woman: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.26 0.06 4.32 0.00 The plot output shows that the two slopes are indeed different, such that there doesnt seem to be much of an association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) for men, but there seems to be a postive association for women. Now take a look at the text output in your console. The simple slopes analysis automatically estimates the regression coefficient (slope) for both levels of the moderator variable (gender: man, woman). Corroborating what we see in the plot, we find that the simple slope for women is indeed statistically significant and positive (b = .26, p &lt; .01), such that for every one unit increase in interview scores, job performance scores tend to go up by .26 units. In contrast, we find that the simple slope for men is nonsignificant (b = -.02, p = .77). In sum, we found evidence of intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the levels/subgroups of the protected class variable (gender). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. Sample Write-Up: Our HR analytic team recently found evidence in support of the criterion-related validity of a new structured interview selection procedure. As a next step, we investigated whether there might be evidence of differential prediction of the structured interview based on the U.S. protected class of gender. Based on a sample of 377 individuals, we applied a three-step process with moderated multiple linear regression, which is consistent with established principles (Society for Industrial &amp; Organizational Psychology 2018; Cleary 1968). Based on our analyses, we found evidence differential prediction (i.e., predictive bias) based on gender for the new structured interview procedure, and specifically, we found evidence of both intercept and slope differences. In the first step, we found evidence of criterion-related validity based on a simple linear regression model, as structured interview scores were positively and significantly associated with job performance (b = .307, p &lt; .001), thereby indicating some degree of job-relatedness and -relevance. In the second step, we added the protected class variable associated with gender to the model, resulting in a multiple linear regression model, and found that the model intercept for women was significantly lower than the intercept for men (b = -.602, p &lt; .001). In the third step, we added the interaction term between the structured interview and gender variables, resulting in a moderated multiple linear regression model, and we found that gender moderated the association between interview scores and job performance scores to a statistically significant extent (b = .284, p = .007). Based on follow-up simple slopes analyses, we found that the association between structured interview scores and job performance scores was statistically significant for women (b = .26, p &lt; .01), such that for every one unit increase in structured interview scores, job performance scores tended to increase by .26 units; in contrast, the associated between structured interview scores and job performance scores for men was nonsignificant (b = -.02, p = .77). In sum, we found evidence of both intercept and slope differences, which means that this interview tool shows predictive bias with respect to gender, making the application of a common regression line (i.e., equation) inappropriate according to U.S. guidelines. If possible, this structured interview should be redesigned or the interviewers should be trained to reduce the predictive bias based on gender. In the interim, we recommend that the structured interview not be used for employee selection purposes. 42.3.3.2 Test Differential Prediction Based on c_age Investigating whether differential prediction exists for c_interview in relation to perf_eval based on the continuous moderator variable c_age will unfold very similarly to the process we used for the dichotomous gender variable from above. The only differences will occur when it comes to estimating and visualizing the intercept differences and simple slopes (assuming we find statistically significant effects). Given this, we will breeze through the steps, which means I will provide less explanation. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.86274 -0.58012 0.01324 0.58922 2.74118 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.70504 0.04945 95.145 &lt; 0.0000000000000002 *** ## c_interview 0.30665 0.04108 7.465 0.000000000000587 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9602 on 375 degrees of freedom ## Multiple R-squared: 0.1294, Adjusted R-squared: 0.1271 ## F-statistic: 55.73 on 1 and 375 DF, p-value: 0.0000000000005866 The regression coefficient for c_interview is statistically significant and positive (b = .307, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .129, adjusted R2 = .127, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable c_age. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) model.2 &lt;- lm(perf_eval ~ c_interview + c_age, data=DiffPred) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + c_age, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9396 -0.5133 -0.0155 0.5659 2.5057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.705040 0.045766 102.806 &lt; 0.0000000000000002 *** ## c_interview 0.557615 0.049317 11.307 &lt; 0.0000000000000002 *** ## c_age 0.059144 0.007404 7.988 0.0000000000000171 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8886 on 374 degrees of freedom ## Multiple R-squared: 0.2563, Adjusted R-squared: 0.2523 ## F-statistic: 64.44 on 2 and 374 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + c_age ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 375 345.72 ## 2 374 295.33 1 50.392 63.816 0.00000000000001707 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.1269002 The regression coefficient associated with c_age is positive and statistically signficant (b = .059, p &lt; .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is positive, it implies that the intercept value is significantly higher for older workers relative to younger workers. The amount of variance explained by c_interview and c_age in perf_eval is large in magnitude (R2 = .256, adjusted R2 = .252, p &lt; .001), which is notably larger than when just c_interview was included in the model. The significant F-value (63.816, p &lt; .001) in from the nested model comparison indicates that adding c_age significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .127), which indicates that adding c_age to the model explained an additional 12.7% of the variance in perf_eval. Using the probe_interaction function from the interactions package, we will visualize the intercept differences of the model.2 model we specified above. Note that the function automatically categorizes the continuous moderator variable by its mean and 1 standard deviation (SD) below and above the mean. As you can see, individuals whose age is 1 SD above the mean have the highest intercept value. # Visualize intercept differences probe_interaction(model.2, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.029929095154546558888 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 ## ## Slope of c_interview when c_age = 0.000000000000001243921 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 ## ## Slope of c_interview when c_age = 8.029929095154550111602 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.56 0.05 11.31 0.00 Step Three: Now add the interaction term between c_interview and c_age to the model using the * symbol. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (c_age) model.3 &lt;- lm(perf_eval ~ c_interview * c_age, data=DiffPred) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * c_age, data = DiffPred) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.98160 -0.48292 -0.02569 0.54557 2.44720 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.795272 0.051339 93.404 &lt; 0.0000000000000002 *** ## c_interview 0.572163 0.048683 11.753 &lt; 0.0000000000000002 *** ## c_age 0.069471 0.007812 8.893 &lt; 0.0000000000000002 *** ## c_interview:c_age 0.014671 0.004010 3.658 0.00029 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8743 on 373 degrees of freedom ## Multiple R-squared: 0.2821, Adjusted R-squared: 0.2763 ## F-statistic: 48.85 on 3 and 373 DF, p-value: &lt; 0.00000000000000022 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + c_age ## Model 2: perf_eval ~ c_interview * c_age ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 374 295.33 ## 2 373 285.10 1 10.23 13.384 0.0002902 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.02576237 The regression coefficient associated with the interaction term (c_interview:c_age) is positive and statistically signficant (b = .015, p &lt; .001), which indicates that there is evidence of slope differences. The amount of variance explained by c_interview, c_age, and their interaction in relation to perf_eval is large in magnitude (R2 = .282, adjusted R2 = .276, p &lt; .001). The significant F-value (13.384, p &lt; .001) in from the nested model comparison indicates that adding the interactoin term significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .026), which indicates that adding the interaction term to the model explained an additional 2.6% of the variance in perf_eval. Given the statistically significant interaction term, we will use the probe_interaction function from the interactions package to probe the interaction. Now were ready to use the probe_interaction to plot this multiplicative model that we named model.3. # Probe the significant interaction (slope differences) probe_interaction(model.3, pred=c_interview, modx=c_age, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when c_age = -8.029929095154546558888 (- 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.45 0.06 8.09 0.00 ## ## Slope of c_interview when c_age = 0.000000000000001243921 (Mean): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.57 0.05 11.75 0.00 ## ## Slope of c_interview when c_age = 8.029929095154550111602 (+ 1 SD): ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.69 0.06 11.40 0.00 The plot and the simple slopes analyses output show that the two slopes are indeed different, such that the association between the selection tool (c_interview, Interview) and the criterion (perf_eval, Job Performance) is stronger for older workers (b = .69, p &lt; .01), than for average-aged workers (b = .57, p &lt; .01) and younger workers (b = .45, p &lt; .01). Notably, all three simple slopes are statistically significant and postive, but the slope is stronger for older workers. In sum, we found evidence of intercept and slope differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the protected class variable (c_age). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.3.3.3 Test Differential Prediction Based on race Investigating whether differential prediction exists for c_interview in relation to perf_eval based on moderator variable race will unfold in the much the same way as the dichotomous gender variable from above; however, we will filter the race variable within the regression functions to focus on just comparing the Asian and White individuals. The decision to focus on these two subgroups, specifically, is arbitrary and just for the sake of demonstration. Note that in the DiffPred data frame object that both asian and white are lowercase category labels in the race variable. Step One: Regress the criterion variable perf_eval on the selection tool variable c_interview. Add the subset=(race==\"asian\" | race==\"white\") argument to subset the data such that only those individuals who are Asian and White are retained. In doing so, we effectively dichotomize the race variable within the function. If you need to review conditional/logical statements when filtering data, check out the chapter on filtering data. # Regress perf_eval (criterion) on c_interview (selection tool) model.1 &lt;- lm(perf_eval ~ c_interview, data=DiffPred, subset=(race==&quot;asian&quot; | race==&quot;white&quot;)) summary(model.1) ## ## Call: ## lm(formula = perf_eval ~ c_interview, data = DiffPred, subset = (race == ## &quot;asian&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.78941 -0.60097 -0.04034 0.58952 2.75581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.75118 0.05071 93.696 &lt; 0.0000000000000002 *** ## c_interview 0.34840 0.04286 8.128 0.00000000000000796 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9395 on 344 degrees of freedom ## Multiple R-squared: 0.1611, Adjusted R-squared: 0.1587 ## F-statistic: 66.06 on 1 and 344 DF, p-value: 0.000000000000007962 The regression coefficient for c_interview is statistically significant and positive (b = .348, p &lt; .001), which provides evidence of criterion-related validity. The amount of variance explained by c_interview in perf_eval is medium-large in magnitude (R2 = .161, adjusted R2 = .159, p &lt; .001). Step Two: Regress the criterion variable perf_eval on the selection tool variable c_interview and the protected class variable race. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool) and protected class (race) model.2 &lt;- lm(perf_eval ~ c_interview + race, data=DiffPred, subset=(race==&quot;asian&quot; | race==&quot;white&quot;)) summary(model.2) ## ## Call: ## lm(formula = perf_eval ~ c_interview + race, data = DiffPred, ## subset = (race == &quot;asian&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.76671 -0.61101 -0.04633 0.61482 2.67395 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.91778 0.06951 70.754 &lt; 0.0000000000000002 *** ## c_interview 0.40664 0.04546 8.945 &lt; 0.0000000000000002 *** ## racewhite -0.37273 0.10819 -3.445 0.000642 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.925 on 343 degrees of freedom ## Multiple R-squared: 0.1892, Adjusted R-squared: 0.1844 ## F-statistic: 40.01 on 2 and 343 DF, p-value: 0.000000000000000241 # Nested model comparison anova(model.1, model.2) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview ## Model 2: perf_eval ~ c_interview + race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 344 303.66 ## 2 343 293.50 1 10.156 11.869 0.0006415 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Change in R-squared summary(model.2)$r.squared - summary(model.1)$r.squared ## [1] 0.0280581 The regression coefficient associated with race (or more specifically racewhite) is negative and statistically signficant (b = -.373, p = .001) when controlling for c_interview, which indicates that there is evidence of intercept differences. Because the coefficient is negative and because white is appended to the race variable name, it implies that the intercept value is significantly lower for white individuals compared to asian individuals. The amount of variance explained by c_interview and race in perf_eval is medium-large in magnitude (R2 = .189, adjusted R2 = .184, p &lt; .001). The significant F-value (11.869, p = .001) from the nested model comparison indicates that adding race significantly improved model fit, and because there was a significant improvement in model fit, we should interpret the change in R2 (R2 = .028), which indicates that adding race to the model explained an additional 2.8% of the variance in perf_eval. Using the probe_interaction function from the interactions package, we will visualize the intercept differences. As input to the probe_interaction function, use the the model.2 model object from above, and specify the arguments in the probe_interaction function as you did before. # Visualize intercept differences probe_interaction(model.2, pred=c_interview, modx=race, johnson_neyman=FALSE, x.label=&quot;Interview Score&quot;, y.label=&quot;Job Performance&quot;) ## SIMPLE SLOPES ANALYSIS ## ## Slope of c_interview when race = white: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.41 0.05 8.94 0.00 ## ## Slope of c_interview when race = asian: ## ## Est. S.E. t val. p ## ------ ------ -------- ------ ## 0.41 0.05 8.94 0.00 Step Three: Now add the interaction term between c_interview and race to the model using the * symbol. In addition, do a nested model comparison, and estimate the change in R2. # Regress perf_eval (criterion) on c_interview (selection tool), protected class (race), # and interaction between selection tool and protected class model.3 &lt;- lm(perf_eval ~ c_interview * race, data=DiffPred, subset=(race==&quot;asian&quot; | race==&quot;white&quot;)) summary(model.3) ## ## Call: ## lm(formula = perf_eval ~ c_interview * race, data = DiffPred, ## subset = (race == &quot;asian&quot; | race == &quot;white&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.80582 -0.60528 -0.06719 0.57676 2.63035 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.89594 0.07214 67.863 &lt; 0.0000000000000002 *** ## c_interview 0.36167 0.06054 5.974 0.00000000582 *** ## racewhite -0.37405 0.10815 -3.459 0.000612 *** ## c_interview:racewhite 0.10301 0.09163 1.124 0.261728 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9247 on 342 degrees of freedom ## Multiple R-squared: 0.1922, Adjusted R-squared: 0.1851 ## F-statistic: 27.12 on 3 and 342 DF, p-value: 0.0000000000000009343 # Nested model comparison anova(model.2, model.3) ## Analysis of Variance Table ## ## Model 1: perf_eval ~ c_interview + race ## Model 2: perf_eval ~ c_interview * race ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 343 293.50 ## 2 342 292.42 1 1.0806 1.2638 0.2617 # Change in R-squared summary(model.3)$r.squared - summary(model.2)$r.squared ## [1] 0.002985186 The regression coefficient associated with the interaction term (c_interview:racewhite) is not statistically signficant (b = .103, p = .262), which indicates that there is no evidence of slope differences. Further, adding the interaction term did not significantly improve model fit, so we wont interpret change in R2. Given all this, we wont probe the interaction because, well, there is not an interaction to probe. In sum, we found evidence of intercept differences for the selection tool (c_interview) in relation to the criterion (perf_eval) that were conditional upon the protected class variable (race). Thus, it would be inappropriate to use a common regression line when predicting scores on the criterion based on the interview selection tool. 42.3.4 APA-Style Results Table If you want to present the results of your moderated multiple linear regression (MMLR) model to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. Using the lm function from base R, as we did above, lets begin by estimating a MMLR model and naming the model object (model.3). # Estimate multiple linear regression model model.3 &lt;- lm(perf_eval ~ c_interview * gender, data=DiffPred) If you havent already, install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.reg.table function from apaTables is pretty straightforward. Simply enter your regression model object (model.3) as the sole parenthetical argument. This will generate a table as output in your Console. # Create APA-style regression table apa.reg.table(model.3) ## ## ## Regression results using perf_eval as the criterion ## ## ## Predictor b b_95%_CI sr2 sr2_95%_CI Fit ## (Intercept) 5.26** [5.04, 5.48] ## c_interview -0.02 [-0.19, 0.14] .00 [-.00, .00] ## genderwoman -0.72** [-0.98, -0.47] .07 [.02, .11] ## c_interview:genderwoman 0.28** [0.08, 0.49] .02 [-.01, .04] ## R2 = .198** ## 95% CI[.13,.26] ## ## ## Note. A significant b-weight indicates the semi-partial correlation is also significant. ## b represents unstandardized regression weights. ## sr2 represents the semi-partial correlation squared. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If we add a filename= as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file APA Multiple Linear Regression Table.doc. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner. # Create APA-style regression table and write to working directory apa.reg.table(model.3, filename=&quot;APA Moderated Multiple Linear Regression Table.doc&quot;) ## ## ## Regression results using perf_eval as the criterion ## ## ## Predictor b b_95%_CI sr2 sr2_95%_CI Fit ## (Intercept) 5.26** [5.04, 5.48] ## c_interview -0.02 [-0.19, 0.14] .00 [-.00, .00] ## genderwoman -0.72** [-0.98, -0.47] .07 [.02, .11] ## c_interview:genderwoman 0.28** [0.08, 0.49] .02 [-.01, .04] ## R2 = .198** ## 95% CI[.13,.26] ## ## ## Note. A significant b-weight indicates the semi-partial correlation is also significant. ## b represents unstandardized regression weights. ## sr2 represents the semi-partial correlation squared. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The apa.reg.table function from the apaTables package can table moderated multiple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise. "],["crossvalidation.html", "Chapter 43 Statistically &amp; Empirically Cross-Validating a Selection Tool 43.1 Conceptual Overview 43.2 Tutorial", " Chapter 43 Statistically &amp; Empirically Cross-Validating a Selection Tool In this chapter, we will learn how to perform both statistical and empirical cross-validation, and we will do so in the context of validating an employee selection tool by estimating a multiple linear regression model. For a review of multiple linear regression, please refer to the chapter on estimating incremental validity. 43.1 Conceptual Overview Cross-validation is a process through which we determine how well a regression model estimated using a sample from drawn from a particular population generalizes to another sample from that same population. More specifically, the term cross-validity refers to whether the [regression coefficients] derived from one sample can predict outcomes to the same degree in the population as a whole or in other samples drawn from the same population (Cascio and Aguinis 2005, 173). At its core, cross-validation represents a step toward true predictive analytics, as it involves training our data on one sample and then testing our data on one or more other samples from that population. In general, we can distinguish between two approaches to cross-validation: statistical and empirical. 43.1.1 Review of Statistical Cross-Validation Statistical cross-validation is performed without collecting additional data from the underlying population from which the original regression model was estimated. There are a couple of different statistical indicators of a models cross-validity, which can be used in tandem to inform judgments of regarding cross-validity: adjusted R2 and squared cross-validity (Raju et al. 1997). The adjusted R2 (i.e., \\(R^2_{adj}\\), squared multiple correlation) (Ezekiel 1930; Wherry 1931) appears in most statistical platforms regression output (including many R functions), and it provides a more accurate estimate of the amount of variance explained by the predictor variable(s) in the outcome variable than the unadjusted R2 (\\(R^2_{unadj}\\)). Specifically, the \\(R^2_{adj}\\) corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The \\(R^2_{adj}\\) is a better indicator of the magnitude of the association in the underlying population than the \\(R^2_{unadj}\\). The formula for \\(R^2_{adj}\\) is: \\(R^2_{adj} = \\frac{N-1}{N-k-1}(1-R^2_{unadj})\\) where \\(N\\) refers to the sample size, \\(k\\) refers to the number of predictor variables in the model, and \\(R^2_{unadj}\\) is the unadjusted R2, which reflects the sample-specific variance explained by the predictor variables in relation to the outcome variable. Again, this \\(R^2_{adj}\\) is readily produced by most regression functions, so additional steps are not required to compute it. In contrast, the squared cross-validity (\\(\\rho^2_{c}\\)) (Browne 1975) often requires additional computation beyond what our regression function computes. As you can see in the formula below, the \\(\\rho^2_{c}\\) formula uses the \\(R^2_{adj}\\) as input. \\(\\rho^2_{c} = \\frac{(N-k-3)(R^2_{adj})^2+R^2_{adj}}{(N-2k-2)(R^2_{adj})+k}\\) Both \\(R^2_{adj}\\) and \\(\\rho^2_{c}\\) can be used inform our understanding of a models squared cross-validity; however, preference should be given to the \\(\\rho^2_{c}\\), as \\(R^2_{adj}\\) by itself tends to overestimate the squared cross-validity. If we take the square root of \\(\\rho^2_{c}\\), we get an estimate of cross-validity (\\(\\rho_{c}\\)). 43.1.2 Review of Empirical Cross-Validation Empirical cross-validation requires that we sample additional data from the same population, or that we split our original sample (if its large enough) into subsamples, where the subsamples are sometimes referred to as holdout samples. Regardless of which approach we use, the logic is similar: One (sub)sample serves as our training (i.e., initial, original, derivation) data, and the other (sub)sample serves as our test (i.e., cross-validation) data. Using either approach, we estimate the regression model from the first (sub)sample and then estimate the extent to which the model fits the second (sub)sample as well as its and predictive accuracy. 43.1.2.1 Sample Write-Up We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we first performed statistical cross-validation. Specifically, we found that the adjusted R2 was .348 and the squared cross-validity (\\(\\rho^2_{c}\\)) was .341. The latter indicates that in the population, the three selection tools collectively explain 34.1% of the variability in the criterion. Next, we performed empirical cross-validation by assessing the models performance in a new same of data drawn from the sample population. The squared cross-validity (\\(\\rho^2_{c}\\)) was .341, which indicates that the model explained 34.1% of the variance in the criterion when applied to the second sample, which was only a slight decrease in performance compared to the original sample (\\(R^2_{unadj}\\) = .359, \\(R^2_{adj}\\) = .348). Further, estimates of root mean square error (RMSE) indicated that the RMSE for the second sample is 13.6% larger than the RMSE for the first sample, which indicates that prediction errors didnt increase substantially when applying the model to the second sample. Overall, the model performed relatively well applied to a second sample of data, and it doesnt appear as though we over fit the model when estimating it using the first sample. 43.2 Tutorial This chapters tutorial demonstrates how to perform statistical and empirical cross-validation using R. 43.2.1 Functions &amp; Packages Introduced Function Package lm base R summary base R sqrt base R predict base R R2 caret mean base R RMSE caret abs base R MAE caret cbind base R 43.2.2 Initial Steps If you havent already, save the files called Sample1.csv and Sample2.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data files for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data files called Sample1.csv and Sample2.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) objects Sample1 &lt;- read_csv(&quot;Sample1.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## PerfEval = col_double(), ## WorkSample = col_double(), ## SJT = col_double(), ## Consc = col_double() ## ) Sample2 &lt;- read_csv(&quot;Sample2.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## PerfEval = col_double(), ## WorkSample = col_double(), ## SJT = col_double(), ## Consc = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(Sample1) ## [1] &quot;PerfEval&quot; &quot;WorkSample&quot; &quot;SJT&quot; &quot;Consc&quot; names(Sample2) ## [1] &quot;PerfEval&quot; &quot;WorkSample&quot; &quot;SJT&quot; &quot;Consc&quot; # View number of rows/cases in data frame (tibble) objects nrow(Sample1) ## [1] 182 nrow(Sample2) ## [1] 166 # View variable type for each variable in data frame (tibble) objects str(Sample1) ## spec_tbl_df[,4] [182 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ PerfEval : num [1:182] 5.1 5 3.5 5 4 4.4 5.2 5.1 3.8 3.1 ... ## $ WorkSample: num [1:182] 7.5 8.1 7.6 6.6 6 6 8 8.8 6.1 6.5 ... ## $ SJT : num [1:182] 7.9 5.6 4.4 5.7 8.5 5.8 6.1 4.7 4.5 5.1 ... ## $ Consc : num [1:182] 4.9 2.4 3.1 4.5 3.6 4.8 4.7 4.9 3.2 4.6 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. PerfEval = col_double(), ## .. WorkSample = col_double(), ## .. SJT = col_double(), ## .. Consc = col_double() ## .. ) str(Sample2) ## spec_tbl_df[,4] [166 x 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ PerfEval : num [1:166] 4 4.3 3.9 5.6 4.5 4.9 4.2 4.7 4.5 5.6 ... ## $ WorkSample: num [1:166] 6.7 9 6.8 7.6 4.5 8.1 6.5 7.1 6.1 5.3 ... ## $ SJT : num [1:166] 4.4 3.7 5.6 6.8 6.9 7.4 6.4 7 7.2 10 ... ## $ Consc : num [1:166] 4.3 1.7 4.5 5.2 3.8 4.3 3.7 5.1 4.1 4.3 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. PerfEval = col_double(), ## .. WorkSample = col_double(), ## .. SJT = col_double(), ## .. Consc = col_double() ## .. ) # View first 6 rows of data frame (tibble) objects head(Sample1) ## # A tibble: 6 x 4 ## PerfEval WorkSample SJT Consc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 7.5 7.9 4.9 ## 2 5 8.1 5.6 2.4 ## 3 3.5 7.6 4.4 3.1 ## 4 5 6.6 5.7 4.5 ## 5 4 6 8.5 3.6 ## 6 4.4 6 5.8 4.8 head(Sample2) ## # A tibble: 6 x 4 ## PerfEval WorkSample SJT Consc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 6.7 4.4 4.3 ## 2 4.3 9 3.7 1.7 ## 3 3.9 6.8 5.6 4.5 ## 4 5.6 7.6 6.8 5.2 ## 5 4.5 4.5 6.9 3.8 ## 6 4.9 8.1 7.4 4.3 The first data frame (Sample1) has 182 cases and the following 4 variables: PerfEval, WorkSample, SJT, and Consc. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). Imagine that the Sample1 data were collected as part of a criterion-related validation study - specifically, a concurrent validation design in which job incumbents were administered the three selection tools 90 days after entering the organization. PerfEval is the criterion (outcome) of interest, and it is a 90-day-post-hire measure of supervisor-rated job performance, where scores could range from 1 (low performance) to 7 (high performance). The WorkSample variable contains scores from a work sample assessment, and scores could range from 1-10, with 10 indicating a strong performance on the assessment. SJT refers to a situational judgment test in which scores can range from 1-10, with 10 indicating higher proficiency. Consc refers to an a conscientiousness personality inventory in which scores can range from 1-7, with 7 indicating higher conscientiousness. Proactivity refers to a proactive personality inventory in which scores can range from 1-15, with 15 indicating higher proactive personality. 43.2.3 Perform Statistical Cross-Validation In order to perform statistical cross-validation, we first need to estimate our model. In this chapter, well estimate a multiple linear regression in which we treat the three selection tool variables (WorkSample, SJT, Consc) as predictors and the variable PerfEval as our criterion (outcome). In other words, we are estimating the criterion-related and incremental validity of these three selection tools in relation to the outcome. In doing so, we will determine whether each selection to explains unique variance in the outcome, and how much variance, collectively, all three selection tools explain in the outcome. For more information on multiple linear regression, check out the chapter on estimating incremental validity. Note that in this chapter we will use the lm function from base R to estimate our multiple linear regression model, which is discussed in the chapter supplement to the aforementioned chapter. # Estimate multiple linear regression model model1 &lt;- lm(PerfEval ~ WorkSample + SJT + Consc, data=Sample1) # Print summary of results summary(model1) ## ## Call: ## lm(formula = PerfEval ~ WorkSample + SJT + Consc, data = Sample1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62345 -0.39876 0.01343 0.32258 1.51027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.30283 0.34833 3.740 0.000248 *** ## WorkSample 0.28430 0.03854 7.376 0.00000000000596 *** ## SJT 0.08118 0.03082 2.634 0.009190 ** ## Consc 0.23241 0.04652 4.996 0.00000139215819 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.589 on 178 degrees of freedom ## Multiple R-squared: 0.3592, Adjusted R-squared: 0.3484 ## F-statistic: 33.25 on 3 and 178 DF, p-value: &lt; 0.00000000000000022 In the output, you can see that each of the regression coefficients associated with the predictor variables (i.e., selection tools) are statistically significant, which indicates that they each explain unique variance in the outcome (PerfEval) when controlling for the predictors in the model. Further, the \\(R^2_{unadj}\\) (unadjusted R2) is .359, and the \\(R^2_{adj}\\) (adjusted R2) is .348, both of which provide us with estimates of model fit and the amount of variance explained in the outcome variable collectively by the three predictor variables. Next, lets define a function that will compute the squared cross-validity (\\(\\rho^2_{c}\\)) based on the Browne (1975) formula when we type the name of a regression model object as the sole argument. Lets name this function rho2 for \\(\\rho^2_{c}\\). As a reminder, the Browne formula is: \\(\\rho^2_{c} = \\frac{(N-k-3)(R^2_{adj})^2+R^2_{adj}}{(N-2k-2)(R^2_{adj})+k}\\) You dont need to change anything in the function script below. Simply run it as is. # Write function for squared cross-validity formula (Browne, 1975) rho2 &lt;- function(x) { N &lt;- nobs(model1) # model sample size k &lt;- length(coef(model1)) - 1 # number of predictor variables R2_adj &lt;- summary(model1)$adj.r.squared # adjusted R-squared ((N - k - 3) * (R2_adj^2) + R2_adj) / ((N - (2 * k) - 2) * R2_adj + k) # Browne formula } Using the new function called rho2, type the name of our regression model object as the sole parenthetical argument. # Estimate squared cross-validity rho2(model1) ## [1] 0.3412245 The output indicates that the squared cross-validity (\\(\\rho^2_{c}\\)) is .341, which indicates that, in the population, the predictor variables included in the model explain 34.1% of the variability in the outcome variable. If you recall, the \\(R^2_{unadj}\\) was .359, and the \\(R^2_{adj}\\) was .348. Thus, compared to \\(R^2_{unadj}\\), the model fit didnt shrink too much. To estimate the cross-validity (\\(\\rho_{c}\\)), just take the square root of (\\(\\rho^2_{c}\\)). # Estimate cross-validity sqrt(rho2(model1)) ## [1] 0.5841442 Thus, our cross-validity (multiple correlation) is .584. Technical Write-Up: We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we performed statistical cross-validation. Specifically, we found that the adjusted R2 was .348 and the squared cross-validity (\\(\\rho^2_{c}\\)) was .341. The latter indicates that in the population, the three selection tools collectively explain 34.1% of the variability in the criterion. 43.2.4 Perform Empirical Cross-Validation There are different ways that we can go about performing empirical cross-validation, but regardless of the approach, we are entering the realm of predictive analytics. Why? Well, we are testing the model fit and predictive accuracy of our model estimated using one sample in a different sample from the same population. To get things started, lets estimate our multiple linear regression model that contains the three selection tool variables (WorkSample, SJT, Consc) as predictors and PerfEval as the outcome (criterion) variable. We will estimate our model using the first sample data frame, which we named Sample1 above. # Estimate multiple linear regression model model1 &lt;- lm(PerfEval ~ WorkSample + SJT + Consc, data=Sample1) # Print summary of results summary(model1) ## ## Call: ## lm(formula = PerfEval ~ WorkSample + SJT + Consc, data = Sample1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.62345 -0.39876 0.01343 0.32258 1.51027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.30283 0.34833 3.740 0.000248 *** ## WorkSample 0.28430 0.03854 7.376 0.00000000000596 *** ## SJT 0.08118 0.03082 2.634 0.009190 ** ## Consc 0.23241 0.04652 4.996 0.00000139215819 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.589 on 178 degrees of freedom ## Multiple R-squared: 0.3592, Adjusted R-squared: 0.3484 ## F-statistic: 33.25 on 3 and 178 DF, p-value: &lt; 0.00000000000000022 In the output, you can see that each of the regression coefficients associated with the predictor variables (i.e., selection tools) are statistically significant, which indicates that they each explain unique variance in the outcome (PerfEval) when controlling for the predictors in the model. Further, the \\(R^2_{unadj}\\) (unadjusted R2) is .359, and the \\(R^2_{adj}\\) (adjusted R2) is .348, both of which provide us with estimates of model fit and the amount of variance explained in the outcome variable collectively by the three predictor variables. Lets kick things up a notch by applying our model that we named model1 to a new data frame called (Sample2). Using the predict function from base R, enter the name of the model object as the first argument and the name of the second data frame object as the second argument. Lets assign these predicted values using the &lt;- assignment operator to an object called predictions. The predict function is using our regression model equation from the first sample to estimate the fitted (predicted) values of the outcome variable in the second sample based on the values for the different predictor variables. # Predict values on outcome using second sample predictions &lt;- predict(model1, Sample2) Next, we will evaluate the quality of our models fit to the new dataset (Sample2). Well use several functions from the caret package, so be sure to install and access that package if you havent already. The caret package is renowned for its predictive modeling and machine learning functions. # Install caret package if you haven&#39;t already install.packages(&quot;caret&quot;) # Access caret package library(caret) For comparison, first, lets extract the unadjusted r.squared value from the our regression model summary by appending r.squared to the end of summary(model1) using the $ operator. Second, using the R2 function from the caret package, enter the name of the predictions object we created above as the first argument, and as the second argument, enter the name of the second data frame (Sample2), followed by the the $ operator and the name of the outcome variable (PerfEval). The first operation will extract our \\(R^2_{unadj}\\) from the original sample, and the second operation will estimate our \\(\\rho^2_{c}\\) based on our new model and its performance in the second sample. # Unadjusted R-squared based on the first sample summary(model1)$r.squared ## [1] 0.359162 # Adjusted R-squared based on the first sample summary(model1)$adj.r.squared ## [1] 0.3483614 # Squared cross-validity based on the second sample R2(predictions, Sample2$PerfEval) ## [1] 0.3412069 As one would expect, the \\(\\rho^2_{c}\\) is slightly lower the \\(R^2_{unadj}\\) when the model is applied to new data, but fortunately, it doesnt decrease that much. Further, the \\(\\rho^2_{c}\\) is only slightly lower than the \\(R^2_{adj}\\). This is our first indication that the model performed fairly well with the new data from the population. To calculate the multiple R and cross-validity (\\(\\rho_c\\)) values, we simply take the square root (sqrt) of the two values we calculated above. # Multiple R for first sample sqrt(summary(model1)$r.squared) ## [1] 0.5993013 # Cross-validity for second sample sqrt(R2(predictions, Sample2$PerfEval)) ## [1] 0.5841292 Similarly, the multiple R and cross-validity (\\(\\rho_c\\)) values are comparable, with the cross-validity value only slightly lower than the multiple R value. The mean squared error (MSE) is another way that we can test how well our model performed. As the label implies, the MSE is the mean of the error (residual) values squared. First, lets estimate the MSE for the initial sample, which reflects the extent to which the fitted (predicted) values based on the model deviate from the actual outcome variables in the initial sample (Sample1) on which the model was estimated. To estimate the MSE for the model based on the initial sample, we extract the residuals from the model (model1), square them, and then take the mean using the mean function from base R. Second, lets estimate the MSE for the second sample based on the extent to which the fitted (predicted) values from our model (model1) deviate from the actual outcome variables in the second sample (Sample2). To estimate the MSE for the model (model1) in relation to the second sample, we subtract the predicted values (predictions) from the outcome variable (PerfEval) values for the second sample (Sample2), square the difference, and then compute the mean using the mean function from base R. # MSE (Mean Squared Error) for first sample mean(model1$residuals^2) ## [1] 0.3393501 # MSE (Mean Squared Error) for second sample mean((Sample2$PerfEval - predictions)^2) ## [1] 0.4381229 In isolation, each MSE value does not mean much; however, each MSE value takes on mean when we compare it to another MSE value. As expected, the MSE is higher when we apply the model (model1) estimated from the first sample (Sample1) to the second sample (Sample2). With that said, we dont want our MSE in the second, validation sample to be too much higher than the original, test sample. Often, however, we opt for the root mean squared error for evaluating the quality of our models fit because it is in the same units as the outcome variable. To calculate the root mean squared error (RMSE), we simply take the square root of the MSE. For our predicted (fitted) values based on our second, validation sample, we can simply apply the RMSE function from the caret package; as the first argument, type the name of the predictions object we created above, and as the second argument, type the name of the second sample (Sample2), followed by the $ operator and the name of the outcome variable (PerfEval). # RMSE (Root Mean Squared Error) for first sample sqrt(mean(model1$residuals^2)) ## [1] 0.5825376 # RMSE (Root Mean Squared Error) for second sample RMSE(predictions, Sample2$PerfEval) ## [1] 0.6619085 Again, we expect to see that the RMSE for our model (model1) when applied to the second sample to be larger, as we expect larger errors in prediction when we apply our model to new data. That said, we dont want the differences in RMSE to be too large. But how much is too much you might ask? Good question, and there isnt a great answer. What is considered a large difference depends in part on the context and what youre predicting. That said, its sometimes helpful to calculate how much larger one RMSE is than the other. Below, I simply calculate how much larger (as a percentage) is the RMSE from the second sample than the first sample. # Percentage difference in RMSE (Root Mean Squared Error) values (RMSE(predictions, Sample2$PerfEval) - sqrt(mean(model1$residuals^2))) / sqrt(mean(model1$residuals^2)) * 100 ## [1] 13.62503 We see that the RMSE for the second, validation sample is 13.6% larger than the RMSE for the first, test sample, which is actually pretty good. If it were me, I would allow my concern to grow when that percentage difference hits 30% or higher, but thats not a strict rule. The mean absolute error (MAE) is another common indicator of a models fit to data. The MAE is just the average of the errors (residuals) absolute values. We want the difference between MAE values to be as small as possible. # MAE (Mean Absolute Error) for first sample mean(abs(model1$residuals)) ## [1] 0.4535265 # MAE (Mean Absolute Error) for second sample MAE(predictions, Sample2$PerfEval) ## [1] 0.5213717 As we did with the RMSE values, lets calculate how much larger the MAE is when the model is applied to the second sample than the first sample. # Percentage difference in MAE (Mean Absolute Error) values (MAE(predictions, Sample2$PerfEval) - mean(abs(model1$residuals))) / mean(abs(model1$residuals)) * 100 ## [1] 14.95948 We see that the MAE for the second, validation sample is 15.0% larger than the MAE for the first, test sample, which again is actually pretty good. Finally, lets estimate the 95% prediction intervals around each fitted (predicted) value based on our model (model1) when applied to the second, validation sample (Sample2). We will use the predict function from base R. As the first argument, enter the name of your model object (model1). As the second argument, type newdata= followed by the name of the data frame associated with the second sample (Sample2). As the third argument, type interval=\"prediction\" to indicate that you want to estimate prediction intervals; the default is the 95% prediction interval. Lets assign the prediction interval and fitted values to an object called pred.int so that we can append those vectors to the second sample data frame, which we do in the following step using the cbind function from base R. Use the head function from base R to view the first 6 rows of your updated Sample2 data frame and to marvel at your work. # Estimate 95% prediction intervals pred.int &lt;- predict(model1, newdata=Sample2, interval=&quot;prediction&quot;) # Join fitted (predicted) values and upper and lower prediction interval values to data frame Sample2 &lt;- cbind(Sample2, pred.int) # View first 6 rows head(Sample2) ## PerfEval WorkSample SJT Consc fit lwr upr ## 1 4.0 6.7 4.4 4.3 4.564170 3.396242 5.732099 ## 2 4.3 9.0 3.7 1.7 4.556964 3.349621 5.764306 ## 3 3.9 6.8 5.6 4.5 4.736498 3.570208 5.902788 ## 4 5.6 7.6 6.8 5.2 5.224037 4.051437 6.396638 ## 5 4.5 4.5 6.9 3.8 4.025462 2.840158 5.210766 ## 6 4.9 8.1 7.4 4.3 5.205726 4.032809 6.378642 Unlike the confidence interval, the prediction interval is specific to each value and represents uncertainty around specific values of the outcome. In contrast, the confidence interval represents uncertainty around the point estimate of a regression coefficient associated with the relation between a predictor variable and the outcome variable. Thus, there is a single confidence interval for a regression coefficient, but there is a different prediction interval for each fitted (predicted) value of the outcome variable. Technical Write-Up: We performed a concurrent validation study based on a sample 182 job incumbents. Given that all three selection tools (i.e., conscientiousness inventory, work sample, situational judgment test) showed evidence of incremental validity, we included all three in a multiple linear regression model, with performance evaluation scores as the criterion. All three selection tools showed evidence of incremental validity with respect to one another. To understand how well this estimated model might generalize to the broader population, we performed empirical cross-validation by assessing the fit and predictive accuracy of the model in a second sample drawn from the sample population. The squared cross-validity (\\(\\rho^2_{c}\\)) was .341, which indicates that the model explained 34.1% of the variance in the criterion when applied to the second sample, which was only a slight decrease in performance compared to the original sample (\\(R^2_{unadj}\\) = .359, \\(R^2_{adj}\\) = .348). Further, estimates of root mean square error (RMSE) indicated that the RMSE for the second sample is 13.6% larger than the RMSE for the first sample, which indicates that were prediction errors didnt increase substantially when applying the model to the second sample. Overall, the model performed relatively well applied to a second sample of data, and it doesnt appear as though we overfit the model when estimating it using the first sample. 43.2.5 Summary In this chapter, we learned how to statistically and empirically cross-validate a regression model. To perform statistical cross-validation, we estimated the cross-validity based on the R-squared value, sample size, and number of predictor variables in the model. To perform empirical cross-validation, we fit our regression model using one sample, and then estimated how well that model fit data from a second sample. "],["turnover.html", "Chapter 44 Introduction to Employee Separation &amp; Retention", " Chapter 44 Introduction to Employee Separation &amp; Retention Link to lecture video: https://youtu.be/I-kn24DQITI "],["turnoverrate.html", "Chapter 45 Computing Monthly &amp; Annual Turnover Rates 45.1 Conceptual Overview 45.2 Tutorial", " Chapter 45 Computing Monthly &amp; Annual Turnover Rates In this chapter, we will learn how to compute monthly and annual turnover rates. 45.1 Conceptual Overview The turnover rate is a specific type of human resource (HR) metric, and allows us to describe past turnover in the organization relative to the total number of employees. In other words, the turnover rate is a specific type of descriptive analytics. A monthly turnover rate refers to the turnover rate for a given month. There are various considerations one can entertain when computing monthly turnover rates, such as how broadly to define separations. That is, do we consider all forms of separations or, perhaps, just voluntary turnover. In this chapter, we will focus on a very broad and simple conceptualization and approach to computing monthly turnover rates, but please note that there are other variations. We will focus on all separations that took place in the organization (i.e., both involuntary and voluntary turnover) and coarsely account for the average number of employees in a given month. Specifically, we will use the following formula to compute a monthly turnover rate: \\(TurnoverRate_{Monthly} = \\frac{Separations_{Total}}{Employees_{Average}} \\times 100\\) where \\(Separations_{Total}\\) refers to the total number of employee separations in a given month, and \\(Employees_{Average}\\) refers to the average number of employees who were working in a given month. Note that we multiply the initial fraction by 100 to convert the monthly turnover rate from a proportion to a percentage. An annual turnover rate refers to the turnover rate for a given year. Like monthly turnover rates, there are various considerations one can entertain when computing annual turnover rates, and in this tutorial we will keep things very simple by assuming that we are working with just one years worth of data and that we have access to separations data for all 12 months within that year. There are other ways in which we could compute annual turnover rates, but for the sake of learning, we will choose what is perhaps the most straight forward approach. Specifically, we will use the following formula to compute an annual turnover rate: \\(TurnoverRate_{Annual} = \\frac{Separations_{YearSumTotal}}{Employees_{AverageAcrossMonths}} \\times 100\\) where \\(Separations_{YearSumTotal}\\) refers to the sum total of employee separations for the year, and \\(Employees_{AverageAcrossMonths}\\) refers to the average number of employees who were working across the 12 months in the year. Note that we multiply the initial fraction by 100 to convert the annual turnover rate from a proportion to a percentage. 45.2 Tutorial This chapters tutorial demonstrates how to compute both monthly and annual turnover rates in R. 45.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/StdNp94V9Pk 45.2.2 Functions &amp; Packages Introduced Function Package sum base R mean base R plot base R 45.2.3 Initial Steps If you havent already, save the file called turnover_rate.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called turnover_rate.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object tr &lt;- read_csv(&quot;turnover_rate.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## Month = col_double(), ## Separations = col_double(), ## Ave_Employees = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(tr) ## [1] &quot;Month&quot; &quot;Separations&quot; &quot;Ave_Employees&quot; # Print number of rows in data frame (tibble) object nrow(tr) ## [1] 12 # Print data frame (tibble) object print(tr) ## # A tibble: 12 x 3 ## Month Separations Ave_Employees ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12 127 ## 2 2 9 107 ## 3 3 6 119 ## 4 4 4 127 ## 5 5 2 113 ## 6 6 3 107 ## 7 7 5 110 ## 8 8 1 125 ## 9 9 6 116 ## 10 10 7 124 ## 11 11 4 117 ## 12 12 3 127 The data frame object we named tr contains three variables: Month, Separations, and Ave_Employees. The Month variable includes 12 unique values, which correspond to the 12 months of the year, where 1 refers to January, 2 refers to February, 3 refers to March, and so on. The Separations variable includes the total number of separations (i.e., number of individuals who turned over) from the organization in a given month. The Ave_Employees variable includes the average number of employees who worked in the organization in a given month; for this example, lets assume that the average number of employees was calculated by adding together the number of employees at the beginning of the month and the number of employees at the end of the month, and then dividing that sum by 2. 45.2.4 Compute Monthly Turnover Rates When computing monthly turnover rates, we will create a new variable containing monthly turnover rates. To calculate the turnover rate for each month, we will use simple arithmetic by dividing the number of separations for the month divided by the average number of employees who worked in the organization that month; we can then multiply the resulting quotient by 100 to convert the proportion to a percentage. Because we are applying these arithmetic operations row by row (because in these data each row represents a unique month), our task will be relatively simple. To illustrate how the underlying math and the logic of the operations, lets break this process of computing monthly turnover rates into three steps. If youre feeling confident and already understand the logic, you can skip directly to the third step, as that is what were building up to. As the first step, lets compute the monthly turnover rates as proportions and print them directly to our console. In other words, we wont create a new variable just yet; rather, we will just peak at the monthly turnover rates (as proportions) as we get comfortable. Type in the name of the data frame object (tr) followed by the $ operator and the name of the separations variable (Separations); note that the $ operator is used to indicate which data frame object a variable belongs to. Next, type in the division symbol (/). After that, type in the name of the data frame object (tr) followed by the $ operator and the name of the average number of employees variable (Ave_Employees). Run that line of code. # Compute monthly turnover rates (as proportions) and print to the Console tr$Separations / tr$Ave_Employees ## [1] 0.09448819 0.08411215 0.05042017 0.03149606 0.01769912 0.02803738 0.04545455 0.00800000 0.05172414 ## [10] 0.05645161 0.03418803 0.02362205 In our console, we should see a vector of the monthly turnover rates as proportions. As the second step, if we wish to view the monthly turnover rates as percentages, we just need to multiply the quotient from above by 100. In the code below, I include parentheses around the chunk of code used to compute the proportions, as I think its easier to conceptualize what were doing. That being said, because we are working with division and multiplication here, mathematical orders of operation do not require the parentheses. Its up to you whether you decide to include them. # Compute monthly turnover rates (as percentages) and print to the Console (tr$Separations / tr$Ave_Employees) * 100 ## [1] 9.448819 8.411215 5.042017 3.149606 1.769912 2.803738 4.545455 0.800000 5.172414 5.645161 3.418803 ## [12] 2.362205 In our console, we should see a vector of monthly turnover rates as percentages. As the third and final step, we will assign the vector of monthly turnover rate percentages we created above to a new variable in our data frame object. After you get the hang of computing monthly turnover rates, you can skip directly to this third step instead of building up to it using the first two steps I demonstrated above. We are simply taking the equation we used to compute monthly turnover rates as percentages and using the &lt;- operator to assign the resulting vector of values to a new variable in our data frame object. You can call the new variable whatever youd like, and here I call it TR_Monthly and use the $ operator to signal that I wish to attach the vector/variable to the tr data frame object we have been working with thus far. # Compute monthly turnover rates (as percentages) and assign to variable tr$TR_Monthly &lt;- (tr$Separations / tr$Ave_Employees) * 100 To verify that we accomplished what we set out to do, lets print the tr data frame object using the print function from base R. # Print data frame object to Console print(tr) ## # A tibble: 12 x 4 ## Month Separations Ave_Employees TR_Monthly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12 127 9.45 ## 2 2 9 107 8.41 ## 3 3 6 119 5.04 ## 4 4 4 127 3.15 ## 5 5 2 113 1.77 ## 6 6 3 107 2.80 ## 7 7 5 110 4.55 ## 8 8 1 125 0.8 ## 9 9 6 116 5.17 ## 10 10 7 124 5.65 ## 11 11 4 117 3.42 ## 12 12 3 127 2.36 We now have a new variable called TR_Monthly added to our tr data frame, and this new variable contains the monthly turnover rates as percentages. For example, we can see that for month 1 (January) the monthly turnover rate in this organization was approximately 9.45%. Often interpreting an HR metric like a turnover rate requires comparing it to other other turnover rates within the same organization (e.g., between units), to other time points (e.g., January compared to February), or to other organizations within the same industry (e.g., benchmarking). So what can we observe or describe about these monthly turnover rates? Well, we could state the following: Monthly turnover rates were highest in the first month (January) but then declined through the fifth month (May), increasing through the seventh month (July), dropping quickly to the eighth month (August), and then increasing, plateauing, and declining in the remaining months of the year. Optional: Given that we have access to an organizations monthly turnover rates for an entire year, we can plot the turnover rates as a line graph to facilitate interpretation of trends. If you choose to create this optional data visualization, feel free to follow along; otherwise, you can skip down to the section called Compute Annual Turnover Rates. We will create a very simple plot using the aptly named plot function from base R. We will start with the most basic version of the line graph plot, and then add more arguments to refine it a bit. For the first argument in our plot function, lets specify the Month variable from the tr data frame object as our x-axis variable using the x= argument: x=tr$Month. As the second argument, lets specify the TR_Monthly variable (we just created) from the tr data frame object as our y-axis variable using the y= argument: y=tr$TR_Monthly. plot(x=tr$Month, y=tr$TR_Monthly) By default, the function as specified above provides us with scatter plot in our Plots window. If we wish to convert this plot to a line graph, we can add the following argument: type=\"l\". plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;) Next, we can create more meaningful x- and y-axis labels by using the xlab and ylab arguments. Note that anything we can write whatever wed like in the quotation marks (\" \"). plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;, xlab=&quot;Month&quot;, ylab=&quot;Monthly Turnover Rate (%)&quot;) Finally, if we wish to adjust the y-axis limits, we can use the ylim argument followed by = and a vector of two values, where the first value is the lower limit of the y-axis and the second value is the upper limit. Here, we create a vector containing the lower- and upper-limits of 0 and 25 using the c (combine) function from base R. plot(x=tr$Month, y=tr$TR_Monthly, type=&quot;l&quot;, xlab=&quot;Month&quot;, ylab=&quot;Monthly Turnover Rate (%)&quot;, ylim=c(0,25)) The resulting plot allows us to potentially identify and describe any trends or patterns in monthly turnover rates over the course of an entire year. Here, we can see that monthly turnover rates were highest in the first month (January) but then declined through the fifth month (May), increasing through the seventh month (July), dropping quickly to the eighth month (August), and then increasing, plateauing, and declining in the remaining months of the year. If we were to have access to data over multiple years, its possible we might begin to see some seasonality to the turnover rates. With access to more years worth of data, we could also build forecasting models (e.g., perform time-series analyses). 45.2.5 Compute Annual Turnover Rate Its time to shift gears and learn how to compute the annual turnover rate. Because we have 12 months of data from an organization for a single year, our task is relatively straight forward. When we have 12 months of turnover data for a single year, we can compute the annual turnover rate by simply summing the number of separations across the 12 months and then dividing that sum by the average number of employees across the 12 months. If we then multiply that quotient by 100, we can convert the annual turnover rate to a percentage. Because we will need to calculate the sum and average based on values within columns, we will use the sum and mean functions, respectively, from base R. As our numerator, we type the name of the sum function; as the first argument, we enter the name of our data frame object (tr) followed by the $ operator and the name of our separations variable (Separations), and as the second argument, we enter na.rm=TRUE, which would allow us to compute the sum even if there were missing data. [You dont technically need the na.rm=TRUE argument here given that we have complete data, but it can be good to get in the habit of including it for other applications and contexts.] As the denominator, we enter the name of the mean function; as the first argument, we enter the name of our data frame object (tr) followed by the $ operator and the name of our separations variable (Ave_Employees), and as the second argument, we enter na.rm=TRUE. # Compute annual turnover rate (as proportion) and print to the Console sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE) ## [1] 0.5243129 As a proportion, our annual turnover rate is approximately .5243. If we wish to convert this to a percentage, we can simply multiply our equation from above by 100. # Compute annual turnover rate (as percentage) and print to the Console (sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE)) * 100 ## [1] 52.43129 After converting our annual turnover rate to a percentage, we see the value is approximately 52.43%. Finally, if we wish to assign the annual turnover rate to an object that we can subsequently reference in other functions and operations, we simply apply the &lt;- operator. Here, Ive arbitrarily named this object TR_Annual. # Compute annual turnover rate and assign to object TR_Annual &lt;- (sum(tr$Separations, na.rm=TRUE) / mean(tr$Ave_Employees, na.rm=TRUE)) * 100 We can print this object to our Console using the print function. # Print annual turnover rate object to the Console print(TR_Annual) ## [1] 52.43129 Like the monthly turnover rates, an annual turnover rate is typically evaluated by comparing it to prior years, comparing between units, or comparing to industry benchmarks. 45.2.6 Summary In this chapter, we learned how to compute monthly and annual turnover rates. For the annual turnover rates, we learned how to apply the sum and mean functions from base R. "],["turnoverchisquare.html", "Chapter 46 Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence 46.1 Conceptual Overview 46.2 Tutorial 46.3 Chapter Supplement", " Chapter 46 Estimating the Association Between Two Categorical Variables Using Chi-Square (\\(\\chi^2\\)) Test of Independence In this chapter, we will learn how to estimate whether theoretically or practically relevant categorical (nominal, ordinal) variables are associated with employees decisions to voluntarily turn over. In doing so, we will learn how to estimate the chi-square (\\(\\chi^2\\)) test of independence. In a previous chapter, we learned how to apply the \\(\\chi^2\\) test of independence to determine whether there was evidence of disparate impact. 46.1 Conceptual Overview A chi-square (\\(\\chi^2\\)) test of independence is a type of categorical nonparametric statistical analysis that can be used to estimate the association between two categorical (nominal, ordinal) variables. As a reminder, categorical variables do not have inherent numeric values, and thus we often just describe how many cases belong to each category (i.e., counts, frequencies). It is very common to represent the association between two categorical variables as a contingency table, which is often referred to as a cross-tabulation. A \\(\\chi^2\\) test of independence is calculated by comparing the observed data in the contingency table (i.e., what we actually found in the sample) to a model in which the variables and data in another table are distributed to be independent of one another (i.e., expected data); there are other types of expected models, but we will focus on just an independent model in this review. The observed data includes the raw counts (i.e., frequencies) from the sample. The row and column marginals represent the sums of each row and column, respectively. The expected data includes the counts (i.e., frequencies) that would be expected if the two categorical variables were independent of one another (i.e., not associated). The expected values for each cell are calculated by multiplying the corresponding row and column marginals and dividing that product by the total sample size. In essence, a \\(\\chi^2\\) test of independence is used to test the null hypothesis that the two categorical variables are independent of one another (i.e., no association between them). If we reject the null hypothesis because the \\(\\chi^2\\) value relative to the degrees of freedom of the model is large enough, we conclude that the categorical variables are not in fact independent of one another but instead are contingent upon one another. In this chapter, we will focus specifically on a 2x2 \\(\\chi^2\\) test, which refers to the fact there are two categorical variables with two levels each (e.g., age: old vs. young; height: tall vs. short). As an added bonus, a 2x2 \\(\\chi^2\\) value can be converted to a phi (\\(\\phi\\)) coefficient, which can be interpreted as a Pearson product-moment correlation and thus as an effect size or indicator of practical significance. The formula for calculating \\(\\chi^2\\) is as follows: \\(\\chi^2 = \\sum_{i = 1}^{n} \\frac{(O_i - E_i)^2}{E_i}\\) where \\(O_i\\) refers to the observed values and \\(E_i\\) refers to expected values. The degrees of freedom (\\(df\\)) of a \\(\\chi^2\\) test is equal to the product of the number of levels of the first variable minus one and the number of levels of the second variable minus one. For example, a 2x2 \\(\\chi^2\\) test has 1 \\(df\\): \\((2-1)(2-1) = 1\\) 46.1.0.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; The nonoccurrences are included, where nonoccurrences refer to the instances in which an event or category does not occur. For example, when we investigate employee voluntary turnover, we often like to focus on the turnover event occurrences (i.e., quitting), but we also need to include nonoccurrences, which would be examples of the turnover event not occurring (i.e., staying). Failure to do so, could lead to misleading results. 46.1.0.2 Statistical Significance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning we reject the null hypothesis that the variables are independent. If the p-value associated with \\(\\chi^2\\) value is equal to or greater than .05 (or whatever two- or one-tailed alpha level we set), then we fail to reject the null hypothesis that the categorical variables are independent. Put differently, if the p-value associated with a \\(\\chi^2\\) value is equal to or greater than .05, we conclude that there is no association between the predictor variable and the outcome variable in the population. If we were to calculate the test by hand, then we might compare our calculated \\(\\chi^2\\) value to a table of critical values for a \\(\\chi^2\\) distribution. If our calculated value were larger than the critical value given the number of degrees of freedom (df) and the desired alpha level (i.e., significance level cutoff for p-value), we would conclude that there is evidence of an association between the categorical variables. Here is a website that lists critical values. Alternatively, we can calculate the exact p-value using statistical software if we know the exact \\(\\chi^2\\) value and the df, and most software programs compute the exact p-value automatically when estimating the \\(\\chi^2\\) test of independence. 46.1.0.3 Practical Significance By itself, the \\(\\chi^2\\) value is not an effect size; however, in the special case of a 2x2 \\(\\chi^2\\) test of independence, we can compute a phi (\\(\\phi\\)) coefficient, which can be interpreted as a Pearson product-moment correlation and thus as an effect size or indicator of practical significance. The size of a \\(\\phi\\) coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the \\(\\phi\\) values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. \\(\\phi\\) Description .10 Small .30 Medium .50 Large Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. 46.1.0.4 Sample Write-Up Our organization created a new onboarding program a year ago. Today (a year later), we wish to investigate whether participating in the new vs. old onboarding program is associated with employees decisions to quit the organization for the subsequent six months. Based on a sample of 100 newcomers (N = 100)  half of whom went through the new onboarding program and half of whom went through the old onboarding program  we estimated a 2x2 chi-square (\\(\\chi^2\\)) test of independence. The \\(\\chi^2\\) test of independence was statistically significant (\\(\\chi^2\\) = 7.84, df = 1, p = .04), such that those who participated in the new onboarding program were less likely to quit in their first year. Note: If the p-value were equal to or greater than our alpha level (e.g., .05, two-tailed), then we would typically state that the association between the two variables is not statistically significant, and we would not proceed forward with interpreting the effect size (i.e., level of practical significance) because the test of statistical significance indicates that it is very unlikely based on our sample that a true association between these two variables exists in the population. 46.2 Tutorial This chapters tutorial demonstrates how a chi-square test of independence can be used to identify whether an association exists between a categorical (nominal, ordinal) variable and dichotomous turnover decisions (i.e., stay vs. quit). 46.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/l8v3kx6zNuQ 46.2.2 Functions &amp; Packages Introduced Function Package xtabs base R print base R xtabs base R chisq.test base R prop.table base R phi psych 46.2.3 Initial Steps If you havent already, save the file called ChiSquareTurnover.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called ChiSquareTurnover.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object cst &lt;- read_csv(&quot;ChiSquareTurnover.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## Onboarding = col_character(), ## Turnover = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(cst) ## [1] &quot;EmployeeID&quot; &quot;Onboarding&quot; &quot;Turnover&quot; # Print number of rows in data frame (tibble) object nrow(cst) ## [1] 75 # Print structure of data frame (tibble) object str(cst) ## spec_tbl_df[,3] [75 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID: chr [1:75] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E8&quot; ... ## $ Onboarding: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ Turnover : chr [1:75] &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. Onboarding = col_character(), ## .. Turnover = col_character() ## .. ) # Print first 6 rows of data frame (tibble) object head(cst) ## # A tibble: 6 x 3 ## EmployeeID Onboarding Turnover ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E1 No Quit ## 2 E2 No Quit ## 3 E3 No Quit ## 4 E8 No Quit ## 5 E14 No Quit ## 6 E22 No Quit There are 3 variables and 75 cases (i.e., employees) in the cst data frame: EmployeeID, Onboarding, and Turnover. Per the output of the str (structure) function above, all of the variables are of type character. EmployeeID is the unique employee identifier variable. Onboarding is a variable that indicates whether new employees participated in the onboarding program, and it has two levels: did not participate (No) and did participate (Yes) Turnover is a variable that indicates whether these individuals left the organization during their first year; it also has two levels: Quit and Stayed. 46.2.4 Create a Contingency Table for Observed Data Before running the chi-square test, we need to create a contingency (cross-tabulation) table for the Onboarding and Turnover variables. To do so, we will use the xtabs function from base R. In the function parentheses, as the first argument type the ~ operator followed by the two variables names, separated by the + operator (i.e., ~ Onboarding + Turnover). As the second argument, type data= followed by the name of the data frame to which both variables belong (cst). We will use the &lt;- assignment operator to name the table object (tbl). For more information on creating tables, check out the chapter on summarizing two or more categorical variables using cross-tabulations. # Create two-way contingency table tbl &lt;- xtabs(~ Onboarding + Turnover, data=cst) Lets print the new tbl object to your Console. # Print table print(tbl) ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 As you can see, we have created a 2x2 contingency table, as it has two variables (Onboarding, Turnover) with each variable consisting of two levels (i.e., categories). This table includes our observed data. 46.2.5 Estimate Chi-Square (\\(\\chi^2\\)) Test of Independence Using the contingency table we created called tbl, we will insert it as the sole parenthetical argument in the chisq.test function from base R. # Estimate chi-square test of independence chisq.test(tbl) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tbl ## X-squared = 23.179, df = 1, p-value = 0.000001476 In the output, note that the \\(\\chi^2\\) value is 23.179 with 1 degree of freedom (df), and the associated p-value is less than .05, leading us to reject the null hypothesis that these two variables are independent of one another (\\(\\chi^2\\) = 23.176, df = 1, p &lt; .001). In other words, we have concluded that there is a statistically significant association between onboarding participation and whether someone quits in the first year. To understand the nature of this association, we need to look back to the contingency table values. In fact, sometimes it is helpful to look at the column (or row) proportions by applying the prop.table function from base R to the tbl object we created. As the second argument, type the numeral 2 to request the column marginal proportions. # Print column marginal proportions prop.table(tbl, 2) ## Turnover ## Onboarding Quit Stayed ## No 0.7272727 0.1320755 ## Yes 0.2727273 0.8679245 As you can see from the contingency table, of those employees who stayed, proportionally more employees had participated in the onboarding program (86.8%). Alternatively, for those who quit, proportionally more employees did not participate in the onboarding program (72.7%). Given our interpretation of the table along with the significant \\(\\chi^2\\) test, we can conclude that employees were significantly more likely to stay through the end of their first year if they participated in the onboarding program. In the special case of a 2x2 contingency table (or a 1x4 vector), we can compute a phi (\\(\\phi\\)) coefficient, which can be interpreted like a Pearson product-moment correlation coefficient and thus as an effect size. To calculate the phi coefficient, we will use the phi function from the psych package. If you havent already, install and access the psych package. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Within the phi function parentheses, insert the name of the 2x2 contingency table object (tbl) as the sole argument. # Estimate phi coefficient phi(tbl) ## [1] 0.587685 The \\(\\phi\\) coefficient for the association between Onboarding and Turnover is .59, which by conventional correlation standards is a large effect (\\(\\phi\\) = .59). Here is a table containing conventional rules of thumb for interpreting a Pearson correlation coefficient (r) or a phi coefficient (\\(\\phi\\)) as an effect size: \\(\\phi\\) Description .10 Small .30 Medium .50 Large Technical Write-Up: Based on a sample of 75 employees, we investigated whether participating in an onboarding program was associated with new employees decisions to quit in the first 12 months. Using a chi-square (\\(\\chi^2\\)) test of independence, we found that new employees who participated in the onboarding program were less likely to quit during their first 12 months on the job (\\(\\chi^2\\) = 23.176, df = 1, p &lt; .001). Specifically, of those employees who stayed at the organization, proportionally more employees had participated in the onboarding program (86.8%), and of those employees who quit the organization, proportionally more employees did not participate in the onboarding program (72.7%). The magnitude of this effect can be described as large (\\(\\phi\\) = .59). 46.2.5.1 Optional: Print Observed and Expected Counts In some instances, it might be helpful to view the observed and expected, as these are the basis for estimating the \\(\\chi^2\\) test of independence. First, we need to create an object that contains the information from the chisq.test function. Using the &lt;- assignment operator, lets name this object chisq. # Create chi-square object chisq &lt;- chisq.test(tbl) We can request the observed counts/frequencies by typing the name of the chisq object, followed by the $ operator and observed. These will look familiar because they are the counts from our original contingency table. # Request observed counts/frequencies chisq$observed ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 To request the expected counts/frequencies, type the name of the chisq object, followed by the $ operator and expected. # Request expected counts/frequencies chisq$expected ## Turnover ## Onboarding Quit Stayed ## No 6.746667 16.25333 ## Yes 15.253333 36.74667 We can descriptively see departures from the observed and expected values in a systematic manner. To understand which cells were most influential in the calculation of the \\(\\chi^2\\) value, it is straightforward to request the Pearson residuals. Pearson residuals with larger absolute values have bigger contributions to the \\(\\chi^2\\) value. # Request Pearson residuals chisq$residuals ## Turnover ## Onboarding Quit Stayed ## No 3.562489 -2.295234 ## Yes -2.369277 1.526473 46.2.6 Summary In this chapter, we learned how to estimate a chi-square (\\(\\chi^2\\)) test of independence using the chisq.test function from base R. 46.3 Chapter Supplement In this chapter supplement, you will have an opportunity to learn how to compute and interpret an odds ratio and its 95% confidence interval, where the odds ratio is another method we can use to describe the association between two categorical variables from a 2x2 contingency table. 46.3.1 Functions &amp; Packages Introduced Function Package sqrt base R exp base R log base R print base R 46.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object cst &lt;- read_csv(&quot;ChiSquareTurnover.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmployeeID = col_character(), ## Onboarding = col_character(), ## Turnover = col_character() ## ) # Print the names of the variables in the data frame (tibble) object names(cst) ## [1] &quot;EmployeeID&quot; &quot;Onboarding&quot; &quot;Turnover&quot; # View variable type for each variable in data frame (tibble) object str(cst) ## spec_tbl_df[,3] [75 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmployeeID: chr [1:75] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E8&quot; ... ## $ Onboarding: chr [1:75] &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... ## $ Turnover : chr [1:75] &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; &quot;Quit&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmployeeID = col_character(), ## .. Onboarding = col_character(), ## .. Turnover = col_character() ## .. ) # View first 6 rows of data frame (tibble) object head(cst) ## # A tibble: 6 x 3 ## EmployeeID Onboarding Turnover ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 E1 No Quit ## 2 E2 No Quit ## 3 E3 No Quit ## 4 E8 No Quit ## 5 E14 No Quit ## 6 E22 No Quit 46.3.3 Compute Odds Ratio for 2x2 Contingency Table When communicating the association between two categorical variables, sometimes it helps to the express the association as an odds ratio. An odds ratio allows us to describe the association between two variables as, given X, the odds of experiencing the event are Y times greater. To get started, lets create a 2x2 contingency table object called tbl, which will house the counts for our two categorical variables (Onboarding, Turnover), where each variable has two levels. # Create two-way contingency table tbl &lt;- xtabs(~ Onboarding + Turnover, data=cst) # Print table print(tbl) ## Turnover ## Onboarding Quit Stayed ## No 16 7 ## Yes 6 46 Im going to demonstrate how to manually compute the odds ratio for a 2x2 contingency table, but note that you can estimate a logistic regression model to arrive at the same end, which is covered in the chapter on identifying predictors of turnover. I should also note that there are some great functions out there like the oddsratio.wald function from the epitools package that will do these calculations for us; nonetheless, I think its worthwhile to unpack how relatively simple these calculations are, as this can serve as a nice warm up for the following chapter. When we have a 2x2 contingency table of counts, the odds ratio is relatively straightforward to calculate if we conceptualize our contingency table as follows. Because we are expecting that those who participate in the treatment (i.e., onboarding program) will be more likely to stay in the organization, lets frame the act of staying as the focal event, which means that those who quit did not experience the event of staying. Condition No Event (Quit) Event (Stayed) Control (No Onboarding) D C Treatment (Onboarding) B A The formula for computing an odds ratio is: \\(OR = \\frac{\\frac{A}{C}}{\\frac{B}{D}}\\) where: A is the number of individuals who participated in the treatment condition (e.g., onboarding program) and who also experienced the event in question (e.g., stayed); B is the number of individuals who participated in the treatment condition (e.g., onboarding program) but who did not experience the event in question (e.g., quit); C is the number of individuals who participated in the control condition (e.g., no onboarding program) but who also experienced the event in question (e.g., stayed); D is the number of individuals who participated in the control condition (e.g., no onboarding program) and who did not experience the event in question (e.g., quit). The formula can also be written in the following manner after cross-multiplication, which is what well use when computing the odds ratio, as I think it is easier to read in R: \\(OR = \\frac{A \\times D}{B \\times C}\\) To make our calculations clearer, lets reference specific cells from our contingency table object (tbl) using brackets ([ ]), where the first value within brackets references the row number and the second references the column number. Well assign the count value from each score to objects A, B, C, and D, to be consistent with the above formula. # Assign counts to objects A &lt;- tbl[2,2] B &lt;- tbl[2,1] C &lt;- tbl[1,2] D &lt;- tbl[1,1] Next, we will plug objects A, B, C, and D into the formula (see above) using the multiplication (*) and division (/) operators, and assign the result to an object called OR using the &lt;- assignment operator. # Calculate odds ratio of someone *staying* # who participated in the treatment (onboarding program) OR &lt;- (A * D) / (B * C) # Print odds ratio print(OR) ## [1] 17.52381 The odds ratio is 17.52, which indicates that for those who participated in the onboarding program the odds of staying was 17.52 greater. If an odds ratio is equal to 1.00, then it signifies no association  or equal odds of experiencing the focal event. If an odds ratio is greater than 1.00, then it signifies a positive association between the two variables, such that the higher level on the first variable (e.g., receiving treatment) is associated with the higher level on the second variable (e.g., experiencing event). Alternatively, if we would like to, instead, understand the odds of someone quitting after participating in the onboarding program, we would flip the numerator and denominator from our above code. Effectively, in this example, this simple operation switches our focal event from staying to quitting, thereby giving us a different perspective on the association between the two variables. # Calculate odds ratio of someone *quitting* # who participated in the treatment (onboarding program) OR_alt &lt;- (B * C) / (A * D) # flip numerator and denominator # Print odds ratio print(OR_alt) ## [1] 0.05706522 The new odds ratio (OR_alt) is approximately .06 (with rounding). Because an odds ratio that is less than 1.00 signifies a negative association, this odds ratio indicates that those who participated in the treatment (onboarding program) are less likely to have experienced the event of quitting. If we compute the reciprocal of the odds ratio, we arrive back at our original odds ratio. # Calculate reciprocal of odds ratio 1 / OR_alt ## [1] 17.52381 This takes us back to our original interpretation, which is that for those who participated in the onboarding program the odds of staying was 17.52 greater. I should note that an odds ratio of 17.52 is a HUGE effect. When working with a 2x2 contingency table, I recommend computing the reciprocal of any odds ratio less than 1.00, as it often leads to a more straightforward interpretation  just remember that the we have to conceptually flip the event and non-event when interpreting the now positive association between the two categorical variables. As a significance test for the odds ratio, we can compute the 95% confidence intervals. Because an odds ratio of 1.00 signifies no association between the two variables, any confidence interval that does not include 1.00 will be considered statistically significant. To compute the confidence interval, we first need to compute the standard error of the odds ratio using the following formula. \\(SE = \\sqrt{\\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\) # Calculate standard error (SE) of odds ratio SE &lt;- sqrt((1/A) + (1/B) + (1/C) + (1/D)) To construct the lower and upper limits of a 95% confidence interval, we need to first multiply the standard error (SE) by 1.96. To compute the lower limit, we subtract that product from the natural log of odds ratio (log(OR)) and then exponentiate the resulting value using the exp function from base R, where the exp function takes Eulers constant (\\(e\\)) and adds a specified exponent to it. To compute the upper limit, we add that product to the natural log of odds ratio and then exponentiate the resulting value. # Calculate 95% confidence interval for odds ratio exp(log(OR) - 1.96*SE) # lower limit ## [1] 5.122538 exp(log(OR) + 1.96*SE) # upper limit ## [1] 59.94761 The 95% confidence interval ranges from 5.12 to 59.95 (95% CI[5.12, 59.95]). Because this interval does not include 1.00, we can conclude that the odds ratio of 17.52 is statistically significantly different from 1.00. "],["logistic.html", "Chapter 47 Identifying Predictors of Turnover Using Logistic Regression 47.1 Conceptual Overview 47.2 Tutorial 47.3 Chapter Supplement", " Chapter 47 Identifying Predictors of Turnover Using Logistic Regression In this chapter, we will learn how to estimate a (binary) logistic regression model in order to identify potential predictors of employee voluntary turnover, when voluntary turnover is operationalized as a dichotomous (i.e., binary) variable (e.g., stay vs. quit). 47.1 Conceptual Overview Logistic regression (logit model) is part of the family of generalized linear models (GLMs). Assuming statistical assumptions have been satisfied, a binary logistic regression model is appropriate when the outcome variable of interest is dichotomous (i.e., binary) and when the predictor variable(s) of interest is/are continuous (interval, ratio) or categorical (nominal, ordinal). Unlike ordinary least squares (OLS) estimation, which was covered previously in the context of simple linear regression and multiple linear regression, logistic regression coefficients are typically estimated using maximum likelihood (ML). There are also extensions of the logistic regression like multinomial and ordinal logistic regression, where these extensions are appropriate when the categorical outcome variable is nominal or ordinal with three or more levels/categories. Logistic regression can be used to determine the odds that a dichotomous event occurs (e.g., stay vs. quit) given higher or lower values/levels on one or more predictor variables, where odds refers to the probability of an event occurring (p) relative to the probability of the event not occurring (1 - p). As such, an odds value of 1 can be interpreted as 1 to 1 odds; or in other words, the probability of the event occurring is equal to the probability of the event not occurring, which would be akin to flipping a fair coin. For example, if we find that the probability of quitting is .75 (p = .75; i.e., event occurring), then by extension, the probability of not quitting is .25 (1 - p = .25; i.e., event not occurring). Given these probabilities, the odds of quitting are 3 (.75 / .25 = 3), and the odds of not quitting are 1/3 or .33, which is calculated as: .25 / (1 - .25). A logit transformation of the odds of quitting and of the odds of not quitting will be symmetrical, where a logit transformation refers to taking the natural log (\\(\\ln\\)) of both values (i.e., logarithmic transformation). For example, a logit transformation of 3 and 1/3 yields 1.10 and -1.10, which are symmetrical: \\(\\ln(3) = 1.10\\) and \\(\\ln(1/3) = -1.10\\). 47.1.1 Review of Logistic Regression Just as there is a distinction between simple and multiple linear regression models, we can also draw a distinction between simple and multiple logistic regression models. When there is a single predictor variable and a dichotomous outcome variable, we can apply what is referred to as a simple logistic regression model. More specifically, a simple logistic regression refers to the bivariate linear association between a predictor variable and a dichotomous outcome variable that has undergone a logit transformation, as shown in the equation below. \\(logit(p) = \\log(odds) = \\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1)\\) where \\(logit(p)\\) represents the logit transformation of the outcome, \\(\\log(odds)\\) represents the log(arithmic) odds, \\(\\ln\\) represents the natural log, \\(p\\) represents the probability of an event occurring, \\(b_0\\) represents the intercept value, and \\(b_1\\) represents the regression coefficient (i.e., slope, weight) of the association between the predictor variable \\(X_1\\) and the logit transformation of the outcome variable. And when we have two or more predictor variables and a single dichotomous outcome variable, we estimate what is called a multiple logistic regression model, where an example of a multiple logistic regression model follows. \\(logit(p) = \\log(odds) = \\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1) + b_2(X_2)\\) where \\(b_2\\) represents the regression coefficient (i.e., slope, weight) of the association between the second predictor variable \\(X_2\\) and the logit transformation of the outcome variable. Logistic Function: Logistic regression is predicated on the logistic function. The scatter plot figure shown below illustrates the logistic function when there is a continuous (interval, ratio) predictor variable called \\(X\\) and a dichotomous outcome variable called \\(Y\\). Because a linear function would not not closely approximate the association between these two variables, we instead use a logistic function, which is a sigmoidal (or sigmoid) function and takes the visual form of an S-curve. The sigmoidal (sigmoid) function is shown in red and represents the probability of an event occurring at each level/value of the predictor variable. Just like simple and multiple linear regression models, regression coefficients are estimated in simple logistic regression models, but these coefficients are not perhaps as easily interpretable in their original form because the outcome variable undergoes a logarithmic transformation, as noted above. The regression coefficients in a logistic regression model represent the change in log odds (i.e., logit transformation) for every one unit change in the predictor variable. Odds Ratio: To make a statistically significant regression coefficient (\\(b_i\\)) easier to interpret, we often convert the coefficient to an odds ratio. To do so, we exponentiate the coefficient using Eulers number (\\(e\\)). Eulers number (\\(e\\)) is an irrational number and mathematical constant that is approximately equal to 2.71828. \\(e^{b_i}\\) An odds ratio that is less than 1.00 indicates a negative association between the predictor variable and outcome variable, and an odds ratio that is greater than 1.00 indicates a positive association. An odds ratio equal to 1.00 indicates that there is no association between the variables. Note: In the case of a multiple logistic regression model where we have two or more predictor variables, we would describe an odds ratios more accurately as an adjusted odds ratio because we are statistically controlling for the other predictor variable(s) in the model. Example of Interpreting a Negative Association: As an example, lets imagine in a simple logistic regression model we find that the coefficient for the association between job satisfaction (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is -.42  that is, we find a negative association. If we interpret the coefficient in its original form, we might say something like: For every one unit increase in job satisfaction, there is as .42 unit reduction in log odds of quitting. Such language will not typically be well-received by organizational stakeholders; however, if we convert the coefficient to an odds ratio by exponentiating it, we might find it easier to explain the finding to ourselves and others. \\(e^{-.42} = .66\\) In this example, the odds ratio of .66 is less than 1.00, which reflects back to us that the association is indeed negative, which we already knew from the original log odds coefficient value of -.42. We can interpret this odds ratio as follows: For every unit increase in job satisfaction, the odds of quitting is reduced by 34% (1 - .66 = .34). Alternatively, we can interpret the odds ratio from a different vantage point if we compute its reciprocal, which is 1.52 (1 / .66 = 1.52); in doing so, we can interpret the finding as: For every unit increase in job satisfaction, the odds of quitting is reduced by 1 in 1.52. Or, we could frame the finding in terms of not quitting: For every unit increase in job satisfaction, the odds of not quitting is 1.52 times greater  or has a 1.52 times higher likelihood. Example of Interpreting a Positive Association: Now that weve worked through an example of a negative association, lets practice interpreting a positive association. Lets imagine a different simple logistic regression model in which we find that the coefficient for the association between turnover intentions (continuous predictor variable) and voluntary turnover (dichotomous outcome variable; i.e., stay = 0 vs. quit = 1) is .89. When interpreting the coefficient in its original form, we might say: For every one unit increase in turnover intentions, there is as .89 unit increase in log odds of quitting. Just as we did before, we can exponentiate the coefficient to find the odds ratio. \\(e^{.99} = 2.44\\) Because the odds ratio of 2.44 is greater than 1.00, it confirms to us that the association between turnover intentions and voluntary turnover is positive. We can interpret this finding as: For every unit increase in turnover intentions, the odds of quitting is 2.44 times greater  or has a 2.44 times higher likelihood. As another example, lets imagine that we observe an odds ratio of 1.29 with respect to negative affectivity in relation to voluntary turnover. We could interpret this finding as: For every unit increase in negative affectivity, the odds of quitting is 1.29 times greater. Or, because 1.29 times greater is equivalent to saying that there was a 29% increase, we could say: For every unit increase in negative affectivity, the odds of quitting increases by 29%. Predicted Probabilities: Our logistic regression coefficients can also be used to predict the probability of the event occurring for different value(s) of the predictor variable(s). The equations that follow can help us to understand algebraically how we can determine the probability (\\(p\\)) of an event occurring based on the coefficients we might estimate for simple logistic regression model. \\(\\ln(\\frac{p}{1-p}) = b_0 + b_1(X_1)\\) \\(\\frac{p}{1-p} = e^{b_0 + b_1(X_1)}\\) \\(p = \\frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}}\\) where \\(e\\) represents Eulers number. As an example, lets first imagine that our estimated intercept (\\(b_0\\)) is .32 and our estimated regression coefficient associated with the predictor variable (\\(b_1\\)) is -.09. Next, lets imagine that someone has a score of 3 on the predictor variable (\\(X_1\\)). If we plug those values into the equation, we will find that the probability of a person with a score of 3 on the predictor variable is .51. \\(p = \\frac{e^{b_0 + b_1(X_1)}}{1+e^{b_0 + b_1(X_1)}} = \\frac{e^{.32 + (-.09 \\times 3)}}{1+e^{.32 + (-.09 \\times 3)}} = .51\\) If we set a probability threshold of .50 for experiencing the event, then we would classify anyone who scores a 3 on the predictor variable as having been predicted to experience the event in question. If however, the computed probability had been less than the probability threshold of .50, then we would have classified any associated with cases as having been predicted to not experience the event in question. 47.1.1.1 Statistical Assumptions The statistical assumptions that should be met prior to running and/or interpreting estimates from a simple or multiple logistic regression model include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Data are free of bivariate/multivariate outliers; The association between any continuous predictor variable(s) and the logit transformation of the outcome variable is linear; The outcome variable is dichotomous; For a multiple logistic regression model, there is no (multi)collinearity between predictor variables. The fifth statistical assumption refers to the concept of collinearity (multicollinearity). This can be a tricky concept to understand, so lets take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple logistic regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights  and even the signs  of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The tolerance statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that R2 value from 1 (i.e., 1 - R2). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the variance inflation factor (VIF) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated. Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapters data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., multilevel logit model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 47.1.1.2 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero. In other words, if a regression coefficients p-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent. In contrast, if the regression coefficients p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the regression coefficient is equal to zero. Put differently, if a regression coefficients p-value is equal to or greater than .05, we conclude that the regression coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. Keep in mind that in the context of a multiple logistic regression model, the association between each predictor variable and the logit transformation of the outcome variable must be interpreted with statistical control in mind, as we are effectively testing whether each predictor variable shows evidence of incremental validity in the presence of any other predictor variables. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (b) is a point estimate of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do not include zero, then this tells us the same thing as a p-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error. Note: In a logistic regression model, we may also construct confidence intervals around the odds ratios. 47.1.1.3 Practical Significance In its original form, a logistic regression coefficient is not an effect size; that is, it doesnt provide an indication of practical significance. The odds ratio, however, can be conceptualized as an effect size. With that being said, there are some caveats. First, an odds ratio that is computed directly from an unstandardized logistic regression coefficient needs to be interpreted based on the raw scaling of the predictor variable, as the interpretation of the odds ratio has to do with the change in odds for unit change in the predictor variable. If we wish to compare odds ratios within or between models, we need to take this scaling issue into account. Second, in the case of a multiple logistic regression model, statistical control is at play, which means that the odds ratios are more accurately described as adjusted odds ratios. There are different thresholds we can apply when interpreting the magnitude of an odds ratio, and below I provide some thresholds that we will use in this tutorial. With that being said, thresholds for qualitatively interpreting effect sizes should be context dependent, and there are other thresholds we might apply. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large At the model level, we cant compute a true R2 when estimating a logistic regression model. We can, however, compute a pseudo-R2, There are different formulas available for computing pseudo-R2 (e.g., Cox &amp; Snell, McFadden), and in this chapter well focus on the following Nagelkerke (1991) formula: \\(pseudo-R^2 = \\frac{1 - (\\frac{L(M_{null})}{L(M_{full})})^{2/N}}{1-L(M_{null})^{2/N}}\\) where \\(L(M_{null})\\) is the likelihood of the outcome variable given a null, intercept-only model, \\(L(M_{full})\\) is the likelihood of the outcome variable given the predictor variable(s) in the model, and \\(N\\) is the sample size. Its important to note, though, that as the name implies, a pseudo-R2 is not the same thing as a true R2. Thus, while we need to be cautious in our interpretations. In addition to pseudo-R2, we can also describe the classification accuracy of the model by using a confusion matrix (or classification table). A confusion matrix presents the percentage of correctly predicted values on the outcome variable; if a probability for a case based on the model is equal to or greater than .50, then it would be classified as a probability of 1, and all else would be classified as 0. For example, using a confusion matrix, we can make statements like: The model correctly classified 55.9% of the employees as either stay or quit. To indicate how well your model fit the data and performed, I recommend reporting either pseudo-R2, model classification accuracy, or both. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. 47.1.1.4 Sample Write-Up A voluntary turnover study was conducted based on a sample 99 employees from the past year, some of whom quit the company and some of whom stayed. The focal outcome variable is turnover behavior (quit vs. stay), and because it is dichotomous, we used logistic regression. We were, specifically, interested in the extent to which employees self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]). For every one unit increase in negative affectivity, the odds of quitting were 3.304 times greater, when controlling for the other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]). For every one unit increase in turnover intentions, the odds of quitting were 2.451 times greater, when controlling for other predictor variables in the model. Both of these significant associations can be described as medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to correct classify 78.9% of employees from our sample using the estimated multiple logistic regression model. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. 47.2 Tutorial This chapters tutorial demonstrates how to estimate simple and multiple logistic regression models using R. 47.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/O7gRceyeyT8 47.2.2 Functions &amp; Packages Introduced Function Package Logit lessR log base R PseudoR2 DescTools exp base R glm base R merge base R data.frame base R mutate dplyr ifelse base R c base R predict base R detach base R 47.2.3 Initial Steps If you havent already, save the file called Turnover.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called Turnover.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;Turnover.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## ID = col_character(), ## Turnover = col_double(), ## JS = col_double(), ## OC = col_double(), ## TI = col_double(), ## NAff = col_double() ## ) # Print the names of the variables in the data frame (tibble) objects names(td) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;NAff&quot; # View variable type for each variable in data frame str(td) ## spec_tbl_df[,6] [99 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ... ## $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ... ## $ JS : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ... ## $ OC : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ... ## $ TI : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ... ## $ NAff : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_character(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. NAff = col_double() ## .. ) # View first 6 rows of data frame head(td) ## # A tibble: 6 x 6 ## ID Turnover JS OC TI NAff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EMP559 1 4.96 5.32 0.51 1.87 ## 2 EMP561 1 1.72 1.47 4.08 2.48 ## 3 EMP571 1 1.64 0.87 2.65 2.84 ## 4 EMP589 1 3.01 2.15 4.17 2.43 ## 5 EMP592 1 3.04 1.94 3.27 2.76 ## 6 EMP601 1 3.81 3.81 3.01 3.67 There are 5 variables and 99 cases (i.e., employees) in the td data frame: ID, Turnover, JS, OC, TI, and NAff. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio), except for the ID variable, which is of type character. ID is the unique employee identifier variable. Imagine that these data were collected as part of a turnover study within an organization to determine the drivers/predictors of turnover based on a sample of employees who stayed and leaved during the past year. The variables JS, OC, TI, and NAff were collected as part of an annual survey and were later joined with the Turnover variable. Survey respondents rated each survey item using a 7-point response scale, ranging from strongly disagree (0) to strongly agree (6). JS contains the average of each employees responses to 10 job satisfaction items. OC contains the average of each employees responses to 7 organizational commitment items. TI contains the average of each employees responses to 3 turnover intentions items, where higher scores indicate higher levels of turnover intentions. NAff contains the average of each employees responses to 10 negative affectivity items. Turnover is a variable that indicates whether these individuals left the organization during the prior year, with 1 = quit and 0 = stayed. Note: If the Turnover variable were to include the character values of quit and stay instead of 1 and 0, the functions covered in this tutorial would automatically convert the character values to 0 and 1 (behind the scenes), where 0 would be assigned to the character value that comes first alphabetically. 47.2.4 Estimate Simple Logistic Regression Model Well begin by specifying a simple logistic regression model, which means the model will include just one predictor variable. Lets begin by regressing Turnover on JS using the Logit function from the lessR package. If you havent already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) As the first argument in the Logit function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame to which both of the variables belong (td). Lets begin by specifying JS as the predictor variable. # Estimate simple logistic regression model Logit(Turnover ~ JS, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 69 6.00 1 0 0.3162 0.1215 ## 97 5.59 0 0 0.3562 0.1120 ## 70 5.48 0 0 0.3673 0.1090 ## 73 5.48 1 0 0.3673 0.1090 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover predict fitted std.err ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover predict fitted std.err ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 39 39.8 8 31 20.5 ## Turnover 1 59 60.2 10 49 83.1 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Recall: 83.05 ## Precision: 61.25 Note: In some instances, you might receive the error message shown below. You can ignore this message, as it just indicates that you have a poor predictor variable in the model that results in fitted/predicted values that are all the same. If you get this message, proceed forward with your interpretation of the output. \\(\\color{red}{\\text{Error:}}\\) \\(\\color{red}{\\text{All predicted values are 0.}}\\) \\(\\color{red}{\\text{Something is wrong here.}}\\) The output generates the model coefficient estimates, the odds ratios and their confidence intervals, model fit information (i.e., AIC), outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model (which we probably already knew), the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis after excluding cases with missing data (N = 98). 47.2.4.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Bivariate Outliers: To determine whether the data are free of bivariate outliers, lets take a look at the text output section called Analysis of Residuals and Influence. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cooks distance (cooks). The case associated with row number 66 has the highest Cooks distance value (.085), followed by the cases associated with row numbers 67 and 71, which have Cooks distance values of .062 and .049. A liberal threshold Cooks distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we may want to estimate our model once more after removing the cases associated with row numbers 66, 66, and 71 from our data frame; however, these Cooks distance values dont look too concerning or out of the ordinary, and thus I wouldnt recommend removing the associated cases. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between our predictor variable JS and its logarithmic transformation to our logistic regression model  but not the main effect for the logarithmic transformation of JS. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of the predictor variable JS followed by the + operator. After the + operator, type the name of the predictor variable JS, followed by the : operator and the log function from base R with the predictor variable JS as its sole parenthetical argument. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable Logit(Turnover ~ JS + JS:log(JS), data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 5.4461 3.2062 1.699 0.089 -0.8379 11.7300 ## JS -2.9840 2.1693 -1.376 0.169 -7.2357 1.2677 ## JS:log(JS) 1.1696 0.9778 1.196 0.232 -0.7467 3.0860 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 231.8457 0.4326 124248.7675 ## JS 0.0506 0.0007 3.5527 ## JS:log(JS) 3.2208 0.4739 21.8896 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 124.621 on 95 degrees of freedom ## ## AIC: 130.6211 ## ## Number of iterations to convergence: 5 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 7 1.38 0 0.8639 -0.8639 -2.105 -0.4827 0.158262 ## 69 6.00 1 0.5290 0.4710 1.221 0.5442 0.090322 ## 12 1.72 0 0.8029 -0.8029 -1.852 -0.3440 0.063819 ## 31 1.77 0 0.7936 -0.7936 -1.821 -0.3267 0.055946 ## 97 5.59 0 0.5044 -0.5044 -1.239 -0.3947 0.049267 ## 73 5.48 1 0.4993 0.5007 1.224 0.3561 0.039962 ## 70 5.48 0 0.4993 -0.4993 -1.221 -0.3554 0.039735 ## 58 5.43 1 0.4972 0.5028 1.224 0.3418 0.036908 ## 13 1.96 0 0.7577 -0.7577 -1.713 -0.2696 0.034590 ## 80 5.04 0 0.4853 -0.4853 -1.174 -0.2378 0.017543 ## 1 4.96 1 0.4840 0.5160 1.225 0.2327 0.017368 ## 33 4.88 1 0.4830 0.5170 1.225 0.2184 0.015319 ## 61 2.52 0 0.6572 -0.6572 -1.476 -0.1788 0.012429 ## 74 2.56 0 0.6506 -0.6506 -1.462 -0.1758 0.011886 ## 75 2.57 0 0.6490 -0.6490 -1.459 -0.1751 0.011761 ## 84 4.66 1 0.4823 0.5177 1.221 0.1854 0.011051 ## 63 4.65 1 0.4823 0.5177 1.221 0.1842 0.010899 ## 67 2.65 0 0.6363 -0.6363 -1.433 -0.1701 0.010877 ## 96 2.82 0 0.6108 -0.6108 -1.384 -0.1623 0.009530 ## 21 4.64 0 0.4824 -0.4824 -1.159 -0.1737 0.009334 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 84 4.66 1 0 0.4823 0.08526 ## 63 4.65 1 0 0.4823 0.08471 ## 21 4.64 0 0 0.4824 0.08416 ## 53 4.63 0 0 0.4824 0.08363 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover predict fitted std.err ## 70 5.48 0 0 0.4993 0.15604 ## 73 5.48 1 0 0.4993 0.15604 ## 55 3.97 1 1 0.5005 0.06559 ## 16 3.95 0 1 0.5015 0.06547 ## 8 3.92 1 1 0.5031 0.06531 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover predict fitted std.err ## 66 1.19 1 1 0.8945 0.08584 ## 48 1.05 1 1 0.9147 0.08191 ## 88 0.67 1 1 0.9582 0.06150 ## 24 0.23 1 1 0.9874 0.02972 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 39 39.8 14 25 35.9 ## Turnover 1 59 60.2 9 50 84.7 ## --------------------------------------------------- ## Total 98 65.3 ## ## Accuracy: 65.31 ## Recall: 84.75 ## Precision: 66.67 Because the interaction term (JS:log(JS)) regression coefficient of 1.1696 in the Model Coefficients table is nonsignificant (p = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is nonlinear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (Curvilinear Effects in Logistic Regression) from Osborne (2015). Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well. Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. Just for the sake of demonstration, lets transform the JS variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (JS_0) that is equal to JS minus the minimum value of JS. # ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero td$JS_0 &lt;- td$JS - min(td$JS, na.rm=TRUE) Now that we have a variable called JS_0 with at least one score equal to zero, lets try the try the Box-Tidwell test. Im going to add the argument brief=TRUE to reduce the amount of output generated by the function. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with predictor containing zero value(s)] Logit(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, brief=TRUE) If you ran the script above, you likely got an error message that looked like this: \\(\\color{red}{\\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\\) The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on JS_0, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above). Using the first approach, create a new variable in which the JS_0 variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the JS_0 variable, which results in the lowest score on the new variable (JS_1) being 1. As verification, I include the min function from base R. # Linear transformation that results in lowest score being 1 td$JS_1 &lt;- td$JS_0 + abs(min(td$JS_0, na.rm=TRUE)) + 1 # Verify that new lowest score is 1 min(td$JS_1, na.rm=TRUE) ## [1] 1 It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with transformed predictor = 1] Logit(Turnover ~ JS_1 + JS_1:log(JS_1), data=td, brief=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS_1 ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 8.0985 5.0081 1.617 0.106 -1.7171 17.9141 ## JS_1 -4.0594 2.9774 -1.363 0.173 -9.8950 1.7762 ## JS_1:log(JS_1) 1.5097 1.2264 1.231 0.218 -0.8939 3.9133 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 3289.6100 0.1796 60256846.1183 ## JS_1 0.0173 0.0001 5.9072 ## JS_1:log(JS_1) 4.5254 0.4090 50.0661 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 124.575 on 95 degrees of freedom ## ## AIC: 130.5747 ## ## Number of iterations to convergence: 4 There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (JS_1:log(JS_1)) is nonsignificant (b = 1.510, p = .218). Thus, we dont see evidence that the assumption of linearity has been violated. We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the rows= argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that were back to using the variable called JS_0 that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with only cases with scores greater than zero] Logit(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, rows=(JS_0 &gt; 0), brief=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS_0 ## ## Number of cases (rows) of data: 97 ## Number of cases retained for analysis: 97 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 4.6652 2.8216 1.653 0.098 -0.8650 10.1953 ## JS_0 -2.6157 1.9936 -1.312 0.189 -6.5230 1.2916 ## JS_0:log(JS_0) 1.0392 0.9280 1.120 0.263 -0.7796 2.8579 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 106.1854 0.4211 26777.8281 ## JS_0 0.0731 0.0015 3.6387 ## JS_0:log(JS_0) 2.8269 0.4586 17.4256 ## ## ## Model Fit ## ## Null deviance: 130.725 on 96 degrees of freedom ## Residual deviance: 124.616 on 94 degrees of freedom ## ## AIC: 130.6162 ## ## Number of iterations to convergence: 4 We lost one case because that person had a score of zero on the JS_0 continuous predictor variable, and we see that the interaction term (JS_0:log(JS_0)) is non significant (b = 1.0392, p = .263). To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your continuous predictor variable has scores that are equal to or less than zero. Now that weve met the assumption of linearity, were finally ready to interpret the model results! 47.2.4.2 Interpret Model Results Basic Analysis: The Basic Analysis section of the original output first displays a table called the Model Coefficients, which includes the regression coefficients (slopes, weights) and their standard errors, z-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (JS) in relation to the outcome variable (Turnover) is often of substantive interest. Here, we see that the unstandardized regression coefficient for JS is -.438, and its associated p-value is less than .05 (b = -.438, p = .025). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. Further, the 95% confidence interval ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (JS), the logistic function decreases by .438 units  or that for every one unit increase in the predictor variable (JS) the logit transformation of the outcome variable decreases by .438. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) To that end, to aid our interpretation of the significant finding, we can move our attention to the table called Odds ratios and confidence intervals. To convert our regression coefficient to an odds ratio, the Logit function has already exponentiated it. Behind the scenes, this is what happened: \\(e^{-.438} = .646\\) We could also do this manually by using the exp function from base R. Any difference between the Logit output and the output below is attributable to rounding. # For the sake of demonstration: # Exponentiate logistic regression coefficient to convert to odds ratio # Note that the Logit function already does this for us exp(-.438) ## [1] 0.6453258 In the Odds ratios and confidence intervals table, we see that indeed the odds ratio is approximately .646. Because the odds ratio is less than 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (\\(100 \\times .354\\)) for every one unit increase in job satisfaction (JS). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of not quitting (i.e., staying): The odds of not quitting are 1.548 times as likely for every one unit increase in job satisfaction (JS). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large In the Model Fit table, note that we dont have an estimate of R-squared (R2) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-R-squared (R2) values, but for now lets focus on what is produced in the Logit function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the models fit to the data by looking at the Specified Confusion Matrices table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 39 (39.8%) and 59 (60.2%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%). Overall, we tend to be most interested in the overall percentage of correct classifications, which is 58.2%  so not a monumental amount of prediction accuracy when using just JS (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Forecasts: In the output section called Forecasts, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, were not performing true predictive analytics. As such, we wont pay much attention to interpreting this section of the output in this tutorial. With that said, if youre curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to train or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after weve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. Nagelkerke pseudo-R2: To compute Nagelkerkes pseudo-R2, we will need to install and access the DescTools package so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) To use the function, well need to re-estimate our simple logistic regression model using the glm function from base R. To request a logistic regression model as a specific type of generalized linear model, well add the family=binomial argument. Using the &lt;- assignment operator, we will assign the resulting estimated model to an object that well arbitrarily call model1. # Estimate simple logistic regression model and assign to object model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) In the PseudoR2 function, we will specify the name of the model object (model1) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerkes formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model1, &quot;Nagel&quot;) ## Nagelkerke ## 0.07258382 The estimated Nagelkerke pseudo-R2 is .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS explains 7.3% of the variance in Turnover. Because the DescTools package also has a function called Logit, lets detach the package before moving forward so that we dont inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (JS) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (b = -.438, p = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. Dealing with Bivariate Outliers: If you recall above, we found that the cases associated with row numbers 66 and 67 in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cooks distance value greater than 1.0). If you were to decide to remove cases 66 and 67, heres what you would do. First, look at the data frame (using the View function) and determine which cases row numbers 66 and 67 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that they are associated with ID equal to EMP861 and EMP862, respectively. Next, with respect to estimating the logistic regression model, the model should be specified just as it was earlier in the tutorial, but now lets add an additional argument: rows=(!ID %in% c(\"EMP861\",\"EMP862\")); the rows argument subsets the data frame within the Logit function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which ID is not (!) within the vector containing EMP861 and EMP862. Please consider revisiting the chapter on filtering (subsetting) data if you would like to see the full list of logical operators or to review how to filter out cases from a data frame before specifying the model. # Simple logistic regression model with outlier/influential cases removed Logit(Turnover ~ JS, data=td, rows=(!ID %in% c(&quot;EMP861&quot;,&quot;EMP862&quot;))) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 97 ## Number of cases retained for analysis: 96 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8799 0.7067 2.660 0.008 0.4948 3.2649 ## JS -0.4388 0.1997 -2.198 0.028 -0.8301 -0.0475 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.5528 1.6402 26.1785 ## JS 0.6448 0.4360 0.9536 ## ## ## Model Fit ## ## Null deviance: 128.887 on 95 degrees of freedom ## Residual deviance: 123.664 on 94 degrees of freedom ## ## AIC: 127.6641 ## ## Number of iterations to convergence: 4 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 96 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3202 0.6798 1.5612 0.3758 0.08581 ## 7 1.38 0 0.7815 -0.7815 -1.7807 -0.2965 0.06696 ## 73 5.48 1 0.3717 0.6283 1.4394 0.2963 0.04900 ## 12 1.72 0 0.7549 -0.7549 -1.7039 -0.2564 0.04677 ## 58 5.43 1 0.3769 0.6231 1.4281 0.2889 0.04624 ## 31 1.77 0 0.7509 -0.7509 -1.6928 -0.2507 0.04425 ## 13 1.96 0 0.7349 -0.7349 -1.6508 -0.2291 0.03562 ## 1 4.96 1 0.4264 0.5736 1.3247 0.2239 0.02592 ## 33 4.88 1 0.4350 0.5650 1.3076 0.2136 0.02334 ## 61 2.52 0 0.6844 -0.6844 -1.5305 -0.1720 0.01814 ## 97 5.59 0 0.3605 -0.3605 -0.9631 -0.2060 0.01765 ## 84 4.66 1 0.4589 0.5411 1.2617 0.1870 0.01736 ## 74 2.56 0 0.6806 -0.6806 -1.5221 -0.1685 0.01729 ## 70 5.48 0 0.3717 -0.3717 -0.9809 -0.2021 0.01715 ## 63 4.65 1 0.4600 0.5400 1.2596 0.1858 0.01713 ## 75 2.57 0 0.6797 -0.6797 -1.5200 -0.1676 0.01708 ## 80 5.04 0 0.4178 -0.4178 -1.0538 -0.1840 0.01480 ## 77 4.46 1 0.4807 0.5193 1.2209 0.1649 0.01316 ## 96 2.82 0 0.6553 -0.6553 -1.4682 -0.1483 0.01283 ## 39 4.43 1 0.4840 0.5160 1.2149 0.1618 0.01262 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 69 6.00 1 0 0.3202 0.1234 ## 97 5.59 0 0 0.3605 0.1134 ## 70 5.48 0 0 0.3717 0.1103 ## 73 5.48 1 0 0.3717 0.1103 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover predict fitted std.err ## 39 4.43 1 0 0.4840 0.07517 ## 83 4.41 0 0 0.4862 0.07449 ## 64 4.26 1 1 0.5027 0.06955 ## 27 4.15 0 1 0.5147 0.06614 ## 14 4.14 0 1 0.5158 0.06584 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover predict fitted std.err ## 7 1.38 0 1 0.7815 0.07718 ## 48 1.05 1 1 0.8052 0.08013 ## 88 0.67 1 1 0.8300 0.08191 ## 24 0.23 1 1 0.8556 0.08194 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 38 39.6 8 30 21.1 ## Turnover 1 58 60.4 9 49 84.5 ## --------------------------------------------------- ## Total 96 59.4 ## ## Accuracy: 59.38 ## Recall: 84.48 ## Precision: 62.03 47.2.4.3 Optional: Compute Predicted Probabilities Based on Sample Data The Logit function makes it easy to compute the probabilities of the even occurring based on the sample observations. In fact, all we have to do is add the pred_all=TRUE argument to the function. # Simple logistic regression model with predicted probabilities Logit(Turnover ~ JS, data=td, pred_all=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 69 6.00 1 0 0.3162 0.12145 ## 97 5.59 0 0 0.3562 0.11198 ## 70 5.48 0 0 0.3673 0.10900 ## 73 5.48 1 0 0.3673 0.10900 ## 58 5.43 1 0 0.3724 0.10758 ## 80 5.04 0 0 0.4131 0.09555 ## 1 4.96 1 0 0.4217 0.09291 ## 33 4.88 1 0 0.4302 0.09023 ## 84 4.66 1 0 0.4540 0.08274 ## 63 4.65 1 0 0.4551 0.08240 ## 21 4.64 0 0 0.4561 0.08206 ## 53 4.63 0 0 0.4572 0.08172 ## 45 4.59 0 0 0.4616 0.08036 ## 47 4.57 0 0 0.4638 0.07968 ## 77 4.46 1 0 0.4757 0.07597 ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## 29 4.11 0 1 0.5140 0.06491 ## 23 4.10 0 1 0.5151 0.06462 ## 89 4.07 0 1 0.5184 0.06376 ## 99 4.06 0 1 0.5195 0.06348 ## 94 4.03 0 1 0.5228 0.06265 ## 55 3.97 1 1 0.5293 0.06105 ## 16 3.95 0 1 0.5315 0.06054 ## 8 3.92 1 1 0.5348 0.05978 ## 42 3.90 0 1 0.5369 0.05929 ## 57 3.87 1 1 0.5402 0.05858 ## 40 3.83 0 1 0.5446 0.05767 ## 6 3.81 1 1 0.5467 0.05724 ## 50 3.79 0 1 0.5489 0.05681 ## 98 3.69 1 1 0.5597 0.05489 ## 86 3.63 1 1 0.5662 0.05390 ## 11 3.62 1 1 0.5672 0.05375 ## 76 3.50 0 1 0.5801 0.05222 ## 54 3.49 1 1 0.5812 0.05211 ## 93 3.45 0 1 0.5854 0.05174 ## 56 3.44 0 1 0.5865 0.05166 ## 87 3.42 1 1 0.5886 0.05151 ## 28 3.37 1 1 0.5939 0.05119 ## 18 3.35 1 1 0.5960 0.05109 ## 82 3.34 0 1 0.5971 0.05105 ## 92 3.33 1 1 0.5981 0.05101 ## 46 3.32 0 1 0.5992 0.05097 ## 22 3.27 1 1 0.6044 0.05085 ## 59 3.27 1 1 0.6044 0.05085 ## 36 3.26 0 1 0.6055 0.05084 ## 60 3.25 0 1 0.6065 0.05083 ## 71 3.23 0 1 0.6086 0.05082 ## 78 3.22 1 1 0.6096 0.05082 ## 91 3.21 1 1 0.6107 0.05083 ## 35 3.19 1 1 0.6127 0.05085 ## 32 3.15 0 1 0.6169 0.05094 ## 37 3.15 1 1 0.6169 0.05094 ## 43 3.05 1 1 0.6272 0.05141 ## 5 3.04 1 1 0.6282 0.05147 ## 4 3.01 1 1 0.6313 0.05168 ## 79 3.01 1 1 0.6313 0.05168 ## 20 2.98 0 1 0.6343 0.05192 ## 30 2.96 0 1 0.6364 0.05209 ## 25 2.94 1 1 0.6384 0.05228 ## 81 2.93 1 1 0.6394 0.05237 ## 96 2.82 0 1 0.6504 0.05359 ## 51 2.79 1 1 0.6534 0.05397 ## 26 2.73 1 1 0.6593 0.05478 ## 34 2.67 1 1 0.6652 0.05565 ## 67 2.65 0 1 0.6671 0.05595 ## 65 2.62 1 1 0.6700 0.05642 ## 75 2.57 0 1 0.6749 0.05721 ## 74 2.56 0 1 0.6758 0.05738 ## 61 2.52 0 1 0.6797 0.05804 ## 95 2.48 1 1 0.6835 0.05871 ## 17 2.46 1 1 0.6853 0.05905 ## 41 2.38 1 1 0.6928 0.06044 ## 9 2.35 1 1 0.6956 0.06096 ## 19 2.33 1 1 0.6975 0.06131 ## 68 2.28 1 1 0.7021 0.06220 ## 72 2.08 1 1 0.7201 0.06571 ## 15 2.03 1 1 0.7245 0.06657 ## 90 2.00 1 1 0.7271 0.06708 ## 13 1.96 0 1 0.7305 0.06775 ## 38 1.96 1 1 0.7305 0.06775 ## 62 1.92 1 1 0.7340 0.06842 ## 49 1.84 1 1 0.7407 0.06971 ## 31 1.77 0 1 0.7466 0.07079 ## 85 1.76 1 1 0.7474 0.07095 ## 2 1.72 1 1 0.7507 0.07154 ## 12 1.72 0 1 0.7507 0.07154 ## 10 1.69 1 1 0.7532 0.07198 ## 3 1.64 1 1 0.7572 0.07270 ## 44 1.43 1 1 0.7737 0.07541 ## 7 1.38 0 1 0.7775 0.07598 ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 39 39.8 8 31 20.5 ## Turnover 1 59 60.2 10 49 83.1 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Recall: 83.05 ## Precision: 61.25 In the output under the Forecasts section, you should see a section called Data, Fitted Values, Standard Errors. This section now contains the predicted probabilities and associated classifications for all cases in the sample. The column labeled fitted contains the predicted probabilities, and the column labeled predict contains the probabilities when applied to a default probability threshold of .50, such that 1 indicates that the probability was .50 or greater (i.e., event predicted to occur) 0 indicates the probability was less than .50 (i.e., event not predicted to occur). If desired, we can also append the predicted probabilities (i.e., fitted values) to our existing data frame by referencing the row names (i.e., row numbers) of each object when we merge. Lets overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (prob_JS), followed by the = operator and our simple logistic regression model from above with $fitted.values to the end. This will extract just the fitted values from the output and then convert the vector to a data frame object. As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( prob_JS = Logit(Turnover ~ JS, data=td, pred_all=TRUE)$fitted.values ), by=&quot;row.names&quot;, all=TRUE) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 69 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 7 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 73 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 58 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 12 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 31 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 13 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 33 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 84 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 63 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 61 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 70 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 74 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 75 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 67 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 80 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 77 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 39 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 69 6.00 1 0 0.3162 0.12145 ## 97 5.59 0 0 0.3562 0.11198 ## 70 5.48 0 0 0.3673 0.10900 ## 73 5.48 1 0 0.3673 0.10900 ## 58 5.43 1 0 0.3724 0.10758 ## 80 5.04 0 0 0.4131 0.09555 ## 1 4.96 1 0 0.4217 0.09291 ## 33 4.88 1 0 0.4302 0.09023 ## 84 4.66 1 0 0.4540 0.08274 ## 63 4.65 1 0 0.4551 0.08240 ## 21 4.64 0 0 0.4561 0.08206 ## 53 4.63 0 0 0.4572 0.08172 ## 45 4.59 0 0 0.4616 0.08036 ## 47 4.57 0 0 0.4638 0.07968 ## 77 4.46 1 0 0.4757 0.07597 ## 39 4.43 1 0 0.4790 0.07497 ## 83 4.41 0 0 0.4812 0.07431 ## 64 4.26 1 0 0.4976 0.06946 ## 27 4.15 0 1 0.5097 0.06609 ## 14 4.14 0 1 0.5107 0.06579 ## 29 4.11 0 1 0.5140 0.06491 ## 23 4.10 0 1 0.5151 0.06462 ## 89 4.07 0 1 0.5184 0.06376 ## 99 4.06 0 1 0.5195 0.06348 ## 94 4.03 0 1 0.5228 0.06265 ## 55 3.97 1 1 0.5293 0.06105 ## 16 3.95 0 1 0.5315 0.06054 ## 8 3.92 1 1 0.5348 0.05978 ## 42 3.90 0 1 0.5369 0.05929 ## 57 3.87 1 1 0.5402 0.05858 ## 40 3.83 0 1 0.5446 0.05767 ## 6 3.81 1 1 0.5467 0.05724 ## 50 3.79 0 1 0.5489 0.05681 ## 98 3.69 1 1 0.5597 0.05489 ## 86 3.63 1 1 0.5662 0.05390 ## 11 3.62 1 1 0.5672 0.05375 ## 76 3.50 0 1 0.5801 0.05222 ## 54 3.49 1 1 0.5812 0.05211 ## 93 3.45 0 1 0.5854 0.05174 ## 56 3.44 0 1 0.5865 0.05166 ## 87 3.42 1 1 0.5886 0.05151 ## 28 3.37 1 1 0.5939 0.05119 ## 18 3.35 1 1 0.5960 0.05109 ## 82 3.34 0 1 0.5971 0.05105 ## 92 3.33 1 1 0.5981 0.05101 ## 46 3.32 0 1 0.5992 0.05097 ## 22 3.27 1 1 0.6044 0.05085 ## 59 3.27 1 1 0.6044 0.05085 ## 36 3.26 0 1 0.6055 0.05084 ## 60 3.25 0 1 0.6065 0.05083 ## 71 3.23 0 1 0.6086 0.05082 ## 78 3.22 1 1 0.6096 0.05082 ## 91 3.21 1 1 0.6107 0.05083 ## 35 3.19 1 1 0.6127 0.05085 ## 32 3.15 0 1 0.6169 0.05094 ## 37 3.15 1 1 0.6169 0.05094 ## 43 3.05 1 1 0.6272 0.05141 ## 5 3.04 1 1 0.6282 0.05147 ## 4 3.01 1 1 0.6313 0.05168 ## 79 3.01 1 1 0.6313 0.05168 ## 20 2.98 0 1 0.6343 0.05192 ## 30 2.96 0 1 0.6364 0.05209 ## 25 2.94 1 1 0.6384 0.05228 ## 81 2.93 1 1 0.6394 0.05237 ## 96 2.82 0 1 0.6504 0.05359 ## 51 2.79 1 1 0.6534 0.05397 ## 26 2.73 1 1 0.6593 0.05478 ## 34 2.67 1 1 0.6652 0.05565 ## 67 2.65 0 1 0.6671 0.05595 ## 65 2.62 1 1 0.6700 0.05642 ## 75 2.57 0 1 0.6749 0.05721 ## 74 2.56 0 1 0.6758 0.05738 ## 61 2.52 0 1 0.6797 0.05804 ## 95 2.48 1 1 0.6835 0.05871 ## 17 2.46 1 1 0.6853 0.05905 ## 41 2.38 1 1 0.6928 0.06044 ## 9 2.35 1 1 0.6956 0.06096 ## 19 2.33 1 1 0.6975 0.06131 ## 68 2.28 1 1 0.7021 0.06220 ## 72 2.08 1 1 0.7201 0.06571 ## 15 2.03 1 1 0.7245 0.06657 ## 90 2.00 1 1 0.7271 0.06708 ## 13 1.96 0 1 0.7305 0.06775 ## 38 1.96 1 1 0.7305 0.06775 ## 62 1.92 1 1 0.7340 0.06842 ## 49 1.84 1 1 0.7407 0.06971 ## 31 1.77 0 1 0.7466 0.07079 ## 85 1.76 1 1 0.7474 0.07095 ## 2 1.72 1 1 0.7507 0.07154 ## 12 1.72 0 1 0.7507 0.07154 ## 10 1.69 1 1 0.7532 0.07198 ## 3 1.64 1 1 0.7572 0.07270 ## 44 1.43 1 1 0.7737 0.07541 ## 7 1.38 0 1 0.7775 0.07598 ## 66 1.19 1 1 0.7916 0.07790 ## 48 1.05 1 1 0.8015 0.07904 ## 88 0.67 1 1 0.8266 0.08096 ## 24 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 39 39.8 8 31 20.5 ## Turnover 1 59 60.2 10 49 83.1 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Recall: 83.05 ## Precision: 61.25 If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 prob_JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 We can then take those predicted probabilities and apply a threshold by which scores higher than that threshold will indicate that the event is likely to happen. A common threshold for dichotomous outcomes is .50. Well use the mutate function from the dplyr package and the ifelse function base R to make this happen. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) # Classify probabilities as 1 (Quit) or 0 (Stay) # Using threshold of .50 td &lt;- mutate(td, Pred_Turnover = ifelse(prob_JS &gt;= .50, # apply threshold 1, # if logical statement is true (&quot;Quit&quot;) 0) # if logical statement is false (&quot;Stay&quot;) ) Its important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain fresh data on the JS predictor variable, which we could then use to plug into our model; to do this with a model estimated using the Logit model, setting up the model as follows may be the best course of action. # Assign simple logistic regression model to an object mod &lt;- Logit(Turnover ~ JS, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 98 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 1.8554 0.6883 2.695 0.007 0.5063 3.2044 ## JS -0.4378 0.1958 -2.236 0.025 -0.8216 -0.0540 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 6.3939 1.6591 24.6415 ## JS 0.6455 0.4397 0.9475 ## ## ## Model Fit ## ## Null deviance: 131.746 on 97 degrees of freedom ## Residual deviance: 126.341 on 96 degrees of freedom ## ## AIC: 130.3413 ## ## Number of iterations to convergence: 4 ## ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 98 cases (rows) of data] ## -------------------------------------------------------------------- ## JS Turnover fitted residual rstudent dffits cooks ## 66 6.00 1 0.3162 0.6838 1.5688 0.3725 0.08496 ## 67 1.38 0 0.7775 -0.7775 -1.7682 -0.2877 0.06241 ## 71 5.48 1 0.3673 0.6327 1.4476 0.2949 0.04889 ## 54 5.43 1 0.3724 0.6276 1.4363 0.2877 0.04618 ## 4 1.72 0 0.7507 -0.7507 -1.6920 -0.2486 0.04353 ## 25 1.77 0 0.7466 -0.7466 -1.6810 -0.2429 0.04117 ## 5 1.96 0 0.7305 -0.7305 -1.6393 -0.2219 0.03314 ## 1 4.96 1 0.4217 0.5783 1.3332 0.2239 0.02609 ## 27 4.88 1 0.4302 0.5698 1.3162 0.2138 0.02353 ## 83 4.66 1 0.4540 0.5460 1.2703 0.1875 0.01757 ## 60 4.65 1 0.4551 0.5449 1.2682 0.1863 0.01733 ## 58 2.52 0 0.6797 -0.6797 -1.5199 -0.1668 0.01693 ## 97 5.59 0 0.3562 -0.3562 -0.9554 -0.2021 0.01693 ## 68 5.48 0 0.3673 -0.3673 -0.9731 -0.1985 0.01648 ## 72 2.56 0 0.6758 -0.6758 -1.5115 -0.1635 0.01615 ## 73 2.57 0 0.6749 -0.6749 -1.5095 -0.1626 0.01596 ## 64 2.65 0 0.6671 -0.6671 -1.4929 -0.1563 0.01454 ## 79 5.04 0 0.4131 -0.4131 -1.0457 -0.1813 0.01431 ## 75 4.46 1 0.4757 0.5243 1.2296 0.1656 0.01336 ## 33 4.43 1 0.4790 0.5210 1.2235 0.1625 0.01282 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS Turnover predict fitted std.err ## 66 6.00 1 0 0.3162 0.1215 ## 97 5.59 0 0 0.3562 0.1120 ## 68 5.48 0 0 0.3673 0.1090 ## 71 5.48 1 0 0.3673 0.1090 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS Turnover predict fitted std.err ## 33 4.43 1 0 0.4790 0.07497 ## 82 4.41 0 0 0.4812 0.07431 ## 61 4.26 1 0 0.4976 0.06946 ## 20 4.15 0 1 0.5097 0.06609 ## 6 4.14 0 1 0.5107 0.06579 ## ## ... for the last 4 rows of sorted data ... ## ## JS Turnover predict fitted std.err ## 63 1.19 1 1 0.7916 0.07790 ## 43 1.05 1 1 0.8015 0.07904 ## 87 0.67 1 1 0.8266 0.08096 ## 17 0.23 1 1 0.8525 0.08116 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 39 39.8 8 31 20.5 ## Turnover 1 59 60.2 10 49 83.1 ## --------------------------------------------------- ## Total 98 58.2 ## ## Accuracy: 58.16 ## Recall: 83.05 ## Precision: 61.25 # Extract intercept and predictor coefficient b0 = unname(mod$coefficients[&quot;(Intercept)&quot;]) # extract intercept b1 = unname(mod$coefficients[&quot;JS&quot;]) # extract predictor coefficient # Specify equation to estimate predicted probabilities # using sample data on which model was estimated td$prob_JS_alt &lt;- exp(b0 + b1*td$JS) / (1 + exp(b0 + b1*td$JS)) If we so desired, we could plug in new values for JS from another data frame object like we did in the chapter on predicting criterion scores using simple linear regression. Or we could specify a vector of values for JS that we wish to compute the predicted probabilities of turnover. # Create vector of plausible values for JS vec_JS &lt;- c(0,1,2,3,4,5,6) # &quot;Plug in&quot; vector of values in equation exp(b0 + b1*vec_JS) / (1 + exp(b0 + b1*vec_JS)) ## [1] 0.8647542 0.8049594 0.7270717 0.6322888 0.5260465 0.4173924 0.3162077 The 7 predicted probabilities for turning over corresponding to the 7 values we listed in our vector have been printed in order to our Console. 47.2.5 Estimate Multiple Logistic Regression Model Now we will expand our initial model by specifying a multiple logistic regression model, which means the model will include more than one predictor variable. Lets begin by regressing Turnover on JS, NAff, and TI using the Logit function from the lessR package. If you havent already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) As the first argument in the Logit function, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables are typed to the right of the ~ symbol. Separate predictor variables with the + symbol. As the second argument, type data= followed by the name of the data frame to which both of the variables belong (td). Lets estimate a model with JS, NAff, and TI as the predictor variables. # Estimate multiple logistic regression model Logit(Turnover ~ JS + NAff + TI, data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## Predictor Variable 2: NAff ## Predictor Variable 3: TI ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 95 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) -3.9286 1.8120 -2.168 0.030 -7.4800 -0.3772 ## JS -0.2332 0.2215 -1.053 0.293 -0.6674 0.2010 ## NAff 1.1952 0.4996 2.392 0.017 0.2160 2.1743 ## TI 0.8967 0.3166 2.832 0.005 0.2761 1.5172 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 0.0197 0.0006 0.6858 ## JS 0.7920 0.5130 1.2227 ## NAff 3.3041 1.2411 8.7963 ## TI 2.4514 1.3180 4.5593 ## ## ## Model Fit ## ## Null deviance: 127.017 on 94 degrees of freedom ## Residual deviance: 105.229 on 91 degrees of freedom ## ## AIC: 113.2285 ## ## Number of iterations to convergence: 4 ## ## ## Collinearity ## ## Tolerance VIF ## JS 0.885 1.129 ## NAff 0.973 1.028 ## TI 0.903 1.108 ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 95 cases (rows) of data] ## -------------------------------------------------------------------- ## JS NAff TI Turnover fitted residual rstudent dffits cooks ## 6 4.14 3.07 4.33 0 0.93448 -0.9345 -2.4565 -0.4581 0.15185 ## 1 4.96 1.87 0.51 1 0.08371 0.9163 2.3396 0.4680 0.13427 ## 66 6.00 1.45 2.37 1 0.18699 0.8130 1.9301 0.5252 0.10092 ## 42 4.57 2.01 4.27 0 0.77499 -0.7750 -1.8127 -0.5016 0.08228 ## 96 2.82 2.05 4.25 0 0.84220 -0.8422 -1.9794 -0.3798 0.05870 ## 74 3.50 3.28 1.37 0 0.59961 -0.5996 -1.4140 -0.4473 0.04692 ## 27 4.88 1.76 1.81 1 0.20749 0.7925 1.8220 0.3639 0.04554 ## 26 3.15 2.84 3.48 0 0.86430 -0.8643 -2.0397 -0.3080 0.04250 ## 67 1.38 2.30 3.18 0 0.79411 -0.7941 -1.8204 -0.3391 0.03973 ## 61 4.26 1.73 2.10 1 0.27461 0.7254 1.6390 0.3009 0.02635 ## 68 5.48 2.27 2.78 0 0.49981 -0.4998 -1.2109 -0.3241 0.02182 ## 4 1.72 2.11 1.78 0 0.44724 -0.4472 -1.1203 -0.3091 0.01882 ## 25 1.77 2.24 1.16 0 0.34887 -0.3489 -0.9604 -0.3157 0.01803 ## 52 3.44 2.35 3.37 0 0.75019 -0.7502 -1.6856 -0.2364 0.01720 ## 54 5.43 3.01 2.43 1 0.64141 0.3586 0.9747 0.3071 0.01719 ## 69 3.23 2.84 2.18 0 0.66088 -0.6609 -1.4931 -0.2582 0.01719 ## 8 3.95 2.87 1.36 0 0.45014 -0.4501 -1.1216 -0.2899 0.01661 ## 75 4.46 2.81 1.80 1 0.50095 0.4991 1.2014 0.2785 0.01611 ## 77 3.01 1.53 3.01 1 0.47428 0.5257 1.2459 0.2729 0.01593 ## 64 2.65 1.94 3.27 0 0.66911 -0.6691 -1.5074 -0.2437 0.01552 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS NAff TI Turnover predict fitted std.err ## 1 4.96 1.87 0.51 1 0 0.08371 0.05860 ## 97 5.59 1.60 1.62 0 0 0.13385 0.07684 ## 99 4.06 2.19 0.65 0 0 0.15774 0.08794 ## 66 6.00 1.45 2.37 1 0 0.18699 0.10944 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS NAff TI Turnover predict fitted std.err ## 77 3.01 1.53 3.01 1 0 0.4743 0.11354 ## 53 3.87 1.72 3.05 1 0 0.4899 0.09955 ## 68 5.48 2.27 2.78 0 0 0.4998 0.13673 ## 75 4.46 2.81 1.80 1 1 0.5009 0.11987 ## 78 3.92 2.80 1.70 1 1 0.5070 0.11250 ## ## ... for the last 4 rows of sorted data ... ## ## JS NAff TI Turnover predict fitted std.err ## 17 0.23 2.38 4.20 1 1 0.9327 0.04890 ## 6 4.14 3.07 4.33 0 1 0.9345 0.04905 ## 19 2.73 3.64 3.36 1 1 0.9426 0.04146 ## 95 2.48 3.18 4.74 1 1 0.9719 0.02322 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 37 38.9 23 14 62.2 ## Turnover 1 58 61.1 6 52 89.7 ## --------------------------------------------------- ## Total 95 78.9 ## ## Accuracy: 78.95 ## Recall: 89.66 ## Precision: 78.79 Note: In some instances, you might receive the error message shown below. You can ignore this message, as it just indicates that you have a poor predictor variable in the model that results in fitted/predicted values that are all the same. If you get this message, proceed forward with your interpretation of the output. \\(\\color{red}{\\text{Error:}}\\) \\(\\color{red}{\\text{All predicted values are 0.}}\\) \\(\\color{red}{\\text{Something is wrong here.}}\\) The output generates the model coefficient estimates, the odds ratios and confidence intervals, model fit information (i.e., AIC), collinearity diagnostics, outlier detection, forecasts, and a confusion matrix. At the top of the output, we get information about which variables were included in our model, the number of cases (e.g., employees) in the data, and the number of cases retained for the analysis (N = 95) after removing cases with missing data on any of the model variables. 47.2.5.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a multiple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Multivariate Outliers: To determine whether the data are free of multivariate outliers, lets take a look at the text output section called Analysis of Residuals and Influence. We should find a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: Studentized residual (rstdnt), number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and Cooks distance (cooks). The case associated with row number 14 has the highest Cooks distance value (.152), followed by the cases associated with row numbers 1 (.134), 69 (.101), and 47 (.082). A liberal threshold Cooks distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 14, 1, 69 and 47 from our data frame; though, I wouldnt necessarily recommend this, as these Cooks distance values arent too concerning. In general, we should be wary of removing outliers or influential cases and should do so only when we have a very strong justification for doing so. No (Multi)Collinearity Between Predictor Variables: To assess whether (multi)collinearity might be an issue, check out the table called Collinearity in the output. This table shows two indices of collinearity: tolerance and valence inflation factor (VIF). Because the VIF is just the reciprocal of the tolerance (1/tolerance), lets focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are very lower levels of collinearity. In the table, we can see that the tolerance statistics are all closer to 1.00 and well above .20, thereby indicating that collinearity is not likely an issue. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To investigate the assumption of linearity between continuous predictor variables and the logit transformation of the outcome variable, we can add the interaction between each continuous predictor variable and its logarithmic (i.e., natural log) transformation. We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between each predictor variable (i.e., JS, NAff, TI) and its logarithmic transformation to our logistic regression model  but not the main effect for the logarithmic transformation. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of each predictor variable JS followed by the + operator. After the + operator, type the name of the same predictor variable JS, followed by the : operator and the log function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all continuous predictor variables in the model. # Box-Tidwell diagnostic test of linearity between continuous predictor variables and # logit transformation of outcome variable Logit(Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + TI + TI:log(TI), data=td) ## ## Response Variable: Turnover ## Predictor Variable 1: JS ## Predictor Variable 2: NAff ## Predictor Variable 3: TI ## ## Number of cases (rows) of data: 99 ## Number of cases retained for analysis: 95 ## ## ## ## BASIC ANALYSIS ## ## Model Coefficients ## ## Estimate Std Err z-value p-value Lower 95% Upper 95% ## (Intercept) 0.9206 9.8331 0.094 0.925 -18.3519 20.1932 ## JS -4.5845 2.6176 -1.751 0.080 -9.7149 0.5459 ## NAff -0.4107 6.4047 -0.064 0.949 -12.9636 12.1422 ## TI 3.2888 3.0158 1.091 0.275 -2.6221 9.1998 ## JS:log(JS) 2.0053 1.1844 1.693 0.090 -0.3160 4.3266 ## NAff:log(NAff) 0.9559 3.5321 0.271 0.787 -5.9670 7.8788 ## TI:log(TI) -1.2101 1.5232 -0.794 0.427 -4.1956 1.7754 ## ## ## Odds ratios and confidence intervals ## ## Odds Ratio Lower 95% Upper 95% ## (Intercept) 2.5108 0.0000 588545072.9802 ## JS 0.0102 0.0001 1.7261 ## NAff 0.6632 0.0000 187626.1824 ## TI 26.8110 0.0726 9894.6983 ## JS:log(JS) 7.4284 0.7291 75.6872 ## NAff:log(NAff) 2.6010 0.0026 2640.6975 ## TI:log(TI) 0.2982 0.0151 5.9024 ## ## ## Model Fit ## ## Null deviance: 127.017 on 94 degrees of freedom ## Residual deviance: 100.854 on 88 degrees of freedom ## ## AIC: 114.8541 ## ## Number of iterations to convergence: 5 ## ## ## Collinearity ## ## Tolerance VIF ## JS 0.030 32.947 ## NAff 0.012 82.947 ## TI 0.073 13.775 ## ## ## ## ANALYSIS OF RESIDUALS AND INFLUENCE ## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance ## [sorted by Cook&#39;s Distance] ## [res_rows = 20 out of 95 cases (rows) of data] ## -------------------------------------------------------------------- ## JS NAff TI Turnover fitted residual rstudent dffits cooks ## 1 4.96 1.87 0.51 1 0.03086 0.9691 3.2985 0.9563 0.63056 ## 6 4.14 3.07 4.33 0 0.91111 -0.9111 -2.3981 -0.6526 0.14155 ## 67 1.38 2.30 3.18 0 0.91516 -0.9152 -2.3760 -0.5617 0.10829 ## 68 5.48 2.27 2.78 0 0.73899 -0.7390 -1.7745 -0.6741 0.07686 ## 66 6.00 1.45 2.37 1 0.55250 0.4475 1.2378 0.7960 0.07043 ## 42 4.57 2.01 4.27 0 0.72166 -0.7217 -1.7245 -0.6507 0.06898 ## 74 3.50 3.28 1.37 0 0.50709 -0.5071 -1.3129 -0.6965 0.05735 ## 96 2.82 2.05 4.25 0 0.72223 -0.7222 -1.6950 -0.5521 0.04977 ## 79 5.04 1.22 2.92 0 0.42774 -0.4277 -1.1658 -0.6487 0.04597 ## 4 1.72 2.11 1.78 0 0.53924 -0.5392 -1.3118 -0.4774 0.02800 ## 27 4.88 1.76 1.81 1 0.25761 0.7424 1.6982 0.3893 0.02590 ## 25 1.77 2.24 1.16 0 0.31999 -0.3200 -0.9531 -0.5030 0.02531 ## 26 3.15 2.84 3.48 0 0.83033 -0.8303 -1.9278 -0.3366 0.02494 ## 81 3.34 1.27 2.96 0 0.33222 -0.3322 -0.9633 -0.4596 0.02133 ## 77 3.01 1.53 3.01 1 0.41376 0.5862 1.3781 0.4014 0.02101 ## 21 3.37 1.65 4.16 1 0.57404 0.4260 1.1068 0.4163 0.01894 ## 61 4.26 1.73 2.10 1 0.26659 0.7334 1.6620 0.3239 0.01759 ## 70 2.08 1.62 4.55 1 0.75821 0.2418 0.7970 0.3930 0.01463 ## 52 3.44 2.35 3.37 0 0.68034 -0.6803 -1.5371 -0.2832 0.01210 ## 58 2.52 1.30 2.13 0 0.24723 -0.2472 -0.7985 -0.3562 0.01207 ## ## ## FORECASTS ## ## Probability threshold for predicting : 0.5 ## ## ## Data, Fitted Values, Standard Errors ## [sorted by fitted value] ## [to save space only some intervals printed, pred_all=TRUE to see all] ## -------------------------------------------------------------------- ## JS NAff TI Turnover predict fitted std.err ## 1 4.96 1.87 0.51 1 0 0.03086 0.05764 ## 99 4.06 2.19 0.65 0 0 0.04451 0.06863 ## 88 4.07 2.21 1.21 0 0 0.14009 0.09587 ## 46 3.79 1.45 1.83 0 0 0.15073 0.09141 ## ## ... for the rows of data where fitted is close to 0.5 ... ## ## JS NAff TI Turnover predict fitted std.err ## 49 4.63 2.45 2.01 0 0 0.4820 0.1237 ## 86 3.42 2.23 2.57 1 0 0.4962 0.1032 ## 28 2.67 1.95 2.68 1 0 0.4997 0.1070 ## 74 3.50 3.28 1.37 0 1 0.5071 0.2402 ## 13 2.98 2.32 2.41 0 1 0.5145 0.1018 ## ## ... for the last 4 rows of sorted data ... ## ## JS NAff TI Turnover predict fitted std.err ## 95 2.48 3.18 4.74 1 1 0.9499 0.063267 ## 19 2.73 3.64 3.36 1 1 0.9537 0.060780 ## 87 0.67 2.56 3.08 1 1 0.9890 0.020304 ## 17 0.23 2.38 4.20 1 1 0.9988 0.003638 ## -------------------------------------------------------------------- ## ## ## ---------------------------- ## Specified confusion matrices ## ---------------------------- ## ## Probability threshold for predicting : 0.5 ## ## Baseline Predicted ## --------------------------------------------------- ## Total %Tot 0 1 %Correct ## --------------------------------------------------- ## 0 37 38.9 21 16 56.8 ## Turnover 1 58 61.1 8 50 86.2 ## --------------------------------------------------- ## Total 95 74.7 ## ## Accuracy: 74.74 ## Recall: 86.21 ## Precision: 75.76 Because the p-values associated with each of the interaction terms and their regression coefficients (JS:log(JS), NAff:log(NAff), TI:log(TI)) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio). Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a previous section on simple logistic regression. 47.2.5.2 Interpret Model Results Basic Analysis: The Basic Analysis section of the original output first displays a table called the Model Coefficients, which includes the regression coefficients (slopes, weights) and their standard errors, z-values, p-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (JS) in relation to the logit transformation of the outcome variable (Turnover) is not statistically significant when controlling for the other predictor variables in the model because the associated p-value is equal to or greater than .05 (b = -.233, p = .293, 95% CI[-0.667, .201]). The regression coefficient for NAff is 1.195 and its associated p-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (b = 1.195, p = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for TI is .897 and its associated p-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (b = .897, p = .005, 95% CI[.276, 1.517]). Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, in some contexts we might not write out the regression equation with that variable included; instead, we might re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) To that end, to aid our interpretation of the significant finding, we can move our attention to the table called Odds ratios and confidence intervals. To convert our regression coefficients to odds ratios, the function is simply exponentiating them. Behind the scenes, this is what happened: \\(e^{1.195} = 3.304\\) We can also do this manually using the exp function from base R. Any difference between the Logit output and the output below is attributable to rounding. # For the sake of demonstration: # Exponentiate logistic regression coefficient to convert to odds ratio # Note that the Logit function already does this for us exp(1.195) ## [1] 3.303558 In the Odds ratios and confidence intervals table, we see that the odds ratios are automatically computed for us. We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belong to NAff and TI. Regarding NAff, its odds ratio of 3.304 is greater than 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times greater for every one unit increase in NAff when controlling for other predictor variables in the model. Regarding TI, its odds ratio of 2.451 is also greater than 1, and thus, the odds of quitting are 2.451 times greater for every one unit increase in TI when controlling for other predictor variables in the model. Note that the odds ratio (OR) can be conceptualized as a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large In the Model Fit table, note that we dont have an estimate of R-squared (R2) like we would with a traditional linear regression model. There are ways to compute what are often referred to as pseudo-R-squared (R2) values, but for now lets focus on what is produced in the Logit function output. As you can see, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the models fit to the data by looking at the Specified Confusion Matrices table at the end of the output. This table makes model fit assessments fairly intuitive. First, in the baseline section (which is akin to a null model without any predictors), the confusion matrix provides information about actual the counts and percentages of employees who stayed and quit the organization, which were 37 (38.9%) and 58 (61.1%), respectively. [Remember that for the Turnover variable, 0 = stayed and 1 = quit in our data.] In the predicted section, the table provides information about who would be predicted to stay and who would be predicted to quit based on our logistic regression model. Anyone who has a predicted probability of .50 or higher is predicted to quit, and anyone who has a predicted probability that is less than .50 is predicted to stay. Further, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, this cross-tabulation helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 89.7% accuracy. Overall, we tend to be most interested in the overall percentage of correct classifications, which is 78.9%. That is, based on the logistic regression model that included the predictor variables of JS, NAff, and TI, we were able to predict actual turnover behavior from the same sample 78.9% of the time, which was a big improvement over the simple logistic regression model in which we had just JS as a predictor variable. Forecasts: In the output section called Forecasts, information about the actual outcome and the predicted and fitted values are presented (along with the standard error). This section moves us toward what would be considered true predictive analytics and machine learning; however, because we only have a single dataset to train our model and test it, were not performing true predictive analytics. As such, we wont pay much attention to interpreting this section of the output in this tutorial. With that said, if youre curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one training dataset that we use to train or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a test dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a validation dataset to evaluate the training dataset(s), and after weve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. Nagelkerke pseudo-R2: To compute Nagelkerkes pseudo-R2, we will need to install and access the DescTools package (if we havent already) so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) To use the function, well need to re-estimate our multiple logistic regression model using the glm function from base R. To request a logistic regression model as a specific type of generalized linear model, well add the family=binomial argument. Using the &lt;- assignment operator, we will assign the resulting estimated model to an object that well arbitrarily call model2. # Estimate simple logistic regression model and assign to object model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) In the PseudoR2 function, we will specify the name of the model object (model2) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerkes formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model2, &quot;Nagel&quot;) ## Nagelkerke ## 0.2779516 The estimated Nagelkerke pseudo-R2 is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-R2 was just .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS, NAff, and TI explain 27.8% of the variance in Turnover. Because the DescTools package also has a function called Logit, lets detach the package before moving forward so that we dont inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-R2 was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover. 47.2.6 Summary In this chapter, we learned how to estimate simple and multiple logistic regression models using the Logit function from lessR. 47.3 Chapter Supplement In addition to the Logit function from the lessR package covered above, we can use the glm function from base R to estimate a simple and multiple logistic regression models and the predict function from base R to predict probabilities of the event in question occurring. Because these functions come from base R, we do not need to install and access an additional package. 47.3.1 Functions &amp; Packages Introduced Function Package glm base R log base R exp base R plot base R cooks.distance base R summary base R confint base R / MASS coef base R subset base R na.omit base R xtabs base R print base R prop.table base R predict base R sum base R scale base R vif car PseudoR2 DescTools c base R merge base R data.frame base R mutate dplyr ifelse base R 47.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object td &lt;- read_csv(&quot;Turnover.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## ID = col_character(), ## Turnover = col_double(), ## JS = col_double(), ## OC = col_double(), ## TI = col_double(), ## NAff = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(td) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;NAff&quot; # View variable type for each variable in data frame (tibble) object str(td) ## spec_tbl_df[,6] [99 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : chr [1:99] &quot;EMP559&quot; &quot;EMP561&quot; &quot;EMP571&quot; &quot;EMP589&quot; ... ## $ Turnover: num [1:99] 1 1 1 1 1 1 0 1 1 1 ... ## $ JS : num [1:99] 4.96 1.72 1.64 3.01 3.04 3.81 1.38 3.92 2.35 1.69 ... ## $ OC : num [1:99] 5.32 1.47 0.87 2.15 1.94 3.81 0.83 3.88 3.03 2.82 ... ## $ TI : num [1:99] 0.51 4.08 2.65 4.17 3.27 3.01 3.18 1.7 2.44 2.58 ... ## $ NAff : num [1:99] 1.87 2.48 2.84 2.43 2.76 3.67 2.3 2.8 2.71 2.07 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_character(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. NAff = col_double() ## .. ) # View first 6 rows of data frame (tibble) object head(td) ## # A tibble: 6 x 6 ## ID Turnover JS OC TI NAff ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EMP559 1 4.96 5.32 0.51 1.87 ## 2 EMP561 1 1.72 1.47 4.08 2.48 ## 3 EMP571 1 1.64 0.87 2.65 2.84 ## 4 EMP589 1 3.01 2.15 4.17 2.43 ## 5 EMP592 1 3.04 1.94 3.27 2.76 ## 6 EMP601 1 3.81 3.81 3.01 3.67 47.3.3 Simple Logistic Regression Model Using glm Function from Base R The glm function from base R stands for generalized linear model (GLM), making it an appropriate function for estimating logistic regression models and other models in the GLM family; logistic regression model is a GLM with a logit link function. Lets begin by specifying JS as a predictor variable for Turnover. Using the &lt;- assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., model1). Type the name of the glm function. As the functions first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. As the third argument, type family=binomial, which by default estimates a logit (logistic) model. # Estimate simple logistic regression model model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) Note: You wont see a summary of the results in the Console, as we have not requested those. As a next step, we will use the model1 object to determine whether we have satisfied the statistical assumptions of a simple logistic regression model. 47.3.3.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Bivariate Outliers: To determine whether the data are free of bivariate outliers, lets look at Cooks distance (D) values across cases. Using the plot function from base R, type the name of our model object (model1) as the first argument, and as the second argument, enter the numeral 4 to request the fourth diagnostic plot, which is the Cooks distance plot. # Diagnostics plot: Cook&#39;s Distance plot plot(model1, 4) The case associated with row number 66 has the highest Cooks distance value, followed by the cases associated with row numbers 67 and 71  although 66 and 67 seem to be clearest potential outliers of the three. We can also print the cases with the highest Cooks distances. To do so, lets create an object called cooksD to which we will assign a vector of Cooks distance values using the cooks.distance function from base R. Just type the name of the logistic regression model object (model1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and typing the cooksD object name as the first object and decreasing=TRUE as the second argument; this will sort the Cooks distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(model1) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top 20 Cook&#39;s distance values head(cooksD, n=20) ## 69 7 73 58 12 31 13 1 33 ## 0.08495602 0.06240864 0.04888996 0.04617863 0.04352935 0.04117397 0.03313842 0.02608930 0.02352896 ## 84 63 61 97 70 74 75 67 80 ## 0.01756755 0.01733141 0.01693066 0.01692984 0.01648318 0.01614505 0.01595550 0.01453516 0.01431210 ## 77 39 ## 0.01336026 0.01281733 Again, we see cases associated with row numbers 66 (.085), 67 (.062), and 71 (.049) having the highest Cooks distance values, with 66 and 67 still looking like the most influential cases of the lot. As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 66 and 67 from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cooks distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cooks distance values are all well-below the more liberal threshold, so I would say were safe to leave the associated cases in the data. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add the interaction between the predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between our predictor variable JS and its logarithmic transformation to our logistic regression model  but not the main effect for the logarithmic transformation of JS. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ operator. To the right of the ~ operator, type the name of the predictor variable JS followed by the + operator. After the + operator, type the name of the predictor variable JS, followed by the : operator and the log function from base R with the predictor variable JS as its sole parenthetical argument. Finally, we will use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (boxtidwell). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable boxtidwell &lt;- glm(Turnover ~ JS + JS:log(JS), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS + JS:log(JS), family = binomial, ## data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9972 -1.1790 0.7179 1.0762 1.2077 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.4461 3.2062 1.699 0.0894 . ## JS -2.9840 2.1693 -1.376 0.1690 ## JS:log(JS) 1.1696 0.9778 1.196 0.2316 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 124.62 on 95 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.62 ## ## Number of Fisher Scoring iterations: 5 Because the interaction term (JS:log(JS)) regression coefficient of 1.1696 in the Coefficients table is nonsignificant (p = .232), we have no reason to believe that the association between the continuous predictor variable and the logit transformation of the outcome variable is nonlinear. If the interaction term had been statistically significant, then we might have evidence that the assumption was violated, and one potential solution would be to estimate a polynomial model of some kind to better fit the data; for more information on estimating nonlinear associations, check out Chapter 7 (Curvilinear Effects in Logistic Regression) from Osborne (2015). Finally, we only apply this test when the predictor variable in question is continuous (interval, ratio). In practice, however, note that for reasons of parsimony, we sometimes we might choose to estimate a linear model over a nonlinear/polynomial model when the former fits the data reasonably well. Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. Just for the sake of demonstration, lets transform the JS variable so that its lowest score is equal to zero. This will give us an opportunity to test the two approaches I described above. Simply, create a new variable (JS_0) that is equal to JS minus the minimum value of JS. # ONLY FOR DEMONSTRATION PURPOSES: Create new predictor variable where lowest score is zero td$JS_0 &lt;- td$JS - min(td$JS, na.rm=TRUE) Now that we have a variable called JS_0 with at least one score equal to zero, lets try the try the Box-Tidwell test. Im going to add the argument brief=TRUE to reduce the amount of output generated by the function. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with predictor containing zero value(s)] boxtidwell &lt;- glm(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, family=binomial) # Print summary of results summary(boxtidwell) If you ran the script above, you likely got an error message that looked like this: \\(\\color{red}{\\text{Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in &#39;x&#39;}}\\) The reason we got this error message is because the log of zero is undefined, and because we had at least one case with a value of zero on JS_0, it broke down the operations. If you see a message like that, then proceed with one or both of the following approaches (which I described above). Using the first approach, create a new variable in which the JS_0 variable is linearly transformed such that the lowest score is 1. The equation below simply adds 1 and the absolute value of the minimum value to each score on the JS_0 variable, which results in the lowest score on the new variable (JS_1) being 1. As verification, I include the min function from base R. # Linear transformation that results in lowest score being 1 td$JS_1 &lt;- td$JS_0 + abs(min(td$JS_0, na.rm=TRUE)) + 1 # Verify that new lowest score is 1 min(td$JS_1, na.rm=TRUE) ## [1] 1 It worked! Now, using this new transformed variable, enter it into the Box-Tidwell test. # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with transformed predictor = 1] boxtidwell &lt;- glm(Turnover ~ JS_1 + JS_1:log(JS_1), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS_1 + JS_1:log(JS_1), family = binomial, ## data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0004 -1.1792 0.7129 1.0780 1.2090 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.099 5.008 1.617 0.106 ## JS_1 -4.059 2.977 -1.363 0.173 ## JS_1:log(JS_1) 1.510 1.226 1.231 0.218 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 124.57 on 95 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.57 ## ## Number of Fisher Scoring iterations: 4 There is no error message this time, and now we can see that the interaction term between the predictor variable and the log of the predictor variable (JS_1:log(JS_1)) is nonsignificant (b = 1.510, p = .218). Thus, we dont see evidence that the assumption of linearity has been violated. We could (also) use the second approach if we have proportionally very few values that are zero (or less than zero). To do so, we would just use the rows= argument to specify that we want to drop cases for which the predictor variable is equal to or less than zero. Note that were back to using the variable called JS_0 that forced to have at least one score equal to zero (solely for the purposes of demonstration in this tutorial). # Box-Tidwell diagnostic test of linearity between continuous predictor variable and # logit transformation of outcome variable [with only cases with scores greater than zero] boxtidwell &lt;- glm(Turnover ~ JS_0 + JS_0:log(JS_0), data=td, family=binomial, subset=(JS_0 &gt; 0)) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS_0 + JS_0:log(JS_0), family = binomial, ## data = td, subset = (JS_0 &gt; 0)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9868 -1.1763 0.7365 1.0743 1.2079 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.665 2.822 1.653 0.0982 . ## JS_0 -2.616 1.994 -1.312 0.1895 ## JS_0:log(JS_0) 1.039 0.928 1.120 0.2628 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 130.72 on 96 degrees of freedom ## Residual deviance: 124.62 on 94 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.62 ## ## Number of Fisher Scoring iterations: 4 We lost one case because that person had a score of zero on the JS_0 continuous predictor variable, and we see that the interaction term (JS_0:log(JS_0)) is non significant (b = 1.039, p = .263). To summarize, the two approaches we just implemented would only be used when testing the statistical assumption of linearity using the Box-Tidwell approach, and only if your continuous predictor variable has scores that are equal to or less than zero. Now that weve met the assumption of linearity, were finally ready to interpret the model results! 47.3.3.2 Interpret Model Results Now were ready to interpret the results of the simple logistic regression model. For clarity, lets re-specify our original simple logistic regression model from above. To apply the glm function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variable is typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. Third, type the argument family=binomial, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the &lt;- assignment operator; here, I again arbitrarily named the object model1. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (model1). # Estimate simple logistic regression model model1 &lt;- glm(Turnover ~ JS, data=td, family=binomial) # Print summary of results summary(model1) ## ## Call: ## glm(formula = Turnover ~ JS, family = binomial, data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7337 -1.2149 0.7806 0.9945 1.5175 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.8554 0.6883 2.695 0.00703 ** ## JS -0.4378 0.1958 -2.236 0.02538 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 126.34 on 96 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.34 ## ## Number of Fisher Scoring iterations: 4 The output generates the model coefficient estimates and model fit (i.e., AIC) information. Coefficients: The Coefficients section of the output displays the regression coefficients (slopes, weights) and their standard errors, z-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the regression coefficient for the predictor variable (JS) in relation to the outcome variable (Turnover) is often of substantive interest. Here, we see that the unstandardized regression coefficient for JS is -.438, and its associated p-value is less than .05 (b = -.438, p = .025). Given that the p-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. The conceptual interpretation of logistic regression coefficients is not as straightforward as traditional linear regression coefficients, though. We can, however, interpret the significant regression coefficient as follows: For every one unit increase in the predictor variable (JS), the logit transformation of the outcome variable (Turnover) decreases by .438 units. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. Confidence Intervals: To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the confint function from base R with the name of our model as the sole argument (model1). # Estimate 95% confidence intervals confint(model1) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.5644186 3.28485115 ## JS -0.8412956 -0.06705288 The 95% confidence interval for JS ranges from -.822 to -.054 (i.e., 95% CI[-.822, -.054]), which indicates that the true population parameter for association likely falls somewhere between those two values. Odds Ratios: If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = 1.855 -.438 \\times JS_{observed}\\) To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it - for example: \\(e^{-.438} = .646\\) We can also do this using R by applying the exp function from base R. Specifically, within the exp function parentheses, type the name of the coef function from base R, which extracts the regression coefficients from the model object. As the sole argument within the coef function parentheses, enter the name of the model we previously specified (model1). # Exponentiate logistic regression coefficient to convert to odds ratio exp(coef(model1)) ## (Intercept) JS ## 6.3939475 0.6454756 In the output, we see that indeed the odds ratio is approximately .646. Because the odds ratio is less than 1, it implies a negative association between the predictor and outcome variables, which we already knew from the negative regression coefficient on which it is based. Interpreting an odds ratio that is less than 1 takes some getting used to. To aid our interpretation, subtract the odds ratio value of .646 from 1 which yields .354 (i.e., 1 - .646 = .354). Now, using that difference value, we can say something like: The odds of quitting are reduced by 35.4% (100 \\(\\times\\) .354) for every one unit increase in job satisfaction (JS). Alternatively, we could take the reciprocal of .646, which is 1.548 (1 / .646), and interpret the effect in terms of not quitting (i.e., staying): The odds of not quitting are 1.548 times as likely for every one unit increase in job satisfaction (JS). If you have never worked with odds before, keep practicing the interpretation and it will come to you at some point. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe them qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large To get the 95% confidence intervals for the odds ratio, we can nest the confint function within the exp function. # Exponentiate logistic regression coefficient to convert to odds ratio exp(confint(model1)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 1.7584252 26.7050090 ## JS 0.4311515 0.9351458 Model Fit &amp; Performance: Returning to the original output of the simple logistic regression model, lets examine the section that contains information about model fit. Note that we dont have an estimate of R-squared (R2) like we would with a traditional linear regression model. Later on, we will compute the pseudo-R-squared (R2) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (N = 98). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the models fit to the data initially by creating a confusion matrix in which we display the models accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 98 for the analysis. Thus, we should drop the case with missing data prior to estimating our baseline data and confusion matrix. Well start by using the subset function from base R to select just the two variables from our data frame that we used in our logistic regression model: Turnover and JS. As the first argument, type the name of the data frame (td). As the second argument, type select= followed by the c (combine) function from base R. Within the c function parentheses, enter the names of the two variables we wish to retain (Turnover, JS). Using the &lt;- assignment operator, create a new data frame object, and here I arbitrarily call this object td_short. # Create new short data frame with just the retained variables td_short &lt;- subset(td, select=c(Turnover, JS)) Next, apply the na.omit function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (td_short). Using the &lt;- assignment operator, overwrite the td_short data frame by entering its name to the left of the &lt;- assignment operator. # Drop cases with missing data in the short data frame td_short &lt;- na.omit(td_short) There are different ways we can estimate our baseline, but lets keep it simple and use the xtabs function from base R. As the first argument, type the ~ followed the name of the short data frame td_short, followed by the $ and the outcome variable (Turnover). Use the &lt;- operator to create and name a new table object, where here I arbitrarily call it table1. Use the print function to view the table object (table1). Remember that for the Turnover variable quit = 1 and stay = 0. # Number of cases at each level of outcome variable table1 &lt;- xtabs(~ td_short$Turnover) # Print the table print(table1) ## td_short$Turnover ## 0 1 ## 39 59 As you can see, 39 people actually stayed and 59 people actually quit. To convert these to proportions, apply the prop.table function from base R to the table object we created in the previous step (table1). # Proportion of cases at each level of outcome variable prop.table(table1) ## td_short$Turnover ## 0 1 ## 0.3979592 0.6020408 Based on this output, we see that 39.8% of people stayed and 60.2% quit. Now were ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities pred.prob &lt;- predict(model1, type=&quot;response&quot;) We now need to dichotomize the vector of predicted probabilities (pred.prob), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, were setting our threshold for experiencing the event in question as .50. As the first step, lets create a new vector called dich.pred.prob (or whatever you would like to call it) based on the values from the pred.prob vector you created in the previous step. Next, for the dich.pred.prob, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) dich.pred.prob &lt;- pred.prob dich.pred.prob[dich.pred.prob &gt;= .50] &lt;- 1 dich.pred.prob[dich.pred.prob &lt; .50] &lt;- 0 Building on the xtabs function from above, use the + symbol to add the new dich.pred.prob vector as the column variable in the table. Also, create a new table object using the &lt;- operator called table2 (or whatever you would like to call it). Use the print function to print the table2 object. # Number of cases at each level of outcome variable table2 &lt;- xtabs(~ td_short$Turnover + dich.pred.prob) print(table2) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 8 31 ## 1 10 49 This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. By applying the prop.table function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (table2), and as the second argument, enter the numeral 1 to indicate that we want the row proportions. # Row proportions prop.table(table2, 1) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 0.2051282 0.7948718 ## 1 0.1694915 0.8305085 Of those who actually stayed (0), we were only able to predict their turnover behavior with 20.5% accuracy using our model (compared to our baseline of 39.8%). Of those who actually quit (1), our model fared much better, as we were able to predict that outcome with 83.1% accuracy (compared to our baseline of 60.2%). Finally, lets determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, were interested in the cells that correspond to 0 on the Turnover variable and 0 on the dich.pred.prob vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (0), followed by a comma and the name of the column label (0). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (1), followed by a comma and the name of the column label (1). Now add those together to form the numerator. In the denominator, apply the sum function from base R to the table object (table2) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (JS) and the outcome variable (Turnover) in our original model. # Estimate overall percentage of correct classifications (table2[&quot;0&quot;,&quot;0&quot;] + table2[&quot;1&quot;,&quot;1&quot;]) / sum(table2) ## [1] 0.5816327 The overall percentage of correct classifications 58.2%, which is not a monumental amount of prediction accuracy when using just JS (job satisfaction) as a predictor in the model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Nagelkerke pseudo-R2: To compute Nagelkerkes pseudo-R2, we will need to install and access the DescTools package so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) In the PseudoR2 function, we will specify the name of the model object (model1) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerkes formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model1, &quot;Nagel&quot;) ## Nagelkerke ## 0.07258382 The estimated Nagelkerke pseudo-R2 is .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS explains 7.3% of the variance in Turnover. Because the DescTools package also has a function called Logit, lets detach the package before moving forward so that we dont inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees self-reported job satisfaction is associated with their decisions to quit or stay, and thus job satisfaction (JS) was used as continuous predictor variable in our simple logistic regression. In total, due to missing data, 98 employees were included in our analysis. Results indicated that, indeed, job satisfaction was associated with turnover behavior to a statistically significant extent, and the association was negative (b = -.438, p = .025, 95% CI[-.822, -.054]). That is, the odds of quitting were reduced by 35.4% for every one unit increase in job satisfaction (OR = .646), which was a small-medium effect. Overall, using our estimate simple logistic regression model, we were able to predict actual turnover behavior in our sample with 58.2% accuracy, which suggests there is quite a bit of room for improvement. Finally, the estimated Nagelkerke pseudo-R2 was .073. We can cautiously conclude that job satisfaction explains 7.3% of the variance in voluntary turnover. Dealing with Bivariate Outliers: If you recall above, we found that the cases associated with row numbers 66 and 67 in this sample may be potential bivariate outliers. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison a case, unless the case appears to have a dramatic influence on the estimated regression line (i.e., has a Cooks distance value greater than 1.0). If you were to decide to remove cases 66 and 67, heres what you would do. First, look at the data frame (using the View function) and determine which cases row numbers 66 and 67 are associated with; because we have a unique identifier variable (ID) in our data frame, we can see that they are associated with ID equal to EMP861 and EMP862, respectively. Next, with respect to estimating the logistic regression model, I suggest naming the unstandardized regression model something different, and here I name it model1_b. The model should be specified just as it was earlier in the tutorial, but now lets add an additional argument: subset=(!ID %in% c(\"EMP861\",\"EMP862\")); the subset argument subsets the data frame within the glm function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which ID is not (!) within the vector containing EMP861 and EMP862. Please consider revisiting the chapter on filtering (subsetting) data if you would like to see the full list of logical operators or to review how to filter out cases from a data frame before specifying the model. # Simple logistic regression model with outlier/influential cases removed model1_b &lt;- glm(Turnover ~ JS, data=td, family=binomial, subset=(!ID %in% c(&quot;EMP861&quot;,&quot;EMP862&quot;))) # Print summary of results summary(model1_b) ## ## Call: ## glm(formula = Turnover ~ JS, family = binomial, data = td, subset = (!ID %in% ## c(&quot;EMP861&quot;, &quot;EMP862&quot;))) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7441 -1.2209 0.7819 0.9887 1.5092 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.8799 0.7067 2.660 0.00781 ** ## JS -0.4388 0.1997 -2.198 0.02797 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 128.89 on 95 degrees of freedom ## Residual deviance: 123.66 on 94 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 127.66 ## ## Number of Fisher Scoring iterations: 4 Standardizing Continuous Predictor Variable: Optionally, sometimes we may decide to standardize the continuous predictor variable in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the scale function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the chapter on centering and standardizing variables. # Estimate simple logistic regression model # with standardized continuous predictor variable model1_c &lt;- glm(Turnover ~ scale(JS), data=td, family=binomial) # Print summary of results summary(model1_c) ## ## Call: ## glm(formula = Turnover ~ scale(JS), family = binomial, data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7337 -1.2149 0.7806 0.9945 1.5175 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4385 0.2132 2.057 0.0397 * ## scale(JS) -0.5022 0.2247 -2.236 0.0254 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 131.75 on 97 degrees of freedom ## Residual deviance: 126.34 on 96 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 130.34 ## ## Number of Fisher Scoring iterations: 4 47.3.3.3 Compute Predicted Probabilities When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didnt append this new vector to of values to our data frame object (td). For the sake of clarity, we will re-do this step using the predict function from base R once more. Well begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities based on # sample used to estimate the logistic regression model pred.prob &lt;- predict(model1, type=&quot;response&quot;) Now lets add the pred.prob vector as a new variable called pred.prod.JS in the td data frame object. Lets overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (pred.prod.JS), followed by the = operator and the name of the vector object containing the predicted probabilities (pred.prod). As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( pred.prob.JS = pred.prob), by=&quot;row.names&quot;, all=TRUE) If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 We can now dichotomize the predicted probabilities (pred.prob.JS), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, were setting our threshold for experiencing the event in question as .50. As the first step, lets create a new variable called dich.pred.prob.JS (or whatever we would like to call it) based on the values from the pred.prob.JS variable we just created. Next, for the dich.pred.prob.JS, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) td$dich.pred.prob.JS &lt;- td$pred.prob.JS td$dich.pred.prob.JS[td$dich.pred.prob.JS &gt;= .50] &lt;- 1 td$dich.pred.prob.JS[td$dich.pred.prob.JS &lt; .50] &lt;- 0 # Print first 6 rows of data frame object head(td) ## Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS ## 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 ## 2 10 EMP614 1 1.69 2.82 2.58 2.07 1.46 2.46 0.7531576 1 ## 3 11 EMP617 1 3.62 1.08 3.53 2.74 3.39 4.39 0.5672481 1 ## 4 12 EMP619 0 1.72 3.88 1.78 2.11 1.49 2.49 0.7507079 1 ## 5 13 EMP634 0 1.96 3.02 3.41 NA 1.73 2.73 0.7305327 1 ## 6 14 EMP636 0 4.14 2.69 4.33 3.07 3.91 4.91 0.5107466 1 Its important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain fresh data on the JS predictor variable, which we could then use to plug into our model; to do this with a model estimated using the glm model, will create a toy data frame with fresh JS values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object  just like we did in the chapter on predicting criterion scores using simple linear regression. In this toy data frame that were naming fresh_td, we use the data.frame function from base R. As the first argument, well create an ID variable with some fictional employee ID numbers, and as the second argument, well create a JS variable with some fictional job satisfaction scores. # Create toy data frame object for illustration purposes fresh_td &lt;- data.frame(ID=c(&quot;EMP1000&quot;,&quot;EMP1001&quot;,&quot;EMP1002&quot;, &quot;EMP1003&quot;,&quot;EMP1004&quot;,&quot;EMP1005&quot;), JS=c(4.5, 1.6, 0.7, 5.9, 2.1, 3.0)) When estimating the predicted probabilities of turnover based on these new JS scores, well do the following: Using the &lt;- assignment operator, we will create a new variable called prob_JS that we will append to the fresh_td toy data frame object (using the $ operator). To the right of the &lt;- assignment operator, type the name of the predict function from base R. As the first argument, type the name of the logistic regression model object we estimated using the td (original) data frame. As the second argument, type newdata= followed by the name of the data frame object containing fresh data (fresh_td). Its important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated. As the third argument, insert type=\"response\", which will request the predicted probabilities. # Use simple logistic regression model to estimate # predicted probabilities of turnover for &quot;fresh&quot; data fresh_td$prob_JS &lt;- predict(model1, # name of logistic regression model newdata=fresh_td, # name of &quot;fresh&quot; data frame type=&quot;response&quot;) # Print data frame object print(fresh_td) ## ID JS prob_JS ## 1 EMP1000 4.5 0.4713805 ## 2 EMP1001 1.6 0.7604090 ## 3 EMP1002 0.7 0.8247569 ## 4 EMP1003 5.9 0.3257483 ## 5 EMP1004 2.1 0.7182989 ## 6 EMP1005 3.0 0.6322888 47.3.4 Multiple Logistic Regression Using glm Function from Base R Just as we did for simple logistic regression, we can estimate a multiple logistic regression model using the glm function from base R. Lets specify a multiple logistic regression model with JS, NAff, and TI as predictor variables and Turnover as the dichotomous outcome variable. Using the &lt;- assignment operator, we will come up with a name of an object to which we will assign the estimated model (e.g., model2). Type the name of the glm function. As the functions first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables, separated by the + operator, are typed to the right of the ~ symbol. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. As the third argument, type family=binomial, which by default estimates a logit (logistic) model. # Estimate multiple logistic regression model model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) Note: You wont see a summary of the results in the Console, as we have not requested those. As a next step, we will use the model1 object to determine whether we have satisfied the statistical assumptions of a multiple logistic regression model. 47.3.4.1 Test Statistical Assumptions To determine whether its appropriate to interpret the results of a simple logistic regression model, we need to first test the following statistical assumptions. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Outcome Variable Is Dichotomous: We already know that the outcome variable called Turnover is dichotomous (1 = quit, 0 = stayed), which means we have satisfied this assumption. Data Are Free of Multivariate Outliers: To determine whether the data are free of multivariate outliers, lets look at Cooks distance (D) values across cases. Using the plot function from base R, type the name of our model object (model1) as the first argument, and as the second argument, enter the numeral 4 to request the fourth diagnostic plot, which is the Cooks distance plot. # Diagnostics plot: Cook&#39;s Distance plot plot(model2, 4) The case associated with row number 6 has the highest Cooks distance value, followed by the cases associated with row numbers 1 and 66. The cases associated with row numbers 6 and 1 seem to be clearest potential outliers of the three. We can also print the cases with the highest Cooks distances. Lets create an object called cooksD that we will assign a vector of Cooks distance values to using the cooks.distance function from base R. Just enter the name of the logistic regression model object (model1) as the sole parenthetical argument. Next, update the object we called cooksD by applying the sort function from base R and entering the cooksD object as the first object and decreasing=TRUE as the second argument; this will sort the Cooks distance values in descending order. Finally, apply the head function from base R, and as the first argument enter the name of the cooksD object that we just sorted; as the second argument, type n=20 to show the top 20 rows, as opposed to the default top 6 rows. # Estimate Cook&#39;s distance values cooksD &lt;- cooks.distance(model2) # Sort Cook&#39;s distance values in descending order cooksD &lt;- sort(cooksD, decreasing=TRUE) # View top 20 Cook&#39;s distance values head(cooksD, n=20) ## 6 1 66 42 96 74 27 26 67 ## 0.15184792 0.13426790 0.10091817 0.08227834 0.05869871 0.04691783 0.04554332 0.04249992 0.03973226 ## 61 68 4 25 52 54 69 8 75 ## 0.02634920 0.02182251 0.01882168 0.01802907 0.01720366 0.01718997 0.01718628 0.01661234 0.01611474 ## 77 64 ## 0.01593173 0.01552323 Again, we see cases associated with row numbers 6 (.152), 1 (.134), and 66 (.101) having the highest Cooks distance values. As a sensitivity analysis, we could estimate our model once more after removing the cases associated with row numbers 6 and 1 (and maybe even 66) from our data frame. With that being said, we should be wary of removing outlier or influential cases and should do so only when we have good justification for doing so. A liberal threshold Cooks distance is 1, which means that we would grow concerned if any of these values exceeded 1, whereas a more conservative threshold is 4 divided by the sample size (4 / 98 = .041). These Cooks distance values are all well-below the more liberal threshold, so I would say were safe to leave the associated cases in the data. No (Multi)Collinearity Between Predictor Variables: To assess whether (multi)collinearity might be an issue, lets estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you havent already, install the car package which contains the vif function we will use to compute tolerance. # Install car package if you haven&#39;t already install.packages(&quot;car&quot;) Now, access the car package using the library function. # Access car package library(car) Apply the vif function to the model object (model2). # Compute VIF statistic vif(model2) ## JS NAff TI ## 1.068428 1.020933 1.067496 Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (R2) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - R2), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approach .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide 1 by the VIF. # Compute tolerance statistic 1 / vif(model2) ## JS NAff TI ## 0.9359549 0.9794966 0.9367719 This table indicates that there are low levels of collinearity. Specifically, we can see that the tolerance statistics are all closer to 1.00 and definitely above .20, thereby indicating that collinearity is not likely an issue. Association Between Any Continuous Predictor Variable and Logit Transformation of Outcome Variable Is Linear: To test the assumption of linearity between a continuous predictor variable and the logit transformation of the outcome variable, we can add to the model the interaction term between any continuous predictor variable and its logarithmic (i.e., natural log) transformation. [Note: We do not perform the following test/approach for categorical predictor variables.] We will use an approach that is commonly referred to as the Box-Tidwell approach (Hosmer and Lemeshow 2000). To apply this approach, we need to add the interaction term between each predictor variable and its logarithmic transformation to our logistic regression model  but not the main effect for the logarithmic transformation of the predictor variable. In our regression model formula, specify the dichotomous outcome variable Turnover to the left of the ~ symbol. To the right of the ~ symbol, type the name of each predictor variable JS followed by the + symbol. After the + symbol, type the name of the same predictor variable JS, followed by the : symbol and the log function from base R with the predictor variable as its sole parenthetical argument. Repeat that process for all continuous predictor variables in the model. Be sure to name the model object, and here I name it arbitrarily boxtidwell. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (boxtidwell). # Box-Tidwell diagnostic test of linearity between continuous predictor variables and # logit transformation of outcome variable boxtidwell &lt;- glm(Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + TI + TI:log(TI), data=td, family=binomial) # Print summary of results summary(boxtidwell) ## ## Call: ## glm(formula = Turnover ~ JS + JS:log(JS) + NAff + NAff:log(NAff) + ## TI + TI:log(TI), family = binomial, data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2213 -0.8788 0.4455 0.8257 2.6375 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.9206 9.8331 0.094 0.9254 ## JS -4.5845 2.6176 -1.751 0.0799 . ## NAff -0.4107 6.4047 -0.064 0.9489 ## TI 3.2888 3.0158 1.091 0.2755 ## JS:log(JS) 2.0053 1.1844 1.693 0.0904 . ## NAff:log(NAff) 0.9559 3.5321 0.271 0.7867 ## TI:log(TI) -1.2101 1.5232 -0.794 0.4269 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 100.85 on 88 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 114.85 ## ## Number of Fisher Scoring iterations: 5 Because the p-values associated with each of the interaction terms and their regression coefficients (JS:log(JS), NAff:log(NAff), TI:log(TI)) are equal to or greater than the conventional two-tailed alpha level of .05, we have no reason to believe that any of the associations between the continuous predictor variables and the logit transformation of the outcome variable are nonlinear. If one of the interaction terms had been statistically significant, then we might have evidence that the assumption was violated, meaning we would assume a nonlinear association instead, and one potential solution would be to apply a transformation to the predictor variable in question (e.g., logarithmic transformation). Finally, we only apply this test when the predictor variables in question are continuous (interval, ratio). Important Note: If the variable for which you are applying the Box-Tidwell approach described above has one or more cases with a score of zero, then you will receive an error message when you run the model with the log function. The reason for the error is that the log of zero or any negative value is (mathematically) undefined. There are many reasons why your variable might have cases with scores equal to zero, some of which include: (a) zero is a naturally occurring value for the scale on which the variable is based, or (b) the variable was grand-mean centered or standardized, such that the mean is now equal to zero. There are some approaches to dealing with this issue and neither approach I will show you is going to be perfect, but each approach will give you an approximate understanding of whether violation of the linearity assumption might be an issue. First, we can add a positive numeric constant to every score on the continuous predictor variable in question that will result in the new lowest score being 1. Why 1 you might ask? Well, the rationale is somewhat arbitrary; the log of 1 is zero, and there is something nice about grounding the lowest logarithmic value at 0. Due note, however, that the magnitude of the linear transformation will have some effect on the p-value associated with the Box-Tidwell interaction term. Second, if there are proportionally very few cases with scores of zero on the predictor variable in question, we can simply subset those cases out for the analysis. These approaches to addressing zero values (including the error message you might encounter) are demonstrated in a previous section on simple logistic regression. 47.3.4.2 Interpret Model Results Now were ready to interpret the results of the multiple logistic regression model. For clarity, lets re-specify our original simple logistic regression model from above. To apply the glm function, type the name of the function. As the first argument, specify the logistic regression model, wherein the dichotomous outcome variable is typed to the left of the ~ symbol, and the predictor variables are typed to the right of the ~ symbol, with each predictor variable separated by the + operator. As the second argument, type data= followed by the name of the data frame (td) to which both of the variables belong. Third, type the argument family=binomial, which by default estimates a logit (logistic) model. Be sure to name and create an object based on your model using the &lt;- assignment operator; here, I again arbitrarily named the object model2. Finally, use the summary function from base R to generate the model output, and within the parentheses, enter the name of the model you created in the prior step (model2). # Estimate multiple logistic regression model model2 &lt;- glm(Turnover ~ JS + NAff + TI, data=td, family=binomial) # Print summary of results summary(model2) ## ## Call: ## glm(formula = Turnover ~ JS + NAff + TI, family = binomial, data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3347 -0.9248 0.5881 0.8403 2.2273 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.9286 1.8120 -2.168 0.03015 * ## JS -0.2332 0.2215 -1.053 0.29253 ## NAff 1.1952 0.4996 2.392 0.01674 * ## TI 0.8967 0.3166 2.832 0.00462 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 105.23 on 91 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 113.23 ## ## Number of Fisher Scoring iterations: 4 The output generates the model coefficient estimates and model fit (i.e., AIC) information. To estimate the 95% confidence intervals for the regression coefficients based on their standard errors, we can apply the confint function from base R with the name of our model as the sole argument (model2). # Estimate 95% confidence intervals confint(model2) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -7.6905274 -0.5144811 ## JS -0.6793832 0.1980706 ## NAff 0.2606377 2.2375600 ## TI 0.3091687 1.5633542 Coefficients: The Coefficients section of the output from the summary of the glm function generates the model coefficient estimates and model fit (i.e., AIC) information, and the output from the confint provides the confidence intervals. To begin, the Coefficients section displays the regression coefficients (slopes, weights) and their standard errors, z-values, and p-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation. The estimate of the unstandardized regression coefficient for the predictor variable (JS) in relation to the logit transformation of the outcome variable (Turnover) is not statistically significant when controlling for the other predictor variables in the model because the associated p-value is equal to or greater than .05 (b = -.233, p = .293, 95% CI[-0.667, .201]). The regression coefficient for NAff is 1.195 and its associated p-value is less than .05, indicating that the association is statistically significant when controlling for the other predictor variables in the model (b = 1.195, p = .017, 95% CI[.216, 2.174]). Finally, the regression coefficient for TI is .897 and its associated p-value is less than .05, indicating that the association is also statistically significant when controlling for the other predictor variables in the model (b = .897, p = .005, 95% CI[.276, 1.517]). Given that one of the predictor variables did not share a statistically significant association with the outcome when included in the model, we typically would not write out the regression equation with that variable included. Instead, we would typically re-estimate the model without that variable. For illustrative purposes, however, we will write out the equation. Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows: \\(\\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) where \\(p\\) refers to, in this example, as the probability of quitting. If you recall from earlier in the tutorial, we can also interpret our findings with respect to \\(\\log(odds)\\). \\(\\log(odds) = \\ln(\\frac{p}{1-p}) = -3.929 - .233 \\times JS_{observed} + 1.195 \\times NAff_{observed} + .897 \\times TI_{observed}\\) To that end, to aid our interpretation of the significant finding, we can convert our logistic regression coefficient to an odds ratio by simply exponentiating it  for example: \\(e^{1.195} = 3.304\\) We can also do this using R by applying the exp function from base R. Specifically, within the exp function parentheses, type the name of the coef function from base R, which extracts the regression coefficients from the model object. As the sole argument within the coef function parentheses, enter the name of the model we previously specified (model2). # Exponentiate logistic regression coefficient to convert to odds ratio exp(coef(model2)) ## (Intercept) JS NAff TI ## 0.0196714 0.7919969 3.3041473 2.4513941 We should only interpret those odds ratios in which their corresponding regression coefficient was statistically significant; accordingly, in this example, we will just interpret the odds ratios belonging to NAff and TI. Regarding NAff, its odds ratio of 3.304 is greater than 1, which implies a positive association between the predictor and the outcome variables, which we already knew from the negative regression coefficient on which it is based. Thus, the odds of quitting are 3.304 times as likely for every one unit increase in NAff when controlling for other predictor variables in the model. Regarding TI, its odds ratio of 2.451 is also greater than 1, and thus, the odds of quitting are 2.451 times as likely for every one unit increase in TI when controlling for other predictor variables in the model. Note that the odds ratio (OR) is a type of effect size, and thus we can compare odds ratios and describe an odds ratio qualitatively using descriptive language. There are different rules of thumb, and for the sake of parsimony, I provide rules of thumb for when odds ratios are greater than 1 and less than 1. Both odds ratios are about medium in terms of their magnitude. Odds Ratio &gt; 1 Odds Ratio &lt; 1 Description 1.2 .8 Small 2.5 .4 Medium 4.3 .2 Large To get the 95% confidence intervals for the odds ratio, we can nest the confint function within the exp function. # Exponentiate logistic regression coefficient to convert to odds ratio exp(confint(model2)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.000457137 0.5978107 ## JS 0.506929579 1.2190484 ## NAff 1.297757439 9.3704394 ## TI 1.362292195 4.7748100 Model Fit &amp; Performance: Returning to the original output of the multiple logistic regression model, lets examine the section that contains information about model fit. Note that we dont have an estimate of R-squared (R2) like we would with a traditional linear regression model. Later on, we will compute the pseudo-R-squared (R2) value. As you can see in the output, we get the null deviance and residual deviance values (and their degrees of freedom) as well as the Akaike information criterion (AIC) value. If we add 1 to the degrees of freedom of our null deviance estimate, we get the number of cases retained for the analysis (N = 95). By themselves, these values are not very meaningful; however, they can be used to compare nested models, which is beyond the scope of this tutorial. For our purposes, we will assess the models fit to the data initially by creating a confusion matrix in which we display the models accuracy in predicting the outcome. Before doing so, recall that in our analysis above, we lost one case because of missing data on either the predictor, outcome, or both, which dropped our sample size from 99 to 95 for the analysis. Thus, we should drop the cases with missing data prior to estimating our baseline data and confusion matrix. Well start by using the subset function from base R to select just the variables from our data frame that we used in our logistic regression model: Turnover, JS, NAff, and TI. As the first argument, type the name of the data frame (td). As the second argument, type select= followed by the c (combine) function from base R. Within the c function parentheses, enter the names of the two variables we wish to retain (Turnover, JS). Using the &lt;- assignment operator, create a new data frame object, and here I arbitrarily call this object td_short. # Create new short data frame with just the retained variables td_short &lt;- subset(td, select=c(Turnover, JS, NAff, TI)) Next, apply the na.omit function from base R to drop cases in our data frame that are missing values on one or more variables. In the function parentheses, enter the name of the data frame we just created (td_short). Using the &lt;- assignment operator, overwrite the td_short data frame by entering its name to the left of the &lt;- assignment operator. # Drop cases with missing data in the short data frame td_short &lt;- na.omit(td_short) There are different ways we can estimate our baseline, but lets keep it simple and use the xtabs function from base R. As the first argument, type the ~ followed the name of the short data frame td_short, followed by the $ and the outcome variable (Turnover). Use the &lt;- operator to create and name a new table object, where here I arbitrarily call it table1. Use the print function to view the table object (table1). Remember that for the Turnover variable quit = 1 and stay = 0. # Number of cases at each level of outcome variable table1 &lt;- xtabs(~ td_short$Turnover) # Print the table print(table1) ## td_short$Turnover ## 0 1 ## 37 58 As you can see, 37 people actually stayed and 58 people actually quit. To convert these to proportions, apply the prop.table function from base R to the table object we created in the previous step (table1). # Proportion of cases at each level of outcome variable prop.table(table1) ## td_short$Turnover ## 0 1 ## 0.3894737 0.6105263 Based on this output, we see that 38.9% of people stayed and 61.1% quit. Now were ready to estimate the predicted probabilities of someone quitting based on our logistic regression model. Begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model2). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities pred.prob &lt;- predict(model2, type=&quot;response&quot;) We now need to dichotomize the vector of predicted probabilities (pred.prob), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, were setting our threshold for experiencing the event in question as .50. As the first step, lets create a new vector called dich.pred.prob (or whatever you would like to call it) based on the values from the pred.prob vector you created in the previous step. Next, for the dich.pred.prob, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) dich.pred.prob &lt;- pred.prob dich.pred.prob[dich.pred.prob &gt;= .50] &lt;- 1 dich.pred.prob[dich.pred.prob &lt; .50] &lt;- 0 Building on the xtabs function from above, use the + symbol to add the new dich.pred.prob vector as the column variable in the table. Also, create a new table object using the &lt;- operator called table2 (or whatever you would like to call it). Use the print function to print the table2 object. # Number of cases at each level of outcome variable table2 &lt;- xtabs(~ td_short$Turnover + dich.pred.prob) print(table2) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 23 14 ## 1 6 52 This table is our confusion matrix, where the rows represent the actual turnover observations (i.e., true state of affairs), and the columns represent the predicted turnover occurrences. In other words, a cross-tabulation is shown in which the rows represent actual/observed turnover behavior (0 = stay, 1 = quit), and the columns represent predicted turnover behavior (0 = stay, 1 = quit). Thus, the cross-tabulation (i.e., confusion matrix) helps us understand how accurate our model predictions were relative to the observed data, thereby providing us with an indication of how well the model fit the data. By applying the prop.table function, we can calculate the row proportions, which will give us an idea of how accurately our logistic regression model classified people as staying and as leaving relative to the actual, observed data for turnover. As the first argument, type the name of the table object (table2), and as the second argument, enter the numeral 1 to indicate that we want the row proportions. # Row proportions prop.table(table2, 1) ## dich.pred.prob ## td_short$Turnover 0 1 ## 0 0.6216216 0.3783784 ## 1 0.1034483 0.8965517 Of those who actually stayed (0), we were able to predict their turnover behavior with 62.2% accuracy using our model. Of those who actually quit (1), our model even better, as we were able to predict that outcome with 89.7% accuracy. Not too shabby. Finally, lets determine the percentage of correct classifications (i.e., model accuracy) for both quit and stay behavior. In other words, we want to determine the percentage of correct decisions that were made based on our model relative to the overall number of decisions made by our model (i.e., sample size). We will use basic arithmetic to do so. First, specify the numerator value, which will be calculated by adding those correct predictions; in this context, the correct decisions are those in which the model accurately predicted who would stay (0) and who would quit (1). Thus, were interested in the cells that correspond to 0 on the Turnover variable and 0 on the dich.pred.prob vector, which is the upper-left cell in our 2x2 table. To reference that cell and its value, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (0), followed by a comma and the name of the column label (0). To reference the lower-right cell, which represents the number of correct predictions regarding quitting, type the name of the table object (table2), and within brackets beside it ([ ]), type the name of the row label (1), followed by a comma and the name of the column label (1). Now add those together to form the numerator. In the denominator, apply the sum function from base R to the table object (table2) to calculate how many predictions were made - or in other words, the sample size for those who had data for both the predictor variable (JS) and the outcome variable (Turnover) in our original model. # Estimate overall percentage of correct classifications (table2[&quot;0&quot;,&quot;0&quot;] + table2[&quot;1&quot;,&quot;1&quot;]) / sum(table2) ## [1] 0.7894737 The overall percentage of correct classifications 78.9%, which is a pretty good accuracy in prediction when using the three predictor variables in our model. If we were to add additional predictor variables to the model, our hope would be that our percentage of correct predictions would increase to a notable extent. Nagelkerke pseudo-R2: To compute Nagelkerkes pseudo-R2, we will need to install and access the DescTools package so that we can use its PseudoR2 function. # Install package install.packages(&quot;DescTools&quot;) # Access package library(DescTools) In the PseudoR2 function, we will specify the name of the model object (model2) as the first argument. As the second argument, type \"Nagel\" to request pseudo-R2 calculated using Nagelkerkes formula. # Compute Nagelkerke pseudo-R-squared PseudoR2(model2, &quot;Nagel&quot;) ## Nagelkerke ## 0.2779516 The estimated Nagelkerke pseudo-R2 is .278, which is a big improvement over the simple logistic regression model we estimated above for which pseudo-R2 was just .073. Remember, a pseudo-R2 is not the exact same thing as a true R2, so we should interpret it with caution. With caution, we can conclude that JS, NAff, and TI explain 27.8% of the variance in Turnover. Because the DescTools package also has a function called Logit, lets detach the package before moving forward so that we dont inadvertently attempt to use the Logit function from DescTools as opposed to the one from lessR. # Detach package detach(&quot;package:DescTools&quot;, character.only=TRUE) Technical Write-Up of Results: A turnover study was conducted based on a sample of 99 employees from the past year, some of whom quit the company and some of whom stayed. Turnover behavior (quit vs. stay) (Turnover) is our outcome of interest, and because it is dichotomous, we used logistic regression. We, specifically, were interested in the extent to which employees self-reported job satisfaction, negative affectivity, and turnover intentions were associated with their decisions to quit or stay, and thus all three were was used as continuous predictor variables in our multiple logistic regression model. In total, due to missing data, 95 employees were included in our analysis. Results indicated that, indeed, job satisfaction was not associated with turnover behavior to a statistically significant extent (b = -.233, p = .293, 95% CI[-0.667, .201]). Negative affectivity, however, was positively and significantly associated with turnover behavior (b = 1.195, p = .017, 95% CI[.216, 2.174]), such that the odds of quitting were 3.304 times as likely for every one unit increase in negative affectivity, when controlling for other predictor variables in the model. Similarly, turnover intentions were also positively and significantly associated with turnover behavior (b = .897, p = .005, 95% CI[.276, 1.517]), such that the odds of quitting were 2.451 times as likely for every one unit increase in turnover intentions, when controlling for other predictor variables in the model. Both of these significant associations were medium in magnitude. Overall, based on our estimated multiple logistic regression model, we were able to predict actual turnover behavior in our sample with 78.9% accuracy. Finally, the estimated Nagelkerke pseudo-R2 was .278. We can cautiously conclude that job satisfaction, negative affectivity, and turnover intentions explain 27.8% of the variance in voluntary turnover. Standardizing Continuous Predictor Variables: Optionally, sometimes we may decide to standardize the continuous predictor variables in our model, as this may make it easier to compare the odds ratios between variables. To do so, we just need to apply the scale function to the continuous predictor variable in the model. For more information on standardizing variables, please check out the chapter on centering and standardizing variables. # Estimate simple logistic regression model # with standardized continuous predictor variable model2_b &lt;- glm(Turnover ~ scale(JS) + scale(NAff) + scale(TI), data=td, family=binomial) # Print summary of results summary(model2_b) ## ## Call: ## glm(formula = Turnover ~ scale(JS) + scale(NAff) + scale(TI), ## family = binomial, data = td) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3347 -0.9248 0.5881 0.8403 2.2273 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.5669 0.2415 2.348 0.01888 * ## scale(JS) -0.2675 0.2542 -1.053 0.29253 ## scale(NAff) 0.6190 0.2588 2.392 0.01674 * ## scale(TI) 0.7685 0.2713 2.832 0.00462 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 127.02 on 94 degrees of freedom ## Residual deviance: 105.23 on 91 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 113.23 ## ## Number of Fisher Scoring iterations: 4 47.3.4.3 Compute Predicted Probabilities When creating a confusion matrix above, we already estimated the predicted probabilities of the model when applied to the sample on which it was estimated; however, we didnt append this new vector to of values to our data frame object (td). For the sake of clarity, we will re-do this step using the predict function from base R once more. Well begin by specifying the name of the object to which you want to assign the vector of predicted probabilities (pred.prob), using the &lt;- assignment operator. Next, type the name of the predict function from base R. As the first argument, type the name of our logistic regression model (model1). As the second argument, enter the argument type=\"response\" to indicate that you want to predict the outcome (response) variable. # Estimate predicted probabilities based on # sample used to estimate the logistic regression model pred.prob &lt;- predict(model2, type=&quot;response&quot;) Now lets add the pred.prob vector as a new variable called pred.prod.allpred in the td data frame object. Lets overwrite the existing td data frame object by using the &lt;- assignment operator. Specify the name of the merge function from base R. For more information on this function, please refer to this chapter supplement from the chapter on joining data frames. As the first argument in the merge function, specify x= followed by the name of the td data frame object. As the second argument in the merge function, specify y= followed by the data.frame function from base R. As the sole argument within the data.frame function specify a name for the new variable that will contain the predicted probabilities based on JS scores (pred.prod.allpred), followed by the = operator and the name of the vector object containing the predicted probabilities (pred.prod). As the third argument in the merge function, type by=\"row.names\", which will match rows from the x and y data frame objects based on their respective row names (i.e., row numbers). As the fourth argument in the merge function, type all=TRUE to request a full merge, such that all rows with data will be retained from both data frame objects when merging. # Simple logistic regression model with predicted probabilities # added as new variable in existing data frame object td &lt;- merge(x=td, y=data.frame( pred.prod.allpred = pred.prob), by=&quot;row.names&quot;, all=TRUE) If we print the first six rows from the td data frame object, we will see the new column containing the predicted probabilities based on our simple logistic regression model. # Print first 6 rows of data frame object head(td) ## Row.names Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS ## 1 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 ## 2 10 18 EMP643 1 3.35 2.94 3.60 2.08 3.12 4.12 0.5960009 1 ## 3 11 19 EMP644 1 2.33 0.80 3.22 2.45 2.10 3.10 0.6974856 1 ## 4 12 2 EMP561 1 1.72 1.47 4.08 2.48 1.49 2.49 0.7507079 1 ## 5 13 20 EMP647 0 2.98 2.16 2.41 2.32 2.75 3.75 0.6343220 1 ## 6 14 21 EMP677 0 4.64 2.92 1.48 2.43 4.41 5.41 0.4561403 0 ## pred.prod.allpred ## 1 0.08371016 ## 2 0.73187048 ## 3 0.79306191 ## 4 0.90827169 ## 5 0.57694329 ## 6 0.31447250 We can now dichotomize the predicted probabilities (pred.prod.allpred), such that any case with a predicted probability of .50 or higher is assigned a 1 (quit), and any case with a predicted probability that is less than .50 is assigned a 0 (stay). That is, were setting our threshold for experiencing the event in question as .50. As the first step, lets create a new variable called dich.pred.prod.allpred (or whatever we would like to call it) based on the values from the pred.prod.allpred variable we just created. Next, for the dich.pred.prod.allpred, dichotomize the values as either a 1 or a 0 in accordance with the approach described above. For more information on the data-management operations below, check out chapter on cleaning data. # Dichotomize probabilities, where .50 or greater is 1 (quit) and less than .50 is 0 (stay) td$dich.pred.prod.allpred &lt;- td$pred.prod.allpred td$dich.pred.prod.allpred[td$dich.pred.prod.allpred &gt;= .50] &lt;- 1 td$dich.pred.prod.allpred[td$dich.pred.prod.allpred &lt; .50] &lt;- 0 # Print first 6 rows of data frame object head(td) ## Row.names Row.names ID Turnover JS OC TI NAff JS_0 JS_1 pred.prob.JS dich.pred.prob.JS ## 1 1 1 EMP559 1 4.96 5.32 0.51 1.87 4.73 5.73 0.4216566 0 ## 2 10 18 EMP643 1 3.35 2.94 3.60 2.08 3.12 4.12 0.5960009 1 ## 3 11 19 EMP644 1 2.33 0.80 3.22 2.45 2.10 3.10 0.6974856 1 ## 4 12 2 EMP561 1 1.72 1.47 4.08 2.48 1.49 2.49 0.7507079 1 ## 5 13 20 EMP647 0 2.98 2.16 2.41 2.32 2.75 3.75 0.6343220 1 ## 6 14 21 EMP677 0 4.64 2.92 1.48 2.43 4.41 5.41 0.4561403 0 ## pred.prod.allpred dich.pred.prod.allpred ## 1 0.08371016 0 ## 2 0.73187048 1 ## 3 0.79306191 1 ## 4 0.90827169 1 ## 5 0.57694329 1 ## 6 0.31447250 0 Its important to keep in mind that these predicted probabilities are estimated based on the same data we used to estimate the model in the first place. Given this, I would describe this process as predict-ish analytics as opposed to true predictive analytics. To reach predictive analytics, we would need to obtain fresh data on the predictor variables, which we could then use to plug into our model; to do this with a model estimated using the glm model, will create a toy data frame with fresh predictor variable values sampled from the same population. In a real life situation, we would more than likely read in a new data file and assign that to an object  just like we did in the chapter on applying a compensatory approach to selection decisions using multiple linear regression. In this toy data frame that were naming fresh_td, we use the data.frame function from base R. As the first argument, well create an ID variable with some fictional employee ID numbers. As the second argument, well create a JS variable with some fictional job satisfaction scores. As the third argument, well create a NAff variable with some fictional negative affectivity scores. As the fourth argument, well create a TI variable with some fictional turnover intentions scores. # Create toy data frame object for illustration purposes fresh_td &lt;- data.frame(ID=c(&quot;EMP1000&quot;,&quot;EMP1001&quot;,&quot;EMP1002&quot;, &quot;EMP1003&quot;,&quot;EMP1004&quot;,&quot;EMP1005&quot;), JS=c(4.5, 1.6, 0.7, 5.9, 2.1, 3.0), NAff=c(0.9, 3.3, 6.0, 4.2, 4.0, 1.9), TI=c(6.0, 5.5, 4.8, 3.0, 0.1, 1.1)) When estimating the predicted probabilities of turnover based on these new predictor variable scores, well do the following: Using the &lt;- assignment operator, we will create a new variable called prob_allpred that we will append to the fresh_td toy data frame object (using the $ operator). To the right of the &lt;- assignment operator, type the name of the predict function from base R. As the first argument, type the name of the logistic regression model object we estimated using the td (original) data frame. As the second argument, type newdata= followed by the name of the data frame object containing fresh data (fresh_td). Its important that the predictor variable name is the same in this data frame as it is in the original data frame on which the model was estimated. As the third argument, insert type=\"response\", which will request the predicted probabilities. # Use multiple logistic regression model to estimate # predicted probabilities of turnover for &quot;fresh&quot; data fresh_td$prob_allpred &lt;- predict(model2, # name of logistic regression model newdata=fresh_td, # name of &quot;fresh&quot; data frame type=&quot;response&quot;) # Print data frame object print(fresh_td) ## ID JS NAff TI prob_allpred ## 1 EMP1000 4.5 0.9 6.0 0.8142131 ## 2 EMP1001 1.6 3.3 5.5 0.9897887 ## 3 EMP1002 0.7 6.0 4.8 0.9993788 ## 4 EMP1003 5.9 4.2 3.0 0.9172278 ## 5 EMP1004 2.1 4.0 0.1 0.6111323 ## 6 EMP1005 3.0 1.9 1.1 0.2024548 "],["kfold.html", "Chapter 48 Applying k-Fold Cross-Validation to Logistic Regression 48.1 Conceptual Overview 48.2 Tutorial", " Chapter 48 Applying k-Fold Cross-Validation to Logistic Regression In this chapter, we will learn how to apply k-fold cross-validation to logistic regression. As a specific type of cross-validation, k-fold cross-validation can be a useful framework for training and testing models. 48.1 Conceptual Overview In general, cross-validation is an integral part of predictive analytics, as it allows us to understand how a model estimated on one data set will perform when applied to one or more new data sets. Cross-validation was initially introduced in the chapter on statistically and empirically cross-validating a selection tool using multiple linear regression. k-fold cross-validation is a specific type of cross-validation that is commonly applied when carrying out predictive analytics. 48.1.1 Review of Predictive Analytics True predictive analytics involves training (i.e., estimating, building) a model using one or more samples of data and then evaluating (i.e., testing, validating) how well the model performs when applied to a separate sample of data drawn from the same population. Assuming we have a large enough data set to begin with, we often assign 80% of the cases to a training data set and 20% to a test data set. The term predictive analytics is a big umbrella term, and predictive analytics can be applied using many different types of models, including common regression model types like ordinary least squares (OLS) multiple linear regression or maximum likelihood logistic regression. When building a model using a predictive analytics framework, one of our goals is to minimize prediction errors (i.e., improve prediction accuracy) when the model is applied to fresh or new data (e.g., test data). Fewer prediction errors when applying the model to new data indicate that the model makes more accurate predictions. At the same time, we want to avoid overfitting our model to the data on which it is trained, as this can lead to a model that fits our training data well but that doesnt generalize to our test data; applying a trained model to new data can help us evaluate the extent to which we might have overfit a model to the original data. Fortunately, there are multiple cross-validation frameworks that we can apply when training a model, and these frameworks can help us improve a models predictive performance/accuracy while also reducing the occurrence of overfitting. One such cross-validation framework is k-fold cross-validation. Predictive analytics involves estimating a model based on past data, and then applying that model to new data in order to evaluate the accuracy of the models predictions. 48.1.2 Review of k-Fold Cross-Validation Often, when applying a predictive analytics process, we do some sort of variation of the two-phase process described above: (a) train the model on one set of data, and (b) test the model on a separate set of data. We typically put a lot of thought and consideration into how to train a model using the training data. Not only do we need to identify an appropriate type of statistical model, but we also need to consider whether we will incorporate validation into the model-training process. A common validation approach is k-fold cross-validation. To perform k-fold cross-validation, we often do the following: We split the data into a training data set and a test data set. Commonly, 80% of the data are randomly selected for the training data set, while the remaining 20% end up in the test data set. We randomly split the training data into two or more (sub)samples, which will allow us to train the model in an iterative process. The total number of samples we split the data into will be equal to k, where k refers to the number of folds (i.e., resamples). For example, if we randomly split the training data into five samples, then our k will be 5, and thus we will be performing 5-fold cross-validation. We then use the k samples to train (i.e., estimate, build) our model, which is accomplished through iterating the model-estimation process across k folds of data. For each fold, we estimate the model based on pooled data from k-1 samples. That is, if we split our training data into 5 samples, then for the first fold we might estimate the model based on data pooled together from the first four samples (out of five); we then use the fifth holdout (i.e., kth) sample as the validation sample, which means we assess the estimated models predictive performance/accuracy on data that werent used for that specific model estimation process. This process repeats until every sample has served as the validation sample, for a total of k folds. The models estimated performance across the k folds is then synthesized. Specifically, the k cross-validated models are then tossed out, but the estimated error from those models can be synthesized (e.g., averaged) to arrive at a performance estimate for the final model. With some types of models, the estimated error (e.g., root mean-squared error, R2) from those models is used to inform the final model estimation. Note that the final models parameter estimates are typically estimated using all of the available training data. The k-fold cross-validation framework can be extended by taking the final trained model and evaluating its performance based on the test data set that  to this point  has yet to be used; this is how k-fold cross-validation can be used in a broader predictive analytics (i.e., predictive modeling) context. It is important to note, however, that there are other cross-validation approaches we might choose, such as empirical cross-validation and leave-one-out cross-validation (LOOCV)  but those are beyond the scope of this chapter. James, Witten, Hastie, and Tibshirani (2013) provide a nice introduction to LOOCV; though, in short, LOOCV is a specific instance of k-fold cross-validation in which k is equal to the number of observations in the training data. The figure below provides a visual overview of the k-fold cross-validation process, which we will bring to life in the tutorial portion of this chapter. In this chapter, we will focus on applying k-fold cross-validation to the estimation and evaluation of a multiple logistic regression model. The use of logistic regression implies that our outcome variable will be dichotomous. For background information on logistic regression, including the statistical assumptions that should be satisfied, check out the prior chapter on logistic regression. I must note, however, that other types of statistical models can be applied within a k-fold cross-validation framework  or any cross-validation framework for that matter. For example, assuming the appropriate statistical assumptions have been met, we might also apply more traditional regression-based approaches like ordinary least squares (OLS) multiple linear regression, or we could apply supervised statistical learning models like Lasso regression, which will be covered in a later chapter. Overview of a k-fold cross-validation process 48.1.3 Conceptual Video For a more in-depth review of k-fold cross-validation, please check out the following conceptual video. Link to conceptual video: https://youtu.be/kituDjzXwfE 48.2 Tutorial This chapters tutorial demonstrates how to estimate and evaluate a multiple logistic regression model using k-fold cross-validation. 48.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/BQ1VAZ7jNYQ 48.2.2 Functions &amp; Packages Introduced Function Package as.factor base R set.seed base R nrow base R createDataPartition caret as.data.frame base R trainControl caret train caret print base R summary base R varImp caret predict base R confusionMatrix caret RMSE caret abs base R MAE caret cbind base R drop_na dplyr 48.2.3 Initial Steps If you havent already, save the file called PredictiveAnalytics.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called PredictiveAnalytics.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object df &lt;- read_csv(&quot;PredictiveAnalytics.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## ID = col_double(), ## Turnover = col_double(), ## JS = col_double(), ## OC = col_double(), ## TI = col_double(), ## Naff = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(df) ## [1] &quot;ID&quot; &quot;Turnover&quot; &quot;JS&quot; &quot;OC&quot; &quot;TI&quot; &quot;Naff&quot; # Print variable type for each variable in data frame (tibble) object str(df) ## spec_tbl_df[,6] [1,000 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ ID : num [1:1000] 1 2 3 4 5 6 7 8 9 10 ... ## $ Turnover: num [1:1000] 1 0 1 1 0 1 1 1 0 1 ... ## $ JS : num [1:1000] 3.78 4.52 2.26 2.48 4.12 2.99 2.44 2.31 3.87 4.62 ... ## $ OC : num [1:1000] 4.41 2.38 3.17 2.06 6 0.4 4.07 3.04 3.02 1.19 ... ## $ TI : num [1:1000] 4.16 2.74 2.76 4.32 1.37 3.24 2.63 1.94 3.69 1.96 ... ## $ Naff : num [1:1000] 1.82 2.28 2.25 2.11 2.01 2.73 2.91 2.62 2.89 2.45 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ID = col_double(), ## .. Turnover = col_double(), ## .. JS = col_double(), ## .. OC = col_double(), ## .. TI = col_double(), ## .. Naff = col_double() ## .. ) # Print first 6 rows of data frame (tibble) object head(df) ## # A tibble: 6 x 6 ## ID Turnover JS OC TI Naff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3.78 4.41 4.16 1.82 ## 2 2 0 4.52 2.38 2.74 2.28 ## 3 3 1 2.26 3.17 2.76 2.25 ## 4 4 1 2.48 2.06 4.32 2.11 ## 5 5 0 4.12 6 1.37 2.01 ## 6 6 1 2.99 0.4 3.24 2.73 # Print number of rows in data frame (tibble) object nrow(df) ## [1] 1000 The data frame (df) has 1000 cases and the following 5 variables: ID, Turnover, JS, OC, TI, and NAff. Per the output of the str (structure) function above, all of the variables are of type numeric (continuous: interval/ratio). ID is the unique employee identifier variable. Imagine that these data were collected as part of a turnover study within an organization to determine the drivers/predictors of turnover based on a sample of employees who stayed and leaved during the past year. The variables JS, OC, TI, and Naff were collected as part of an annual survey and were later joined with the Turnover variable. Survey respondents rated each survey item using a 7-point response scale, ranging from strongly disagree (0) to strongly agree (6). JS contains the average of each employees responses to 10 job satisfaction items. OC contains the average of each employees responses to 7 organizational commitment items. TI contains the average of each employees responses to 3 turnover intentions items, where higher scores indicate higher levels of turnover intentions. Naff contains the average of each employees responses to 10 negative affectivity items. Turnover is a variable that indicates whether these individuals left the organization during the prior year, with 1 = quit and 0 = stayed. 48.2.4 Apply k-Fold Cross-Validation Using Logistic Regression In this tutorial, we will apply k-fold cross-validation to estimate and evaluate a multiple logistic regression model. You can think of k-fold cross-validation as an enhanced version of the conventional empirical cross-validation process described and demonstrated in previous chapter. We can perform k-fold cross-validation with different types of regression models, and in this tutorial, we will focus on applying it using multiple logistic regression. Because we have access to 1,000 cases, we will randomly split our data into training and test data frames, and then within the training data frame, we will perform k-fold cross-validation to come up with a more accurate model. We will then we will apply the final model to the test data frame data frame to see how well it predicts with a completely new set of data. Note: The type of model we will apply in our k-fold cross-validation approach will delete any cases with missing data on any variables in the model (i.e., listwise deletion). We dont have any missing data in df data frame object, but if we did, I would recommend deleting cases with missing data on the focal variables prior to randomly splitting the data. The drop_na function from the tidyr package works well for this purpose. # In the event of missing data on focal variables, # perform listwise deletion prior to randomly splitting/partitioning # the data # Install dplyr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;tidyr&quot;) # Access readr package library(tidyr) # Drop rows with missing data on focal variables df &lt;- df %&gt;% drop_na(Turnover, JS, Naff, TI, OC) # Print number of rows in updated data frame (tibble) object nrow(df) ## [1] 1000 Because we didnt have missing data for any of the focal variables, the number of rows (i.e., cases) remains at 1000. Now were ready to install and access the caret package, if we havent already. The caret package has a number of different functions that are useful for running predictive analytics and various machine learning models/algorithms. # Install caret package if you haven&#39;t already install.packages(&quot;caret&quot;) # Access caret package library(caret) Using the createDataPartition function from the caret package, we will split the data such that 80% of the cases will be randomly assigned to one split, and the remaining 20% of cases will be assigned to the other split. Before doing so, lets use the set.seed function from base R and include a number of your choosing to create random operations that are reproducible. # Set random seed for subsequent random selection and assignment operations set.seed(1985) Next, lets perform the data split/partition. Well start by creating a name for the index of unique cases that will identify who will be included in the 80% split of the original data frame; here, I call this object index and assign values to it using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the createDataPartition function. As the first argument, type the name of the data frame (df), followed by the $ operator and the name of the outcome variable (Turnover). As a the second argument, set p=.8 to note that you wish to randomly select 80% (.8) of the cases from the data frame. As the third argument, type list=FALSE to indicate that we want the values in matrix form to facilitate, which will facilitate our ability to reference them in a subsequent step. As the fourth argument, type times=1 to indicate that we only want to request a single partition of the data. # Partition data and create index matrix of selected values index &lt;- createDataPartition(df$Turnover, p=.8, list=FALSE, times=1) Note: I have noticed that the createDataPartition sometimes adds an extra case to the partitioned data when the outcome variable is of type factor as opposed to numeric, integer, or character. An extra case added to the training data frame is not likely to be of much consequence, but if if you would like to avoid this, try converting the outcome variable from type factor to type numeric, integer, or character, whichever makes the most sense given the variable. You can always convert the outcome back to type factor after this process, which I demonstrate in this tutorial. Sometimes this works, and sometimes it doesnt. Well use the index matrix object we created above to assign cases to the training data frame (which we will name train_df) and to the test data frame (which we will name test_df). Beginning with the training data frame, we can assign the random sample of 80% of the original cases to the train_df object by using the &lt;- assignment operator. Specifically, using matrix notation, we will specify the name of the original data frame (df), followed by brackets ([ ]). In the brackets, include the name of the index object we created above, followed by a comma (,). The placement of the index object before the comma indicates that we are selecting rows with the selected unique row numbers (i.e., index) from the matrix. Next, we do essentially the same thing with the test_df, except that we insert a minus (-) symbol before index to indicate that we dont want cases associated with the unique row numbers contained in the index object. # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] If you received the error message shown below, then you will want to convert the original data frame df from a tibble to a conventional data frame; after that, repeat the steps above. If you did not receive this message, then you can skip this step and proceed on to the step in which you verify the number of rows in each data frame. \\(\\color{red}{\\text{Error: `i` must have one dimension, not 2.}}\\) # Convert data frame object (df) to a conventional data frame object df &lt;- as.data.frame(df) # Create test and train data frames train_df &lt;- df[index,] test_df &lt;- df[-index,] To check our work and to be sure that 80% of the cases ended up in the train_df data frame and the remaining 20% ended up in the test_df data frame, lets apply the nrow function from base R to each data frame object. # Verify number of rows (cases) in each data frame nrow(train_df) ## [1] 800 nrow(test_df) ## [1] 200 Indeed, 800 (80%) of the original 1,000 cases ended up in the train_df data frame, and 200 (20%) of the original 1,000 cases ended up in the test_df data frame. For the train_df and test_df data frames weve created, lets re-label our dichotomous outcome variables (Turnover) levels from 1 to quit and from 0 to stay. This will make our interpretations of the results a little bit easier later on. # Re-label values of outcome variable for train_df train_df$Turnover[train_df$Turnover==1] &lt;- &quot;quit&quot; train_df$Turnover[train_df$Turnover==0] &lt;- &quot;stay&quot; # Re-label values of outcome variable for test_df test_df$Turnover[test_df$Turnover==1] &lt;- &quot;quit&quot; test_df$Turnover[test_df$Turnover==0] &lt;- &quot;stay&quot; In addition, lets convert the outcome variable to a variable of type factor. To do so, we apply the as.factor function from base R. # Convert outcome variable to factor for each data frame train_df$Turnover &lt;- as.factor(train_df$Turnover) test_df$Turnover &lt;- as.factor(test_df$Turnover) Now were ready to specify the type of training method we wish to apply, the number of folds, and that we wish to save all of our predictions. Lets name this object containing these specifications as ctrlspecs using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the trainControl function from the caret package. As the first argument in the trainControl function, specify the method by typing method=\"cv\", where \"cv\" represents cross-validation. As the second argument, type number=10 to indicate that we want 10 folds (i.e., 10 resamples), which means we will specifically use 10-fold cross-validation. As the third argument, type savePredictions=\"all\" to save all of the hold-out predictions for each of our resamples from the train_df data frame, where hold-out predictions refer to those cases that werent used in training each iteration of the model based on each fold of data but were used for validating the model trained at each fold. As the fourth argument, type classProbs=TRUE, as this tells the function to compute class probabilities (along with predicted values) for classification models in each resample. # Specify type of training method used and the number of folds ctrlspecs &lt;- trainControl(method=&quot;cv&quot;, number=10, savePredictions=&quot;all&quot;, classProbs=TRUE) To train (i.e., estimate, build) our multiple logistic regression model, we will use the train function from the caret package. To begin, come up with a name for your model object; here, I name the model object model and use the &lt;- assignment operator to assign the model information to it. Type the name of the train function to the right of the &lt;- operator. As the first argument in the train function, we will specify our multiple logistic regression model. To the left of the tilde (~) operator, type the name of the dichotomous outcome variable called Turnover. To the right of the tilde (~) operator, type the names of the predictor variables we wish to include in the model, each separated by a + operator. As the second argument, type data= followed by the name of the training data frame object (train_df). As the third and fourth arguments, specify that we want to estimate a estimate a logistic regression model which is part of the generalized linear model (glm) and binomial family of analyses. To do so, type the argument method=\"glm\" and the argument family=binomial. As the fifth argument, type trControl= followed by the name of the object that includes your model specifications, which we created above (ctrlspecs). Finally, if you so desire, you can use the print function from base R to request some basic information about the training process. # Set random seed for subsequent random selection and assignment operations set.seed(1985) # Specify logistic regression model to be estimated using training data # and k-fold cross-validation process model1 &lt;- train(Turnover ~ JS + Naff + TI + OC, data=train_df, method=&quot;glm&quot;, family=binomial, trControl=ctrlspecs) # Print information about model print(model1) ## Generalized Linear Model ## ## 800 samples ## 4 predictor ## 2 classes: &#39;quit&#39;, &#39;stay&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 721, 720, 721, 719, 720, 721, ... ## Resampling results: ## ## Accuracy Kappa ## 0.6987682 0.3890541 In the general information provided in our output about our model/algorithm (i.e., generalized linear model), we find out which method was applied (i.e., cross-validated), the number of folds (i.e., 10), and the size of the same sizes in training each iteration of the model (e.g., 721, 720, 721, etc.). The function created 10 roughly equal-sized and randomly-assigned folds (per our request) from the train_df data frame by resampling from the original 800 cases. In total, the model was cross-validated 10 times across 10 folds, such that in each instance 9 samples (e.g., samples 1-9; i.e., k-1) were used to estimate the model and, more importantly, its classification/prediction error when applied to the validation kth sample (e.g., holdout data, out-of-sample data). This was repeated until every sample served as the validation sample once, resulting in k folds (i.e., 10 folds in this tutorial). These cross-validated models were then tossed out, but the estimated error, which by default is classification accuracy for this type of model, was averaged across the folds to provide an estimate of the overall predictive performance of the final model. The final model and its parameters (e.g., regression coefficients) were estimated based on the entire training data frame (train_df). In this way, the final model was built and trained to improve accuracy and, in the case of logistic regression, the percentage of correct classifications for the dichotomous outcome variable (Turnover). In the output, we also received estimates of the models performance in terms of the average accuracy and Cohens kappa values across the folds (i.e., resamples). Behind the scenes, the train function has done a lot for us. The accuracy statistic represents the proportion of total correctly classified cases out of all classification instances; in the case of a logistic regression model with a dichotomous outcome variable, a correctly classified case would be one in which the actual observed outcome for the case (e.g., observed quit) matches what was predicted based on the estimated model (e.g., predicted quit). Cohens kappa (\\(\\kappa\\)) is another classification accuracy statistic, but it differs from the accuracy statistic in that it accounts for the baseline probabilities from your null model that contains no predictor variables. In other words, it accounts for the proportion of cases in your data that were observed to have experienced one level of the dichotomous outcome (e.g., quit) versus the other level (e.g., stay). This is useful when there isnt a 50%/50% split in the observed levels/classes of your dichotomous outcome variable. Like so many effect size metrics, Cohens (\\(\\kappa\\)) has some recommended thresholds for interpreting the classification strength different \\(\\kappa\\) values, as shown below (Landis and Koch 1977). Cohens \\(\\kappa\\) Description .81-1.00 Almost perfect .61-.80 Substantial .41-60 Moderate .21-.40 Fair .00-.20 Slight &lt; .00 Poor Based on our k-fold cross-validation framework, model accuracy was .70 (70%) and Cohens kappa was .39, which would be considered fair given the thresholds provided above. Thus, our model did an okay job of classifying cases correctly estimated using k-fold cross-validation and when tested using the training data frame (train_df). We can also use the summary function from base R to view the results of our final model, just as we would if we were estimating a multiple logistic regression model without cross-validation when using the glm function from base R. If you so desire, you could also specify the equation necessary to predict the likelihood that an individual quits; if you need a refresher, refer to the chapter on logistic regression and specifically the chapter supplement, which demonstrates how to estimate a multiple logistic regression using the glm function from base R. # Print results of final model estimated using training data summary(model1) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3463 -0.9258 -0.4827 0.9650 2.4701 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.17201 0.61856 3.511 0.000446 *** ## JS 0.14929 0.07355 2.030 0.042386 * ## Naff -0.42540 0.15747 -2.702 0.006902 ** ## TI -0.99246 0.11133 -8.915 &lt; 0.0000000000000002 *** ## OC 0.25999 0.06994 3.717 0.000201 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1102.55 on 799 degrees of freedom ## Residual deviance: 907.54 on 795 degrees of freedom ## AIC: 917.54 ## ## Number of Fisher Scoring iterations: 4 As a next step, we will evaluate the importance of different predictor variables in our model. Variables with higher importance values contribute more to model estimation  and in this case, model classification. # Estimate the importance of different predictors varImp(model1) ## glm variable importance ## ## Overall ## TI 100.000 ## OC 24.509 ## Naff 9.758 ## JS 0.000 As you can see, TI was the most important variable when it came to predicting Turnover (1.00), and it was followed by OC (24.51) and then Naff (9.86). JS was deemed an unimportant predictor, as shown in the model summary above, and it had 0.00 importance based on this estimation process. Remember way back when in this tutorial when we partitioned our data into the train_df and test_df? Well, now were going to apply the final multiple logistic regression model we estimated using k-fold cross-validation and the train_df data frame to our test_df data frame. First, using our final model, we need to estimate predicted classes/values for individuals outcomes (Turnover) based on their predictor variables scores in the test_df data frame. Lets call the object to which we will assign these predictions predictions by using the &lt;- assignment operator. To the right of the &lt;- operator, type the name of the predict function from base R. As the first argument in the predict function, type the name of the model object we built using the train function (model1), and as the second argument, type newdata= followed by the name of the testing data frame (test_df). # Predict outcome using model from training data based on testing data predictions &lt;- predict(model1, newdata=test_df) Using the confusionMatrix function from the caret package, we will assess how well our model fit the test data. Type the name of the confusionMatrix function. As the first argument, type data= followed by the name of the object to which you assigned the predicted values/classes on the outcome in the previous step (predictions). As the second argument, type the name of the test data frame (test_df), followed by the $ operator and the name of the dichotomous outcome variable (Turnover). # Create confusion matrix to assess model fit/performance on test data confusionMatrix(data=predictions, test_df$Turnover) ## Confusion Matrix and Statistics ## ## Reference ## Prediction quit stay ## quit 85 37 ## stay 17 61 ## ## Accuracy : 0.73 ## 95% CI : (0.6628, 0.7902) ## No Information Rate : 0.51 ## P-Value [Acc &gt; NIR] : 0.0000000001757 ## ## Kappa : 0.4576 ## ## Mcnemar&#39;s Test P-Value : 0.009722 ## ## Sensitivity : 0.8333 ## Specificity : 0.6224 ## Pos Pred Value : 0.6967 ## Neg Pred Value : 0.7821 ## Prevalence : 0.5100 ## Detection Rate : 0.4250 ## Detection Prevalence : 0.6100 ## Balanced Accuracy : 0.7279 ## ## &#39;Positive&#39; Class : quit ## The output provides us with a lot of useful information that can help us assess how well the model fit and performed with the test data, which I review below. Contingency Table: The output first provides a contingency table displaying the 2x2 table of observed quit vs. stay in relation to predicted quit vs. stay. Model Accuracy: We can see that the accuracy is .73 (73%) (95% CI[.66, .79]). The p-value is a one-sided test that helps us understand if the model is more accurate than a model with no information (e.g., predictors); here, we see that that p-value is less than .05, which would lead us to conclude the model is more accurate to a statistically significant extent. Cohens \\(\\kappa\\): Cohens \\(\\kappa\\) is .46, which would be considered moderate classification strength by the thresholds shown above. Sensitivity: Sensitivity represents the number of cases in which the positive class (e.g., quit) was accurately predicted divided by the total number of cases whose observed data fall into the positive class (e.g., quit). In other words, sensitivity represents the proportion of people who were correctly classified as having quit relative to the total number of people who actually quit. In this case, sensitivity is .83. Specificity: Specificity can be conceptualized as the opposite of sensitivity in that it represents the number of cases in which the negative class (e.g., stay) was accurately predicted divided by the total number of cases whose observed data fall into the negative class (e.g., stay). In other words, specificity represents the proportion of people who were correctly classified as having stayed relative to the total number of people who actually stayed. In this case, sensitivity is .62. Prevalence: Prevalence represents the number of cases whose observed data indicate that they were in the positive class (e.g., quit) divided by the total number of cases in the sample. In other words, prevalence represents the proportion of people who actually quit relative to the entire sample. In this example, prevalence is .51. When interpreting sensitivity, it is useful to compare it to prevalence. Balanced Accuracy: Balanced accuracy is calculated by adding sensitivity to specificity and then dividing that sum by 2. In this example, the balanced accuracy is .73, and thus provides another estimate of the models overall classification accuracy. To learn about some of the other information in the output, check out the documentation for the confusionMatrix function. # View documentation for confusionMatrix function from caret package ?caret::confusionMatrix Summary of Results: Overall, the multiple logistic regression model we trained using k-fold cross-validation performed reasonably well when applied to the test data. Adding additional predictors to the model or measuring the predictors with great reliability could improve the predictive and classification performance of the model. 48.2.5 Summary In this chapter, we dipped our toes into predictive analytics by using the k-fold cross-validation approach in which we trained our model using one set of data and then applied to a separate set of data to evaluate the extent to which it correctly classified cases. "],["survival.html", "Chapter 49 Understanding Length of Service Using Survival Analysis 49.1 Conceptual Overview 49.2 Tutorial", " Chapter 49 Understanding Length of Service Using Survival Analysis In this chapter, we will learn how to perform a survival analysis in order to understand employees length of service prior to leaving the organization. 49.1 Conceptual Overview The delightfully named survival analysis refers to various statistical techniques that can be used to investigate how long it takes for an event to occur. How did it get that name? Well, the term survival comes from medical and health studies that predicted, well, survival time. Of course, survival analysis can also be applied to other phenomena of interest. For example, in organizations, we might wish to understand the probability of individuals voluntarily leaving the organization by certain time points and the factors, predictors, or drivers (i.e., covariates) that might help us understand how long certain individuals are likely stay  or their expected length of service. I should note that survival analysis also goes by other names, such as failure analysis, time-to-event analysis, duration analysis, event-history analysis, and accelerated failure time analysis. 49.1.1 Censoring Due to normal data acquisition processes, we often collect data at a specific point in time, where historical data might only be available for certain individuals (or based on whether they meet the inclusions criteria to be part of our sample). Further, for some individuals, the event in question (e.g., voluntary turnover) will not have occurred (at least in the context of the acquired data), which means that their length of survival until the event remains unknown. Alternatively, other individuals may have experienced another event (e.g., involuntary turnover) that was the reason for their exit from the study. Individuals who fall into these categories are referred to as censored cases. There are different types of censoring, such as right censoring, left censoring, and interval censoring. Right-censored cases are those cases for which the event has not yet occurred when data acquisition is completed and the study has ended, or those cases that dropped out of study data before the end for reasons other than the focal event; for example, if we would like to understand the survival rate for individuals who voluntarily turn over (i.e., focal event), then examples of right-censored individuals include those individuals who are still employed at the organization by the end of the study and those individuals who dropped out of the study prematurely due because they were fired (i.e., involuntarily turned over). Left-censored cases are those cases for which the focal event occurred prior to the study commencing. In organizational research, this would often mean that a case was mistakenly included in the study sample, when the case should have been excluded. An example of a left-censored case - albeit an unlikely one - would be if for some reason it was later determined that an employee included a study sample had voluntarily turned over (i.e., focal event) prior to the beginning of the 5-year study period. In organizational research, usually such cases will just be removed from the sample prior to analysis. Interval-censored cases are those cases for which the focal event occurred between two time points but where the exact time of the event remains unknown. As an example, suppose that for some reason, the exact day on which an employee voluntarily turned over (i.e., focal event) is unknown, but organizational records indicate that the individual was employed at the end of the prior month but not employed by the end of the subsequent month. This individual would be interval censored. In this chapter, we will focus on right-censored cases, as this is perhaps the most common type of censoring we concern ourselves with in organizational research. 49.1.2 Types of Survival Analysis There are different techniques we can use to carry out survival analysis, and different techniques are suited to answering different questions. The following are survival analyses that you might encounter: Kaplan-Meier analysis, Cox proportional hazards model (i.e., Cox regression), log-logistic parametric model, Weibull model, survival tree analysis, and survival random forest analysis. In this tutorial, we will focus on implementing Kaplan-Meier analysis and the Cox proportional hazards model. Before doing so, however, we will learn about what is referred to as the life table. 49.1.2.1 Life Table A life table is a descriptive tool and contains important information regarding length of survival and survival rate, and it is descriptive in nature. A life table displays the proportion of survivors at specific time points and, specifically, between two time points, where the latter is referred to as a time interval (\\(i\\)). The time interval width chosen affects the survival probabilities, so its important to choose the size of the interval wisely and thoughtfully. One approach is to create a new time interval at each subsequent time at which the event occurs; this is usually the default approach. Another approach is to create time intervals that are meaningful for the context; for example, if the focal event is voluntary turnover, then we might decide that the survival probability/rate should be assessed at 30, 60, and 90 days post hire and then 6 months, 1 year, 2 years, 3 years, and so forth and so on. Lets consider an example of a life table. Imagine you wish to know how many people remained with the organization between their 61st day of work until the end of their probationary period at 89 days post hire. First, we need to estimate \\(r_i\\), where \\(r_i\\) refers to the adjusted number of individuals at risk during the time interval in question, which takes into account how many individuals entered the interval and how many individuals were censored. \\(r_i = n_i - \\frac{1}{2}c_i\\) where \\(n_i\\) refers to the number of individuals who entered the interval (e.g., 60-89 days), and \\(c_i\\) refers to the number of individuals who were censored in the time interval. For example, if 100 individuals entered the interval and 10 individuals were censored, then \\(r_i\\) is equal to 95. \\(r_i = 100 - \\frac{1}{2}*10 = 95\\) Next, we can calculate \\(q_i\\), which refers to the proportion of individuals who experienced the event during the time interval in question (i.e., did not survive). \\(q_i = \\frac{d_i}{r_i}\\) where \\(d_i\\) refers to the number of individuals who experienced the event during the time interval (i.e., did not survive). Lets assume that 15 individuals experienced the event during this interval; this means that \\(q_i\\) is equal to .16. \\(q_i = \\frac{15}{95} = .16\\) To calculate \\(p_i\\), which is the proportion of individuals who did not experience the event during the time interval, we just subtract \\(q_i\\) from \\(1\\). You can think of \\(p_i\\) as the survival rate for a specific time interval. \\(p_i = 1 - q_i\\) If we plug our \\(q_i\\) value of .14 into the formula, we arrive at a \\(p_i\\) equal to .84. \\(p_i = 1 - .16 = .84\\) Finally, to calculate \\(s_i\\), which refers to the proportion of individuals who survived past/through the time interval in question (i.e., cumulative survival rate or cumulative survival proportion/probability), we use the following formula. \\(s_i = s_{i-1} * p_i\\) where \\(s_{i-1}\\) refers to the proportion of individuals who survived past/through the previous time interval. Lets assume in this example that \\(s_{i-1}\\) is equal to .86, which would indicate that 86% of individuals survived past the previous time interval (i.e., did not experience the event). \\(s_i = .86 * .84 = .72\\) As you can see, in this example, the proportion of individuals who survived past/through the focal time interval is .72. In other words, the cumulative survival rate is .72 for that time interval. When we assemble all of these estimates together, we end up with a life table like the one shown below. Interval (\\(i\\)) \\(n_i\\) \\(c_i\\) \\(r_i\\) \\(d_i\\) \\(q_i\\) \\(p_i\\) \\(s_i\\) 0-29 days 120 2 119 15 .13 .87 .87 30-59 days 103 1 102.5 2 .02 .98 .85 60-89 days 100 10 95 15 .16 .84 .71 49.1.2.2 Kaplan-Meier Analysis Kaplan-Meier analysis is a nonparametric method, which means that it does not have the same distributional assumptions as a parametric method. This analysis allows us to estimate the standard error and confidence interval of the survival rate (\\(s_i\\)) at each time interval and across the entire study, which helps us to account for and understand uncertainty owing to sampling error. Using Kaplan-Meier analysis, we can also understand how independent groups of cases (e.g., race/ethnicity groups) differ with respect to their survival rates and their overall curves/trajectories. Kaplan-Meier analysis is an example of a descriptive approach to survival analysis, and it is also referred to as the product-limit method/estimator. 49.1.2.3 Cox Proportional Hazards Model The Cox proportional hazards model is a semiparametric method in that it makes a distributional assumption about the covariates in the model and their log-linear relation to the hazard (i.e., instantaneous rate of experiencing focal event), but does not make a distributional assumption about the hazard function itself. This analysis allows us to investigate how categorical and continuous covariates (e.g., predictor variables) might affect individuals length of survival. 49.1.2.4 Statistical Assumptions Given that Kaplan-Meier analysis and the Cox proportional hazards model are examples of nonparametric and semiparametric methods, respectively, we dont need to meet many of the statistical assumptions commonly associated with traditional parametric methods. Kaplan-Meier analysis makes the following assumptions: Experiencing the focal event and being censored are mutually exclusive and independent; (b) censoring is similar between independent groups (i.e., by levels of categorical covariate); No interventions or trends were introduced during the study time frame that may have influenced the probability of experiencing the focal event. As described above, the Cox proportional hazards model is a semiparametric method and has a distributional assumption about the covariates in the model and their relation to the hazard. In addition, this type of model makes the following assumptions: Time-to-event values of cases are independent of one another; Hazard ratio is constant over time; Model covariates have multiplicative effect on the hazard (i.e., association are not linear). 49.1.2.5 Statistical Signficance Using null hypothesis significance testing (NHST), we interpret a p-value that is less than .05 (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the coefficient is equal to zero. In other words, if a coefficients p-value is less than .05, we conclude that the coefficient differs from zero to a statistically significant extent. In contrast, if the coefficients p-value is equal to or greater than .05, then we fail to reject the null hypothesis that the coefficient is equal to zero. Put differently, if a coefficients p-value is equal to or greater than .05, we conclude that the coefficient does not differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population. When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline p-values signify significance or nonsignificance. For our purposes, lets be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, p = .049 would be considered statistically significant, and p = .050 would be considered statistically nonsignificant. 49.1.3 Conceptual Video For a more in-depth conceptual review of survival, please check out the following conceptual video. Link to conceptual video: https://youtu.be/874ZcEMlUas 49.2 Tutorial This chapters tutorial demonstrates how to perform survival analysis using R. 49.2.1 Video Tutorials As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below. Link to video tutorial: https://youtu.be/Bubo_7R7h0Q 49.2.2 Functions &amp; Packages Introduced Function Package survfit survival Surv survival hist base R subset base R print base R summary base R c base R plot base R ggsurvplot survminer coxph survival scale base R factor base R drop_na tidyr anova base R 49.2.3 Initial Steps If you havent already, save the file called Survival.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called Survival.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object survdat &lt;- read_csv(&quot;Survival.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## id = col_double(), ## Start_date = col_character(), ## Gender = col_character(), ## Race = col_character(), ## Pay_hourly = col_double(), ## Pay_sat = col_double(), ## Turnover = col_double(), ## Turnover_date = col_character(), ## LOS = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(survdat) ## [1] &quot;id&quot; &quot;Start_date&quot; &quot;Gender&quot; &quot;Race&quot; &quot;Pay_hourly&quot; &quot;Pay_sat&quot; ## [7] &quot;Turnover&quot; &quot;Turnover_date&quot; &quot;LOS&quot; # View variable type for each variable in data frame (tibble) object str(survdat) ## spec_tbl_df[,9] [701 x 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:701] 1073 1598 1742 1125 1676 ... ## $ Start_date : chr [1:701] &quot;5/4/2014&quot; &quot;9/30/2016&quot; &quot;3/14/2018&quot; &quot;9/7/2014&quot; ... ## $ Gender : chr [1:701] &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; &quot;Man&quot; ... ## $ Race : chr [1:701] &quot;Black&quot; &quot;Black&quot; &quot;Black&quot; &quot;Black&quot; ... ## $ Pay_hourly : num [1:701] 12.6 17.7 12.6 15.2 14.7 ... ## $ Pay_sat : num [1:701] 4.67 3 4.33 3 3.33 ... ## $ Turnover : num [1:701] 1 1 0 1 0 1 2 1 1 0 ... ## $ Turnover_date: chr [1:701] &quot;1/14/2017&quot; &quot;5/7/2018&quot; NA &quot;4/13/2017&quot; ... ## $ LOS : num [1:701] 986 584 293 949 622 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. Start_date = col_character(), ## .. Gender = col_character(), ## .. Race = col_character(), ## .. Pay_hourly = col_double(), ## .. Pay_sat = col_double(), ## .. Turnover = col_double(), ## .. Turnover_date = col_character(), ## .. LOS = col_double() ## .. ) # View first 6 rows of data frame (tibble) object head(survdat) ## # A tibble: 6 x 9 ## id Start_date Gender Race Pay_hourly Pay_sat Turnover Turnover_date LOS ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1073 5/4/2014 Man Black 12.6 4.67 1 1/14/2017 986 ## 2 1598 9/30/2016 Man Black 17.7 3 1 5/7/2018 584 ## 3 1742 3/14/2018 Man Black 12.6 4.33 0 &lt;NA&gt; 293 ## 4 1125 9/7/2014 Man Black 15.2 3 1 4/13/2017 949 ## 5 1676 4/19/2017 Man Black 14.7 3.33 0 &lt;NA&gt; 622 ## 6 1420 12/25/2015 Man Black 15.9 4.33 1 7/31/2018 949 There are 9 variables and 701 cases (i.e., individuals) in the survdat data frame: id, Start_date, Gender, Race, Pay_hourly, Pay_sat, Turnover, Turnover_date, and LOS. In this tutorial we will focus on just Race, Pay_hourly, Pay_sat, Turnover, and LOS. the Race variable is categorical (nominal) and contains information about the race for each case in the sample. The Pay_hourly variable contains numeric data regarding the hourly pay rate of each individual when they either last worked in the organization or their current hourly pay rate. The Pay_sat variable comes from a pay satisfaction survey, and represents the overall perceived satisfaction with pay, ranging from 1 (low) to 5 (high). The Turnover variable indicates whether a case stayed in the organization (0), voluntarily quit (1), or involuntarily left. Finally, the LOS variable stands for length of service, and it represents how many days an individual worked in the organization prior to leaving or how long those who were employed at the time of the data collection had worked in the organization thus far. 49.2.4 Create a Censoring Variable Lets assume that our focal event for which we wish to estimate survival is voluntary turnover. Note that our Turnover variable has three levels: 0 = stayed, 1 = voluntary turnover, 2 = involuntary turnover. Given that, we need to create a variable that can be used to classify the right-censored cases, which are those individuals who are still employed at the organization at the end of the study (Turnover = 0) and those who left the organization for reasons other than voluntary turnover (Turnover = 2). Now were ready to create a new variable called censored in which cases with Turnover equal to 1 are set to 1; this will indicate the individuals who experienced the event (i.e., voluntary turnover) within the study. Next cases with Turnover equal to 0 or 2 are set to 1 for the censored variable, and these cases represented are our right-censored cases. # Create censoring variable survdat$censored[survdat$Turnover == 1] &lt;- 1 survdat$censored[survdat$Turnover == 0 | survdat$Turnover == 2] &lt;- 0 49.2.5 Inspect Distribution of Length of Service To better understand our data, lets run a simple histogram for length of service (LOS) for all cases in our data frame (survdat) using the hist function from base R. # Inspect distribution for LOS for all cases hist(survdat$LOS) Interestingly, length of service looks normally distributed in this sample. Lets drill down a bit deeper by looking at length of service for just those individuals who stayed in the organization for the duration of the study. To do so, lets bring in the subset function from base R and integrate it within our hist function. # Inspect distribution for LOS for those who stayed hist(subset(survdat, Turnover==0)$LOS) This distribution also looks relatively normal. Next, lets inspect the distributions for those who voluntarily turned over. # Inspect distribution for LOS for those who voluntarily turned over hist(subset(survdat, Turnover==1)$LOS) Both of these distributions seem to be relatively normal as well, although the distribution for just those individuals who involuntarily turned over has many fewer cases. Please note that it is not unusual to see nonnormal distributions of length of service; for example, in some industries, there tends to be a large number of first day no shows for certain jobs and a higher turnover during the first 90 days on-the-job than after the first 90 days. This can result in a positively skewed distribution. 49.2.6 Conduct Kaplan-Meier Analysis &amp; Create Life Table Well begin by taking a descriptive approach to survival analysis. That is, well creating a life table and conduct a Kaplan-Meier analysis. If you havent already, be sure to install and access the survival package. # Install survival package if you haven&#39;t already install.packages(&quot;survival&quot;) # Access survival package library(survival) To begin, we will use the survfit and Surv functions from the survival package, which allow us to fit a survival model based on a survival object. In this case, a survival object contains a vector of censored and non-censored values for the cases in our data frame, where the censored values are followed by a plus (+) sign. [Remember, we created a variable called censored above, which will serve as the basis for distinguishing between censored and non-censored cases in our data frame (survdat).] That is, the behind-the-scenes vector shows how long each person survived before the event in question, and if they were still surviving at the time the data were acquired throughout the study or left the study for some other reason, then they are considered to be (right) censored. Lets name our Kaplan-Meier analysis model km_fit1 using the &lt;- operator. Next, type the name of the survfit function from the survival package. As the first argument, specify the survival model by specifying the outcome to the left of the ~ (tilde) symbol and the predictor to the right. As the outcome, we type the Surv function with the survival length variable (length of service: LOS) as the first argument, and the variable that indicates which cases should be censored (censored) should be typed as the second argument; cases with a zero (0) on the second variable (censored) are treated as censored because they did not experience the event in question (i.e., voluntary turnover). As our predictor, we will specify the numeral 1 because we want to begin by estimating a null model (i.e., no predictor variables); this value goes to the right of the ~ (tilde) symbol. As the second argument in the survfit function, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. As the final argument, type type=\"kaplan-meier\" to specify that we wish to use Kaplan-Meier estimation. Use the print function from base R to view the basic descriptive survival information for the entire sample. # Conduct Kaplan-Meier analysis km_fit1 &lt;- survfit(Surv(LOS, censored) ~ 1, data=survdat, type=&quot;kaplan-meier&quot;) # Print basic descriptive survival information print(km_fit1) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## n events median 0.95LCL 0.95UCL ## 701 463 1059 1022 1095 In the output, we find some basic descriptive information about our survival model. The n column indicates the number of total cases in our sample (e.g., 701). The events column indicates how many cases experienced the event over the course of the study (e.g., 463), where in this example, the event in question is voluntary turnover. The median column indicates the median length of time (e.g., 1059), where time in this example is measured in days of length of service before someone experiences the event; this is determined based on the time to event when the cumulative survival rate is .50. The 0.95LCL and 0.95UCL contain the upper and lower limits of the 95% confidence interval surrounding the median value (e.g., 1022-1095). Based on this output, we can concluded that 479 individuals out of 701 individuals voluntarily turned over over the course of the study time frame, and the median length of service prior to leaving was 1059 days (95% CI[1022, 1095]). Note: If the overall median length of survival (i.e., time to event) and/or the associated lower/upper confidence interval limits is/are NA, it indicates a cumulative survival rate of .50 was not reached for those estimates. Usually, you can see this visually in a survival plot, as the survival curve and/or its confidence interval limits will not cross a cumulative survival rate/probability of .50. Next, lets summarize the Kaplan-Meier analysis object we created (km_fit1) using the summary function from base R. Just enter the name of the model object as the sole parenthetical argument in the summary function. # Summarize results of Kaplan-Meier analysis using default time intervals # and create a life table summary(km_fit1) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 701 12 0.9829 0.00490 0.9733 0.9925 ## 110 682 1 0.9814 0.00510 0.9715 0.9915 ## 146 681 3 0.9771 0.00566 0.9661 0.9883 ## 183 677 4 0.9713 0.00632 0.9590 0.9838 ## 219 672 3 0.9670 0.00677 0.9538 0.9804 ## 256 667 4 0.9612 0.00732 0.9470 0.9757 ## 292 659 1 0.9597 0.00745 0.9453 0.9745 ## 329 655 1 0.9583 0.00758 0.9435 0.9733 ## 365 652 6 0.9495 0.00832 0.9333 0.9659 ## 402 641 7 0.9391 0.00911 0.9214 0.9571 ## 438 633 8 0.9272 0.00991 0.9080 0.9469 ## 475 616 4 0.9212 0.01030 0.9012 0.9416 ## 511 610 8 0.9091 0.01101 0.8878 0.9310 ## 548 601 13 0.8895 0.01205 0.8662 0.9134 ## 584 584 11 0.8727 0.01284 0.8479 0.8982 ## 621 566 11 0.8557 0.01357 0.8296 0.8828 ## 657 549 12 0.8370 0.01431 0.8095 0.8656 ## 694 532 16 0.8119 0.01520 0.7826 0.8422 ## 730 506 11 0.7942 0.01577 0.7639 0.8257 ## 767 487 25 0.7534 0.01694 0.7210 0.7874 ## 803 454 20 0.7203 0.01774 0.6863 0.7559 ## 840 428 12 0.7001 0.01818 0.6653 0.7366 ## 876 406 24 0.6587 0.01897 0.6225 0.6969 ## 913 366 20 0.6227 0.01956 0.5855 0.6622 ## 949 339 19 0.5878 0.02004 0.5498 0.6284 ## 986 313 22 0.5465 0.02047 0.5078 0.5881 ## 1022 281 23 0.5017 0.02081 0.4626 0.5442 ## 1059 247 18 0.4652 0.02101 0.4258 0.5082 ## 1095 224 25 0.4133 0.02107 0.3740 0.4567 ## 1132 189 16 0.3783 0.02103 0.3392 0.4218 ## 1168 163 15 0.3435 0.02092 0.3048 0.3870 ## 1205 138 14 0.3086 0.02077 0.2705 0.3521 ## 1241 114 13 0.2734 0.02057 0.2359 0.3169 ## 1278 96 10 0.2449 0.02030 0.2082 0.2882 ## 1314 84 8 0.2216 0.01997 0.1857 0.2644 ## 1351 69 6 0.2023 0.01973 0.1672 0.2449 ## 1387 60 6 0.1821 0.01941 0.1478 0.2244 ## 1424 51 10 0.1464 0.01860 0.1141 0.1878 ## 1460 32 10 0.1007 0.01753 0.0715 0.1416 ## 1497 21 4 0.0815 0.01661 0.0546 0.1215 ## 1533 15 1 0.0760 0.01637 0.0499 0.1159 ## 1570 14 1 0.0706 0.01607 0.0452 0.1103 ## 1606 11 2 0.0578 0.01550 0.0341 0.0978 ## 1643 7 1 0.0495 0.01533 0.0270 0.0908 ## 1679 6 1 0.0413 0.01483 0.0204 0.0835 ## 1935 1 1 0.0000 NaN NA NA By default, this function begins each new time interval at the time in which a new event (i.e., voluntary turnover) occurs. This explains why the time intervals are not consistent widths in the time column of the life table output. The n.risk column corresponds to the the number of individuals who entered the time interval (\\(i\\)), which I introduced earlier in this tutorial using the notation \\(n_i\\). The n.event column contains the number of individuals who experienced the event during the time interval (i.e., did not survive), which I introduced earlier using the notation \\(d_i\\). The survival column contains the the proportion of individuals who survived past/through the time interval in question, which is referred to as the (cumulative) survival rate (\\(s_i\\)). The std.err column provides the standard errors (SEs) associated with the cumulative survival rate points estimates, and the lower 95% CI and upper 95% CI columns provide the 95% confidence interval around the point cumulative survival rate point estimate. Thus, the survfit function in combination with the summary function provides us with an abbreviated life table. We can, however, exert more control over the width of the time intervals in our life table output, and we might exert this control because we have business-related or theoretically meaningful time intervals that we would like to impose. Lets adapt our previous summary function by adding an additional argument relating to the time intervals. Specifically, as the second argument, type times= followed by the c (combine) function from base R to specify a vector of different time intervals for which you would like to estimate survival rates. In this example, I set the intervals at 30 days, 60 days, 90 days post hire, and up to 30 subsequent 90-day intervals after the initial 90 days (by using the 90*(1:30) notation). # Summarize results of Kaplan-Meier analysis using pre-specified time intervals # and create a life table summary(km_fit1, times=c(30, 60, 90*(1:30))) ## Call: survfit(formula = Surv(LOS, censored) ~ 1, data = survdat, type = &quot;kaplan-meier&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 701 0 1.0000 0.00000 1.0000 1.0000 ## 60 701 0 1.0000 0.00000 1.0000 1.0000 ## 90 682 12 0.9829 0.00490 0.9733 0.9925 ## 180 677 4 0.9771 0.00566 0.9661 0.9883 ## 270 659 11 0.9612 0.00732 0.9470 0.9757 ## 360 652 2 0.9583 0.00758 0.9435 0.9733 ## 450 616 21 0.9272 0.00991 0.9080 0.9469 ## 540 601 12 0.9091 0.01101 0.8878 0.9310 ## 630 549 35 0.8557 0.01357 0.8296 0.8828 ## 720 506 28 0.8119 0.01520 0.7826 0.8422 ## 810 428 56 0.7203 0.01774 0.6863 0.7559 ## 900 366 36 0.6587 0.01897 0.6225 0.6969 ## 990 281 61 0.5465 0.02047 0.5078 0.5881 ## 1080 224 41 0.4652 0.02101 0.4258 0.5082 ## 1170 138 56 0.3435 0.02092 0.3048 0.3870 ## 1260 96 27 0.2734 0.02057 0.2359 0.3169 ## 1350 69 18 0.2216 0.01997 0.1857 0.2644 ## 1440 32 22 0.1464 0.01860 0.1141 0.1878 ## 1530 15 14 0.0815 0.01661 0.0546 0.1215 ## 1620 7 4 0.0578 0.01550 0.0341 0.0978 ## 1710 4 2 0.0413 0.01483 0.0204 0.0835 ## 1800 3 0 0.0413 0.01483 0.0204 0.0835 ## 1890 1 0 0.0413 0.01483 0.0204 0.0835 The output includes a life table with the time intervals that we specified in the time column, along with the other columns that we reviewed in the previous output shown above. Looking at this life table, descriptively, we can see that the cumulative survival rate begins to drop more precipitously beginning at about 720 days post hire. We can create a data visualization (i.e., line chart) of our km_fit1 model by using the plot function from base R. Simply, enter the name of the model object as the sole parenthetical argument of the plot function. # Plot the cumulative survival rates (probabilities) plot(km_fit1) The data visualization shown above provides a visual depiction of the survival curve/trajectory along with the 95% confidence interval for each time interval. Corroborating what we saw in our first life table output using the default time-interval settings, the cumulative survival rate begins to drop more precipitously when length of service is between 500 and 100 days. Note that the x-axis shows the length of service (in days), and the y-axis shows the cumulative survival rate (i.e., probability). We can also create a more aesthetically pleasing data visualization of our km_fit1 model by using the ggsurvplot function from the survminer package. If you havent already, install and access the survminer package. # Install survminer package if you haven&#39;t already install.packages(&quot;survminer&quot;) # Access survminer package library(survminer) Using the ggsurvplot function, type the name of the Kaplan-Meier analysis object (km_fit1) as the first argument. As the second argument, type data= followed by the name of the data frame object (survdat). The following arguments are optional. Setting risk.table=TRUE requests the risk table as output. Setting conf.int=TRUE requests that the 95% confidence intervals be displayed to show uncertainty. Finally, because the ggsurvplot function is built in part on the ggplot2 package, we can request different data visualization themes from ggplot2; by setting ggtheme=theme_minimal(), we are requesting a minimalist theme. # Plot the cumulative survival rates (probabilities) ggsurvplot(km_fit1, data=survdat, risk.table=TRUE, conf.int=TRUE, ggtheme=theme_minimal()) The plot graphically displays the cumulative survival rates/probabilities and the risk table. The shading around the red line represents the 95% confidence intervals. The risk table below the plot displays the average number of cases at risk of experiencing the event (\\(r_i\\)) at the time intervals displayed on the x-axis. Moving forward, lets extend our Kaplan-Meier analysis model by adding a categorical covariate (i.e., predictor variable) to our Kaplan-Meier analysis model to see if we can observe differences between groups/categories in terms of survival curves. Specifically, lets replace the numeral 1 from our previous model with the categorical Race variable, and lets name this model object km_fit2. # Conduct Kaplan-Meier analysis with categorical covariate km_fit2 &lt;- survfit(Surv(LOS, censored) ~ Race, data=survdat, type=&quot;kaplan-meier&quot;) print(km_fit2) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## n events median 0.95LCL 0.95UCL ## Race=Black 283 190 1022 986 1095 ## Race=HispanicLatino 79 57 1022 876 1132 ## Race=White 339 216 1059 1022 1095 In the output, we find basic descriptive information about our survival model; however, because we added the categorical covariate variable called Race, we now see the overall survival information for the study time frame displayed by Race categories. For example, the n column indicates the number of total cases in our sample by Race categories (e.g., 283, 79, 339), and the events column indicates how many cases experienced the event over the course of the study for each Race category (e.g., 190, 57, 216). This information allows us to segment the survival information by levels/categories of the categorical covariate added to the model. As we did before, lets summarize the updated model object (km_fit2) using the default time intervals. # Summarize results of Kaplan-Meier analysis using default time intervals # and create a life table summary(km_fit2) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## Race=Black ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 283 6 0.9788 0.00856 0.9622 0.9957 ## 146 272 3 0.9680 0.01049 0.9477 0.9888 ## 183 268 3 0.9572 0.01210 0.9337 0.9812 ## 256 264 1 0.9535 0.01258 0.9292 0.9785 ## 292 263 1 0.9499 0.01305 0.9247 0.9758 ## 329 261 1 0.9463 0.01350 0.9202 0.9731 ## 365 260 3 0.9354 0.01474 0.9069 0.9647 ## 402 255 7 0.9097 0.01724 0.8765 0.9441 ## 438 248 1 0.9060 0.01755 0.8723 0.9411 ## 475 244 3 0.8949 0.01848 0.8594 0.9318 ## 511 241 4 0.8800 0.01961 0.8424 0.9193 ## 548 236 7 0.8539 0.02136 0.8131 0.8968 ## 584 227 5 0.8351 0.02249 0.7922 0.8804 ## 621 219 3 0.8237 0.02313 0.7796 0.8703 ## 657 212 3 0.8120 0.02376 0.7668 0.8599 ## 694 205 4 0.7962 0.02458 0.7494 0.8458 ## 730 195 1 0.7921 0.02479 0.7450 0.8422 ## 767 191 8 0.7589 0.02638 0.7089 0.8124 ## 803 180 5 0.7378 0.02728 0.6862 0.7933 ## 840 173 5 0.7165 0.02811 0.6635 0.7738 ## 876 165 9 0.6774 0.02944 0.6221 0.7377 ## 913 152 12 0.6239 0.03090 0.5662 0.6875 ## 949 136 8 0.5872 0.03169 0.5283 0.6528 ## 986 126 12 0.5313 0.03253 0.4712 0.5990 ## 1022 110 8 0.4927 0.03291 0.4322 0.5616 ## 1059 98 8 0.4525 0.03315 0.3919 0.5223 ## 1095 89 9 0.4067 0.03312 0.3467 0.4771 ## 1132 74 8 0.3627 0.03299 0.3035 0.4335 ## 1168 63 6 0.3282 0.03272 0.2699 0.3990 ## 1205 53 2 0.3158 0.03264 0.2579 0.3867 ## 1241 47 5 0.2822 0.03244 0.2253 0.3535 ## 1278 41 6 0.2409 0.03177 0.1860 0.3120 ## 1314 33 3 0.2190 0.03130 0.1655 0.2898 ## 1351 26 3 0.1937 0.03090 0.1417 0.2648 ## 1387 21 3 0.1661 0.03034 0.1161 0.2376 ## 1424 18 4 0.1292 0.02866 0.0836 0.1995 ## 1460 11 6 0.0587 0.02336 0.0269 0.1281 ## 1497 5 1 0.0470 0.02144 0.0192 0.1149 ## 1606 3 2 0.0157 0.01464 0.0025 0.0979 ## 1935 1 1 0.0000 NaN NA NA ## ## Race=HispanicLatino ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 79 2 0.9747 0.0177 0.94065 1.000 ## 256 77 1 0.9620 0.0215 0.92079 1.000 ## 438 74 3 0.9230 0.0302 0.86569 0.984 ## 475 71 1 0.9100 0.0325 0.84859 0.976 ## 511 69 1 0.8968 0.0346 0.83159 0.967 ## 548 68 1 0.8836 0.0365 0.81496 0.958 ## 584 67 1 0.8705 0.0382 0.79863 0.949 ## 621 66 3 0.8309 0.0428 0.75112 0.919 ## 657 62 3 0.7907 0.0466 0.70444 0.887 ## 694 58 2 0.7634 0.0488 0.67351 0.865 ## 730 56 4 0.7089 0.0524 0.61330 0.819 ## 767 51 2 0.6811 0.0539 0.58324 0.795 ## 803 49 3 0.6394 0.0557 0.53901 0.758 ## 876 44 3 0.5958 0.0573 0.49341 0.719 ## 949 40 3 0.5511 0.0585 0.44753 0.679 ## 1022 34 5 0.4701 0.0601 0.36585 0.604 ## 1059 25 2 0.4325 0.0609 0.32815 0.570 ## 1095 23 3 0.3761 0.0610 0.27357 0.517 ## 1132 17 1 0.3539 0.0613 0.25201 0.497 ## 1168 16 3 0.2876 0.0606 0.19023 0.435 ## 1205 13 4 0.1991 0.0558 0.11491 0.345 ## 1241 8 1 0.1742 0.0541 0.09476 0.320 ## 1314 5 1 0.1394 0.0533 0.06582 0.295 ## 1351 4 1 0.1045 0.0501 0.04084 0.267 ## 1387 3 2 0.0348 0.0330 0.00545 0.223 ## 1533 1 1 0.0000 NaN NA NA ## ## Race=White ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 73 339 4 0.9882 0.00586 0.9768 1.000 ## 110 333 1 0.9852 0.00656 0.9725 0.998 ## 183 332 1 0.9823 0.00718 0.9683 0.996 ## 219 330 3 0.9733 0.00877 0.9563 0.991 ## 256 326 2 0.9674 0.00968 0.9486 0.987 ## 365 317 3 0.9582 0.01094 0.9370 0.980 ## 438 311 4 0.9459 0.01241 0.9219 0.971 ## 511 300 3 0.9364 0.01343 0.9105 0.963 ## 548 297 5 0.9207 0.01494 0.8918 0.950 ## 584 290 5 0.9048 0.01629 0.8734 0.937 ## 621 281 5 0.8887 0.01751 0.8550 0.924 ## 657 275 6 0.8693 0.01884 0.8332 0.907 ## 694 269 10 0.8370 0.02072 0.7973 0.879 ## 730 255 6 0.8173 0.02174 0.7758 0.861 ## 767 245 15 0.7673 0.02394 0.7217 0.816 ## 803 225 12 0.7263 0.02541 0.6782 0.778 ## 840 210 7 0.7021 0.02616 0.6527 0.755 ## 876 197 12 0.6594 0.02733 0.6079 0.715 ## 913 174 8 0.6290 0.02809 0.5763 0.687 ## 949 163 8 0.5982 0.02876 0.5444 0.657 ## 986 152 10 0.5588 0.02943 0.5040 0.620 ## 1022 137 10 0.5180 0.02998 0.4625 0.580 ## 1059 124 8 0.4846 0.03028 0.4287 0.548 ## 1095 112 13 0.4284 0.03052 0.3725 0.493 ## 1132 98 7 0.3978 0.03046 0.3423 0.462 ## 1168 84 6 0.3693 0.03041 0.3143 0.434 ## 1205 72 8 0.3283 0.03029 0.2740 0.393 ## 1241 59 7 0.2894 0.03007 0.2360 0.355 ## 1278 50 4 0.2662 0.02980 0.2138 0.332 ## 1314 46 4 0.2431 0.02937 0.1918 0.308 ## 1351 39 2 0.2306 0.02916 0.1800 0.295 ## 1387 36 1 0.2242 0.02905 0.1739 0.289 ## 1424 32 6 0.1822 0.02822 0.1345 0.247 ## 1460 20 4 0.1457 0.02784 0.1002 0.212 ## 1497 15 3 0.1166 0.02688 0.0742 0.183 ## 1570 10 1 0.1049 0.02660 0.0638 0.172 ## 1643 6 1 0.0874 0.02732 0.0474 0.161 ## 1679 5 1 0.0699 0.02687 0.0329 0.149 Just as we generated before when applying the summary function to a Kaplan-Meier analysis object, we find life table information. By default, the summary function begins each new time interval at the time in which a new event (i.e., voluntary turnover) occurs; however, now there are separate life tables for each level of the categorical covariate (i.e., Race). Note that the time intervals displayed in the time column of each table are specific to the cases found within each level/category of the categorical covariate. Just as we did before, we can also set our time intervals. Lets choose the same time intervals as we did in our null model (without the covariate). # Summarize results of Kaplan-Meier analysis using pre-specified time intervals # and create a life table summary(km_fit2, times=c(30, 60, 90*(1:30))) ## Call: survfit(formula = Surv(LOS, censored) ~ Race, data = survdat, ## type = &quot;kaplan-meier&quot;) ## ## Race=Black ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 283 0 1.0000 0.00000 1.0000 1.0000 ## 60 283 0 1.0000 0.00000 1.0000 1.0000 ## 90 272 6 0.9788 0.00856 0.9622 0.9957 ## 180 268 3 0.9680 0.01049 0.9477 0.9888 ## 270 263 4 0.9535 0.01258 0.9292 0.9785 ## 360 260 2 0.9463 0.01350 0.9202 0.9731 ## 450 244 11 0.9060 0.01755 0.8723 0.9411 ## 540 236 7 0.8800 0.01961 0.8424 0.9193 ## 630 212 15 0.8237 0.02313 0.7796 0.8703 ## 720 195 7 0.7962 0.02458 0.7494 0.8458 ## 810 173 14 0.7378 0.02728 0.6862 0.7933 ## 900 152 14 0.6774 0.02944 0.6221 0.7377 ## 990 110 32 0.5313 0.03253 0.4712 0.5990 ## 1080 89 16 0.4525 0.03315 0.3919 0.5223 ## 1170 53 23 0.3282 0.03272 0.2699 0.3990 ## 1260 41 7 0.2822 0.03244 0.2253 0.3535 ## 1350 26 9 0.2190 0.03130 0.1655 0.2898 ## 1440 11 10 0.1292 0.02866 0.0836 0.1995 ## 1530 4 7 0.0470 0.02144 0.0192 0.1149 ## 1620 1 2 0.0157 0.01464 0.0025 0.0979 ## 1710 1 0 0.0157 0.01464 0.0025 0.0979 ## 1800 1 0 0.0157 0.01464 0.0025 0.0979 ## 1890 1 0 0.0157 0.01464 0.0025 0.0979 ## ## Race=HispanicLatino ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 79 0 1.0000 0.0000 1.00000 1.000 ## 60 79 0 1.0000 0.0000 1.00000 1.000 ## 90 77 2 0.9747 0.0177 0.94065 1.000 ## 180 77 0 0.9747 0.0177 0.94065 1.000 ## 270 76 1 0.9620 0.0215 0.92079 1.000 ## 360 75 0 0.9620 0.0215 0.92079 1.000 ## 450 71 3 0.9230 0.0302 0.86569 0.984 ## 540 68 2 0.8968 0.0346 0.83159 0.967 ## 630 62 5 0.8309 0.0428 0.75112 0.919 ## 720 56 5 0.7634 0.0488 0.67351 0.865 ## 810 45 9 0.6394 0.0557 0.53901 0.758 ## 900 40 3 0.5958 0.0573 0.49341 0.719 ## 990 34 3 0.5511 0.0585 0.44753 0.679 ## 1080 23 7 0.4325 0.0609 0.32815 0.570 ## 1170 13 7 0.2876 0.0606 0.19023 0.435 ## 1260 5 5 0.1742 0.0541 0.09476 0.320 ## 1350 4 1 0.1394 0.0533 0.06582 0.295 ## 1440 1 3 0.0348 0.0330 0.00545 0.223 ## 1530 1 0 0.0348 0.0330 0.00545 0.223 ## ## Race=White ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 30 339 0 1.0000 0.00000 1.0000 1.000 ## 60 339 0 1.0000 0.00000 1.0000 1.000 ## 90 333 4 0.9882 0.00586 0.9768 1.000 ## 180 332 1 0.9852 0.00656 0.9725 0.998 ## 270 320 6 0.9674 0.00968 0.9486 0.987 ## 360 317 0 0.9674 0.00968 0.9486 0.987 ## 450 301 7 0.9459 0.01241 0.9219 0.971 ## 540 297 3 0.9364 0.01343 0.9105 0.963 ## 630 275 15 0.8887 0.01751 0.8550 0.924 ## 720 255 16 0.8370 0.02072 0.7973 0.879 ## 810 210 33 0.7263 0.02541 0.6782 0.778 ## 900 174 19 0.6594 0.02733 0.6079 0.715 ## 990 137 26 0.5588 0.02943 0.5040 0.620 ## 1080 112 18 0.4846 0.03028 0.4287 0.548 ## 1170 72 26 0.3693 0.03041 0.3143 0.434 ## 1260 50 15 0.2894 0.03007 0.2360 0.355 ## 1350 39 8 0.2431 0.02937 0.1918 0.308 ## 1440 20 9 0.1822 0.02822 0.1345 0.247 ## 1530 10 7 0.1166 0.02688 0.0742 0.183 ## 1620 6 1 0.1049 0.02660 0.0638 0.172 ## 1710 3 2 0.0699 0.02687 0.0329 0.149 ## 1800 2 0 0.0699 0.02687 0.0329 0.149 As you can see in the output, each life table now has the same pre-specified time intervals, which can make descriptive comparison of the table values to be a bit easier. Now lets use the same arguments for the ggsurvplot function above, but this time, lets add the arguments pval=TRUE and pval.method= to see, respectively, whether the survival rates differ to a statistically significant extent across levels of the categorical covariate and what statistical test is used to estimate whether such differences exist in the population. # Plot the cumulative survival rates (probabilities) ggsurvplot(km_fit2, data=survdat, risk.table=TRUE, pval=TRUE, pval.method=TRUE, conf.int=TRUE, ggtheme=theme_minimal()) In the plot, three survival rates are presented - one for each race. Note that the p-value is greater than or equal to .05, so we would not conclude that any differences we observed descriptively/visually between the three races are statistically significant. We can see that the log-rank test was used to estimate whether such differences existed across levels of the categorical covariate; well revisit the log-rank test in the context of a Cox proportional hazards model below. In the table, we also find that the risk table is now stratified by levels (i.e., strata) of the categorical covariate. 49.2.7 Estimate Cox Proportional Hazards Model The Cox proportional hazards (Cox PH) model is a semiparametric method and is sometimes referred to as Cox regression. It estimates the log-linear association between a covariate (or multiple covariates) in relation to instantaneous rate of experiencing the focal event (i.e., hazard). This method allows us to introduce continuous covariates (as well as categorical covariates) in a regression-like model. To estimate a Cox PH model, we will use the coxph function from the survival package. Lets name our model cox_reg1 using the &lt;- operator. Next, type the name of the coxph function. As the first argument, specify the survival model by specifying the outcome to the left of the ~ (tilde) symbol and the predictor(s) to the right. As the outcome, we type the Surv function with the survival length variable (LOS) as the first argument, and as the second argument, the censored variable (which indicates which cases should be censored) should be entered. Lets keep this first model simple by simply applying Race as a categorical covariate, as we did above with the Kaplan-Meier analysis. As the second argument in the coxph function, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. Use the summary function from base R to summarize the results of the cox_reg1 object we specified. # Estimate Cox proportional hazards model with categorical covariate cox_reg1 &lt;- coxph(Surv(LOS, censored) ~ Race, data=survdat) # Print summary of results summary(cox_reg1) ## Call: ## coxph(formula = Surv(LOS, censored) ~ Race, data = survdat) ## ## n= 701, number of events= 463 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## RaceHispanicLatino 0.17936 1.19645 0.15147 1.184 0.236 ## RaceWhite -0.14365 0.86619 0.09998 -1.437 0.151 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## RaceHispanicLatino 1.1964 0.8358 0.8891 1.610 ## RaceWhite 0.8662 1.1545 0.7120 1.054 ## ## Concordance= 0.522 (se = 0.014 ) ## Likelihood ratio test= 5.13 on 2 df, p=0.08 ## Wald test = 5.28 on 2 df, p=0.07 ## Score (logrank) test = 5.3 on 2 df, p=0.07 In the output, we receive information about our sample size (n = 701) and the number of individuals in our sample who experienced the event of voluntary turnover (number of events = 463). Next, you will see a table with coefficients, along with their exponentiated values, their standard errors, z-test value (Wald statistic value), and p-value (Pr(&gt;|z|)). Because Race is a categorical variable with three levels/categories (i.e., Black, HispanicLatino, White), we see two lines in the table; these two lines reflect the dummy variables, which were created behind the scenes as part of the function. The Race category of Black comes first alphabetically (before HispanicLatino and White), and thus it serves as the default reference group. When comparing HispanicLatino individuals to Black individuals, we can see that there is not a difference in survival, as evidenced by the nonsignificant p-value (.236). Similarly, when comparing White individuals to Black individuals, there is not a significant difference in terms of survival (p = .151). Because these coefficients are nonsignificant, we will not proceed forward with interpretation of their respective hazard ratios, which appear in the subsequent table. We will, however, skip down to the last set of tests/values that appear in the output. The value labeled as Concordance in the output is an aggregate estimate of how well the model predicted individuals experience of the event in time (i.e., model fit). Specifically, pairs of individuals are compared to determine whether the individual with the temporally earlier experience of the event is correctly predicted by the model. If model is accurate, the pair is considered concordant. The overall concordance score represents the proportion of pairs of individuals whose experience of the event (e.g., voluntary turnover) was accurately predicted. Accordingly, a concordance value of .50 indicates that the model does no better than chance (i.e., 50-50 chance of correctly predicting which individual incurs the event before another). In our data, the concordance value is .522 (SE = .014), which indicates that including the Race covariate in the model slightly improves the accuracy of our predictions but not by much more than a model with no covariates (i.e., null model). The tests labeled as Likelihood ratio test (likelihood-ratio test), Wald test (Wald test), and Score (logrank) test (log-rank test) in the output all focus on whether the model with the covariates fits the data better than a model without the covariates (i.e., null model). Lets focus first on the likelihood-ratio test (LRT), which is chi-square goodness-of-fit test that compares nested models. The LRT is not statistically significant (\\(\\chi^2\\) = 5.13, df = 2, p = .08), which indicates that the model with the Race covariate does not result in significantly better model fit than a null model without the covariate. Thus, we fail to reject the null hypothesis that our model with the covariates fits the same as the null model without the covariates. The Wald test and log-rank test can be interpreted similarly to the LRT and similarly have to do with the overall fit of the model. These three tests are referred to as asymptotically equivalent because their values should converge when the sample size is sufficiently high. If youre working with a smaller sample size, the LRT tends to perform better. Given that, in general I recommend reporting and interpreting the log-rank test and, for space considerations, forgoing the reporting and interpretation of the other two tests. For example, in this example, the log-rank value is 5.3 (df = 2) with a p-value of .07, which indicates that the model with the Race covariate does not perform significantly better than a model with not covariates. Lets extend our Cox PH model by addition two additional continuous covariates: Pay_hourly and Pay_sat. To add these covariates to our model, use the + symbol. Thus, as our first argument in the coxph function, enter the model specifications. As the second argument, type data= followed by the name of the data frame object (survdat) to which the aforementioned variables belong. Using the &lt;- operator, lets name this new model cox_reg2. As before, use the summary function from base R to summarize the results of the cox_reg2 object. # Estimate Cox proportional hazards model with categorical &amp; continuous covariates cox_reg2 &lt;- coxph(Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, data=survdat) # Print summary of results summary(cox_reg2) ## Call: ## coxph(formula = Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, ## data = survdat) ## ## n= 663, number of events= 445 ## (38 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## RaceHispanicLatino 0.12087 1.12848 0.15796 0.765 0.444154 ## RaceWhite -0.04446 0.95651 0.10367 -0.429 0.668008 ## Pay_hourly -0.10971 0.89609 0.03276 -3.349 0.000812 *** ## Pay_sat 0.14135 1.15183 0.08135 1.738 0.082289 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## RaceHispanicLatino 1.1285 0.8862 0.8280 1.5380 ## RaceWhite 0.9565 1.0455 0.7806 1.1720 ## Pay_hourly 0.8961 1.1160 0.8404 0.9555 ## Pay_sat 1.1518 0.8682 0.9821 1.3509 ## ## Concordance= 0.55 (se = 0.016 ) ## Likelihood ratio test= 15.97 on 4 df, p=0.003 ## Wald test = 16 on 4 df, p=0.003 ## Score (logrank) test = 16.02 on 4 df, p=0.003 In the output, effective total sample size is 663, which is due to 38 cases/observations being listwise deleted from the sample size due to missing values on the Pay_sat covariate; you can look at the survdat data frame to verify this, if you would like. Of those 663 individuals included in the analysis, 445 experienced the event. Only the coefficient associated with the covariate Pay_hourly is statistically significant (b = -.110, p &lt; .001) when accounting for the other covariates in the model. Given that, lets proceed forward to interpret its hazard ratio. The hazard ratio and its confidence interval can be found in the second table under the column exp(coef). (Note that exp stands for exponentiation, and for more information on exponentiation, check out the chapter covering logistic regression.) A hazard ratio is the ratio between the hazards (i.e., hazard rates) associated with two (adjacent) levels of a covariate. The hazard ratio associated with Pay_hourly is .896, which can be interpreted similar to an odds ratio from logistic regression. Because the hazard ratio is less than 1, it signifies a negative association, which we already knew from the regression coefficient. If we subtract .896 from 1, we get .104, which means we can interpret the association as follows: For every unit increase in the predictor variable, we can expect a 10.4% decrease in the expected hazard; or in other words, for every additional dollar in hourly pay earned, we can expect a 10.4% decrease in the likelihood of voluntary turnover. The column labeled exp(-coef) is just the reciprocal (i.e., 1 divided by hazard ratio) and thus allows us to interpret the association from a different perspective. Specifically, the reciprocal is 1.116, which can be interpreted as: For every dollar decrease in pay, the expected hazard will be 1.116 greater  or in other words, we expected to see a 11.6% increase in the likelihood of voluntary turnover for every additional dollar earned. Its up to you to decide whether you want to report the hazard ratio, its reciprocal, or both. The columns labeled lower .95 and upper .95 contain the 95% confidence interval lower and upper limits for the hazard ratio, which for Pay_hourly are 95% CI[.840, .956]. Just for fun, lets specify our Cox PH model in equation form to show how we can estimate the log of the overall risk score for survival time prior to experiencing the event (i.e., overall predicted hazard ratio for an individual). Given the nonsignificant of the other covariates in the model, in real life, we might choose to run the model once more with the nonsignificant covariates excluded. For this example, however, Ill include nonsignificant covariates in the equation as well. \\(Log(Overall Risk) = .121*(Race[HL]) - .044*(Race[W]) - .110*(Pay_{hourly}) + .141*(Pay_{sat})\\) For example, if an individual is HispanicLatino, has an hourly pay rate of $16.00, and pay satisfaction equal to 4.00, then we would predict the log of their overall risk score for survival time prior to experiencing the event to be -1.075. \\(Log(Overall Risk) = .121*(1) - .044*(0) - .110*(16.00) + .141*(4.00)\\) \\(Log(Overall Risk) = .121 - 0 - 1.760 + .564\\) \\(Log(Overall Risk) = -1.075\\) If we exponentiate the log of the overall risk score, we arrive at .341. \\(e^{-1.075} = .341\\) Thus, the overall risk of voluntarily turning over for this individual is .341. This means that the expected hazard is 65.9% less when compared to an individual with scores of zero on each of the covariates in the model. This might sound great, but think about what this actually means. For our equation, an individual who earns zero on the covariates is Black, earns .00 dollars/hour, and has a score of .00 on the pay satisfaction scale. For this survival study, the minimum pay earned by an individual is 10.00 dollars/hour (mean = 14.11), the minimum score on the pay satisfaction scale is 2.00, and the sample is 40.4% Black, 11.3% HispanicLatino, and 48.4% White. This is why we might decide to grand-mean center our continuous covariates and specify a different reference group for our categorical covariate - for interpretation purposes. Lets go ahead and grand-mean center the continuous covariates and set the HispanicLatino individuals as the reference group. If you would like a deeper dive into how to grand-mean center variables, check out the chapter on centering and standardizing variables. # Grand-mean center continuous covariates survdat$c_Pay_hourly &lt;- scale(survdat$Pay_hourly, center=TRUE, scale=FALSE) survdat$c_Pay_sat &lt;- scale(survdat$Pay_sat, center=TRUE, scale=FALSE) # Change reference group to HispanicLatino for categorical covariate by re-ordering levels survdat$HL_Race &lt;- factor(survdat$Race, levels=c(&quot;HispanicLatino&quot;, &quot;Black&quot;, &quot;White&quot;)) # Estimate Cox proportional hazards model with categorical &amp; continuous covariates cox_reg3 &lt;- coxph(Surv(LOS, censored) ~ HL_Race + c_Pay_hourly + c_Pay_sat, data=survdat) # Print summary of results summary(cox_reg3) ## Call: ## coxph(formula = Surv(LOS, censored) ~ HL_Race + c_Pay_hourly + ## c_Pay_sat, data = survdat) ## ## n= 663, number of events= 445 ## (38 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## HL_RaceBlack -0.12087 0.88615 0.15796 -0.765 0.444154 ## HL_RaceWhite -0.16533 0.84761 0.15773 -1.048 0.294567 ## c_Pay_hourly -0.10971 0.89609 0.03276 -3.349 0.000812 *** ## c_Pay_sat 0.14135 1.15183 0.08135 1.738 0.082289 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## HL_RaceBlack 0.8862 1.1285 0.6502 1.2077 ## HL_RaceWhite 0.8476 1.1798 0.6222 1.1547 ## c_Pay_hourly 0.8961 1.1160 0.8404 0.9555 ## c_Pay_sat 1.1518 0.8682 0.9821 1.3509 ## ## Concordance= 0.55 (se = 0.016 ) ## Likelihood ratio test= 15.97 on 4 df, p=0.003 ## Wald test = 16 on 4 df, p=0.003 ## Score (logrank) test = 16.02 on 4 df, p=0.003 Using our new output, lets construct the latest incarnation of the equation. \\(Log(Overall Risk) = -.121*(Race[B]) - .165*(Race[W]) - .110*(Pay_{hourly}) + .141*(Pay_{sat})\\) As you can see, the coefficients for Pay_hourly and Pay_sat remain the same, but now we have the HispanicLatino group set as the reference group and the coefficients of the dummy coded variables changed. Lets imagine the same individual as before for whom we would like to estimate the log of overall risk. For the two continuous covariates, we will subtract the grand mean of each of those aforementioned values (i.e., 16.00, 4.00) to arrive at their grand-mean centered values (i.e., 1.89, .32). \\(Log(Overall Risk) = -.121*(0) - .165*(0) - .110*(1.89) + .141*(.32)\\) \\(Log(Overall Risk) = -.162\\) Now lets exponentiate the log of the overall risk. \\(e^{-.284} = .850\\) Keeping in mind that we have changed the reference group for the Race covariate to HispanicLatino and have grand-mean centered the Pay_hourly and Pay_sat covariates, the individual in questions overall risk score is .850. Thus, the expected hazard is 15.0% less for this individual when compared to individuals with scores of zero on each of the covariates in the model  or in other words, when compared to individuals who are HispanicLatino, earn average hourly pay, and have average levels of pay satisfaction. Lets now focus on the model-level performance and fit information in the output. Note that the concordance is .55 (SE = .016), which is an improvement from the concordance for the Cox PH model with just Race as a covariate. This indicates that including the two additional continuous covariates in the model improves the accuracy of our predictions. The significant log-rank test (16.02, df = 4, p = .003) indicates that the this model fits significant better than a null model. We can also perform nested model comparisons between two non-null models. Before doing so, we need to make sure both models are estimated using the same data, which is required for the following comparison when there is missing data on one or more of the variables in the models. (Note: This can change the original model results for a model that was originally estimated with a larger sample because it had less missing data than the the model to which we wish to compare it.) First, we will pull in the drop_na function from the tidyr package to do the heavy lifting for us. Specifically, within the function, we note the name of the data frame followed by the names of all of the variables from the full model. Second, we will use the anova function from base R to perform the nested model comparison between cox_reg1 and cox_reg2. # Access tidyr package # install.packages(&quot;tidyr&quot;) # install if necessary library(tidyr) # Re-estimate Cox PH nested models by dropping all cases with missing data on focal variables cox_reg1 &lt;- coxph(Surv(LOS, censored) ~ Race, data=drop_na(survdat, LOS, censored, Race, Pay_hourly, Pay_sat)) cox_reg2 &lt;- coxph(Surv(LOS, censored) ~ Race + Pay_hourly + Pay_sat, data=drop_na(survdat, LOS, censored, Race, Pay_hourly, Pay_sat)) # Nested model comparison anova(cox_reg1, cox_reg2) ## Analysis of Deviance Table ## Cox model: response is Surv(LOS, censored) ## Model 1: ~ Race ## Model 2: ~ Race + Pay_hourly + Pay_sat ## loglik Chisq Df P(&gt;|Chi|) ## 1 -2459.6 ## 2 -2453.1 13.141 2 0.001401 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the chi-square test (\\(\\chi^2\\) = 13.141, df = 2, p = .001) indicates that the full model with all covariates included fits the data significantly better than the smaller nested model with only the Race covariate included. 49.2.8 Summary In this chapter, we learned the basics of how to conduct a Kaplan-Meier analysis, create a life table, and estimate a Cox proportional hazards (i.e., Cox regression) model. "],["performancemanagement.html", "Chapter 50 Introduction to Employee Performance Management", " Chapter 50 Introduction to Employee Performance Management XXXXX "],["convergentdiscriminantvalidity.html", "Chapter 51 Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations 51.1 Conceptual Overview 51.2 Tutorial 51.3 Chapter Supplement", " Chapter 51 Evaluating Convergent &amp; Discriminant Validity Using Scatter Plots &amp; Correlations In this chapter, we will learn how to generate scatter plots, estimate Pearson product-moment and point-biserial correlations, and create a correlation matrix to evaluate the convergent validity and discriminant validity of a performance measure. 51.1 Conceptual Overview In this section, first, well begin by reviewing concurrent and discriminant validity. Second, we review the Pearson product-moment correlation and point-biserial correlation, and well discuss the statistical assumptions that should be satisfied prior to estimating and interpreting both types of correlations as well as statistical significance and and practical significance in the context of these two correlations. The section will wrap up with a sample-write up of a correlation when used to estimate the criterion-related validity of a selection tool. Finally, we will describe the data visualization called a bivariate scatter plot and how it can be used to understand the association between two continuous (interval, ratio) variables. 51.1.1 Review of Concurrent &amp; Discriminant Validity To understand concurrent and discriminant validity, we first need to define criterion-related validity, where criterion-related validity (criterion validity) refers to the association between a variable and some outcome variable or correlate. The term criterion can be thought of as some outcome or correlate of practical or theoretical interest. Convergent validity is a specific type of criterion-related validity in which the criterion of interest is conceptually similar to the focal variable, such that we would expect for scores on two conceptually similar variables to be associated with one another. For example, imagine that supervisors customer service employees job performance using a behavioral performance evaluation tool. Given that these employees work in customer service, we would expect scores on the behavioral performance evaluation tool to correlate with a measure that assesses a conceptually similar construct like, say, customer satisfaction ratings. In other words, we would expect for the performance evaluation tool and customer satisfaction ratings to show evidence of convergent validity. Unlike convergent validity, we would find evidence of discriminant validity when scores on a conceptually dissimilar criterion are not associated with the focal variable. For example, we would not expect scores on a tool designed to measure job performance to correlate strongly with employee sex  assuming that the tool was intended to be bias free. 51.1.2 Review of Pearson Product-Moment &amp; Point-Biserial Correlation A correlation represents the sign (i.e., direction) and magnitude (i.e., strength) of an association between two variables. Correlation coefficients can range from -1.00 to +1.00, where zero (.00) represents no association, -1.00 represents a perfect negative (inverse) association, and +1.00 represents a perfect positive association. When estimated using data acquired from a criterion-related validation study, a correlation coefficient can be referred to as a validity coefficient. There are different types of correlations we can estimate, and their appropriateness will depend on the measurement scales of the two variables. For instance, the Pearson product-moment correlation is used when both variables are continuous (i.e., have interval or ratio measurement scales), whereas the point-biserial correlation is used when one variable is continuous and the other is truly dichotomous (e.g., has a nominal measurement scale with just two levels or categories)  and by truly dichotomous, I mean that there is no underlying continuum between the two components of the binary. If we assign numeric values to the two levels of the dichotomous variable (e.g., 0 and 1), then the point-biserial correlation will be mathematically equivalent to a Pearson product moment correlation. A Pearson product-moment correlation coefficient (\\(r_{pm}\\)) for a sample can be computed using the following formula: \\(r_{pm} = \\frac{\\sum XY - \\frac{(\\sum X)(\\sum Y)}{n}} {\\sqrt{(\\sum X^{2} - \\frac{\\sum X^{2}}{n}) (\\sum Y^{2} - \\frac{\\sum Y^{2}}{n})}}\\) where \\(X\\) refers to scores from one variable and \\(Y\\) refers to scores from the other variable, \\(n\\) refers to the sample size (i.e., the number of pairs of data corresponding to the number of cases  with complete data). If we assign numeric values to both levels of the dichotomous variable (e.g., 0 and 1), then we could use the formula for a Pearson product-moment correlation to calculate a point-biserial correlation. If, however, the two levels of the dichotomous variable are still non-numeric, then we can calculate a point-biserial correlation coefficient (\\(r_{pb}\\)) for a sample using the following formula: \\(r_{pb} = \\frac{M_1 - M_0}{s_N} \\sqrt{pq}\\) where \\(M_1\\) refers to mean score on the continuous (interval, ratio) variable for just the subset of cases with a score of 1 on the dichotomous variable; \\(M_0\\) refers to mean score on the continuous (interval, ratio) variable for just the subset of cases with a score of 0 on the dichotomous variable; \\(s_N\\) refers to the standard deviation for the continuous (interval, ratio) variable for the entire sample; \\(p\\) refers to the proportion of cases with a score of 0 on the dichotomous variable; and \\(q\\) refers to the proportion of cases with a score of 1 on the dichotomous variable. 51.1.2.1 Statistical Assumptions The statistical assumptions that should be met prior to estimating and/or interpreting a Pearson product-moment correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; Each variable shows a univariate normal distribution; Each variable is free of univariate outliers, and together the variables are free of bivariate outliers; Variables demonstrate a bivariate normal distribution - meaning, each variable is normally distributed at each level/value of the other variable. Often, this roughly takes the form of an ellipse shape, if you were to superimpose an oval that would fit around most cases in a bivariate scatter plot; The association between the two variables is linear. The statistical assumptions that should be met prior to estimating and/or interpreting a point-biserial correlation include: Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual; One of the variables is continuous (i.e., has an interval or ratio measurement scale); One of the variables is dichotomous (i.e., binary); The continuous variable shows a univariate normal distribution at each level of the dichotomous variable; The variance of the continuous variable is approximately equal for each level of the dichotomous variable; The continuous variable is free of univariate outliers at each level of the dichotomous variable. Note: Regarding the first statistical assumption (i.e., cases randomly sampled from population), we will assume in this chapters data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our variables, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An intraclass correlation (ICC) can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the p-values and inferences of statistical significance. 51.1.2.2 Statistical Significance The significance level of a correlation coefficient is determined by the sample size and the magnitude of the correlation coefficient. Specifically, a t-statistic with N-2 degrees of freedom (df) is calculated and compared to a Students t-distribution with N-2 df and a given alpha level (usually two-tailed, alpha = .05). If the calculated t-statistic is greater in magnitude than the chosen t-distribution, we conclude that the population correlation coefficient is significantly greater than zero. We use the following formula to calculate the t-statistic: \\(t = \\frac{r \\sqrt{N-2}}{1-r^{2}}\\) where \\(r\\) refers to the estimated correlation coefficient and \\(N\\) refers to the sample size. Alternatively, the exact p-value can be computed using a statistical software program like R if we know the df and t-value. In practice, however, we dont always report the associated t-value; instead, we almost always report the exact p-value associated with the t-value when reporting information about statistical significance. When using null hypothesis significance testing, we interpret a p-value that is less than our chosen alpha level (which is conventionally .05, two-tailed) to meet the standard for statistical significance. This means that we reject the null hypothesis that the correlation is equal to zero. By rejecting this null hypothesis, we conclude that the correlation is significantly different from zero. If the p-value is equal to or greater than our chosen alpha level (e.g., .05, two-tailed), then we fail to reject the null hypothesis that the correlation is equal to zero; meaning, we conclude that there is no evidence of linear association between the two variables. 51.1.2.3 Practical Significance The size of a correlation coefficient can be described using qualitative labels, such as small, medium, and large. The quantitative values tied to such qualitative labels of magnitude should really be treated as context specific; with that said, there are some very general rules we can apply when interpreting the magnitude of correlation coefficients, which are presented in the table below (Cohen 1992). Please note that the r values in the table are absolute values, which means, for example, that correlation coefficients of .50 and -.50 would both have the same absolute value and thus would both be considered large. r Description .10 Small .30 Medium .50 Large Some people like to also report the coefficient of determination as an indicator of effect size. The coefficient of determination is calculated by squaring the correlation coefficient to create r2. When multiplied by 100, the coefficient of determination (r2) can be interpreted as the percentage of variance/variability shared between the two variables, which is sometimes stated as follows: Variable \\(X\\) explains \\(X\\)% of the variance in Variable \\(Y\\) (or vice versa). Please note that we use the lower-case r in r2 to indicate that we are reporting the variance overlap between only two variables. Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldnt make sense to the interpret the size of something that statistically has no effect. 51.1.2.4 Sample Write-Up Based on a sample of 95 employees (N = 95), we evaluated the convergent and discriminant validity of a performance evaluation tool designed to measure the construct of job performance for sales professionals. With respect to convergent validity, we found that the correlation between overall performance evaluation scores and sales revenue generated is statistically significant, as the p-value is less than the conventional two-tailed alpha level of .05 (r = 0.509, p &lt; .001, 95% CI[.343, .644]). Further, because the correlation coefficient is positive, it indicates that there is a positive linear association between overall performance evaluation ratings and sales revenue generated. Using the commonly used thresholds in the table below, we can conclude that  in terms of practical significance  the correlation coefficient is large in magnitude. With respect to the 95% confidence interval, it is likely that the range from .343 to .644 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. Overall, the statistically significant and large-in-magnitude correlation between overall scores on the performance evaluation and sales revenue generated provides evidence of criterion-related validity  and specifically evidence of convergent validity, as these two conceptually similar variables were indeed correlated to a statistically significant extent. With respect to discriminant validity, we found that the correlation between overall performance evaluation scores and employee sex was not statistically significant, as the p-value was greater than the conventional two-tailed alpha level of .05 (r = -.135, p = .193, 95% CI[-.327, .069]). This leads us to conclude that employees overall performance evaluation ratings are not associated with their sex to a statistical significant extent and, therefore, that there is evidence of discriminant validity. 51.1.3 Review of Bivariate Scatter Plot The bivariate scatter plot (or scatterplot) is a useful data visualization display type when our goal is to visualize the association between two continuous (interval, ratio) variables. Each dot in a scatter plot represents an individual case (i.e., observation) from the sample, and the dots position on the graph represents the cases scores on the each of the variables. A bivariate scatter plot can help us understand whether the statistical assumption of bivariate normality has been met (such as for a Pearson product-moment correlation), and it can help us determine whether the two variables have a linear or nonlinear association. The bivariate scatter plot is a type of data visualization that is indended to depict the nature of the association (or lack thereof) between two continuous (interval, ratio) variables. In this example, there appears to be a relatively strong, positive association between annual sales revenue (\\() generated and the amount of variable pay (\\)) earned. If a bivariate scatter plot is applied to a continuous (interval, ratio) and dichotomous variable, it will take on a qualitatively different appearance and conveys different information. Namely, as shown in the figure below, when a dichotomous variable is involved, the bivariate scatter plot will display two columns (or rows) of dots, each corresponding to a level of the dichotomous variable. In fact, we can use the plot to infer visually whether there might be differences in means for the continuous variable based on the two levels of the dichotomous variable. The bivariate scatter plot can also be used to depict the nature of the association (or lack thereof) between a continuous (interval, ratio) variable and a dichotomous variable. In this example, one can imagine that the mean variable pay ($) is higher for females compared to males. To play around with examples of bivariate scatter plots based on simulated data, check out this free tool. The tool also does a nice job of depicting the concept of shared variance in the context of correlation (i.e., coefficient of determination) using a Venn diagram. 51.2 Tutorial This chapters tutorial demonstrates how to evaluate evidence of concurrent and discriminant validity for a performance measure by using scatter plots, correlations, and a correlation matrix. 51.2.1 Video Tutorial As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorials below. Link to video tutorial: https://youtu.be/VQDZctJn43o Link to video tutorial: https://youtu.be/cEld0s-qpL4 Link to video tutorial: https://youtu.be/MfN8KGQU3MI 51.2.2 Functions &amp; Packages Introduced Function Package ScatterPlot lessR BoxPlot lessR drop_na tidyr nrow base R group_by dplyr summarize dplyr leveneTest car Correlation lessR as.numeric base R corr.test psych lowerCor psych write.csv base R 51.2.3 Initial Steps If you havent already, save the file called PerfMgmtRewardSystemsExample.csv into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., \"H:/RWorkshop\"). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: https://github.com/davidcaughlin/R-Tutorial-Data-Files; once youve followed the link to GitHub, just click Code (or Download) followed by Download ZIP, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book. Next, using the setwd function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to Session &gt; Set Working Directory &gt; Choose Directory. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to Setting a Working Directory and Creating &amp; Saving an R Script. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) Next, read in the .csv data file called PerfMgmtRewardSystemsExample.csv using your choice of read function. In this example, I use the read_csv function from the readr package (Wickham and Hester 2020). If you choose to use the read_csv function, be sure that you have installed and accessed the readr package using the install.packages and library functions. Note: You dont need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months. For refreshers on installing packages and reading data into R, please refer to Packages and Reading Data into R. # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object PerfRew &lt;- read_csv(&quot;PerfMgmtRewardSystemsExample.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Perf_Qual = col_double(), ## Perf_Prod = col_double(), ## Perf_Effort = col_double(), ## Perf_Admin = col_double(), ## SalesRevenue = col_double(), ## BasePay_2018 = col_double(), ## VariablePay_2018 = col_double(), ## Sex = col_character(), ## Age = col_double(), ## EducationLevel = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(PerfRew) ## [1] &quot;EmpID&quot; &quot;Perf_Qual&quot; &quot;Perf_Prod&quot; &quot;Perf_Effort&quot; &quot;Perf_Admin&quot; ## [6] &quot;SalesRevenue&quot; &quot;BasePay_2018&quot; &quot;VariablePay_2018&quot; &quot;Sex&quot; &quot;Age&quot; ## [11] &quot;EducationLevel&quot; # Print variable type for each variable in data frame (tibble) object str(PerfRew) ## spec_tbl_df[,11] [95 x 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:95] 1 2 3 4 5 6 7 8 9 10 ... ## $ Perf_Qual : num [1:95] 3 1 2 2 1 2 5 2 2 2 ... ## $ Perf_Prod : num [1:95] 3 1 1 3 1 2 5 1 3 2 ... ## $ Perf_Effort : num [1:95] 3 1 1 3 1 2 5 1 3 3 ... ## $ Perf_Admin : num [1:95] 4 1 1 1 1 3 5 1 2 2 ... ## $ SalesRevenue : num [1:95] 57563 54123 56245 58291 58354 ... ## $ BasePay_2018 : num [1:95] 53791 52342 50844 52051 48061 ... ## $ VariablePay_2018: num [1:95] 6199 1919 7507 6285 4855 ... ## $ Sex : chr [1:95] &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Age : num [1:95] 39 48 38 35 32 34 57 43 35 47 ... ## $ EducationLevel : num [1:95] 2 4 4 4 2 4 3 2 4 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Perf_Qual = col_double(), ## .. Perf_Prod = col_double(), ## .. Perf_Effort = col_double(), ## .. Perf_Admin = col_double(), ## .. SalesRevenue = col_double(), ## .. BasePay_2018 = col_double(), ## .. VariablePay_2018 = col_double(), ## .. Sex = col_character(), ## .. Age = col_double(), ## .. EducationLevel = col_double() ## .. ) # Print first 6 rows of data frame (tibble) object head(PerfRew) ## # A tibble: 6 x 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female ## 2 2 1 1 1 1 54123 52342 1919 Male ## 3 3 2 1 1 1 56245 50844 7507 Male ## 4 4 2 3 3 1 58291 52051 6285 Male ## 5 5 1 1 1 1 58354 48061 4855 Female ## 6 6 2 2 2 3 57618 53386 4056 Male ## # ... with 2 more variables: Age &lt;dbl&gt;, EducationLevel &lt;dbl&gt; # Print number of rows in data frame (tibble) object nrow(PerfRew) ## [1] 95 There are 11 variables and 95 cases (i.e., employees) in the PerfRew data frame: EmpID, Perf_Qual, Perf_Prod, Perf_Effort, Perf_Admin, SalesRevenue, BasePay_2018, VariablePay_2018, Sex, Age, and EducationLevel. Per the output of the str (structure) function above, all of the variables except for Sex are of type integer (continuous: interval, ratio), and Sex is of type character (nominal, dichotomous). The Perf_Qual, Perf_Prod, Perf_Effort, and Perf_Admin variables reflect four subjective performance-evaluation rating dimensions (as rated by direct supervisors), where Perf_Qual refers to perceived performance quality, Perf_Prod refers to perceived productivity, Perf_Effort refers to perceived effort, and Perf_Admin refers to perceived performance on administrative duties; each dimension was rated on a 1-5 scale in which higher values indicate higher performance (1 = poor performance, 5 = exceptional performance. The SalesRevenue variable is a measure of objective performance, as it reflects the 2018 sales-revenue contributions made by employees (in USD). The BasePay_2018 variable reflects the amount of base pay earned by employees during 2018 (in USD), and the VariablePay_2018 variable reflects the amount of variable pay (e.g., sales commission, bonuses) earned by employees during 2018 (in USD). The Sex variable has two levels: Male and Female, and the Age variable contains each employees age as of December 31, 2018. Finally, EducationLevel is an ordinal variable in which 1 = has high school degree or GED, 2 = completed some college courses, 3 = has Associates degree, 4 = has Bachelors degree, and 5 = complete some graduate courses or graduate degree. Even though the EducationLevel variable technically has an ordinal measurement scale. 51.2.3.1 Create Composite Variable Based on Performance Evaluation Dimensions Technically, each of these performance evaluation dimension variables has an ordinal measurement scale; however, because all four dimensions are intended to assess some aspect of the job performance domain, we will see if we might be justified in creating a composite variable that represents overall performance. In doing so, we will hopefully be able to make the argument that the overall performance composite variable has an interval measurement scale. To determine whether it is appropriate to create a composite variable from the four performance evaluation dimensions, we will follow the logic of the chapter on creating a composite variable based on a multi-item measure. Because that chapter explains the logic and process in detail, we will breeze through the steps in this tutorial. To get started, we will install and access the psych package using the install.packages and library functions, respectively (if you havent already done so). # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Now lets compute Cronbachs alpha for the four-item performance evaluation measure. # Estimate Cronbach&#39;s alpha for the four-item Engagement measure alpha(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)]) ## ## Reliability analysis ## Call: psych::alpha(x = PerfRew[, c(&quot;Perf_Qual&quot;, &quot;Perf_Prod&quot;, &quot;Perf_Effort&quot;, ## &quot;Perf_Admin&quot;)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.94 0.94 0.92 0.78 14 0.011 2.9 1.2 0.77 ## ## lower alpha upper 95% confidence boundaries ## 0.91 0.94 0.96 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Perf_Qual 0.90 0.90 0.86 0.76 9.3 0.017 0.00035 0.76 ## Perf_Prod 0.93 0.93 0.90 0.81 12.7 0.013 0.00130 0.81 ## Perf_Effort 0.92 0.92 0.89 0.78 10.9 0.015 0.00310 0.77 ## Perf_Admin 0.91 0.91 0.88 0.78 10.7 0.015 0.00067 0.77 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Perf_Qual 95 0.94 0.94 0.92 0.88 2.9 1.3 ## Perf_Prod 95 0.89 0.89 0.83 0.81 2.9 1.3 ## Perf_Effort 95 0.91 0.91 0.87 0.84 2.9 1.3 ## Perf_Admin 95 0.92 0.92 0.88 0.85 2.9 1.3 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Perf_Qual 0.17 0.25 0.22 0.20 0.16 0 ## Perf_Prod 0.17 0.27 0.24 0.17 0.15 0 ## Perf_Effort 0.19 0.18 0.27 0.23 0.13 0 ## Perf_Admin 0.16 0.26 0.22 0.20 0.16 0 For all four dimensions, Cronbachs alpha is .94, which is great and indicates an acceptable level of internal consistency reliability for this multi-dimensional measure. The Reliability if an item is dropped table indicates that dropping any of the dimensions/items would decrease Cronbachs alpha, which would be undesirable. Given this, we will feel justified in creating a composite variable based the mean score across these four dimensions for each case. Lets name the composite variable Perf_Overall. # Create composite (overall scale score) variable based on Engagement items PerfRew$Perf_Overall &lt;- rowMeans(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)], na.rm=TRUE) Now we have a variable called Perf_Overall that represents overall job performance, based on the supervisor ratings along the four dimensions. Further, we can argue that this new composite variable has an interval measurement scale. 51.2.3.2 Recode Employee Sex Variable The dichotomous employee sex variable (Sex) currently has the following levels: Female and Male. To make this variable amenable to a point-biserial correlation, we will recode the Female as 1 and Male as 0. To do so, we will use the recode function from the dplyr package. I should note that there are other ways in which we could recode these values, and some of those approaches are covered in the chapter on data cleaning. We will need to install and access the dplyr package using the install.packages and library functions, respectively (if you havent already done so). # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) To use the recode function we will do the following: Specify the name of the data frame object (PerfRew) followed by the $ operator and the name of variable we wish to recode (Sex). In doing so, we will overwrite the existing variable called Sex. Follow this with the &lt;- assignment operator. Specify the name of the recode function to the right of the &lt;- assignment operator. As the first argument in the recode function, specify the name of the the name of the data frame object (PerfRew) followed by the $ operator and the name of variable we wish to recode (Sex). As the second argument, type the name of one of the values we wish to change (Female), followed by the = operator and what wed like to change that value to (1). Note: If we wished to change the value to a non-numeric text (i.e., character) value, then we would need to surround the new value in quotation marks (\" \"). As the third argument, repeat the previous step for the second value we wish to change (Male = 0). # Recode Female as 1 and Male as 0 for Sex variable PerfRew$Sex &lt;- recode(PerfRew$Sex, Female = 1, Male = 0) Weve successfully recoded the Sex variable! Just remember that 1 indicates Female and 0 indicates Male, as this will be important for interpreting a point-biserial correlation, should we estimate one. 51.2.4 Visualize Association Using a Bivariate Scatter Plot A bivariate scatter plot can be used to visualize the association between two continuous (interval, ratio) variables. Lets create a bivariate scatter plot to visualize the association between the Perf_Prod (productivity) dimension of the performance evaluation tool and the SalesRevenue (sales revenue generated) variable, both of which are continuous. The ScatterPlot function from the lessR package does a nice job generating scatter plots  and it even provides an estimate of the correlation by default. If you havent already, install and access the lessR package. # Install package install.packages(&quot;lessR&quot;) # Access package library(lessR) Lets start by visualizing the association between the overall performance evaluation composite variable (Perf_Overall) we created above and the criterion variable of sales revenue generated during the year (SalesRevenue). And lets imagine data frame (PerfRew) contains cases who are all sales professionals, and thus the amount of sales revenue generated should be indicator of job performance. Thus, we can argue that both the Perf_Overall and SalesRevenue variables are conceptually similar as they tap into key aspects of job performance for these sales professionals. To begin, type the name of the ScatterPlot function. As the first two arguments of the function, well type the names of the two variables we wish to visualize: Perf_Overall and SalesRevenue. The variable name that we type after the x= argument will set the x-axis, and the variable name that we type after the y= argument will set the y-axis. Conventionally, we place the criterion variable on the y-axis, as it is the outcome. As the third argument, use the data= argument to provide the name of the data frame to which the two variables belong (PerfRew). # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew) ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, SalesRevenue, enhance=TRUE) # many options ## Plot(Perf_Overall, SalesRevenue, fill=&quot;skyblue&quot;) # interior fill color of points ## Plot(Perf_Overall, SalesRevenue, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Perf_Overall, SalesRevenue, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 In our plot window, we can see a fairly clear positive linear association between Perf_Overall and SalesRevenue. Further, the distribution is ellipse-shaped, which gives us some evidence that the underlying distribution between the two variables is likely bivariate normal  thereby satisfying a key statistical assumption for a Pearson product-moment correlation. Note that the ScatterPlot function automatically provides an estimate of the correlation coefficient in the output (r = .509), along with the associated p-value (p &lt; .001). Visually, this bivariate scatter plot provides initial evidence of convergent validity for these two variables. 51.2.4.1 Optional: Stylizing the ScatterPlot Function from lessR If you would like to optionally stylize your scatter plot, we can use the xlab= and ylab= arguments to change the default names of the x-axis and y-axis, respectively. # Optional: Styling the scatter plot ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, xlab=&quot;Overall Performance Evaluation Score&quot;, ylab=&quot;Annual Sales Revenue Generated ($)&quot;) ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, SalesRevenue, enhance=TRUE) # many options ## Plot(Perf_Overall, SalesRevenue, color=&quot;red&quot;) # exterior edge color of points ## Plot(Perf_Overall, SalesRevenue, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Perf_Overall, SalesRevenue, out_cut=.10) # label top 10% from center as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 We can also superimpose an ellipse by adding the argument ellipse=TRUE, which can visually aid our judgment on whether the distribution is bivariate normal. # Optional: Styling the scatter plot ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, xlab=&quot;Overall Performance Evaluation Score&quot;, ylab=&quot;Annual Sales Revenue Generated ($)&quot;, ellipse=TRUE) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, SalesRevenue, enhance=TRUE) # many options ## Plot(Perf_Overall, SalesRevenue, color=&quot;red&quot;) # exterior edge color of points ## Plot(Perf_Overall, SalesRevenue, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## Plot(Perf_Overall, SalesRevenue, MD_cut=6) # label Mahalanobis dist &gt; 6 as outliers ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 51.2.5 Estimate Correlations Now were ready to practice estimating Pearson product-moment and point-biserial correlations. For both, well work through the statistical assumptions along the way. 51.2.5.1 Estimate Pearson Product-Moment Correlation {estimate_pm_correlation_convergentdiscriminantvalidity} Because both Perf_Overall and SalesRevenue are measures of job performance, we will consider them to be conceptually similar  and even measures of the same overarching construct: job performance. Thus, we will investigate whether there is evidence of convergent validity between these two variables. Given that both Perf_Overall and SalesRevenue are continuous variables, lets see if these two variables satisfy the statistical assumptions for estimating and interpreting a Pearson product-moment correlation. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. Each Variable Shows a Univariate Normal Distribution: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Given that, well begin by checking to see how many cases have data for both continuous variables, and well use a combination of the drop_na function from the tidyr package and the nrow function from base R. First, if you havent already, install and access the tidyr package. # Install package install.packages(&quot;tidyr&quot;) # Access package library(tidyr) First, using the drop_na function, as the first argument, type the name of the data frame object (PerfRew). As the second and third arguments, type the names of the two variables (Perf_Overall, SalesRevenue). Second, following the drop_na function, insert the pipe (%&gt;%) operator. Finally, after the pipe (%&gt;%) operator, type the nrow function without any arguments specified. This code will first create a new data frame (tibble) in which rows with missing data on either variable are removed, and then the number of rows in the new data frame (tibble) will be evaluated. Effectively, this code performs listwise deletion and tells us how many rows were retained after the listwise deletion. Because we arent assigning this data frame to an object, this is only a temporary data frame  and not one that will become part of our Global Environment. # Drop cases (rows) where either Perf_Overall or SalesRevenue # have missing data (NAs) &amp; pipe (%&gt;%) the resulting data # frame to the nrow function drop_na(PerfRew, Perf_Overall, SalesRevenue) %&gt;% nrow() ## [1] 95 After attempting to drop any rows with missing data (NAs), we find that the sample size is 95, which is the same as our original sample size. This indicates that we did not have any missing values on either of these variables. Because 95 &gt; 30, we can assume that the assumption of univariate normality has been met for both variables. To learn how to test this assumption if there are fewer than 30 cases in a sample, please see the end-of-chapter supplement. Each Variable Is Free of Univariate Outliers: To visualize whether there are any univariate outliers for either variable, we will use the BoxPlot function from the lessR package to generate a box-and-whiskers plot (box plot). Lets start with the Perf_Overall variable, and enter it as the first argument in the BoxPlot function. As the second argument, type data= followed by the name of the data frame object (PerfRew). # Create box plot BoxPlot(Perf_Overall, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(Perf_Overall, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- Perf_Overall --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 2.911 ## Stnd Dev : 1.202 ## IQR : 1.875 ## Skew : 0.000 [medcouple, -1 to 1] ## ## Minimum : 1.000 ## Lower Whisker: 1.000 ## 1st Quartile : 2.000 ## Median : 3.000 ## 3rd Quartile : 3.875 ## Upper Whisker: 5.000 ## Maximum : 5.000 ## ## No (Box plot) outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## Perf_Overall 10 2.25 If outliers or other influential cases were present, they would appear beyond the whiskers (i.e., outer bars) in the plot. No potential outliers were detected for Perf_Overall. Now lets create a box plot for the SalesRevenue variable. # Create box plot BoxPlot(SalesRevenue, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## &gt;&gt;&gt; Suggestions ## Plot(SalesRevenue, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(SalesRevenue, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- SalesRevenue --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 60852.88 ## Stnd Dev : 6501.54 ## IQR : 7734.50 ## Skew : 0.04 [medcouple, -1 to 1] ## ## Minimum : 42512.00 ## Lower Whisker: 48157.00 ## 1st Quartile : 57027.00 ## Median : 60779.00 ## 3rd Quartile : 64761.50 ## Upper Whisker: 73329.00 ## Maximum : 76365.00 ## ## ## (Box plot) Outliers: 2 ## ## Small Large ## ----- ----- ## 73 42512.0 50 76365.0 ## ## Number of duplicated values: 0 The box plot indicates that there may be two potential outliers, which appear in red beyond the whiskers of the plot. Generally, I recommend using caution when deciding whether to remove outlier cases, and I tend to err on the side of not removing outliers  unless they are outlandishly separated from the other scores (e.g., +/- 3 standard deviations). If we did decide to remove these cases, then I would recommend running a sensitivity analysis, which means running the focal analysis with and without the cases included. I think we can reasonably conclude that we have satisfied the assumption that the variables are (mostly) free of univariate outliers. Association Between Variables Is Linear, Variables Demonstrate a Bivariate Normal Distribution, &amp; Variables Are Free of Bivariate Outliers: To test these statistical assumptions, we will estimate a bivariate scatter plot using the same function as the previous section. Well add two additional arguments, though. First, lets add the ellipse=TRUE argument, which will superimpose an ellipse; this will facilitate our interpretation of whether there is a bivariate normal distribution. Second, lets add the out_cut=.05 argument, which will helps us visualize the top 5% of cases from the center that might be bivariate outliers. # Create scatter plot using ScatterPlot function from lessR ScatterPlot(x=Perf_Overall, y=SalesRevenue, data=PerfRew, ellipse=TRUE, out_cut=.05) ## [Ellipse with Murdoch and Chow&#39;s function ellipse from their ellipse package] ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, SalesRevenue, enhance=TRUE) # many options ## Plot(Perf_Overall, SalesRevenue, fill=&quot;skyblue&quot;) # interior fill color of points ## Plot(Perf_Overall, SalesRevenue, fit=&quot;lm&quot;, fit_se=c(.90,.99)) # fit line, stnd errors ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Sample Correlation of Perf_Overall and SalesRevenue: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 ## &gt;&gt;&gt; Outlier analysis with Mahalanobis Distance ## ## MD ID ## ----- ----- ## 9.57 73 ## 7.45 50 ## 6.14 15 ## 5.18 13 ## 5.04 66 ## ## 4.85 54 ## 4.81 90 ## 4.43 48 ## ... ... Again, it appears as though the association between the two variables is linear (as opposed to nonlinear). Further, the bivariate distribution seems to be normal, as it takes on more-or-less an ellipse shape. Finally, there may be a few potential bivariate outliers, but none of them look too out-of-bounds or too extreme; thus, lets conclude that none of these more extreme cases are extreme enough to warrant removal. Overall, we can reasonably conclude that the statistical assumptions for a Pearson product-moment correlation have been satisfied. Estimate Pearson Product-Moment Correlation: To estimate a Pearson product-moment correlation, well use the Correlation function from the lessR package. Conveniently, this function will take the same first three arguments as the ScatterPlot function. # Estimate Pearson product-moment correlation using Correlation function from lessR Correlation(x=Perf_Overall, y=SalesRevenue, data=PerfRew) ## Correlation Analysis for Variables Perf_Overall and SalesRevenue ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = 3976.099 ## ## Sample Correlation: r = 0.509 ## ## Hypothesis Test of 0 Correlation: t = 5.701, df = 93, p-value = 0.000 ## 95% Confidence Interval for Correlation: 0.343 to 0.644 Sample Technical Write-Up: Based on a sample of 95 employees (N = 95), we can conclude that the correlation between overall performance evaluation scores and sales revenue generated is statistically significant, as the p-value is less than the conventional two-tailed alpha level of .05 (r = 0.509, p &lt; .001, 95% CI[.343, .644]). Further, because the correlation coefficient is positive, it indicates that there is a positive linear association between Perf_Overall and SalesRevenue. Using the commonly used thresholds in the table below, we can conclude that  in terms of practical significance  the correlation coefficient is large in magnitude, as its absolute value exceeds .50. With respect to the 95% confidence interval, it is likely that the range from .343 to .644 contains the true (population) correlation; that is, the true (population) correlation coefficient is likely somewhere between medium and large in magnitude. Overall, the statistically significant and large-in-magnitude correlation between overall scores on the performance evaluation and sales revenue generated provides evidence of criterion-related validity  and specifically evidence of convergent validity, as these two conceptually related variables were indeed correlated to a statistically significant extent. r Description .10 Small .30 Medium .50 Large 51.2.5.2 Estimate Point-Biserial Correlation {estimate_pb_correlation_convergentdiscriminantvalidity} Well shift gears and practice estimating the point-biserial correlation between the Perf_Overall continuous variable and the Sex dichotomous variable. Lets assume that the Perf_Overall variable is a measure of overall job performance for the employees in this sample and that employees sex should not have a bearing on the performance evaluation ratings they receive from their respective supervisors. Given that, we are looking to see if there is evidence of discriminant validity between these two conceptually distinguishable variables. Before estimating their correlation, lets see if these two variables satisfy the statistical assumptions for estimating and interpreting a point-biserial correlation. Cases Are Randomly Sampled from the Population: As mentioned in the statistical assumptions section, we will assume that the cases (i.e., employees) were randomly sampled from the population, and thus conclude that this assumption has been satisfied. One of the Variables Is Continuous &amp; One of the Variables Is Dichotomous: We can describe the Perf_Overall variable has having an interval measurement scale, and thus it is a continuous variable. We can describe the Sex variable as having a nominal measurement scale and specifically as being a dichotomous variable because it is operationalized as having two levels: 1 = Female, 0 = Male. We have satisfied both of these assumptions. The Continuous Variable Variable Shows a Univariate Normal Distribution at Each Level of the Dichotomous Variable: In accordance with the central limit theorem, a sampling distribution will tend to approximate a normal distribution when it is based on more than 30 cases (N &gt; 30). Lets figure out if the number of cases at each level of the dichotomous variable exceeds 30. Using the drop_na function from the tidyr package: As the first argument, type the name of the data frame object (PerfRew). As the second and third arguments, type the names of the two variables (Perf_Overall, Sex). Following the drop_na function, insert the pipe (%&gt;%) operator. This will pipe the data frame object (tibble) created by the drop_na function to the subsequent function. After the pipe (%&gt;%) operator, type the name of the group_by function from the dplyr package, and as the sole argument, insert the name of the dichotomous variable (Sex). Following the group_by function, insert another pipe (%&gt;%) operator. After the pipe (%&gt;%) operator, type the name of the summarize function from the dplyr package, and as the sole argument, type the name of some new variable that will contain the counts (e.g., count) followed by the = operator and the n function from dplyr. Do not include any arguments in the n function. Note: If youd like a refresher on aggregating and segmenting data, check out the chapter that covers those operations. # Drop cases (rows) where either Perf_Overall or Sex # have missing data (NAs), # pipe (%&gt;%) the resulting data # frame to the group_by function, &amp; # compute the counts for each level # of the Sex variable drop_na(PerfRew, Perf_Overall, Sex) %&gt;% group_by(Sex) %&gt;% summarize(count = n()) ## # A tibble: 2 x 2 ## Sex count ## &lt;dbl&gt; &lt;int&gt; ## 1 0 54 ## 2 1 41 As shown in the resulting table, there are more than 30 cases for each level of the dichotomous Sex variable. Thus, we can assume that the assumption of univariate normality has been met for the continuous variable at both levels of the dichotomous variable. To learn how to test this assumption if there are fewer than 30 cases in each sub-sample, please see the end-of-chapter supplement. Continuous Variable Is Free of Univariate Outliers: To visualize whether there are any univariate outliers for either variable, we will use the BoxPlot function from the lessR package to generate a box-and-whiskers plot (box plot). Type the name of the Perf_Overall variable as the first argument in the BoxPlot function, and as the second argument, type data= followed by the name of the data frame object (PerfRew). # Create box plot BoxPlot(Perf_Overall, data=PerfRew) ## [Violin/Box/Scatterplot graphics from Deepayan Sarkar&#39;s lattice package] ## &gt;&gt;&gt; Suggestions ## Plot(Perf_Overall, out_cut=2, fences=TRUE, vbs_mean=TRUE) # Label two outliers ... ## Plot(Perf_Overall, box_adj=TRUE) # Adjust boxplot whiskers for asymmetry ## --- Perf_Overall --- ## Present: 95 ## Missing: 0 ## Total : 95 ## ## Mean : 2.911 ## Stnd Dev : 1.202 ## IQR : 1.875 ## Skew : 0.000 [medcouple, -1 to 1] ## ## Minimum : 1.000 ## Lower Whisker: 1.000 ## 1st Quartile : 2.000 ## Median : 3.000 ## 3rd Quartile : 3.875 ## Upper Whisker: 5.000 ## Maximum : 5.000 ## ## No (Box plot) outliers ## ## Max Dupli- ## Level cations Values ## ------------------------------ ## Perf_Overall 10 2.25 If outliers or other influential cases were present, they would appear beyond the whiskers (i.e., outer bars) in the plot. No potential outliers were detected for Perf_Overall, which indicates that we have satisifed the statistical assumption that the continuous variable is free of univariate outliers. The Variance of the Continuous Variable Is Approximately Equal for Each Level of the Dichotomous Variable: To test this statistical assumption, we will perform a statistical test called Levenes test using the leveneTest function from the car package. This function will allow us to test whether we can assume the variances to be approximately equal, which is sometimes referred to as homogeneity of variances. If you havent already, install and access the car package. # Install package install.packages(&quot;car&quot;) # Access package library(car) Type the name of the leveneTest function. As the first argument, specify the statistical model. To do so, type the name of the continuous variable (Perf_Overall) to the left of the ~ operator and the name of the dichotomous variable (Sex) to the right of the ~ operator. This function requires that the variable to the right of the ~ operator is non-numeric, so lets wrap the Sex variable in the as.factor function from base R to convert the dichotomous variable to a non-numeric factor within the leveneTest function; this will not permanently change the Sex variable to a factor, though. For the second argument, use data= to specify the name of the data frame (PerfRew). # Compute Levene&#39;s test for equal variances leveneTest(Perf_Overall ~ as.factor(Sex), data=PerfRew) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 2.3502 0.1287 ## 93 This function tests the null hypothesis that the variances are equal, and the output indicates that the p-value (i.e., Pr(&gt;F) = .1287) associated with Levenes test is equal to or greater than a two-tailed alpha level of .05. Thus, we fail to reject the null hypothesis that the variances are equal and thus conclude that the variances are equal. We have satisfied this statistical assumption. Overall, we can reasonably conclude that the statistical assumptions for a point-biserial correlation have been satisfied. Estimate Point-Biserial Correlation: Fortunately, because a point-biserial correlation is equal to a Pearson product-moment correlation when the dichotomous variable has been converted to quantitative values, we can use the same Correlation function from the lessR package that we used for estimating the Pearson product-moment correlation. # Estimate point-biserial / Pearson product-moment correlation using Correlation function from lessR Correlation(x=Perf_Overall, y=Sex, data=PerfRew) ## Correlation Analysis for Variables Perf_Overall and Sex ## ## &gt;&gt;&gt; Pearson&#39;s product-moment correlation ## ## Number of paired values with neither missing, n = 95 ## Number of cases (rows of data) deleted: 0 ## ## Sample Covariance: s = -0.081 ## ## Sample Correlation: r = -0.135 ## ## Hypothesis Test of 0 Correlation: t = -1.312, df = 93, p-value = 0.193 ## 95% Confidence Interval for Correlation: -0.327 to 0.069 Sample Technical Write-Up: Based on a sample of 95 employees (N = 95), we can conclude that the correlation between overall performance evaluation scores and employee sex is not statistically significant, as the p-value was greater than the conventional two-tailed alpha level of .05 (r = -.135, p = .193, 95% CI[-.327, .069]). This leads us to conclude that employees overall performance evaluation ratings are not associated with their sex. Thus, we found evidence of discriminant validity, as these two conceptually dissimilar variables were not found to be correlated. 51.2.6 Create Correlation Matrix When we have a large number of variables for which we wish to evaluate convergent and discriminant validity, often it makes sense to create a correlation matrix as opposed to estimating the correlation for each pair of variables one by one. There are different functions we can use to create a correlation matrix for a set of numeric variables. Here, we will learn how to use thecorr.test and lowerCor functions from the psych package. Before we review the functions, lets drop the EmpID variable from the data frame, as it wouldnt make sense to correlate another variable with a unique identifier variable like EmpID. # Drop EmpID unique identifier variable PerfRew$EmpID &lt;- NULL Previously, we recoded the Sex variable such that Female became 1 and Male became 0. As a precautionary step, we will ensure that the Sex variable is indeed now numeric by applying the as.numeric function. We will overwrite the existing Sex variable by using the &lt;- assignment operator. Using the $ operator, we can attach this new (overwritten) variable to the existing PerfRew data frame object. To the right of the &lt;- assignment operator, type the name of the as.numeric function. As the sole argument, type the name of the data frame object (PerfRew), followed by the $ operator and the name of the variable we wish to convert to numeric (Sex). # Convert Sex variable to type numeric PerfRew$Sex &lt;- as.numeric(PerfRew$Sex) The corr.test function from the psych package has the advantage of producing both the correlation coefficients and a p-values. If you havent already, install and access the psych package using the install.packages and library functions, respectively. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) Type the name of the corr.test function (not to be confused with the cor.test function from base R). As the first argument, type the name of your data frame object (PerfRew). As the second argument, specify method=\"pearson\" to estimate Pearson product-moment correlations. If you were estimating the associations between a set of rank-order variables, you could replace pearson with spearman to estimate Spearman correlations. # Create correlation matrix using corr.test function corr.test(PerfRew, method=&quot;pearson&quot;) ## Call:corr.test(x = PerfRew, method = &quot;pearson&quot;) ## Correlation matrix ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 ## Perf_Qual 1.00 0.77 0.81 0.85 0.46 0.11 0.44 ## Perf_Prod 0.77 1.00 0.76 0.74 0.56 0.17 0.58 ## Perf_Effort 0.81 0.76 1.00 0.77 0.50 0.21 0.48 ## Perf_Admin 0.85 0.74 0.77 1.00 0.34 0.11 0.34 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 0.06 0.91 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 0.08 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 ## Sex Age EducationLevel Perf_Overall ## Perf_Qual -0.10 0.12 -0.02 0.94 ## Perf_Prod -0.21 0.23 -0.06 0.89 ## Perf_Effort -0.18 0.07 0.05 0.91 ## Perf_Admin -0.01 0.12 -0.10 0.92 ## SalesRevenue 0.02 0.10 0.02 0.51 ## BasePay_2018 -0.18 0.40 0.10 0.16 ## VariablePay_2018 -0.07 0.15 0.01 0.50 ## Sex 1.00 -0.02 -0.11 -0.13 ## Age -0.02 1.00 0.12 0.15 ## EducationLevel -0.11 0.12 1.00 -0.04 ## Perf_Overall -0.13 0.15 -0.04 1.00 ## Sample Size ## [1] 95 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 ## Perf_Qual 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Prod 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Effort 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Admin 0.00 0.00 0.00 0.00 0.03 1.00 0.03 ## SalesRevenue 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## BasePay_2018 0.30 0.11 0.04 0.27 0.55 0.00 1.00 ## VariablePay_2018 0.00 0.00 0.00 0.00 0.00 0.44 0.00 ## Sex 0.35 0.04 0.08 0.95 0.83 0.07 0.50 ## Age 0.25 0.03 0.48 0.24 0.34 0.00 0.15 ## EducationLevel 0.81 0.58 0.65 0.35 0.81 0.34 0.93 ## Perf_Overall 0.00 0.00 0.00 0.00 0.00 0.12 0.00 ## Sex Age EducationLevel Perf_Overall ## Perf_Qual 1.00 1.00 1.00 0 ## Perf_Prod 1.00 0.83 1.00 0 ## Perf_Effort 1.00 1.00 1.00 0 ## Perf_Admin 1.00 1.00 1.00 0 ## SalesRevenue 1.00 1.00 1.00 0 ## BasePay_2018 1.00 0.00 1.00 1 ## VariablePay_2018 1.00 1.00 1.00 0 ## Sex 0.00 1.00 1.00 1 ## Age 0.86 0.00 1.00 1 ## EducationLevel 0.30 0.23 0.00 1 ## Perf_Overall 0.19 0.15 0.73 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option The output produced includes three sections: Correlation matrix, Sample Size, and Probability values (p-values). To find the corresponding p-value for a correlation displayed in the matrix, look to the Probability values matrix, and note that the lower diagonal includes the traditional p-values, while the upper diagonal includes the adjusted p-values based on multiple tests (i.e., correcting for family-wise error). To estimate the confidence intervals, assign the results of the function and its argument to an object named whatever you would like (matrixexample) using the &lt;- operator. After that, use the print function from base R with the name of the new object as the first argument and short=FALSE as the second argument. # Create correlation matrix objet with p-values &amp; confidence intervals matrixexample &lt;- corr.test(PerfRew, method=&quot;pearson&quot;) # Print correlation matrix with p-values &amp; confidence intervals print(matrixexample, short=FALSE) ## Call:corr.test(x = PerfRew, method = &quot;pearson&quot;) ## Correlation matrix ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 ## Perf_Qual 1.00 0.77 0.81 0.85 0.46 0.11 0.44 ## Perf_Prod 0.77 1.00 0.76 0.74 0.56 0.17 0.58 ## Perf_Effort 0.81 0.76 1.00 0.77 0.50 0.21 0.48 ## Perf_Admin 0.85 0.74 0.77 1.00 0.34 0.11 0.34 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 0.06 0.91 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 0.08 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 ## Sex Age EducationLevel Perf_Overall ## Perf_Qual -0.10 0.12 -0.02 0.94 ## Perf_Prod -0.21 0.23 -0.06 0.89 ## Perf_Effort -0.18 0.07 0.05 0.91 ## Perf_Admin -0.01 0.12 -0.10 0.92 ## SalesRevenue 0.02 0.10 0.02 0.51 ## BasePay_2018 -0.18 0.40 0.10 0.16 ## VariablePay_2018 -0.07 0.15 0.01 0.50 ## Sex 1.00 -0.02 -0.11 -0.13 ## Age -0.02 1.00 0.12 0.15 ## EducationLevel -0.11 0.12 1.00 -0.04 ## Perf_Overall -0.13 0.15 -0.04 1.00 ## Sample Size ## [1] 95 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 ## Perf_Qual 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Prod 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Effort 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## Perf_Admin 0.00 0.00 0.00 0.00 0.03 1.00 0.03 ## SalesRevenue 0.00 0.00 0.00 0.00 0.00 1.00 0.00 ## BasePay_2018 0.30 0.11 0.04 0.27 0.55 0.00 1.00 ## VariablePay_2018 0.00 0.00 0.00 0.00 0.00 0.44 0.00 ## Sex 0.35 0.04 0.08 0.95 0.83 0.07 0.50 ## Age 0.25 0.03 0.48 0.24 0.34 0.00 0.15 ## EducationLevel 0.81 0.58 0.65 0.35 0.81 0.34 0.93 ## Perf_Overall 0.00 0.00 0.00 0.00 0.00 0.12 0.00 ## Sex Age EducationLevel Perf_Overall ## Perf_Qual 1.00 1.00 1.00 0 ## Perf_Prod 1.00 0.83 1.00 0 ## Perf_Effort 1.00 1.00 1.00 0 ## Perf_Admin 1.00 1.00 1.00 0 ## SalesRevenue 1.00 1.00 1.00 0 ## BasePay_2018 1.00 0.00 1.00 1 ## VariablePay_2018 1.00 1.00 1.00 0 ## Sex 0.00 1.00 1.00 1 ## Age 0.86 0.00 1.00 1 ## EducationLevel 0.30 0.23 0.00 1 ## Perf_Overall 0.19 0.15 0.73 0 ## ## Confidence intervals based upon normal theory. To get bootstrapped values, try cor.ci ## raw.lower raw.r raw.upper raw.p lower.adj upper.adj ## Prf_Q-Prf_P 0.68 0.77 0.84 0.00 0.59 0.88 ## Prf_Q-Prf_E 0.73 0.81 0.87 0.00 0.65 0.90 ## Prf_Q-Prf_A 0.78 0.85 0.89 0.00 0.71 0.92 ## Prf_Q-SlsRv 0.29 0.46 0.61 0.00 0.17 0.68 ## Prf_Q-BP_20 -0.10 0.11 0.30 0.30 -0.20 0.40 ## Prf_Q-VP_20 0.26 0.44 0.59 0.00 0.14 0.67 ## Prf_Q-Sex -0.29 -0.10 0.11 0.35 -0.38 0.20 ## Prf_Q-Age -0.08 0.12 0.31 0.25 -0.19 0.41 ## Prf_Q-EdctL -0.22 -0.02 0.18 0.81 -0.28 0.24 ## Prf_Q-Prf_O 0.91 0.94 0.96 0.00 0.88 0.97 ## Prf_P-Prf_E 0.66 0.76 0.83 0.00 0.58 0.87 ## Prf_P-Prf_A 0.63 0.74 0.82 0.00 0.54 0.86 ## Prf_P-SlsRv 0.40 0.56 0.68 0.00 0.29 0.75 ## Prf_P-BP_20 -0.04 0.17 0.36 0.11 -0.16 0.46 ## Prf_P-VP_20 0.42 0.58 0.70 0.00 0.31 0.76 ## Prf_P-Sex -0.40 -0.21 -0.01 0.04 -0.50 0.11 ## Prf_P-Age 0.03 0.23 0.41 0.03 -0.10 0.51 ## Prf_P-EdctL -0.26 -0.06 0.15 0.58 -0.33 0.23 ## Prf_P-Prf_O 0.84 0.89 0.93 0.00 0.80 0.94 ## Prf_E-Prf_A 0.68 0.77 0.84 0.00 0.60 0.88 ## Prf_E-SlsRv 0.33 0.50 0.64 0.00 0.21 0.71 ## Prf_E-BP_20 0.01 0.21 0.39 0.04 -0.12 0.49 ## Prf_E-VP_20 0.31 0.48 0.62 0.00 0.19 0.70 ## Prf_E-Sex -0.37 -0.18 0.02 0.08 -0.47 0.14 ## Prf_E-Age -0.13 0.07 0.27 0.48 -0.22 0.36 ## Prf_E-EdctL -0.16 0.05 0.25 0.65 -0.23 0.32 ## Prf_E-Prf_O 0.87 0.91 0.94 0.00 0.83 0.96 ## Prf_A-SlsRv 0.15 0.34 0.51 0.00 0.02 0.59 ## Prf_A-BP_20 -0.09 0.11 0.31 0.27 -0.20 0.40 ## Prf_A-VP_20 0.14 0.34 0.50 0.00 0.02 0.59 ## Prf_A-Sex -0.21 -0.01 0.20 0.95 -0.21 0.20 ## Prf_A-Age -0.08 0.12 0.32 0.24 -0.19 0.41 ## Prf_A-EdctL -0.29 -0.10 0.11 0.35 -0.38 0.21 ## Prf_A-Prf_O 0.88 0.92 0.94 0.00 0.84 0.96 ## SlsRv-BP_20 -0.14 0.06 0.26 0.55 -0.23 0.34 ## SlsRv-VP_20 0.86 0.91 0.94 0.00 0.82 0.95 ## SlsRv-Sex -0.18 0.02 0.22 0.83 -0.23 0.28 ## SlsRv-Age -0.10 0.10 0.30 0.34 -0.21 0.39 ## SlsRv-EdctL -0.18 0.02 0.23 0.81 -0.24 0.29 ## SlsRv-Prf_O 0.34 0.51 0.64 0.00 0.22 0.72 ## BP_20-VP_20 -0.12 0.08 0.28 0.44 -0.22 0.36 ## BP_20-Sex -0.37 -0.18 0.02 0.07 -0.47 0.14 ## BP_20-Age 0.21 0.40 0.55 0.00 0.09 0.64 ## BP_20-EdctL -0.11 0.10 0.29 0.34 -0.21 0.39 ## BP_20-Prf_O -0.04 0.16 0.35 0.12 -0.16 0.45 ## VP_20-Sex -0.27 -0.07 0.13 0.50 -0.35 0.22 ## VP_20-Age -0.06 0.15 0.34 0.15 -0.17 0.44 ## VP_20-EdctL -0.19 0.01 0.21 0.93 -0.22 0.24 ## VP_20-Prf_O 0.33 0.50 0.64 0.00 0.21 0.71 ## Sex-Age -0.22 -0.02 0.18 0.86 -0.26 0.23 ## Sex-EdctL -0.30 -0.11 0.10 0.30 -0.40 0.20 ## Sex-Prf_O -0.33 -0.13 0.07 0.19 -0.43 0.18 ## Age-EdctL -0.08 0.12 0.32 0.23 -0.19 0.42 ## Age-Prf_O -0.05 0.15 0.34 0.15 -0.17 0.44 ## EdctL-Prf_O -0.24 -0.04 0.17 0.73 -0.31 0.24 The 95% confidence intervals appear at the bottom of the output, under the section Confidence intervals based upon normal theory. Each row represents a unique correlation; note that the function abbreviates the variable names, so youll have to do your best to interpret them. We can also write the matrixexample object we created to a .csv file by using the write.csv function. As the first argument, enter the name of the correlation matrix object (matrixexample), followed by a $, followed by either r, p, or ci to write the correlation matrix, p-values, or confidence intervals, respectively. As the second argument, come up with a name for the data file that will appear in your working directory, but be sure to keep the .csv extension. The files will appear in your working directory, and you can open them in Microsoft Excel or Google Sheets. # Write correlation matrix to working directory write.csv(matrixexample$r, &quot;Correlation Matrix Example.csv&quot;) # Write p-values to working directory write.csv(matrixexample$p, &quot;p-values Example.csv&quot;) # Write confidence intervals to working directory write.csv(matrixexample$ci, &quot;Confidence Intervals Example.csv&quot;) If you just want to view the lower diagonal of the correlation matrix (with just the correlation coefficients), apply the lowerCor function from the psych package, using the same arguments as the corr.test function. # Create correlation matrix using lowerCor function lowerCor(PerfRew, method=&quot;pearson&quot;) ## Prf_Q Prf_P Prf_E Prf_A SlsRv BP_20 VP_20 Sex Age EdctL Prf_O ## Perf_Qual 1.00 ## Perf_Prod 0.77 1.00 ## Perf_Effort 0.81 0.76 1.00 ## Perf_Admin 0.85 0.74 0.77 1.00 ## SalesRevenue 0.46 0.56 0.50 0.34 1.00 ## BasePay_2018 0.11 0.17 0.21 0.11 0.06 1.00 ## VariablePay_2018 0.44 0.58 0.48 0.34 0.91 0.08 1.00 ## Sex -0.10 -0.21 -0.18 -0.01 0.02 -0.18 -0.07 1.00 ## Age 0.12 0.23 0.07 0.12 0.10 0.40 0.15 -0.02 1.00 ## EducationLevel -0.02 -0.06 0.05 -0.10 0.02 0.10 0.01 -0.11 0.12 1.00 ## Perf_Overall 0.94 0.89 0.91 0.92 0.51 0.16 0.50 -0.13 0.15 -0.04 1.00 If our goal is only to read the sign and magnitude of each correlation, then the viewing just the lower diagonal makes that task much easier. To learn how to make heatmap data visualizations for a correlation matrix and how to present the results in American Psychological Association (APA) style, please check out the chapter supplement. 51.2.7 Summary In this chapter, we learned how to create a scatter plot using the ScatterPlot function from the lessR package, how to estimate a correlation using the Correlation function from the lessR package, and how to create a correlation matrix using the corr.test and lowerCor functions from the psych package. 51.3 Chapter Supplement In this chapter, we will learn how to test the statistical assumption of a univariate normal distribution when we have fewer than 30 cases by using the Shapiro-Wilk test. In addition, we will learn how to create an American Psychological Association (APA) style table and other data visualizations for a correlation matrix. 51.3.1 Functions &amp; Packages Introduced Function Package set.seed base R slice_sample dplyr shapiro.test base R tapply base R apa.cor.table apaTables cor.plot psych corrgram corrgram 51.3.2 Initial Steps If required, please refer to the Initial Steps section from this chapter for more information on these initial steps. # Set your working directory setwd(&quot;H:/RWorkshop&quot;) # Install readr package if you haven&#39;t already # [Note: You don&#39;t need to install a package every # time you wish to access it] install.packages(&quot;readr&quot;) # Access readr package library(readr) # Read data and name data frame (tibble) object PerfRew &lt;- read_csv(&quot;PerfMgmtRewardSystemsExample.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------- ## cols( ## EmpID = col_double(), ## Perf_Qual = col_double(), ## Perf_Prod = col_double(), ## Perf_Effort = col_double(), ## Perf_Admin = col_double(), ## SalesRevenue = col_double(), ## BasePay_2018 = col_double(), ## VariablePay_2018 = col_double(), ## Sex = col_character(), ## Age = col_double(), ## EducationLevel = col_double() ## ) # Print the names of the variables in the data frame (tibble) object names(PerfRew) ## [1] &quot;EmpID&quot; &quot;Perf_Qual&quot; &quot;Perf_Prod&quot; &quot;Perf_Effort&quot; &quot;Perf_Admin&quot; ## [6] &quot;SalesRevenue&quot; &quot;BasePay_2018&quot; &quot;VariablePay_2018&quot; &quot;Sex&quot; &quot;Age&quot; ## [11] &quot;EducationLevel&quot; # Print variable type for each variable in data frame (tibble) object str(PerfRew) ## spec_tbl_df[,11] [95 x 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ EmpID : num [1:95] 1 2 3 4 5 6 7 8 9 10 ... ## $ Perf_Qual : num [1:95] 3 1 2 2 1 2 5 2 2 2 ... ## $ Perf_Prod : num [1:95] 3 1 1 3 1 2 5 1 3 2 ... ## $ Perf_Effort : num [1:95] 3 1 1 3 1 2 5 1 3 3 ... ## $ Perf_Admin : num [1:95] 4 1 1 1 1 3 5 1 2 2 ... ## $ SalesRevenue : num [1:95] 57563 54123 56245 58291 58354 ... ## $ BasePay_2018 : num [1:95] 53791 52342 50844 52051 48061 ... ## $ VariablePay_2018: num [1:95] 6199 1919 7507 6285 4855 ... ## $ Sex : chr [1:95] &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ Age : num [1:95] 39 48 38 35 32 34 57 43 35 47 ... ## $ EducationLevel : num [1:95] 2 4 4 4 2 4 3 2 4 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. EmpID = col_double(), ## .. Perf_Qual = col_double(), ## .. Perf_Prod = col_double(), ## .. Perf_Effort = col_double(), ## .. Perf_Admin = col_double(), ## .. SalesRevenue = col_double(), ## .. BasePay_2018 = col_double(), ## .. VariablePay_2018 = col_double(), ## .. Sex = col_character(), ## .. Age = col_double(), ## .. EducationLevel = col_double() ## .. ) # Print first 6 rows of data frame (tibble) object head(PerfRew) ## # A tibble: 6 x 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female ## 2 2 1 1 1 1 54123 52342 1919 Male ## 3 3 2 1 1 1 56245 50844 7507 Male ## 4 4 2 3 3 1 58291 52051 6285 Male ## 5 5 1 1 1 1 58354 48061 4855 Female ## 6 6 2 2 2 3 57618 53386 4056 Male ## # ... with 2 more variables: Age &lt;dbl&gt;, EducationLevel &lt;dbl&gt; # Print number of rows in data frame (tibble) object head(PerfRew) ## # A tibble: 6 x 11 ## EmpID Perf_Qual Perf_Prod Perf_Effort Perf_Admin SalesRevenue BasePay_2018 VariablePay_2018 Sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 3 3 3 4 57563 53791 6199 Female ## 2 2 1 1 1 1 54123 52342 1919 Male ## 3 3 2 1 1 1 56245 50844 7507 Male ## 4 4 2 3 3 1 58291 52051 6285 Male ## 5 5 1 1 1 1 58354 48061 4855 Female ## 6 6 2 2 2 3 57618 53386 4056 Male ## # ... with 2 more variables: Age &lt;dbl&gt;, EducationLevel &lt;dbl&gt; # Create composite (overall scale score) variable based on Engagement items PerfRew$Perf_Overall &lt;- rowMeans(PerfRew[,c(&quot;Perf_Qual&quot;,&quot;Perf_Prod&quot;,&quot;Perf_Effort&quot;,&quot;Perf_Admin&quot;)], na.rm=TRUE) # Drop EmpID unique identifier variable PerfRew$EmpID &lt;- NULL # Recode Female as 1 and Male as 0 for Sex variable PerfRew$Sex &lt;- dplyr::recode(PerfRew$Sex, Female = 1, Male = 0) # Convert Sex variable to type numeric PerfRew$Sex &lt;- as.numeric(PerfRew$Sex) 51.3.3 shapiro.test Function from Base R Evidence of a univariate normal distribution is a common statistical assumption for parametric statistical analyses like the Pearson product-moment correlation, point-biserial correlation, independent-samples t-test, and paired-samples t-test. In accordance with the central limit theorem, when the sample size exceeds 30 (N &gt; 30), then the sampling distribution tends to approximate normality. Consequently, when we have larger sample sizes, we can generally assume that we have met the assumption of univariate normality. In contrast, when the sample size has 30 or fewer cases, then we should formally test the assumption of univariate normality using a statistical test like the Shapiro-Wilk test and/or a data visualization like a histogram. The null hypothesis for the Shapiro-Wilk test is that the distribution is normal; thus, only when the associated p-value is less than the alpha level of .05 will we conclude that the distribution violates the assumption of normality. For the sake of demonstration, lets randomly select 25 cases from our PerfRew data frame object by using the slice_sample. function from the dplyr package. If you havent already, be sure to install and access the dplyr package. # Install package install.packages(&quot;dplyr&quot;) # Access package library(dplyr) Lets make our random sampling reproducible by setting a seed value using the set.seed function from base R. # Set seed for reproducibility of random sampling set.seed(1985) To randomly sample 25 cases from the PerfRew data frame object, we will do the following: Come up with a name for a new data frame object that will house our randomly sampled cases. In this example, Im naming the new object temp using the &lt;- assignment operator. To the right of the &lt;- assignment operator, type the name of the slice_sample function from the dplyr package. As the first argument in the function, well type the name of the data frame object from which we wish to randomly sample (PerfRew). As the second argument, well type n= followed by the number of cases we wish to randomly sample. This function will by default randomly sample without replacement. # Randomly sample 25 cases from data frame object temp &lt;- slice_sample(PerfRew, n=25) Now we have a data frame object called temp that has fewer than 30 cases, which makes it necessary to formally evaluate whether the assumption of univariate normality has been satisfied. To compute the Shapiro-Wilk normality test for all cases with scores on a single continuous variable in our sample, we can use the shapiro.test function from base R. First, type the name of the shapiro.test function. As the sole argument, type the name of the data frame (temp), followed by the $ operator and the name of the continuous variable in question (Perf_Overall). # Compute Shapiro-Wilk normality test for univariate normal # distribution statistical assumption shapiro.test(temp$Perf_Overall) ## ## Shapiro-Wilk normality test ## ## data: temp$Perf_Overall ## W = 0.96315, p-value = 0.4808 The output indicates that the p-value of .4808 is equal to or greater than the conventional alpha of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed. In other words, we have evidence that the outcome variable is likely normally distributed for both conditions, which suggests that we have met the this statistical assumption for this continuous variable; if another continuous variable also met this statistical assumption, then we would build more confidence that a Pearson product-moment correlation is an appropriate analysis. If our goal is to test the statistical assumption that a continuous variable is normally distributed at each level of a categorical (e.g., dichotomous) variable, then we can use a combination of the tapply and shapiro.test functions from base R. The tapply function will apply the shapiro.test function to the continuous variable for each level of the categorical variable. Type the name of the tapply function from base R. As the first argument, type the name of the data frame object (temp), followed by the $ operator and the name of the continuous variable (Perf_Overall). As the second argument, type the name of the data frame object (temp), followed by the $ operator and the name of the categorical (e.g., dichotomous) variable (Sex). As the third argument, type the name of the shapiro.test function from base R. # Compute Shapiro-Wilk normality test for univariate normal # distribution statistical assumption tapply(temp$Perf_Overall, temp$Sex, shapiro.test) ## $`0` ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.97492, p-value = 0.9106 ## ## ## $`1` ## ## Shapiro-Wilk normality test ## ## data: X[[i]] ## W = 0.80628, p-value = 0.02408 The output indicates that the p-value associated with the Perf_Overall variable for cases where Sex is equal to 1 (Female) is .9106, which is equal to or greater than the conventional two-tailed alpha level of .05; therefore, we fail to reject the null hypothesis that the values are normally distributed within this segment of the data. The p-value associated with the Perf_Overall variable for cases where Sex is equal to 0 (Male) is 02408, which is less than the conventional two-tailed alpha level of .05; therefore, we reject the null hypothesis that the values are normally distributed within this segment of the data, leading us to conclude that there is likely a violation of the assumption of univariate normality for the segment of cases where Sex is equal to 0 (Male). Taken together, the Shapiro-Wilk tests indicate that a parametric statistical analysis like a point-biserial correlation may not be appropriate. 51.3.4 APA-Style Results Table If we wish to present the results of our correlation matrix to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the apaTables package. If you havent already, be sure to install and access the apaTables package using the install.packages and library functions, respectively. # Install package install.packages(&quot;apaTables&quot;) # Access package library(apaTables) The apa.cor.table function is easy to use. Just type the name of the apa.cor.table function, and in the function parentheses, include the name of the data frame (with only variables of type numeric) as the sole argument (PerfRew). The APA-style correlation matrix will appear in your console. # Create APA-style correlation matrix using apa.cor.table function apa.cor.table(PerfRew) ## ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 4 5 ## 1. Perf_Qual 2.93 1.33 ## ## 2. Perf_Prod 2.85 1.30 .77** ## [.68, .84] ## ## 3. Perf_Effort 2.93 1.30 .81** .76** ## [.73, .87] [.66, .83] ## ## 4. Perf_Admin 2.94 1.32 .85** .74** .77** ## [.78, .89] [.63, .82] [.68, .84] ## ## 5. SalesRevenue 60852.88 6501.54 .46** .56** .50** .34** ## [.29, .61] [.40, .68] [.33, .64] [.15, .51] ## ## 6. BasePay_2018 52090.52 2670.02 .11 .17 .21* .11 .06 ## [-.10, .30] [-.04, .36] [.01, .39] [-.09, .31] [-.14, .26] ## ## 7. VariablePay_2018 8131.65 3911.81 .44** .58** .48** .34** .91** ## [.26, .59] [.42, .70] [.31, .62] [.14, .50] [.86, .94] ## ## 8. Sex 0.43 0.50 -.10 -.21* -.18 -.01 .02 ## [-.29, .11] [-.40, -.01] [-.37, .02] [-.21, .20] [-.18, .22] ## ## 9. Age 39.67 8.83 .12 .23* .07 .12 .10 ## [-.08, .31] [.03, .41] [-.13, .27] [-.08, .32] [-.10, .30] ## ## 10. EducationLevel 3.13 1.02 -.02 -.06 .05 -.10 .02 ## [-.22, .18] [-.26, .15] [-.16, .25] [-.29, .11] [-.18, .23] ## ## 11. Perf_Overall 2.91 1.20 .94** .89** .91** .92** .51** ## [.91, .96] [.84, .93] [.87, .94] [.88, .94] [.34, .64] ## ## 6 7 8 9 10 ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## .08 ## [-.12, .28] ## ## -.18 -.07 ## [-.37, .02] [-.27, .13] ## ## .40** .15 -.02 ## [.21, .55] [-.06, .34] [-.22, .18] ## ## .10 .01 -.11 .12 ## [-.11, .29] [-.19, .21] [-.30, .10] [-.08, .32] ## ## .16 .50** -.13 .15 -.04 ## [-.04, .35] [.33, .64] [-.33, .07] [-.05, .34] [-.24, .17] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## If you would like to write (export) the correlation matrix table to your working directory as a .doc or .rtf document, as an additional argument, add filename= followed by what you would like to name the new file in quotation marks (\" \"). Be sure to include either .doc or .rtf as the file extension. Once you have run the code below, open the new file in Microsoft Word. # Create APA-style correlation matrix using apa.cor.table function apa.cor.table(PerfRew, filename=&quot;APA style table.doc&quot;) 51.3.5 cor.plot Function from psych package To create a correlation matrix with a heatmap, we must first create a correlation matrix object using the cor function from base R. We will assign the results of the cor function to an object named something of your choosing (e.g., matrixexample2) using the &lt;- assignment operator. We will apply then cor function to the PerfRew data frame object, and specify that we wish to estimate Pearson product-moment correlations (and by extension point biserial correlations, if applicable) by specifying method=\"pearson\" as the second argument. # Create and name correlation matrix object using cor function matrixexample2 &lt;- cor(PerfRew, method=&quot;pearson&quot;) Lets make a correlation matrix with a heatmap based on the matrixexample2 correlation matrix object we just created, we will use the cor.plot function from the psych package. If you havent already, be sure to install and access the psych package. # Install package install.packages(&quot;psych&quot;) # Access package library(psych) To begin, type the name of the cor.plot function. As the first argument, type the name of the correlation matrix object you just created (matrixexample2). As the second argument, leave the numbers=TRUE argument as is. You can save the image by selecting Export from the Plots window in RStudio. # Create a heatmap correlation matrix cor.plot(matrixexample2, numbers=TRUE) The heatmap displays stronger positive correlations in a darker hue of blue and stronger negative correlations in a darker hue of red. Correlations that are equal to zero will be white. 51.3.6 corrgram Function from corrgram package To create a correlogram heatmap, we will use the corrgram function from the corrgram package. If you havent already, install and access the corrgram package. # Install package install.packages(&quot;corrgram&quot;) # Access package library(corrgram) Lets begin by creating a correlogram heatmap in which the lower diagonal is a gradient-shaded heatmap and the upper diagonal is a series of pies depicting the magnitude of correlations. Type the name of the corrgram function. As the first argument, type the name of the data frame object (PerfRew). As the second argument, specify order=FALSE to keep the order of the variables as they are in the data frame. As the third argument, include the argument lower.panel=panel.shade to create a gradient-shared heatmap in the lower diagonal. As the fourth argument, include the argument upper.panel=panel.pie to convey the magnitude of each correlation as a proportion of a filled-in pie. As the fifth argument, include the argument text.panel=panel.txt to include variable names on the off diagonal. # Create a correlogram heatmap corrgram(PerfRew, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie, text.panel=panel.txt) "],["create-portfolio.html", "Chapter 52 Creating a Data Analytics Portfolio", " Chapter 52 Creating a Data Analytics Portfolio When searching for a job in data analytics, it can be tricky to (a) demonstrate what you know how to do or have done previously and (b) distinguish yourself from other job candidates. Creating a portfolio of your work can be one way to set yourself apart on the job market and can, in some instances, help compensate for fewer years of relevant work experience. In this document, I will provide some suggestions on how to set up a portfolio, with an emphasis on applying the R programming language. Identify a content area or context that excites you. In the Human Resource (HR) Analytics certificate, we obviously focus on the HR context and various HR content areas. Try to be more specific than that, though. For example, are you interested in employee sensing (e.g., engagement surveys), separation and retention, recruitment and selection, training and development, reward systems, or workforce planning? In your portfolio, you can present multiple projects that highlight different content areas and contexts, but I recommend focusing your first portfolio project on an area (a) that you are really passionate about and (b) in which you have some prior work experience (even if not in the application of data analytics/science to that area). Identify an area of data analytics that excites you. Begin by reflecting on what type of work you intrinsically enjoy at the moment and what work you would like to continue doing in the near future. For example, you might enjoy one of the following more than others: question formulation (e.g., problem definition), data acquisition (e.g., measure development), data management (e.g., data wrangling), data analysis (e.g., model building), or storytelling (e.g., data visualization or dashboard creation). Ideally, you will want to create a portfolio that highlights your strengths in each of the areas you wish to emphasize. With that being said, its also good to show at least some level of proficiency in non-emphasized areas as well. Figure out what knowledge, skills, abilities your ideal employer wants  or what it needs but doesnt yet realize it needs. At the very least, youll want to figure out what knowledge, skills, or abilities the employer expects to see in ideal job candidates. You might be able to glean some of these expectations from the job posting itself or from the organizations website. In addition, you find examples of expected knowledge, skills, and abilities in published white papers, peer-reviewed publications, or the LinkedIn profiles of other individuals who hold similar job at that organization. Even better, try to attain direct insider information from those who currently work at that organization. A portfolio also provides you with an opportunity to showcase tools, applications, knowledge, skills, or abilities that the organization likely needs to attain strategic objectives but does not yet realize or recognize. This might be especially relevant in situations in which you suspect that the organizations data analytics capabilities are less mature or when you believe you might be overqualified. Decide whether you want your portfolio to teach, showcase, or do both. Both teaching and showcasing can be useful ways to illustrate your knowledge, skills, and abilities. If you go the teaching route, your portfolio project will likely take the form of a tutorial. A well thought out tutorial can be a good method for showing that you understand the concepts and technical applications well enough to teach another person how to do the same. If you go the showcasing route, your portfolio project will focus less on teaching and more on highlighting what you are capable of doing. If your portfolio consists of multiple projects, you might find it worthwhile to include at least one teaching project and at least one showcasing project. Find an appropriate dataset. There are many different places in which you can find toy datasets or public datasets that are free to use. Though, youll want to be absolutely certain that the dataset youve chosen is free to use and publicly available. That is, you do not want to use proprietary or private data for your portfolio. Examples of repositories for data sets include: My GitHub repository called R Tutorial Data Files, which are all datasets that Ive simulated using R; This GitHub repository called Awesome Public Datasets; Kaggle; This Stanford University website has links to a variety of public datasets. If you cant find an appropriate dataset, you can always simulate one using R, which is more involved and complicated. Rich Landers (University of Minnesota) website includes this tool, which can be used to simulate simple datasets. Create an immersive environment for the intended audience. It can be tempting to just manage, analyze, and visualize a bunch of data without providing any context or backstory. Because context matters, I recommend creating an immersive environment that orients the intended audience to the context, including variable definitions. This can be dibe via writing, audio, or video. By establishing an immersive environment, the problem youre attempting to solve or the question youre attempting to answer will be more meaningful  and ideally will illustrate a clear purpose (e.g., helping an organization to attain a strategic objective). Articulate a clear problem (question) that can be solved (answered) with the available data. Its important to make sure that your portfolio project is problem- or question-focused. That is, use your portfolio to show how you can go from a problem definition (or question formulation) to a solution (or answer). In doing so, you can demonstrate your ability to conduct meaningful and purposeful data analytics. For a refresher on problem definition and question formulation, check out this chapter. Write (and annotate) your code with an emphasis on clarity. Your code provides a behind-the-scenes glimpse at your decision-making processes so make sure its clear and understandable. For positions that expect candidates to have less advanced programming skills, you can focus on writing code that is understandable to a broad audience and that illustrates you understand foundational concepts, operations, and techniques. It might not be the most efficient or elegant code, but it should be clear and free of errors. For positions that expect candidates to have more advanced programming skills, youll want to focus on writing code that is stable, reproducible, efficient, and elegant. Regarding efficiency and elegance, youll want to consider how long it takes your code to run and how this might be more consequential at scale, and ideally, youll want to write less code when possible. Be sure to include clear annotations that help explain your many decisions. If your portfolio project includes data analysis or visualization, make sure that youve chosen an appropriate analysis or visualization given the problem/question and available data. You can run all sorts of analyses and attain results  even when the analyses are not appropriate or meaningful given the problem/question and/or available data. If you are performing statistical analysis of the data, youll want to make sure that the statistical assumptions for a particular analysis have been reasonably satisfied; better yet, demonstrate in your portfolio how you tested relevant statistical assumptions. All else being equal, its best to choose the simplest and most easily interpretable analysis. For example, if youre interested in comparing the means for two independent samples, then there are statistical equivalent ways to analyze the data: independent-samples t-test, one-way analysis of variance, simple linear regression, and structural equation modeling. In this example, the independent-samples t-test will likely be the simplest analysis to run and communicate given the goal of comparing the means for two independent samples. I like to think of it using this metaphor: If your objective is to get some almond milk from your corner grocery story, you could walk (independent-samples t-test) or you could drive a Ferrari (structural equation modeling); both will get you there, but one is less resource intensive. Focus on good storytelling. Make sure your portfolio project tells an accurate yet compelling story. While writing sophisticated code or running advanced analyses may impressive some, at the end of the day, your portfolio project should tell a good (and hopefully memorable) story. For a review of classic storytelling principles, check out this chapter. Solicit friendly feedback prior to sharing your portfolio with an employer. Everyone makes errors, and its better to have a friend or colleague catch those errors prior to sharing the portfolio with an employer. Friends or colleagues can also provide feedback on how intuitive, comprehensible, or appropriate your portfolio is. So who should you ask for feedback? Ideally, you should seek feedback from individuals who have greater expertise in the area than you to make sure youve done everything correctly or appropriately, but it can also be helpful to seek feedback from people who you expect will have a similar level of expertise as the intended audience, as this latter group may help you create a portfolio that is not overly complex for the given audience. Select a platform that will allow you to share your portfolio. There are many different ways in you can share a portfolio, and its important to select a platform that a hiring manager will be familiar with (or at least figure out how to intuitively access) and that has at least some relevance to the position to which youre applying. One of the simplest ways to share a platform would be to share a static document (e.g., PDF). A static document may not impress some hiring managers, but if the position to which youre applying will involve writing a lot of technical reports, then a static document might be appropriate. Further, in some instances, part or all of your portfolio might consist of published articles  although this might be most relevant for more specialized research roles. Further, in RStudio, you can create an RMarkdown file to embed code; in fact, there are packages called blogdown and bookdown that allow you to assemble multiple RMarkdown files into a coherent structure. Other platforms include: YouTube video, a playlist, or an entire channel containing multiple videos; GitHub repository; Medium article; Personal website; Tableau or PowerBI; Shiny web application. Update your portfolio regularly. Youll want your portfolio to feel fresh and contemporary, which means that youll need to update your portfolio with some regularity (e.g., once or twice a year). In addition, be sure to check periodically in which some of the packages/functions youre using have been updated, which could affect how your code works. One way to work around this is to use a dependency management package like packrat. Although using a dependency management package will help to ensure that your code works properly over time, it doesnt guard against stale-looking code that references deprecated functions. "],["literature-search-review.html", "Chapter 53 Conducting a Literature Search &amp; Review", " Chapter 53 Conducting a Literature Search &amp; Review Link to Video Tutorial: https://youtu.be/nqv9CkEMDQY Link to Video Tutorial: https://youtu.be/kSVcSKpXhC4 Link to Video Tutorial: https://youtu.be/v5EMEnlUhac "],["references.html", "References", " References Aguinis, Herman, and Charles A Pierce. 1998. Heterogeneity of Error Variance and the Assessment of Moderating Effects of Categorical Variables: A Conceptual Review. Organizational Research Methods 1 (3): 296314. Ajzen, Icek. 1991. The Theory of Planned Behavior. Organizational Behavior and Human Decision Processes 50: 179211. Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2021. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown. Angoff, W H. 1971. Scales, Norms, and Equivalent Scores. In Educational Measurement, edited by R L Thorndike, 508600. Washington, DC: American Council on Education. Bache, Stefan Milton, and Hadley Wickham. 2020. Magrittr: A Forward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr. Banks, George C, Haley M Woznyj, Ryan S Wesslen, and Roxanne L Ross. 2018. A Review of Best Practice Recommendations for Text Analysis in R (and a User-Friendly App). Journal of Business and Psychology 33 (4): 44559. Basadur, Min, George B Graen, and Terri A Scandura. 1986. Training Effects on Attitudes Toward Divergent Thinking Among Manufacturing Engineers. Journal of Applied Psychology 71: 61217. Basadur, Min, Mark A Runco, and Luis A Vega. 2000. Understanding How Creative Thinking Skills, Attitudes and Behaviors Work Together: A Causal Process Model. Journal of Creative Behavior 34: 77100. Bauer, Talya N., Berrin Erdogan, David E. Caughlin, and Donald M. Truxillo. 2019. Human Resource Management: People, Data, and Analytics. Thousand Oaks, California: Sage. . 2020. Fundamentals of Human Resource Management: People, Data, and Analytics. Thousand Oaks, California: Sage. Bennett, W L, and M S Feldman. 1981. Reconstructing Reality in the Courtroom: Justice and Judgment in American Culture. New Brunswick, New Jersey: Rutgers University Press. Bollen, Kenneth A, and Robert W Jackman. 1985. Regression Diagnostics: An Expository Treatment of Outliers and Influential Cases. Sociological Methods &amp; Research 13 (4): 51042. Bonabeau, Eric. 2002. Agent-Based Modeling: Methods and Techniques for Simulating Human Systems. Proceedings of the National Academy of Sciences 99 (suppl 3): 728087. https://doi.org/10.1073/pnas.082080899. Browne, MW. 1975. Predictive Validity of a Linear Regression Equation. British Journal of Mathematical and Statistical Psychology 28 (1): 7987. Cacioppo, John T, Stephanie Cacioppo, and Richard E Petty. 2018. The Neuroscience of Persuasion: A Review with an Emphasis on Issues and Opportunities. Social Neuroscience 13 (2): 12972. Calder, Muffy, Claire Craig, Dave Culley, Richard de Cani, Christl A Donnelly, Rowan Douglas, Bruce Edmonds, et al. 2018. Computational Modelling for Decision-Making: Where, Why, What, Who and How. Royal Society Open Science 5 (6): 172096. Cascio, WF, and H Aguinis. 2005. Applied Psychology in Human Resource Management (6th Ed.). Upper Saddle River, NJ: Pearson Prentice Hall. Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny. Chevallier, Arnaud. 2016. Strategic Thinking in Complex Problem Solving. New York, New York: Oxford University Press. Cleary, T Anne. 1968. Test Bias: Prediction of Grades of Black and White Students in Integrated Colleges. Journal of Educational Measurement 5 (2): 11524. Cohen, Jacob. 1992. A Power Primer. Psychological Bulletin 112 (1): 15559. Conn, Charles, and Robert McLean. 2018. Bulletproof Problem Solving: The One Skill That Changes Everything. Hoboken, New Jersey: Wiley. Deloitte. 2018. Global Human Capital Trends Report 2018. Westlake, Texas: Deloitte University Press. Denning, S. 1986. Evidence Evaluation in Complex Decision Making. Journal of Personality and Social Psychology 51 (2): 24258. . 2006. Effective Storytelling: Strategic Business Narrative Techniques. Strategy &amp; Leadership 34 (1): 4248. Devine, D J. 2012. Jury Decision Making: The State of the Science. New York, New York: New York University Press. Dictionary.com. n.d. Story. https://www.dictionary.com/browse/story. Dunleavy, Eric, Scott Morris, and Elizabeth Howard. 2015. Measuring Adverse Impact in Employee Selection Decisions. In Practitioners Guide to Legal Issues in Organizations, 126. Springer. Ezekiel, Mordecai. 1930. Methods of Correlation Analysis. Federal Contract Compliance Programs, Office of. 1993. Federal Contract Compliance Manual (SUDOC l 36.8: C 76/993). Washington, DC: U. S. Department of Labor, Employment Standards Administration. Feltz, Carol J, and G Edward Miller. 1996. An Asymptotic Test for the Equality of Coefficients of Variation from k Populations. Statistics in Medicine 15 (6): 64758. Finch, David M, Bryan D Edwards, and J Craig Wallace. 2009. Multistage Selection Strategies: Simulating the Effects on Adverse Impact and Expected Performance for Various Predictor Combinations. Journal of Applied Psychology 94 (2): 318. Firke, Sam. 2021. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://github.com/sfirke/janitor. Gan, Sie H. 2018. How to Design a Spam Filtering System with Machine Learning Algorithm. Available at http:// https://towardsdatascience.com/email-spam-detection-1-2-b0e06a5c0472 (30/04/21). Gerbing, David, The School of Business, and Portland State University. 2021. lessR: Less Code, More Results. https://CRAN.R-project.org/package=lessR. Guzella, Thiago S, and Walmir M Caminhas. 2009. A Review of Machine Learning Approaches to Spam Filtering. Expert Systems with Applications 36 (7): 1020622. Hosmer, D W, and S Lemeshow. 2000. Applied Logistic Regression. New York, NY: John Wiley &amp; Sons. James, G, T Witten D abd Hastie, and R Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. New York, NY: Springer. Jordan, Michael I, and Tom M Mitchell. 2015. Machine Learning: Trends, Perspectives, and Prospects. Science 349 (6245): 25560. Knaflic, Cole N. 2015. Storytelling with Data: A Data Visualization Guide for Business Professionals. Hoboken, New Jersey: Wiley. . 2020. Storytelling with Data: Lets Practice! Hoboken, New Jersey: Wiley. Landis, J R, and G G Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics 33: 15974. Lewontin, Richard C. 1966. On the Measurement of Relative Variability. Systematic Zoology 15 (2): 14142. LiteraryTerms.net. n.d. Story. https://literaryterms.net/story. Merriam-Webster.com. n.d. Story. https://www.merriam-webster.com/dictionary/story. Miller, Edward G. 1991. Asymptotic Test Statistics for Coefficients of Variation. Communications in Statistics-Theory and Methods 20 (10): 335163. Morris, Scott B. 2001. Sample Size Required for Adverse Impact Analysis. Applied HRM Research 6 (1-2): 1332. Morris, Scott B, and Russell E Lobsenz. 2000. Significance Tests and Confidence Intervals for the Adverse Impact Ratio. Personnel Psychology 53 (1): 89111. Mueller, Lorin, Dwayne Norris, Scott Oppler, and SM McPhail. 2007. Implementation Based on Alternate Validation Procedures: Ranking, Cut Scores, Banding, and Compensatory Models. Alternative Validation Strategies: Developing New and Leveraging Existing Validity Evidence, 349405. Muthn, B O, and L K Muthn. 1998-2018. Mplus Version 8.3. Los Angeles, California: Muthn &amp; Muthn. Nagelkerke, N J D. 1991. A Note on a General Definition of the Coefficient of Determination. Biometrika 78 (3): 69192. Osborne, J W. 2015. Best Practices in Logistic Regression. Thousand Oaks, CA: Sage. Oswald, Fred S, and Dan J Putka. 2016. Statistical Methods for Big Data: A Scenic Tour. In Big Data at Work: The Data Science Revolution and Organizational Psychology, edited by Scott Tondandel, Eden B. King, and Jose M. Cortina, 4363. New York, NY: Routledge. Pennington, N, and R Hastie. 1993. The Story Model for Juror Decision Making. In Inside the Juror: The Psychology of Juror Decision Making, edited by R Hastie, 192224. Cambridge, UK: Cambridge University Press. Pfeffer, Jeffrey. 1998. Seven Practices of Successful Organizations. California Management Review 40: 96124. Pfeffer, Jeffrey, and Robert I Sutton. 2000. The Knowing-Doing Gap: How Smart Companies Turn Knowledge into Action. Boston, MA: Harvard business press. Price, Paul C, Rajiv S Jhangiani, I-Chant A Chiang, Dana C Leighton, and Carrie Cuttler. 2017. Research Methods in Psychology (3rd American Ed.). Montreal, Canada: Pressbooks. https://opentext.wsu.edu/carriecuttler/. R Core Team. 2021. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Raju, Nambury S, Reyhan Bilgic, Jack E Edwards, and Paul F Fleer. 1997. Methodology Review: Estimation of Population Validity and Cross-Validity, and the Use of Equal Weights in Prediction. Applied Psychological Measurement 21 (4): 291305. Reiter-Palmon, Roni, and Jody J Illies. 2004. Leadership and Creativity: Understanding Leadership from a Creative Problem-Solving Perspective. Leadership Quarterly 15: 5577. Revelle, William. 2021. Psych: Procedures for Psychological, Psychometric, and Personality Research. https://personality-project.org/r/psych/ https://personality-project.org/r/psych-manual.pdf. Rosseel, Yves. 2012. lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software 48 (2): 136. https://www.jstatsoft.org/v48/i02/. RStudio Team. 2020. RStudio: Integrated Development Environment for R. Boston, MA: RStudio, PBC. http://www.rstudio.com/. Schultz, K S, and D J Whitney. 2005. Measurement Theory in Action: Case Studies and Exercises. Thousand Oaks, CA: Sage. Smith, Eliot R, and Frederica R Conrey. 2007. Agent-Based Modeling: A New Approach for Theory Building in Social Psychology. Personality and Social Psychology Review 11 (1): 87104. Society for Industrial &amp; Organizational Psychology. 2018. Principles for the Validation and Use of Personnel Selection Procedures (5th Ed.). Bowling Green, OH: Pearson Prentice Hall. Stanley, David. 2021. apaTables: Create American Psychological Association (APA) Style Tables. https://CRAN.R-project.org/package=apaTables. Stone-Romero, Eugene F, George M Alliger, and Herman Aguinis. 1994. Type II Error Problems in the Use of Moderated Multiple Regression for the Detection of Moderating Effects of Dichotomous Variables. Journal of Management 20 (1): 16778. Strauss, Anselm, and Juliet M Corbin. 1997. Grounded Theory in Practice. Sage. Wherry, Robert J. 1931. A New Formula for Predicting the Shrinkage of the Coefficient of Multiple Correlation. The Annals of Mathematical Statistics 2 (4): 44057. Wickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org. . 2021a. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr. . 2021b. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy DAgostino McGowan, Romain Franois, Garrett Grolemund, et al. 2019. Welcome to the tidyverse. Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Wickham, Hadley, and Jennifer Bryan. 2019. Readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl. Wickham, Hadley, Romain Franois, Lionel Henry, and Kirill Mller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. Sebastopol, California: OReilly Media, Inc. https://r4ds.had.co.nz/n. Wickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr. Xie, Yihui. 2014. Knitr: A Comprehensive Tool for Reproducible Research in R. In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595. . 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/. . 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown. . 2020. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. . 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/. Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown. "]]
