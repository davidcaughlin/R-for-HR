<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 34 Estimating Incremental Validity Using Multiple Linear Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION</title>
  <meta name="description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 34 Estimating Incremental Validity Using Multiple Linear Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://rforhr.com" />
  <meta property="og:image" content="https://rforhr.comcover.png" />
  <meta property="og:description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="github-repo" content="davidcaughlin/R-for-HR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 34 Estimating Incremental Validity Using Multiple Linear Regression | R for HR: An Introduction to Human Resource Analytics Using R BOOK UNDER CONSTRUCTION" />
  
  <meta name="twitter:description" content="Human resource (HR) analytics is a growing area of HR manage, and the purpose of this book is to show how the R programming language can be used as tool to manage, analyze, and visualize HR data in order to derive insights and to inform decision making. [NOTE: This book is currently under construction.]" />
  <meta name="twitter:image" content="https://rforhr.comcover.png" />

<meta name="author" content="David E. Caughlin" />


<meta name="date" content="2021-12-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="predictingcriterionscores.html"/>
<link rel="next" href="compensatory.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for HR</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#hragrowth"><i class="fa fa-check"></i><b>0.1</b> Growth of HR Analytics</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#skills-gap"><i class="fa fa-check"></i><b>0.2</b> Skills Gap</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#hraplc"><i class="fa fa-check"></i><b>0.3</b> Project Life Cycle Perspective</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#overview-of-hris-hr-analytics"><i class="fa fa-check"></i><b>0.4</b> Overview of HRIS &amp; HR Analytics</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#philosophy"><i class="fa fa-check"></i><b>0.5</b> My Philosophy for This Book</a>
<ul>
<li class="chapter" data-level="0.5.1" data-path="index.html"><a href="index.html#rationalerpref"><i class="fa fa-check"></i><b>0.5.1</b> Rationale for Using R</a></li>
<li class="chapter" data-level="0.5.2" data-path="index.html"><a href="index.html#audiencepref"><i class="fa fa-check"></i><b>0.5.2</b> Audience</a></li>
<li class="chapter" data-level="0.5.3" data-path="index.html"><a href="index.html#structurepref"><i class="fa fa-check"></i><b>0.5.3</b> Structure</a></li>
</ul></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#aboutauthor"><i class="fa fa-check"></i><b>0.6</b> About the Author</a></li>
<li class="chapter" data-level="0.7" data-path="index.html"><a href="index.html#acknowpref"><i class="fa fa-check"></i><b>0.7</b> Acknowledgements</a></li>
</ul></li>
<li class="part"><span><b>I HR Analytics Project Life Cycle</b></span></li>
<li class="chapter" data-level="1" data-path="overviewhraplc.html"><a href="overviewhraplc.html"><i class="fa fa-check"></i><b>1</b> Overview of HR Analytics Project Life Cycle</a></li>
<li class="chapter" data-level="2" data-path="questionformulation.html"><a href="questionformulation.html"><i class="fa fa-check"></i><b>2</b> Question Formulation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="questionformulation.html"><a href="questionformulation.html#adoptstrategicmindset"><i class="fa fa-check"></i><b>2.1</b> Adopting a Strategic Mindset</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="questionformulation.html"><a href="questionformulation.html#strategy"><i class="fa fa-check"></i><b>2.1.1</b> Strategy</a></li>
<li class="chapter" data-level="2.1.2" data-path="questionformulation.html"><a href="questionformulation.html#strategyformulation"><i class="fa fa-check"></i><b>2.1.2</b> Strategy Formulation</a></li>
<li class="chapter" data-level="2.1.3" data-path="questionformulation.html"><a href="questionformulation.html#strategyimplementation"><i class="fa fa-check"></i><b>2.1.3</b> Strategy Implementation</a></li>
<li class="chapter" data-level="2.1.4" data-path="questionformulation.html"><a href="questionformulation.html#strategichrm"><i class="fa fa-check"></i><b>2.1.4</b> Strategic Human Resource Management</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="questionformulation.html"><a href="questionformulation.html#defining-problems-formulating-questions"><i class="fa fa-check"></i><b>2.2</b> Defining Problems &amp; Formulating Questions</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="questionformulation.html"><a href="questionformulation.html#definingaproblem"><i class="fa fa-check"></i><b>2.2.1</b> Defining a Problem</a></li>
<li class="chapter" data-level="2.2.2" data-path="questionformulation.html"><a href="questionformulation.html#formulatingaquestion"><i class="fa fa-check"></i><b>2.2.2</b> Formulating a Question</a></li>
<li class="chapter" data-level="2.2.3" data-path="questionformulation.html"><a href="questionformulation.html#divergentconvergentthinking"><i class="fa fa-check"></i><b>2.2.3</b> Thinking Divergently &amp; Convergently</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="questionformulation.html"><a href="questionformulation.html#summary"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="dataacquisition.html"><a href="dataacquisition.html"><i class="fa fa-check"></i><b>3</b> Data Acquisition</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dataacquisition.html"><a href="dataacquisition.html#employee-surveys"><i class="fa fa-check"></i><b>3.1</b> Employee Surveys</a></li>
<li class="chapter" data-level="3.2" data-path="dataacquisition.html"><a href="dataacquisition.html#rating-forms"><i class="fa fa-check"></i><b>3.2</b> Rating Forms</a></li>
<li class="chapter" data-level="3.3" data-path="dataacquisition.html"><a href="dataacquisition.html#surveillance-monitoring"><i class="fa fa-check"></i><b>3.3</b> Surveillance &amp; Monitoring</a></li>
<li class="chapter" data-level="3.4" data-path="dataacquisition.html"><a href="dataacquisition.html#database-queries"><i class="fa fa-check"></i><b>3.4</b> Database Queries</a></li>
<li class="chapter" data-level="3.5" data-path="dataacquisition.html"><a href="dataacquisition.html#scraping"><i class="fa fa-check"></i><b>3.5</b> Scraping</a></li>
<li class="chapter" data-level="3.6" data-path="dataacquisition.html"><a href="dataacquisition.html#summary-1"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datamanagement.html"><a href="datamanagement.html"><i class="fa fa-check"></i><b>4</b> Data Management</a>
<ul>
<li class="chapter" data-level="4.1" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_clean"><i class="fa fa-check"></i><b>4.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="4.2" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_manipulate"><i class="fa fa-check"></i><b>4.2</b> Data Manipulation &amp; Structuring</a></li>
<li class="chapter" data-level="4.3" data-path="datamanagement.html"><a href="datamanagement.html#datamanage_tools"><i class="fa fa-check"></i><b>4.3</b> Common Data-Management Tools</a></li>
<li class="chapter" data-level="4.4" data-path="datamanagement.html"><a href="datamanagement.html#summary-2"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dataanalysis.html"><a href="dataanalysis.html"><i class="fa fa-check"></i><b>5</b> Data Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="dataanalysis.html"><a href="dataanalysis.html#dataanalysis_toolstechniques"><i class="fa fa-check"></i><b>5.1</b> Tools &amp; Techniques</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="dataanalysis.html"><a href="dataanalysis.html#mathematics"><i class="fa fa-check"></i><b>5.1.1</b> Mathematics</a></li>
<li class="chapter" data-level="5.1.2" data-path="dataanalysis.html"><a href="dataanalysis.html#statistics"><i class="fa fa-check"></i><b>5.1.2</b> Statistics</a></li>
<li class="chapter" data-level="5.1.3" data-path="dataanalysis.html"><a href="dataanalysis.html#machinelearning"><i class="fa fa-check"></i><b>5.1.3</b> Machine Learning</a></li>
<li class="chapter" data-level="5.1.4" data-path="dataanalysis.html"><a href="dataanalysis.html#computationalmodelsimulation"><i class="fa fa-check"></i><b>5.1.4</b> Computational Modeling &amp; Simulations</a></li>
<li class="chapter" data-level="5.1.5" data-path="dataanalysis.html"><a href="dataanalysis.html#textqualitativeanalyses"><i class="fa fa-check"></i><b>5.1.5</b> Text Analyses &amp; Qualitative Analyses</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="dataanalysis.html"><a href="dataanalysis.html#continuum_dataanalytics"><i class="fa fa-check"></i><b>5.2</b> Continuum of Data Analytics</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="dataanalysis.html"><a href="dataanalysis.html#descriptive_analytics"><i class="fa fa-check"></i><b>5.2.1</b> Descriptive Analytics</a></li>
<li class="chapter" data-level="5.2.2" data-path="dataanalysis.html"><a href="dataanalysis.html#predictish_analytics"><i class="fa fa-check"></i><b>5.2.2</b> Predict-ish Analytics</a></li>
<li class="chapter" data-level="5.2.3" data-path="dataanalysis.html"><a href="dataanalysis.html#predictive_analytics"><i class="fa fa-check"></i><b>5.2.3</b> Predictive Analytics</a></li>
<li class="chapter" data-level="5.2.4" data-path="dataanalysis.html"><a href="dataanalysis.html#prescriptive_analytics"><i class="fa fa-check"></i><b>5.2.4</b> Prescriptive Analytics</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="dataanalysis.html"><a href="dataanalysis.html#summary-3"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html"><i class="fa fa-check"></i><b>6</b> Data Interpretation &amp; Storytelling</a>
<ul>
<li class="chapter" data-level="6.1" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#datainterpretation"><i class="fa fa-check"></i><b>6.1</b> Data Interpretation</a></li>
<li class="chapter" data-level="6.2" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#storytelling"><i class="fa fa-check"></i><b>6.2</b> Storytelling</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#structure"><i class="fa fa-check"></i><b>6.2.1</b> Structure</a></li>
<li class="chapter" data-level="6.2.2" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#clarity-parsimony"><i class="fa fa-check"></i><b>6.2.2</b> Clarity &amp; Parsimony</a></li>
<li class="chapter" data-level="6.2.3" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#influence-persuasion"><i class="fa fa-check"></i><b>6.2.3</b> Influence &amp; Persuasion</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#storytelling_withdata"><i class="fa fa-check"></i><b>6.3</b> Storytelling with Data</a></li>
<li class="chapter" data-level="6.4" data-path="datainterpretationstorytelling.html"><a href="datainterpretationstorytelling.html#summary-4"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="deploymentimplementation.html"><a href="deploymentimplementation.html"><i class="fa fa-check"></i><b>7</b> Deployment &amp; Implementation</a></li>
<li class="part"><span><b>II Introduction to R</b></span></li>
<li class="chapter" data-level="8" data-path="overviewR.html"><a href="overviewR.html"><i class="fa fa-check"></i><b>8</b> Overview of R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="8.1" data-path="overviewR.html"><a href="overviewR.html#R_overview"><i class="fa fa-check"></i><b>8.1</b> R Programming Language</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="overviewR.html"><a href="overviewR.html#R_what"><i class="fa fa-check"></i><b>8.1.1</b> What Is R?</a></li>
<li class="chapter" data-level="8.1.2" data-path="overviewR.html"><a href="overviewR.html#R_why"><i class="fa fa-check"></i><b>8.1.2</b> Why Use R?</a></li>
<li class="chapter" data-level="8.1.3" data-path="overviewR.html"><a href="overviewR.html#R_who"><i class="fa fa-check"></i><b>8.1.3</b> Who Uses R?</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="overviewR.html"><a href="overviewR.html#RStudio_overview"><i class="fa fa-check"></i><b>8.2</b> RStudio</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="overviewR.html"><a href="overviewR.html#RStudio_what"><i class="fa fa-check"></i><b>8.2.1</b> What is RStudio?</a></li>
<li class="chapter" data-level="8.2.2" data-path="overviewR.html"><a href="overviewR.html#RStudio_why"><i class="fa fa-check"></i><b>8.2.2</b> Why RStudio?</a></li>
<li class="chapter" data-level="8.2.3" data-path="overviewR.html"><a href="overviewR.html#RStudio_who"><i class="fa fa-check"></i><b>8.2.3</b> Who Uses RStudio?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="overviewR.html"><a href="overviewR.html#packages_overview"><i class="fa fa-check"></i><b>8.3</b> Packages</a></li>
<li class="chapter" data-level="8.4" data-path="overviewR.html"><a href="overviewR.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="install.html"><a href="install.html"><i class="fa fa-check"></i><b>9</b> Installing R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="9.1" data-path="install.html"><a href="install.html#installR"><i class="fa fa-check"></i><b>9.1</b> Downloading &amp; Installing R</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="install.html"><a href="install.html#for-windows-operation-systems"><i class="fa fa-check"></i><b>9.1.1</b> For Windows Operation Systems</a></li>
<li class="chapter" data-level="9.1.2" data-path="install.html"><a href="install.html#for-mac-operating-systems"><i class="fa fa-check"></i><b>9.1.2</b> For Mac Operating Systems</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="install.html"><a href="install.html#installRStudio"><i class="fa fa-check"></i><b>9.2</b> Downloading &amp; Installing RStudio</a></li>
<li class="chapter" data-level="9.3" data-path="install.html"><a href="install.html#summary-6"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gettingstarted.html"><a href="gettingstarted.html"><i class="fa fa-check"></i><b>10</b> Getting Started with R &amp; RStudio</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gettingstarted.html"><a href="gettingstarted.html#orientation-to-rstudio"><i class="fa fa-check"></i><b>10.1</b> Orientation to RStudio</a></li>
<li class="chapter" data-level="10.2" data-path="gettingstarted.html"><a href="gettingstarted.html#createRscript"><i class="fa fa-check"></i><b>10.2</b> Creating &amp; Saving an R Script</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="gettingstarted.html"><a href="gettingstarted.html#creating-a-new-r-script"><i class="fa fa-check"></i><b>10.2.1</b> Creating a New R Script</a></li>
<li class="chapter" data-level="10.2.2" data-path="gettingstarted.html"><a href="gettingstarted.html#using-an-r-script"><i class="fa fa-check"></i><b>10.2.2</b> Using an R Script</a></li>
<li class="chapter" data-level="10.2.3" data-path="gettingstarted.html"><a href="gettingstarted.html#saving-an-r-script"><i class="fa fa-check"></i><b>10.2.3</b> Saving an R Script</a></li>
<li class="chapter" data-level="10.2.4" data-path="gettingstarted.html"><a href="gettingstarted.html#opening-a-saved-r-script"><i class="fa fa-check"></i><b>10.2.4</b> Opening a Saved R Script</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="gettingstarted.html"><a href="gettingstarted.html#creating-an-rstudio-project"><i class="fa fa-check"></i><b>10.3</b> Creating an RStudio Project</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="gettingstarted.html"><a href="gettingstarted.html#creating-a-new-rstudio-project"><i class="fa fa-check"></i><b>10.3.1</b> Creating a New RStudio Project</a></li>
<li class="chapter" data-level="10.3.2" data-path="gettingstarted.html"><a href="gettingstarted.html#opening-an-existing-rstudio-project"><i class="fa fa-check"></i><b>10.3.2</b> Opening an Existing RStudio Project</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="gettingstarted.html"><a href="gettingstarted.html#orientation-to-written-tutorials"><i class="fa fa-check"></i><b>10.4</b> Orientation to Written Tutorials</a></li>
<li class="chapter" data-level="10.5" data-path="gettingstarted.html"><a href="gettingstarted.html#summary-7"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="gentleintro.html"><a href="gentleintro.html"><i class="fa fa-check"></i><b>11</b> Basic Features and Operations of the R Language</a>
<ul>
<li class="chapter" data-level="11.1" data-path="gentleintro.html"><a href="gentleintro.html#r_as_calculator"><i class="fa fa-check"></i><b>11.1</b> R as a Calculator</a></li>
<li class="chapter" data-level="11.2" data-path="gentleintro.html"><a href="gentleintro.html#functions"><i class="fa fa-check"></i><b>11.2</b> Functions</a></li>
<li class="chapter" data-level="11.3" data-path="gentleintro.html"><a href="gentleintro.html#packages"><i class="fa fa-check"></i><b>11.3</b> Packages</a></li>
<li class="chapter" data-level="11.4" data-path="gentleintro.html"><a href="gentleintro.html#variableassignment"><i class="fa fa-check"></i><b>11.4</b> Variable Assignment</a></li>
<li class="chapter" data-level="11.5" data-path="gentleintro.html"><a href="gentleintro.html#typesofdata"><i class="fa fa-check"></i><b>11.5</b> Types of Data</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="gentleintro.html"><a href="gentleintro.html#numeric-data"><i class="fa fa-check"></i><b>11.5.1</b> <code>numeric</code> Data</a></li>
<li class="chapter" data-level="11.5.2" data-path="gentleintro.html"><a href="gentleintro.html#character-data"><i class="fa fa-check"></i><b>11.5.2</b> <code>character</code> Data</a></li>
<li class="chapter" data-level="11.5.3" data-path="gentleintro.html"><a href="gentleintro.html#date-data"><i class="fa fa-check"></i><b>11.5.3</b> <code>Date</code> Data</a></li>
<li class="chapter" data-level="11.5.4" data-path="gentleintro.html"><a href="gentleintro.html#logical-data"><i class="fa fa-check"></i><b>11.5.4</b> <code>logical</code> Data</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="gentleintro.html"><a href="gentleintro.html#vectors"><i class="fa fa-check"></i><b>11.6</b> Vectors</a></li>
<li class="chapter" data-level="11.7" data-path="gentleintro.html"><a href="gentleintro.html#lists"><i class="fa fa-check"></i><b>11.7</b> Lists</a></li>
<li class="chapter" data-level="11.8" data-path="gentleintro.html"><a href="gentleintro.html#dataframes"><i class="fa fa-check"></i><b>11.8</b> Data Frames</a></li>
<li class="chapter" data-level="11.9" data-path="gentleintro.html"><a href="gentleintro.html#annotate"><i class="fa fa-check"></i><b>11.9</b> Annotations</a></li>
<li class="chapter" data-level="11.10" data-path="gentleintro.html"><a href="gentleintro.html#summary-8"><i class="fa fa-check"></i><b>11.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="setwd.html"><a href="setwd.html"><i class="fa fa-check"></i><b>12</b> Setting a Working Directory</a>
<ul>
<li class="chapter" data-level="12.0.1" data-path="setwd.html"><a href="setwd.html#identify-the-current-working-directory"><i class="fa fa-check"></i><b>12.0.1</b> Identify the Current Working Directory</a></li>
<li class="chapter" data-level="12.0.2" data-path="setwd.html"><a href="setwd.html#set-a-new-working-directory"><i class="fa fa-check"></i><b>12.0.2</b> Set a New Working Directory</a></li>
<li class="chapter" data-level="12.1" data-path="setwd.html"><a href="setwd.html#summary-9"><i class="fa fa-check"></i><b>12.1</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>III Data Acquisition &amp; Management</b></span></li>
<li class="chapter" data-level="13" data-path="read.html"><a href="read.html"><i class="fa fa-check"></i><b>13</b> Reading Data into R</a>
<ul>
<li class="chapter" data-level="13.1" data-path="read.html"><a href="read.html#readcsv"><i class="fa fa-check"></i><b>13.1</b> Read a .csv File</a></li>
<li class="chapter" data-level="13.2" data-path="read.html"><a href="read.html#readxlsx"><i class="fa fa-check"></i><b>13.2</b> Read a .xlsx File</a></li>
<li class="chapter" data-level="13.3" data-path="read.html"><a href="read.html#read_summary"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
<li class="chapter" data-level="13.4" data-path="read.html"><a href="read.html#read_supplement"><i class="fa fa-check"></i><b>13.4</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="read.html"><a href="read.html#read_additionalfunctions"><i class="fa fa-check"></i><b>13.4.1</b> Additional Functions to Read a .csv File</a></li>
<li class="chapter" data-level="13.4.2" data-path="read.html"><a href="read.html#read_skiprows"><i class="fa fa-check"></i><b>13.4.2</b> Skip Rows of Data During Read</a></li>
<li class="chapter" data-level="13.4.3" data-path="read.html"><a href="read.html#listdatafiles"><i class="fa fa-check"></i><b>13.4.3</b> List Data File Names in Working Directory</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="addnames.html"><a href="addnames.html"><i class="fa fa-check"></i><b>14</b> Removing, Adding, &amp; Changing Variable Names</a>
<ul>
<li class="chapter" data-level="14.1" data-path="addnames.html"><a href="addnames.html#remove_variablenames"><i class="fa fa-check"></i><b>14.1</b> Remove Variable Names from a Data Frame Object</a></li>
<li class="chapter" data-level="14.2" data-path="addnames.html"><a href="addnames.html#add_variablenames"><i class="fa fa-check"></i><b>14.2</b> Add Variable Names to a Data Frame Object</a></li>
<li class="chapter" data-level="14.3" data-path="addnames.html"><a href="addnames.html#change_variablenames"><i class="fa fa-check"></i><b>14.3</b> Change Specific Variable Names in a Data Frame Object</a></li>
<li class="chapter" data-level="14.4" data-path="addnames.html"><a href="addnames.html#summary-10"><i class="fa fa-check"></i><b>14.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="write.html"><a href="write.html"><i class="fa fa-check"></i><b>15</b> Writing Data from R</a>
<ul>
<li class="chapter" data-level="15.1" data-path="write.html"><a href="write.html#write-data-frame-to-working-directory"><i class="fa fa-check"></i><b>15.1</b> Write Data Frame to Working Directory</a></li>
<li class="chapter" data-level="15.2" data-path="write.html"><a href="write.html#write-table-to-working-directory"><i class="fa fa-check"></i><b>15.2</b> Write Table to Working Directory</a></li>
<li class="chapter" data-level="15.3" data-path="write.html"><a href="write.html#summary-11"><i class="fa fa-check"></i><b>15.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="arrange.html"><a href="arrange.html"><i class="fa fa-check"></i><b>16</b> Arranging (Sorting) Data</a>
<ul>
<li class="chapter" data-level="16.1" data-path="arrange.html"><a href="arrange.html#arrangebyvalues"><i class="fa fa-check"></i><b>16.1</b> Arrange (Sort) Data</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="arrange.html"><a href="arrange.html#opt1_arrange_withpipe"><i class="fa fa-check"></i><b>16.1.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="16.1.2" data-path="arrange.html"><a href="arrange.html#opt1_arrange_withoutpipe"><i class="fa fa-check"></i><b>16.1.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="arrange.html"><a href="arrange.html#summary-12"><i class="fa fa-check"></i><b>16.2</b> Summary</a></li>
<li class="chapter" data-level="16.3" data-path="arrange.html"><a href="arrange.html#arrange_supplement"><i class="fa fa-check"></i><b>16.3</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="arrange.html"><a href="arrange.html#arrange_orderfunction"><i class="fa fa-check"></i><b>16.3.1</b> <code>order</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="join.html"><a href="join.html"><i class="fa fa-check"></i><b>17</b> Joining (Merging) Data</a>
<ul>
<li class="chapter" data-level="17.1" data-path="join.html"><a href="join.html#horizontaljoin"><i class="fa fa-check"></i><b>17.1</b> Horizontal Join (Merge)</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="join.html"><a href="join.html#opt2_join_withpipe"><i class="fa fa-check"></i><b>17.1.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="17.1.2" data-path="join.html"><a href="join.html#opt2_join_withoutpipe"><i class="fa fa-check"></i><b>17.1.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="join.html"><a href="join.html#verticaljoin"><i class="fa fa-check"></i><b>17.2</b> Vertical Join (Merge)</a></li>
<li class="chapter" data-level="17.3" data-path="join.html"><a href="join.html#summary-13"><i class="fa fa-check"></i><b>17.3</b> Summary</a></li>
<li class="chapter" data-level="17.4" data-path="join.html"><a href="join.html#join_supplement"><i class="fa fa-check"></i><b>17.4</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="join.html"><a href="join.html#join_mergefunction"><i class="fa fa-check"></i><b>17.4.1</b> <code>merge</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="filter.html"><a href="filter.html"><i class="fa fa-check"></i><b>18</b> Filtering Data</a>
<ul>
<li class="chapter" data-level="18.1" data-path="filter.html"><a href="filter.html#filter_cases"><i class="fa fa-check"></i><b>18.1</b> Filter Cases from Data Frame</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="filter.html"><a href="filter.html#with-pipes"><i class="fa fa-check"></i><b>18.1.1</b> <em>With</em> Pipes</a></li>
<li class="chapter" data-level="18.1.2" data-path="filter.html"><a href="filter.html#without-pipes"><i class="fa fa-check"></i><b>18.1.2</b> <em>Without</em> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="filter.html"><a href="filter.html#remove-single-variable-from-data-frame"><i class="fa fa-check"></i><b>18.2</b> Remove Single Variable from Data Frame</a></li>
<li class="chapter" data-level="18.3" data-path="filter.html"><a href="filter.html#select_multiplevariables"><i class="fa fa-check"></i><b>18.3</b> Select Multiple Variables from Data Frame</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="filter.html"><a href="filter.html#with-pipe"><i class="fa fa-check"></i><b>18.3.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="18.3.2" data-path="filter.html"><a href="filter.html#without-pipe"><i class="fa fa-check"></i><b>18.3.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="filter.html"><a href="filter.html#remove_multiplevariables"><i class="fa fa-check"></i><b>18.4</b> Remove Multiple Variables from Data Frame</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="filter.html"><a href="filter.html#with-pipe-1"><i class="fa fa-check"></i><b>18.4.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="18.4.2" data-path="filter.html"><a href="filter.html#without-pipe-1"><i class="fa fa-check"></i><b>18.4.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="filter.html"><a href="filter.html#summary-14"><i class="fa fa-check"></i><b>18.5</b> Summary</a></li>
<li class="chapter" data-level="18.6" data-path="filter.html"><a href="filter.html#filter_supplement"><i class="fa fa-check"></i><b>18.6</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="18.6.1" data-path="filter.html"><a href="filter.html#filter_subset_supplement"><i class="fa fa-check"></i><b>18.6.1</b> <code>subset</code> Function from Base R</a></li>
<li class="chapter" data-level="18.6.2" data-path="filter.html"><a href="filter.html#str_detect_supp"><i class="fa fa-check"></i><b>18.6.2</b> Filter by Pattern Contained within String</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="clean.html"><a href="clean.html"><i class="fa fa-check"></i><b>19</b> Cleaning Data</a>
<ul>
<li class="chapter" data-level="19.1" data-path="clean.html"><a href="clean.html#review-data"><i class="fa fa-check"></i><b>19.1</b> Review Data</a></li>
<li class="chapter" data-level="19.2" data-path="clean.html"><a href="clean.html#clean-data"><i class="fa fa-check"></i><b>19.2</b> Clean Data</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="clean.html"><a href="clean.html#replace-a-specific-value-for-a-specific-case"><i class="fa fa-check"></i><b>19.2.1</b> Replace a Specific Value for a Specific Case</a></li>
<li class="chapter" data-level="19.2.2" data-path="clean.html"><a href="clean.html#replace-a-specific-value-for-all-cases-with-a-particular-value"><i class="fa fa-check"></i><b>19.2.2</b> Replace a Specific Value for All Cases with a Particular Value</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="clean.html"><a href="clean.html#rename-variables"><i class="fa fa-check"></i><b>19.3</b> Rename Variables</a></li>
<li class="chapter" data-level="19.4" data-path="clean.html"><a href="clean.html#other-approaches-to-cleaning-data"><i class="fa fa-check"></i><b>19.4</b> Other Approaches to Cleaning Data</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="clean.html"><a href="clean.html#changing-the-case-of-variable-names"><i class="fa fa-check"></i><b>19.4.1</b> Changing the Case of Variable Names</a></li>
<li class="chapter" data-level="19.4.2" data-path="clean.html"><a href="clean.html#changing-the-case-of-character-variable-values"><i class="fa fa-check"></i><b>19.4.2</b> Changing the Case of Character Variable Values</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="clean.html"><a href="clean.html#summary-15"><i class="fa fa-check"></i><b>19.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="aggregatesegment.html"><a href="aggregatesegment.html"><i class="fa fa-check"></i><b>20</b> Aggregating &amp; Segmenting Data</a>
<ul>
<li class="chapter" data-level="20.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#counts_bygroup"><i class="fa fa-check"></i><b>20.1</b> Counts By Group</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#counts_bygroup_pipes"><i class="fa fa-check"></i><b>20.1.1</b> <em>With</em> Pipes</a></li>
<li class="chapter" data-level="20.1.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#counts_bygroup_nopipes"><i class="fa fa-check"></i><b>20.1.2</b> <em>Without</em> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#centraldispersion_bygroup"><i class="fa fa-check"></i><b>20.2</b> Measures of Central Tendency and Dispersion By Group</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#centraldispersion_bygroup_pipes"><i class="fa fa-check"></i><b>20.2.1</b> <em>With</em> Pipes</a></li>
<li class="chapter" data-level="20.2.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#centraldispersion_bygroup_nopipes"><i class="fa fa-check"></i><b>20.2.2</b> <em>Without</em> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="aggregatesegment.html"><a href="aggregatesegment.html#addaggregatedvariable"><i class="fa fa-check"></i><b>20.3</b> Add Variable to Data Frame Containing Aggregated Values</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#addaggregatedvariable_pipes"><i class="fa fa-check"></i><b>20.3.1</b> <em>With</em> Pipes</a></li>
<li class="chapter" data-level="20.3.2" data-path="aggregatesegment.html"><a href="aggregatesegment.html#addaggregatedvariable_nopipes"><i class="fa fa-check"></i><b>20.3.2</b> <em>Without</em> Pipes</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="aggregatesegment.html"><a href="aggregatesegment.html#visualize_bygroup"><i class="fa fa-check"></i><b>20.4</b> Visualize Data By Group</a></li>
<li class="chapter" data-level="20.5" data-path="aggregatesegment.html"><a href="aggregatesegment.html#summary-16"><i class="fa fa-check"></i><b>20.5</b> Summary</a></li>
<li class="chapter" data-level="20.6" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_supplement"><i class="fa fa-check"></i><b>20.6</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="20.6.1" data-path="aggregatesegment.html"><a href="aggregatesegment.html#aggregatesegment_subset_supplement"><i class="fa fa-check"></i><b>20.6.1</b> <code>describeBy</code> Function from <code>psych</code> Package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="manipulate.html"><a href="manipulate.html"><i class="fa fa-check"></i><b>21</b> Manipulating &amp; Restructuring Data</a>
<ul>
<li class="chapter" data-level="21.1" data-path="manipulate.html"><a href="manipulate.html#widetolong"><i class="fa fa-check"></i><b>21.1</b> Wide-to-Long Format Data Manipulation</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="manipulate.html"><a href="manipulate.html#widetolong_withpipe"><i class="fa fa-check"></i><b>21.1.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="21.1.2" data-path="manipulate.html"><a href="manipulate.html#widetolong_withoutpipe"><i class="fa fa-check"></i><b>21.1.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="manipulate.html"><a href="manipulate.html#longtowide"><i class="fa fa-check"></i><b>21.2</b> Long-to-Wide Format Data Manipulation</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="manipulate.html"><a href="manipulate.html#longtowide_withpipe"><i class="fa fa-check"></i><b>21.2.1</b> <em>With</em> Pipe</a></li>
<li class="chapter" data-level="21.2.2" data-path="manipulate.html"><a href="manipulate.html#longtowide_withoutpipe"><i class="fa fa-check"></i><b>21.2.2</b> <em>Without</em> Pipe</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="manipulate.html"><a href="manipulate.html#summary-17"><i class="fa fa-check"></i><b>21.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="center.html"><a href="center.html"><i class="fa fa-check"></i><b>22</b> Centering &amp; Standardizing Variables</a>
<ul>
<li class="chapter" data-level="22.1" data-path="center.html"><a href="center.html#grandmean_center"><i class="fa fa-check"></i><b>22.1</b> Grand-Mean Centering Variables</a>
<ul>
<li class="chapter" data-level="22.1.1" data-path="center.html"><a href="center.html#mean_function_center"><i class="fa fa-check"></i><b>22.1.1</b> Option 1: Basic Arithmetic and <code>mean</code> Function from Base R</a></li>
<li class="chapter" data-level="22.1.2" data-path="center.html"><a href="center.html#scale_function_center"><i class="fa fa-check"></i><b>22.1.2</b> Option 2: <code>scale</code> Function from Base R</a></li>
<li class="chapter" data-level="22.1.3" data-path="center.html"><a href="center.html#mutate_function_center"><i class="fa fa-check"></i><b>22.1.3</b> Option 3: <code>mutate</code> Function from <code>dplyr</code> and <code>mean</code> Function from Base R</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="center.html"><a href="center.html#standardize_center"><i class="fa fa-check"></i><b>22.2</b> Standardizing Variables</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="center.html"><a href="center.html#scale_function_standardize"><i class="fa fa-check"></i><b>22.2.1</b> Option 1: <code>scale</code> Function from Base R</a></li>
<li class="chapter" data-level="22.2.2" data-path="center.html"><a href="center.html#mutate_function_standardize"><i class="fa fa-check"></i><b>22.2.2</b> Option 2: <code>mutate</code> Function from <code>dplyr</code> and <code>scale</code> Function from Base R</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="center.html"><a href="center.html#summary_center"><i class="fa fa-check"></i><b>22.3</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>IV Employee Demographics</b></span></li>
<li class="chapter" data-level="23" data-path="employeedemographics.html"><a href="employeedemographics.html"><i class="fa fa-check"></i><b>23</b> Introduction to Employee Demographics</a></li>
<li class="chapter" data-level="24" data-path="descriptives.html"><a href="descriptives.html"><i class="fa fa-check"></i><b>24</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="24.1" data-path="descriptives.html"><a href="descriptives.html#measurementscales"><i class="fa fa-check"></i><b>24.1</b> Measurement Scales</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="descriptives.html"><a href="descriptives.html#measurementscales_nominal"><i class="fa fa-check"></i><b>24.1.1</b> Nominal</a></li>
<li class="chapter" data-level="24.1.2" data-path="descriptives.html"><a href="descriptives.html#measurementscales_ordinal"><i class="fa fa-check"></i><b>24.1.2</b> Ordinal</a></li>
<li class="chapter" data-level="24.1.3" data-path="descriptives.html"><a href="descriptives.html#measurementscales_interval"><i class="fa fa-check"></i><b>24.1.3</b> Interval</a></li>
<li class="chapter" data-level="24.1.4" data-path="descriptives.html"><a href="descriptives.html#measurementscales_ratio"><i class="fa fa-check"></i><b>24.1.4</b> Ratio</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="descriptives.html"><a href="descriptives.html#constructs_measures_measurementscales"><i class="fa fa-check"></i><b>24.2</b> Constructs, Measures, &amp; Measurement Scales</a></li>
<li class="chapter" data-level="24.3" data-path="descriptives.html"><a href="descriptives.html#typesof_descriptivestatistics"><i class="fa fa-check"></i><b>24.3</b> Types of Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="24.3.1" data-path="descriptives.html"><a href="descriptives.html#descriptivestatistics_counts"><i class="fa fa-check"></i><b>24.3.1</b> Counts</a></li>
<li class="chapter" data-level="24.3.2" data-path="descriptives.html"><a href="descriptives.html#descriptivestatistics_centraltendency"><i class="fa fa-check"></i><b>24.3.2</b> Measures of Central Tendency &amp; Dispersion</a></li>
<li class="chapter" data-level="24.3.3" data-path="descriptives.html"><a href="descriptives.html#video-tutorials"><i class="fa fa-check"></i><b>24.3.3</b> Video Tutorials</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="descriptives.html"><a href="descriptives.html#determine_measurementscale"><i class="fa fa-check"></i><b>24.4</b> Determine the Measurement Scale</a></li>
<li class="chapter" data-level="24.5" data-path="descriptives.html"><a href="descriptives.html#describe_nominal_ordinal"><i class="fa fa-check"></i><b>24.5</b> Describe Nominal &amp; Ordinal (Categorical) Variables</a>
<ul>
<li class="chapter" data-level="24.5.1" data-path="descriptives.html"><a href="descriptives.html#compute_counts"><i class="fa fa-check"></i><b>24.5.1</b> Compute Counts &amp; Frequencies</a></li>
<li class="chapter" data-level="24.5.2" data-path="descriptives.html"><a href="descriptives.html#datavisualizations_nominal_ordinal"><i class="fa fa-check"></i><b>24.5.2</b> Create Data Visualizations</a></li>
</ul></li>
<li class="chapter" data-level="24.6" data-path="descriptives.html"><a href="descriptives.html#describe_interval_ratio"><i class="fa fa-check"></i><b>24.6</b> Describe Interval &amp; Ratio (Continuous) Variables</a>
<ul>
<li class="chapter" data-level="24.6.1" data-path="descriptives.html"><a href="descriptives.html#datavisualizations_interval_ratio"><i class="fa fa-check"></i><b>24.6.1</b> Create Data Visualizations</a></li>
<li class="chapter" data-level="24.6.2" data-path="descriptives.html"><a href="descriptives.html#compute_measures_centraltendency_dispersion"><i class="fa fa-check"></i><b>24.6.2</b> Compute Measures of Central Tendency &amp; Dispersion</a></li>
</ul></li>
<li class="chapter" data-level="24.7" data-path="descriptives.html"><a href="descriptives.html#summary_descriptives"><i class="fa fa-check"></i><b>24.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="crosstabs.html"><a href="crosstabs.html"><i class="fa fa-check"></i><b>25</b> Cross-Tabulations</a>
<ul>
<li class="chapter" data-level="25.0.1" data-path="crosstabs.html"><a href="crosstabs.html#video-tutorial-16"><i class="fa fa-check"></i><b>25.0.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="25.1" data-path="crosstabs.html"><a href="crosstabs.html#twoway_crosstabs"><i class="fa fa-check"></i><b>25.1</b> Two-Way Cross-Tabulation</a>
<ul>
<li class="chapter" data-level="25.1.1" data-path="crosstabs.html"><a href="crosstabs.html#twoway_crosstabs_table"><i class="fa fa-check"></i><b>25.1.1</b> Option 1: Using the <code>table</code> Function</a></li>
<li class="chapter" data-level="25.1.2" data-path="crosstabs.html"><a href="crosstabs.html#twoway_crosstabs_xtabs"><i class="fa fa-check"></i><b>25.1.2</b> Option 2: Using the <code>xtabs</code> Function</a></li>
<li class="chapter" data-level="25.1.3" data-path="crosstabs.html"><a href="crosstabs.html#twoway_crosstabs_CrossTable"><i class="fa fa-check"></i><b>25.1.3</b> Option 3: Using the <code>CrossTable</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="crosstabs.html"><a href="crosstabs.html#threeway_crosstabs"><i class="fa fa-check"></i><b>25.2</b> Three-Way Cross-Tabulation</a>
<ul>
<li class="chapter" data-level="25.2.1" data-path="crosstabs.html"><a href="crosstabs.html#threeway_crosstabs_table"><i class="fa fa-check"></i><b>25.2.1</b> Option 1: Using the <code>table</code> Function</a></li>
<li class="chapter" data-level="25.2.2" data-path="crosstabs.html"><a href="crosstabs.html#threeway_crosstabs_xtabs"><i class="fa fa-check"></i><b>25.2.2</b> Option 2: Using the <code>xtabs</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="crosstabs.html"><a href="crosstabs.html#summary-18"><i class="fa fa-check"></i><b>25.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="pivottables.html"><a href="pivottables.html"><i class="fa fa-check"></i><b>26</b> Pivot Tables</a>
<ul>
<li class="chapter" data-level="26.0.1" data-path="pivottables.html"><a href="pivottables.html#video-tutorial-17"><i class="fa fa-check"></i><b>26.0.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="26.1" data-path="pivottables.html"><a href="pivottables.html#create_pivottables"><i class="fa fa-check"></i><b>26.1</b> Create a Pivot Table</a></li>
<li class="chapter" data-level="26.2" data-path="pivottables.html"><a href="pivottables.html#summary-19"><i class="fa fa-check"></i><b>26.2</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>V Employee Surveys</b></span></li>
<li class="chapter" data-level="27" data-path="employeesurveys.html"><a href="employeesurveys.html"><i class="fa fa-check"></i><b>27</b> Introduction to Employee Surveys</a></li>
<li class="part"><span><b>VI Employee Training</b></span></li>
<li class="chapter" data-level="28" data-path="employeetraining.html"><a href="employeetraining.html"><i class="fa fa-check"></i><b>28</b> Introduction to Employee Training</a>
<ul>
<li class="chapter" data-level="28.1" data-path="employeetraining.html"><a href="employeetraining.html#trainingevaluation_employeetraining"><i class="fa fa-check"></i><b>28.1</b> Training Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="posttestonly.html"><a href="posttestonly.html"><i class="fa fa-check"></i><b>29</b> Evaluating a Post-Test-Only with Control Group Design Using Independent-Samples <em>t</em>-test</a>
<ul>
<li class="chapter" data-level="29.1" data-path="posttestonly.html"><a href="posttestonly.html#review_isttest"><i class="fa fa-check"></i><b>29.1</b> Review of Independent-Samples <em>t</em>-test</a>
<ul>
<li class="chapter" data-level="29.1.1" data-path="posttestonly.html"><a href="posttestonly.html#statisticalassumptions_isttest"><i class="fa fa-check"></i><b>29.1.1</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="29.1.2" data-path="posttestonly.html"><a href="posttestonly.html#statisticalsignficance_isttest"><i class="fa fa-check"></i><b>29.1.2</b> Statistical Significance</a></li>
<li class="chapter" data-level="29.1.3" data-path="posttestonly.html"><a href="posttestonly.html#practicalsignficance_isttest"><i class="fa fa-check"></i><b>29.1.3</b> Practical Significance</a></li>
<li class="chapter" data-level="29.1.4" data-path="posttestonly.html"><a href="posttestonly.html#samplewriteup_isttest"><i class="fa fa-check"></i><b>29.1.4</b> Sample Write-Up</a></li>
<li class="chapter" data-level="29.1.5" data-path="posttestonly.html"><a href="posttestonly.html#video-tutorial-18"><i class="fa fa-check"></i><b>29.1.5</b> Video Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="29.2" data-path="posttestonly.html"><a href="posttestonly.html#estimate_isttest"><i class="fa fa-check"></i><b>29.2</b> Estimate Independent-Samples <em>t</em>-test</a></li>
<li class="chapter" data-level="29.3" data-path="posttestonly.html"><a href="posttestonly.html#visualize-results-using-bar-chart-barchart_isttest"><i class="fa fa-check"></i><b>29.3</b> Visualize Results Using Bar Chart {barchart_isttest}</a></li>
<li class="chapter" data-level="29.4" data-path="posttestonly.html"><a href="posttestonly.html#summary_isttest"><i class="fa fa-check"></i><b>29.4</b> Summary</a></li>
<li class="chapter" data-level="29.5" data-path="posttestonly.html"><a href="posttestonly.html#isttest_supplement"><i class="fa fa-check"></i><b>29.5</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="29.5.1" data-path="posttestonly.html"><a href="posttestonly.html#isttest_function_slr"><i class="fa fa-check"></i><b>29.5.1</b> <code>t.test</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VII Employee Selection</b></span></li>
<li class="chapter" data-level="30" data-path="selection.html"><a href="selection.html"><i class="fa fa-check"></i><b>30</b> Introduction to Employee Selection</a></li>
<li class="chapter" data-level="31" data-path="disparateimpact.html"><a href="disparateimpact.html"><i class="fa fa-check"></i><b>31</b> Investigating Disparate Impact</a>
<ul>
<li class="chapter" data-level="31.0.1" data-path="disparateimpact.html"><a href="disparateimpact.html#video-tutorial-19"><i class="fa fa-check"></i><b>31.0.1</b> Video Tutorial</a></li>
<li class="chapter" data-level="31.1" data-path="disparateimpact.html"><a href="disparateimpact.html#fourfifthsrule"><i class="fa fa-check"></i><b>31.1</b> 4/5ths Rule</a></li>
<li class="chapter" data-level="31.2" data-path="disparateimpact.html"><a href="disparateimpact.html#chisquaretest_disparateimpact"><i class="fa fa-check"></i><b>31.2</b> Chi-Squared (<span class="math inline">\(\chi^2\)</span>) Test of Independence</a></li>
<li class="chapter" data-level="31.3" data-path="disparateimpact.html"><a href="disparateimpact.html#fisherexacttest"><i class="fa fa-check"></i><b>31.3</b> Fisher Exact Test</a></li>
<li class="chapter" data-level="31.4" data-path="disparateimpact.html"><a href="disparateimpact.html#zdifference_test_disparateimpact"><i class="fa fa-check"></i><b>31.4</b> <span class="math inline">\(Z_{D}\)</span> test</a></li>
<li class="chapter" data-level="31.5" data-path="disparateimpact.html"><a href="disparateimpact.html#z_impactratiotest"><i class="fa fa-check"></i><b>31.5</b> <span class="math inline">\(Z_{IR}\)</span> Test</a></li>
<li class="chapter" data-level="31.6" data-path="disparateimpact.html"><a href="disparateimpact.html#summary_disparatetreatment"><i class="fa fa-check"></i><b>31.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html"><i class="fa fa-check"></i><b>32</b> Estimating Criterion-Related Validity Using Correlation</a>
<ul>
<li class="chapter" data-level="32.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#review_correlation"><i class="fa fa-check"></i><b>32.1</b> Review of Correlation</a>
<ul>
<li class="chapter" data-level="32.1.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#statisticalassumptions_correlation"><i class="fa fa-check"></i><b>32.1.1</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="32.1.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#statisticalsignficance_correlation"><i class="fa fa-check"></i><b>32.1.2</b> Statistical Significance</a></li>
<li class="chapter" data-level="32.1.3" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#practicalsignficance_correlation"><i class="fa fa-check"></i><b>32.1.3</b> Practical Significance</a></li>
<li class="chapter" data-level="32.1.4" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#samplewriteup_correlation"><i class="fa fa-check"></i><b>32.1.4</b> Sample Write-Up</a></li>
<li class="chapter" data-level="32.1.5" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#video-tutorial-20"><i class="fa fa-check"></i><b>32.1.5</b> Video Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="32.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#visualize-association-using-a-scatter-plot"><i class="fa fa-check"></i><b>32.2</b> Visualize Association Using a Scatter Plot</a></li>
<li class="chapter" data-level="32.3" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#estimate_correlation"><i class="fa fa-check"></i><b>32.3</b> Estimate Correlation</a></li>
<li class="chapter" data-level="32.4" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#summary_criterionrelatedvalidity"><i class="fa fa-check"></i><b>32.4</b> Summary</a></li>
<li class="chapter" data-level="32.5" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#criterionrelatedvalidity_supplement"><i class="fa fa-check"></i><b>32.5</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="32.5.1" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#cor_function"><i class="fa fa-check"></i><b>32.5.1</b> <code>cor</code> Function from Base R</a></li>
<li class="chapter" data-level="32.5.2" data-path="criterionrelatedvalidity.html"><a href="criterionrelatedvalidity.html#cortest_function"><i class="fa fa-check"></i><b>32.5.2</b> <code>cor.test</code> Function from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="33" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html"><i class="fa fa-check"></i><b>33</b> Predicting Criterion Scores Using Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="33.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#review_slr"><i class="fa fa-check"></i><b>33.1</b> Review of Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="33.1.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#statisticalassumptions_slr"><i class="fa fa-check"></i><b>33.1.1</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="33.1.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#statisticalsignficance_slr"><i class="fa fa-check"></i><b>33.1.2</b> Statistical Signficance</a></li>
<li class="chapter" data-level="33.1.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#practicalsignficance_slr"><i class="fa fa-check"></i><b>33.1.3</b> Practical Significance</a></li>
<li class="chapter" data-level="33.1.4" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#samplewriteup_slr"><i class="fa fa-check"></i><b>33.1.4</b> Sample Write-Up</a></li>
<li class="chapter" data-level="33.1.5" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#review_prediction_slr"><i class="fa fa-check"></i><b>33.1.5</b> Predicting Future Criterion Scores Using Simple Linear Regression</a></li>
<li class="chapter" data-level="33.1.6" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#video-tutorial-21"><i class="fa fa-check"></i><b>33.1.6</b> Video Tutorial</a></li>
<li class="chapter" data-level="33.1.7" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#functions-packages-introduced-17"><i class="fa fa-check"></i><b>33.1.7</b> Functions &amp; Packages Introduced</a></li>
<li class="chapter" data-level="33.1.8" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#initsteps_slr"><i class="fa fa-check"></i><b>33.1.8</b> Initial Steps</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#estimate_slr"><i class="fa fa-check"></i><b>33.2</b> Estimate Simple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="33.2.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#teststatisticalassumptions_slr"><i class="fa fa-check"></i><b>33.2.1</b> Test Statistical Assumptions</a></li>
<li class="chapter" data-level="33.2.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#interpret_slr"><i class="fa fa-check"></i><b>33.2.2</b> Interpret Simple Linear Regression Model Results</a></li>
<li class="chapter" data-level="33.2.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#standardized_slr"><i class="fa fa-check"></i><b>33.2.3</b> Optional: Obtaining Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="33.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_slr"><i class="fa fa-check"></i><b>33.3</b> Predict Criterion Scores</a></li>
<li class="chapter" data-level="33.4" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#summary_predictingcriterionscores"><i class="fa fa-check"></i><b>33.4</b> Summary</a></li>
<li class="chapter" data-level="33.5" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predictingcriterionscores_supplement"><i class="fa fa-check"></i><b>33.5</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="33.5.1" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#lm_function_slr"><i class="fa fa-check"></i><b>33.5.1</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="33.5.2" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#predict_function_slr"><i class="fa fa-check"></i><b>33.5.2</b> <code>predict</code> Function from Base R</a></li>
<li class="chapter" data-level="33.5.3" data-path="predictingcriterionscores.html"><a href="predictingcriterionscores.html#apatable_slr"><i class="fa fa-check"></i><b>33.5.3</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html"><i class="fa fa-check"></i><b>34</b> Estimating Incremental Validity Using Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="34.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#review_mlr"><i class="fa fa-check"></i><b>34.1</b> Review of Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="34.1.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#statisticalassumptions_mlr"><i class="fa fa-check"></i><b>34.1.1</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="34.1.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#statisticalsignficance_mlr"><i class="fa fa-check"></i><b>34.1.2</b> Statistical Signficance</a></li>
<li class="chapter" data-level="34.1.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#practicalsignficance_mlr"><i class="fa fa-check"></i><b>34.1.3</b> Practical Significance</a></li>
<li class="chapter" data-level="34.1.4" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#samplewriteup_mlr"><i class="fa fa-check"></i><b>34.1.4</b> Sample Write-Up</a></li>
<li class="chapter" data-level="34.1.5" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#video-tutorial-22"><i class="fa fa-check"></i><b>34.1.5</b> Video Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="34.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#estimate_mlr"><i class="fa fa-check"></i><b>34.2</b> Estimate Multiple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="34.2.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#teststatisticalassumptions_mlr"><i class="fa fa-check"></i><b>34.2.1</b> Test Statistical Assumptions</a></li>
<li class="chapter" data-level="34.2.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#interpret_mlr"><i class="fa fa-check"></i><b>34.2.2</b> Interpret Multiple Linear Regression Model Results</a></li>
<li class="chapter" data-level="34.2.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#standardized_mlr"><i class="fa fa-check"></i><b>34.2.3</b> Optional: Obtaining Standardized Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="34.3" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#summary_incrementalvalidity"><i class="fa fa-check"></i><b>34.3</b> Summary</a></li>
<li class="chapter" data-level="34.4" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#incrementalvalidity_supplement"><i class="fa fa-check"></i><b>34.4</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="34.4.1" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#lm_function_mlr"><i class="fa fa-check"></i><b>34.4.1</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="34.4.2" data-path="incrementalvalidity.html"><a href="incrementalvalidity.html#apatable_mlr"><i class="fa fa-check"></i><b>34.4.2</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="35" data-path="compensatory.html"><a href="compensatory.html"><i class="fa fa-check"></i><b>35</b> Compensatory Approach to Selection Decisions</a>
<ul>
<li class="chapter" data-level="35.1" data-path="compensatory.html"><a href="compensatory.html#review_compensatory"><i class="fa fa-check"></i><b>35.1</b> Review of Compensatory Approach</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="compensatory.html"><a href="compensatory.html#video-tutorial-23"><i class="fa fa-check"></i><b>35.1.1</b> Video Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="compensatory.html"><a href="compensatory.html#estimate_compensatory"><i class="fa fa-check"></i><b>35.2</b> Estimate Multiple Linear Regression Model</a></li>
<li class="chapter" data-level="35.3" data-path="compensatory.html"><a href="compensatory.html#predictcriterionscores_compensatory"><i class="fa fa-check"></i><b>35.3</b> Predict Criterion Scores</a></li>
<li class="chapter" data-level="35.4" data-path="compensatory.html"><a href="compensatory.html#summary_compensatory"><i class="fa fa-check"></i><b>35.4</b> Summary</a></li>
<li class="chapter" data-level="35.5" data-path="compensatory.html"><a href="compensatory.html#compensatory_supplement"><i class="fa fa-check"></i><b>35.5</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="35.5.1" data-path="compensatory.html"><a href="compensatory.html#lm_predict_functions_compensatory"><i class="fa fa-check"></i><b>35.5.1</b> <code>lm</code> &amp; <code>predict</code> Functions from Base R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="multiplecutoff.html"><a href="multiplecutoff.html"><i class="fa fa-check"></i><b>36</b> Noncompensatory Approach to Selection Decisions</a>
<ul>
<li class="chapter" data-level="36.1" data-path="multiplecutoff.html"><a href="multiplecutoff.html#review_multiplecutoff"><i class="fa fa-check"></i><b>36.1</b> Review of Noncompensatory Approach</a></li>
<li class="chapter" data-level="36.2" data-path="multiplecutoff.html"><a href="multiplecutoff.html#createcutoffscores_multiplecutoff"><i class="fa fa-check"></i><b>36.2</b> Create Cutoff Scores</a></li>
<li class="chapter" data-level="36.3" data-path="multiplecutoff.html"><a href="multiplecutoff.html#applycutoffscores_multiplecutoff"><i class="fa fa-check"></i><b>36.3</b> Apply Cutoff Scores to Make Selection Decisions</a></li>
<li class="chapter" data-level="36.4" data-path="multiplecutoff.html"><a href="multiplecutoff.html#summary_multiplecutoff"><i class="fa fa-check"></i><b>36.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="differentialprediction.html"><a href="differentialprediction.html"><i class="fa fa-check"></i><b>37</b> Testing for Differential Prediction Using Moderated Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="37.1" data-path="differentialprediction.html"><a href="differentialprediction.html#review_mmlr"><i class="fa fa-check"></i><b>37.1</b> Review of Moderated Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="37.1.1" data-path="differentialprediction.html"><a href="differentialprediction.html#moderation_mmlr"><i class="fa fa-check"></i><b>37.1.1</b> Moderation</a></li>
<li class="chapter" data-level="37.1.2" data-path="differentialprediction.html"><a href="differentialprediction.html#centering_mmlr"><i class="fa fa-check"></i><b>37.1.2</b> Centering Predictor &amp; Moderator Variables</a></li>
<li class="chapter" data-level="37.1.3" data-path="differentialprediction.html"><a href="differentialprediction.html#mmlr_mmlr"><i class="fa fa-check"></i><b>37.1.3</b> Moderated Multiple Linear Regression</a></li>
<li class="chapter" data-level="37.1.4" data-path="differentialprediction.html"><a href="differentialprediction.html#statisticalassumptions_mmlr"><i class="fa fa-check"></i><b>37.1.4</b> Statistical Assumptions</a></li>
<li class="chapter" data-level="37.1.5" data-path="differentialprediction.html"><a href="differentialprediction.html#statisticalsignficance_mmlr"><i class="fa fa-check"></i><b>37.1.5</b> Statistical Signficance</a></li>
<li class="chapter" data-level="37.1.6" data-path="differentialprediction.html"><a href="differentialprediction.html#practicalsignficance_mmlr"><i class="fa fa-check"></i><b>37.1.6</b> Practical Significance</a></li>
</ul></li>
<li class="chapter" data-level="37.2" data-path="differentialprediction.html"><a href="differentialprediction.html#review_differentialprediction"><i class="fa fa-check"></i><b>37.2</b> Review of Differential Prediction</a>
<ul>
<li class="chapter" data-level="37.2.1" data-path="differentialprediction.html"><a href="differentialprediction.html#samplewriteup_mmlr"><i class="fa fa-check"></i><b>37.2.1</b> Sample Write-Up</a></li>
<li class="chapter" data-level="37.2.2" data-path="differentialprediction.html"><a href="differentialprediction.html#video-tutorial-24"><i class="fa fa-check"></i><b>37.2.2</b> Video Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="37.3" data-path="differentialprediction.html"><a href="differentialprediction.html#center_mmlr"><i class="fa fa-check"></i><b>37.3</b> Grand-Mean Center Continuous Predictor Variables</a></li>
<li class="chapter" data-level="37.4" data-path="differentialprediction.html"><a href="differentialprediction.html#estimate_mmlr"><i class="fa fa-check"></i><b>37.4</b> Estimate Moderated Multiple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="37.4.1" data-path="differentialprediction.html"><a href="differentialprediction.html#gender_differentialprediction"><i class="fa fa-check"></i><b>37.4.1</b> Test Differential Prediction Based on <code>gender</code></a></li>
<li class="chapter" data-level="37.4.2" data-path="differentialprediction.html"><a href="differentialprediction.html#age_differentialprediction"><i class="fa fa-check"></i><b>37.4.2</b> Test Differential Prediction Based on <code>c_age</code></a></li>
<li class="chapter" data-level="37.4.3" data-path="differentialprediction.html"><a href="differentialprediction.html#race_differentialprediction"><i class="fa fa-check"></i><b>37.4.3</b> Test Differential Prediction Based on <code>race</code></a></li>
</ul></li>
<li class="chapter" data-level="37.5" data-path="differentialprediction.html"><a href="differentialprediction.html#summary_differentialprediction"><i class="fa fa-check"></i><b>37.5</b> Summary</a></li>
<li class="chapter" data-level="37.6" data-path="differentialprediction.html"><a href="differentialprediction.html#differentialprediction_supplement"><i class="fa fa-check"></i><b>37.6</b> Chapter Supplement</a>
<ul>
<li class="chapter" data-level="37.6.1" data-path="differentialprediction.html"><a href="differentialprediction.html#lm_function_mmlr"><i class="fa fa-check"></i><b>37.6.1</b> <code>lm</code> Function from Base R</a></li>
<li class="chapter" data-level="37.6.2" data-path="differentialprediction.html"><a href="differentialprediction.html#apatable_mmlr"><i class="fa fa-check"></i><b>37.6.2</b> APA-Style Results Table</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>VIII Odds &amp; Ends</b></span></li>
<li class="chapter" data-level="38" data-path="create-portfolio.html"><a href="create-portfolio.html"><i class="fa fa-check"></i><b>38</b> Creating a Data Analytics Portfolio</a></li>
<li class="chapter" data-level="39" data-path="literature-search-review.html"><a href="literature-search-review.html"><i class="fa fa-check"></i><b>39</b> Conducting a Literature Search &amp; Review</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R for HR:<br />
<em>An Introduction to Human Resource Analytics Using R</em><br />
BOOK UNDER CONSTRUCTION</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="incrementalvalidity" class="section level1" number="34">
<h1><span class="header-section-number">Chapter 34</span> Estimating Incremental Validity Using Multiple Linear Regression</h1>
<p>Multiple linear regression models allow us to apply statistical control and to examine whether a predictor variable shows incremental validity relative to other predictor variables with respect to an outcome variable. In this chapter, I will begin by reviewing fundamental concepts related to multiple linear regression and incremental validity, and then demonstrate how to estimate incremental validity in the context of employee selection tool validiation.</p>
<div id="review_mlr" class="section level2" number="34.1">
<h2><span class="header-section-number">34.1</span> Review of Multiple Linear Regression</h2>
<p>Like simple linear regression, <strong>multiple linear regression</strong> can provide us with information about the strength and sign of a linear association between a predictor variable and an outcome variable; however, unlike a simple linear regression model, a multiple linear regression model allows us to assess the strength and sign of the associations between <em>two or more</em> predictor variables and a single outcome variable. In doing so, using multiple linear regression, we can infer the association between a predictor variable and and outcome variable while statistically controlling for the associations between other predictor variables and the same outcome variable.</p>
<p>When we statistically control for the effects of other predictor variables in a model, we are able to evaluate whether evidence of incremental validity exists for each predictor variable. <strong>Incremental validity</strong> refers to instances in which a predictor variable explains significant amounts of variance in the outcome variable even when statistically controlling for the effects of other predictor variables in the model. When a predictor variable (or a block of predictor variables) shows evidence of incremental validity, sometimes we use language like: “Over and beyond the variance explained by Predictors W and Z, Predictor X explained significant variance in Outcome Y.”</p>
<p>In the context of employee selection, evaluating whether a selection tool shows evidence of incremental validity can be of value, as evidence of incremental validity can signify that a selection tool explains unique variance in the criterion (i.e., outcome) when accounting for the effects of other selection tools. In other words, if a selection tool shows evidence of incremental validity, we can be more confident that it contributes uniquely to the prediction of criterion scores and thus is not overly redundant with the other selection tools.</p>
<p>In this chapter, we will learn how to estimate an <em>ordinary least squares (OLS)</em> multiple linear regression model, where OLS refers to the process of estimating the unknown components (i.e., parameters) of the regression model by attempting to minimize the sum of squared residuals. The sum of the squared residuals are the result of a process in which the differences between the observed outcome variable values and the predicted outcome variable values are calculated, squared, and then summed in order to identify a model with the least amount of error (residuals). This is where the concept of “best fit” comes into play, as the model is estimated to most closely fit the available data such that the error (residuals) between the predicted and observed outcome variable values are minimized. In other words, the goal is to find the linear model that best fits the data at hand; with that said, a specific type of regression called polynomial regression can be used to test nonlinear associations.</p>
<p>Let’s consider a scenario in which we estimate a multiple linear regression model with two predictor variables and a single outcome variable. The equation for such a model with <em>unstandardized</em> regression coefficients (<span class="math inline">\(b\)</span>) would be as follows:</p>
<p><span class="math inline">\(\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + e\)</span></p>
<p>where <span class="math inline">\(\hat{Y}\)</span> represents the predicted score on the outcome variable (<span class="math inline">\(Y\)</span>), <span class="math inline">\(b_{0}\)</span> represents the <span class="math inline">\(\hat{Y}\)</span>-intercept value (i.e., model constant) when the predictor variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are equal to zero, <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> represent the unstandardized coefficients (i.e., weights, slopes) of the associations between the predictor variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, respectively, and the outcome variable <span class="math inline">\(\hat{Y}\)</span>, and <span class="math inline">\(e\)</span> represents the (residual) error in prediction. Importantly, the unstandardized regression coefficients <span class="math inline">\(b_{1}\)</span> and <span class="math inline">\(b_{2}\)</span> represent the “raw” slopes (i.e., weights, coefficients) – or rather, how many unstandardized units of <span class="math inline">\(\hat{Y}\)</span> increase or decrease as a result of a single unit increase in either <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span> when controlling for the effect of the other predictor variable. That is, unstandardized regression coefficients reflect the nature of the association between two variables when the variables retain their original scaling. Often this is why we choose to use the unstandardized regression coefficients when making predictions about <span class="math inline">\(\hat{Y}\)</span>, as the predicted scores will be have the same scaling as the outcome variable in its original form.</p>
<p>If we wanted to estimate a multiple linear regression model with <em>three</em> predictor variables, our equation would change as follows.</p>
<p><span class="math inline">\(\hat{Y} = b_{0} + b_{1}X_1 + b_{2}X_2 + b_{3}X_3 + e\)</span></p>
<p>We can also obtain <em>standardized</em> regression coefficients. To do so, the predictor variables (e.g., <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>) and outcome variable (<span class="math inline">\(Y\)</span>) scores must be standardized. To standardize variables, we convert the predictor and outcome variables to <em>z</em>-scores, such that their respective means are standardized to 0 and their variances and standard deviations are standardized to 1. When standardized, our multiple linear regression model equation will have a <span class="math inline">\(\hat{Y}\)</span>-intercept value equal to zero (and thus is not typically reported) and the standardized regression coefficient is commonly signified using the Greek letter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math inline">\(\hat{Y} = \beta_{1}X_1 + \beta_{2}X_2 + e\)</span></p>
<p>where <span class="math inline">\(\hat{Y}\)</span> represents the predicted <em>standardized</em> score on the outcome variable (<span class="math inline">\(Y\)</span>), <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> represent the <em>standardized</em> coefficients (i.e., weights, slopes) of the association between the predictor variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, respectively, and the outcome variable <span class="math inline">\(\hat{Y}\)</span>, and <span class="math inline">\(e\)</span> represents the (residual) error in prediction. A standardized regression coefficient (<span class="math inline">\(\beta\)</span>) allows us to also compare the relative magnitude of one <span class="math inline">\(\beta\)</span> to another <span class="math inline">\(\beta\)</span>; with that being said, in the case of a multiple linear regression model, comparing <span class="math inline">\(\beta\)</span> coefficients is only appropriate when the predictor variables in the model share little to no intercorrelation (i.e., have low collinearity). Given that, I recommend that you proceed with caution should you choose to make such comparisons.</p>
<p>In terms of interpretation, in a multiple linear regression model, the standardized regression coefficient (<span class="math inline">\(\beta_{1}\)</span>) indicates the standardized slope when controlling for the effects of other predictor variables in the model – or rather, how many standard units of <span class="math inline">\(\hat{Y}\)</span> increase or decrease as a result of a single standard unit increase in <span class="math inline">\(X_1\)</span> when accounting for the effects of other predictor variables in the model.</p>
<div id="statisticalassumptions_mlr" class="section level3" number="34.1.1">
<h3><span class="header-section-number">34.1.1</span> Statistical Assumptions</h3>
<p>The statistical assumptions that should be met prior to running and/or interpreting estimates from a multiple linear regression model include:</p>
<ul>
<li>Cases are randomly sampled from the population, such that the variable scores for one individual are independent of the variable scores of another individual;</li>
<li>Data are free of multivariate outliers;</li>
<li>The association between the predictor and outcome variables is linear;</li>
<li>There is no (multi)collinearity between predictor variables;</li>
<li>Average residual error value is zero for all levels of the predictor variables;</li>
<li>Variances of residual errors are equal for all levels of the predictor variables, which is referred to as the assumption of homoscedasticity;</li>
<li>Residual errors are normally distributed for all levels of the predictor variables.</li>
</ul>
<p>The fourth statistical assumption refers to the concept of <strong>collinearity</strong> (<strong>multicollinearity</strong>). This can be a tricky concept to understand, so let’s take a moment to unpack it. When two or more predictor variables are specified in a regression model, as is the case with multiple linear regression, we need to be wary of collinearity. Collinearity refers to the extent to which predictor variables correlate with each other. Some level of intercorrelation between predictor variables is to be expected and is acceptable; however, if collinearity becomes substantial, it can affect the weights – and even the signs – of the regression coefficients in our model, which can be problematic from an interpretation standpoint. As such, we should avoid including predictors in a multiple linear regression model that correlate highly with one another. The <strong>tolerance</strong> statistic is commonly computed and serves as an indicator of collinearity. The tolerance statistic is computed by computing the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable), and subtracting that <em>R</em><sup>2</sup> value from 1 (i.e., 1 - <em>R</em><sup>2</sup>). We typically grow concerned when the tolerance statistic falls below .20 and closer to .00. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. From time to time, you might also see the <strong>variance inflation factor</strong> (<strong>VIF</strong>) reported as an indicator of collinearity; the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), and in my opinion, it is redundant to report and interpret both the tolerance and VIF. My recommendation is to focus just on the tolerance statistic when inferring whether the statistical assumption of no collinearity might have been violated.</p>
<p>Finally, with respect to the assumption that cases are randomly sampled from population, we will assume in this chapter’s data that this is not an issue. If we were to suspect, however, that there were some clustering or nesting of cases in units/groups (e.g., by supervisors, units, or facilities) with respect to our outcome variable, then we would need to run some type of multilevel model (e.g., hierarchical linear model, multilevel structural equation model), which is beyond the scope of this tutorial. An <em>intraclass correlation (ICC)</em> can be used to diagnose such nesting or cluster. Failing to account for clustering or nesting in the data can bias estimates of standard errors, which ultimately influences the <em>p</em>-values and inferences of statistical significance.</p>
</div>
<div id="statisticalsignficance_mlr" class="section level3" number="34.1.2">
<h3><span class="header-section-number">34.1.2</span> Statistical Signficance</h3>
<p>Using null hypothesis significance testing (NHST), we interpret a <em>p</em>-value that is <em>less than .05</em> (or whatever two- or one-tailed alpha level we set) to meet the standard for statistical significance, meaning that we reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of the other predictor variables in the model. In other words, if a regression coefficient’s <em>p</em>-value is less than .05, we conclude that the regression coefficient differs from zero to a statistically significant extent when controlling for the effects of other predictor variables in the model. In contrast, if the regression coefficient’s <em>p</em>-value is <em>equal to or greater than .05</em>, then we fail to reject the null hypothesis that the regression coefficient is equal to zero when controlling for the effects of other predictor variables in the model. Put differently, if a regression coefficient’s <em>p</em>-value is equal to or greater than .05, we conclude that the regression coefficient does <em>not</em> differ from zero to a statistically significant extent, leading us to conclude that there is no association between the predictor variable and the outcome variable in the population – when controlling for the effects of other predictor variables in the model. I should note that it is entirely possible for a predictor variable and outcome variable to show a statistically significant association in a simple linear regression but for that same association to be not statistically significant when one or more predictor variables are added to the model.</p>
<p>When setting an alpha threshold, such as the conventional two-tailed .05 level, sometimes the question comes up regarding whether borderline <em>p</em>-values signify significance or nonsignificance. For our purposes, let’s be very strict in our application of the chosen alpha level. For example, if we set our alpha level at .05, <em>p</em> = .049 would be considered statistically significant, and <em>p</em> = .050 would be considered statistically nonsignificant.</p>
<p>Because our regression model estimates are based on data from a sample that is drawn from an underlying population, sampling error will affect the extent to which our sample is representative of the population from which its drawn. That is, a regression coefficient estimate (<em>b</em>) is a <em>point estimate</em> of the population parameter that is subject to sampling error. Fortunately, confidence intervals can give us a better idea of what the true population parameter value might be. If we apply an alpha level of .05 (two-tailed), then the equivalent confidence interval (CI) is a 95% CI. In terms of whether a regression coefficient is statistically significant, if the lower and upper limits of 95% CI do <em>not</em> include zero, then this tells us the same thing as a <em>p</em>-value that is less than .05. Strictly speaking, a 95% CI indicates that if we were to hypothetically draw many more samples from the underlying population and construct CIs for each of those samples, then the true parameter (i.e., true value of the regression coefficient in the population) would likely fall within the lower and upper bounds of 95% of the estimated CIs. In other words, the 95% CI gives us an indication of plausible values for the population parameter while taking into consideration sampling error. A wide CI (i.e., large difference between the lower and upper limits) signifies more sampling error, and a narrow CI signifies less sampling error.</p>
</div>
<div id="practicalsignficance_mlr" class="section level3" number="34.1.3">
<h3><span class="header-section-number">34.1.3</span> Practical Significance</h3>
<p>As a reminder, an effect size is a standardized metric that can be compared across samples. In a multiple linear regression model, an <em>unstandardized</em> regression coefficient (<span class="math inline">\(b\)</span>) is <em>not</em> an effect size. The reason being, an unstandardized regression coefficient estimate is based on the original scaling of the predictor and outcome variables, and thus the same effect will take on different regression coefficients to the extent that the predictor and outcome variables have different scalings across samples.</p>
<p>A <em>standardized</em> regression coefficient (<span class="math inline">\(\beta\)</span>) can be interpreted as an effect size (and thus an indicator of practical significance) given that it is standardized. With that being said, I suggest doing so with caution as collinearity (i.e., correlation) between predictor variables in the model can bias our interpretation of <span class="math inline">\(\beta\)</span> as an effect size. Thus, if your goal is just to understand the bivariate association between a predictor variable and an outcome variable (without introducing statistical control), then I recommend to just estimate a correlation coefficient as an indicator of practical significance, which I discuss in the chapter on <a href="criterionrelatedvalidity.html#practicalsignficance_correlation">estimating criterion-related validity using correlations</a>.</p>
<p>In a multiple linear regression model, we can also describe the magnitude of the effect in terms of the proportion of variance explained in the outcome variable by the predictor variables (i.e., <em>R</em><sup>2</sup>). That is, in a multiple linear regression model, <em>R</em><sup>2</sup> represents the proportion of <em>collective</em> variance explained in the outcome variable by all of the predictor variables. Conceptually, we can think of the overlap between the variability in the predictor variables and and outcome variable as the variance explained (<em>R</em><sup>2</sup>), and <em>R</em><sup>2</sup> is a way to evaluate how well a model fits the data (i.e., model fit). I’ve found that the <em>R</em><sup>2</sup> is often readily interpretable by non-analytics audiences. For example, an <em>R</em><sup>2</sup> of .25 in a multiple linear regression model can be interpreted as: the predictor variable scores explain 25% of the variability in scores on the outcome variable. That is, to convert an <em>R</em><sup>2</sup> from a proportion to a percent, we just multiply by 100.</p>
<div class="figure">
<img src="Rsquared_mlr.png" alt="" />
<p class="caption">As an effect size, <em>R</em><sup>2</sup> indicates the proportion of variance explained by the predictor variables in relation to the outcome variable – or in other words, the shared variance between the variables.</p>
</div>
<p><em>Note: Typically, we only interpret the practical significance of an effect if the effect was found to be statistically significant. The logic is that if an effect (e.g., association, difference) is not statistically significant, then we should treat it as no different than zero, and thus it wouldn’t make sense to the interpret the size of something that statistically has no effect.</em></p>
</div>
<div id="samplewriteup_mlr" class="section level3" number="34.1.4">
<h3><span class="header-section-number">34.1.4</span> Sample Write-Up</h3>
<p>A team of researchers is interested in whether a basketball player’s height and intelligence predict the number of points scored during a 10-game season. In this case, the predictor variables are the basketball players’ heights (in inches) and levels of intelligence, and the outcome variable is the number of points scored by the basketball players. Let’s imagine that the researchers collected data on these variables from a sample of 100 basketball players. Our hypothesis for such a situation might be: Basketball players’ heights and intelligence scores will both be positively related to the number of points players score in a 10-game season, such that taller and more intelligent players will tend to score more points in a season. Imagine that we find that the unstandardized regression coefficients associated with player height (<em>b</em> = 2.31, <em>p</em> = .02) and player intelligence (<em>b</em> = .59, <em>p</em> = .01) in relation to points scored are statistically significant and positive, and that the <em>R</em><sup>2</sup> value is .24 (<em>p</em> &lt; .01). We could summarize the findings as follows: Based on a sample of 100 basketball players, basketball player height was found to predict points scored in a 10-game season, after controlling for player intelligence, such that taller players tended to score more points (<em>b</em> = 2.31, <em>p</em> = .02). Specifically, for every 1-inch increase in height, players tended to score 2.31 additional points during the season, when controlling for player intelligence. In addition, basketball player intelligence was found to predict points scored, when controlling for player height, such that more intelligent players tended to score more points (<em>b</em> = .59, <em>p</em> = .01). Specifically, for every 1 additional point scored on the intelligence test, players tended to score .59 additional points during the season. Further, this reflects a large collective effect, as approximately 24% of the variability in points scored was explained by players’ heights and intelligence, collectively (<em>R</em><sup>2</sup> = .24, <em>p</em> &lt; .01).</p>
</div>
<div id="video-tutorial-22" class="section level3" number="34.1.5">
<h3><span class="header-section-number">34.1.5</span> Video Tutorial</h3>
<p>As usual, you have the choice to follow along with the written tutorial in this chapter or to watch the video tutorial below.</p>
<iframe src="https://youtube.com/embed/AOeJ2byUJdw?rel=0" width="672" height="400px">
</iframe>
<p>Link to Video Tutorial: <a href="https://youtu.be/AOeJ2byUJdw" class="uri">https://youtu.be/AOeJ2byUJdw</a></p>
<p>Additionally, in the following video tutorial, I go into greater depth on how to test the statistical assumptions of a multiple linear regression model – just as I do in the written tutorial below.</p>
<iframe src="https://youtube.com/embed/zyEZop-5K9Q?rel=0" width="672" height="400px">
</iframe>
<p>Link to Video Tutorial: <a href="https://youtu.be/zyEZop-5K9Q" class="uri">https://youtu.be/zyEZop-5K9Q</a></p>
<div id="functions-packages-introduced-18" class="section level4" number="34.1.5.1">
<h4><span class="header-section-number">34.1.5.1</span> Functions &amp; Packages Introduced</h4>
<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>Regression</code></td>
<td><code>lessR</code></td>
</tr>
</tbody>
</table>
</div>
<div id="initsteps_mlr" class="section level4" number="34.1.5.2">
<h4><span class="header-section-number">34.1.5.2</span> Initial Steps</h4>
<p>If you haven’t already, save the file called <strong>“ConcurrentValidation.csv”</strong> into a folder that you will subsequently set as your working directory. Your working directory will likely be different than the one shown below (i.e., <code>"H:/RWorkshop"</code>). As a reminder, you can access all of the data files referenced in this book by downloading them as a compressed (zipped) folder from the my GitHub site: <a href="https://github.com/davidcaughlin/R-Tutorial-Data-Files" class="uri">https://github.com/davidcaughlin/R-Tutorial-Data-Files</a>; once you’ve followed the link to GitHub, just click “Code” (or “Download”) followed by “Download ZIP”, which will download all of the data files referenced in this book. For the sake of parsimony, I recommend downloading all of the data files into the same folder on your computer, which will allow you to set that same folder as your working directory for each of the chapters in this book.</p>
<p>Next, using the <code>setwd</code> function, set your working directory to the folder in which you saved the data file for this chapter. Alternatively, you can manually set your working directory folder in your drop-down menus by going to <em>Session &gt; Set Working Directory &gt; Choose Directory…</em>. Be sure to create a new R script file (.R) or update an existing R script file so that you can save your script and annotations. If you need refreshers on how to set your working directory and how to create and save an R script, please refer to <a href="setwd.html#setwd">Setting a Working Directory</a> and <a href="gettingstarted.html#createRscript">Creating &amp; Saving an R Script</a>.</p>
<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1046-1"><a href="incrementalvalidity.html#cb1046-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set your working directory</span></span>
<span id="cb1046-2"><a href="incrementalvalidity.html#cb1046-2" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(<span class="st">&quot;H:/RWorkshop&quot;</span>)</span></code></pre></div>
<p>Next, read in the .csv data file called <strong>“ConcurrentValidation.csv”</strong> using your choice of read function. In this example, I use the <code>read_csv</code> function from the <code>readr</code> package <span class="citation">(<a href="references.html#ref-R-readr" role="doc-biblioref">Wickham and Hester 2020</a>)</span>. If you choose to use the <code>read_csv</code> function, be sure that you have installed and accessed the <code>readr</code> package using the <code>install.packages</code> and <code>library</code> functions. <em>Note: You don’t need to install a package every time you wish to access it; in general, I would recommend updating a package installation once ever 1-3 months.</em> For refreshers on installing packages and reading data into R, please refer to <a href="gentleintro.html#packages">Packages</a> and <a href="read.html#read">Reading Data into R</a>.</p>
<div class="sourceCode" id="cb1047"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1047-1"><a href="incrementalvalidity.html#cb1047-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install readr package if you haven&#39;t already</span></span>
<span id="cb1047-2"><a href="incrementalvalidity.html#cb1047-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [Note: You don&#39;t need to install a package every </span></span>
<span id="cb1047-3"><a href="incrementalvalidity.html#cb1047-3" aria-hidden="true" tabindex="-1"></a><span class="co"># time you wish to access it]</span></span>
<span id="cb1047-4"><a href="incrementalvalidity.html#cb1047-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;readr&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1048"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1048-1"><a href="incrementalvalidity.html#cb1048-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access readr package</span></span>
<span id="cb1048-2"><a href="incrementalvalidity.html#cb1048-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb1048-3"><a href="incrementalvalidity.html#cb1048-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1048-4"><a href="incrementalvalidity.html#cb1048-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data and name data frame (tibble) object</span></span>
<span id="cb1048-5"><a href="incrementalvalidity.html#cb1048-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;ConcurrentValidation.csv&quot;</span>)</span></code></pre></div>
<pre><code>## 
## -- Column specification -----------------------------------------------------------------------------------------------------------
## cols(
##   EmployeeID = col_character(),
##   SJT = col_double(),
##   EI = col_double(),
##   Interview = col_double(),
##   Performance = col_double()
## )</code></pre>
<div class="sourceCode" id="cb1050"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1050-1"><a href="incrementalvalidity.html#cb1050-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the names of the variables in the data frame (tibble) objects</span></span>
<span id="cb1050-2"><a href="incrementalvalidity.html#cb1050-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df)</span></code></pre></div>
<pre><code>## [1] &quot;EmployeeID&quot;  &quot;SJT&quot;         &quot;EI&quot;          &quot;Interview&quot;   &quot;Performance&quot;</code></pre>
<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1052-1"><a href="incrementalvalidity.html#cb1052-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View variable type for each variable in data frame</span></span>
<span id="cb1052-2"><a href="incrementalvalidity.html#cb1052-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ...
##  $ SJT        : num [1:300] 9 8 7 6 6 5 5 4 3 8 ...
##  $ EI         : num [1:300] 8 6 6 5 5 5 4 2 2 7 ...
##  $ Interview  : num [1:300] 2 3 4 5 6 7 7 8 9 2 ...
##  $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ...
##  - attr(*, &quot;spec&quot;)=
##   .. cols(
##   ..   EmployeeID = col_character(),
##   ..   SJT = col_double(),
##   ..   EI = col_double(),
##   ..   Interview = col_double(),
##   ..   Performance = col_double()
##   .. )</code></pre>
<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1054-1"><a href="incrementalvalidity.html#cb1054-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View first 6 rows of data frame</span></span>
<span id="cb1054-2"><a href="incrementalvalidity.html#cb1054-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##   EmployeeID   SJT    EI Interview Performance
##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 EE23           9     8         2          22
## 2 EE24           8     6         3          11
## 3 EE25           7     6         4           5
## 4 EE26           6     5         5          11
## 5 EE27           6     5         6          12
## 6 EE28           5     5         7          12</code></pre>
<p>The data frame contains 5 variables and 300 cases (i.e., employees): <code>EmployeeID</code>, <code>SJT</code>, <code>EI</code>, <code>Interview</code>, and <code>Performance</code>. Let’s assume that these data were collected as part of a concurrent validation study aimed at estimating the criterion-related validity of selection tools (e.g., procedures, assessments, tests); this means that the selection tools (i.e., <code>SJT</code>, <code>EI</code>, <code>Interview</code>) were administered to job incumbents and the criterion measure (<code>Performance</code>) was administered at about the same time. To begin, <code>EmployeeID</code> is the unique identifier variable. The <code>SJT</code> variable contains the scores on a situational judgment test designed to “tap into” the psychological concepts of emotional intelligence and empathy; potential scores on this variable could range from 1 (<em>low emotional intelligence &amp; empathy</em>) to 10 (<em>emotional intelligence &amp; empathy</em>). The <code>EI</code> variable contains scores on an emotional intelligence assessment; potential scores on this variable could range from 1 (<em>low emotional intelligence</em>) to 10 (<em>emotional intelligence</em>). The <code>Interview</code> variable contains the scores for a structured interview designed to assess interviewees’ level of interpersonal skills; potential scores on this variable could range from 1 (<em>poor interpersonal skills</em>) to 15 (<em>interpersonal skills</em>). Finally, the <em>criterion</em> for this concurrent validation study is the <code>Performance</code> variable, which contains the job performance evaluation ratings for the job incumbents; potential scores on this variable could range from 1 (<em>does not meet performance standards</em>) to 30 (<em>exceeds performance standards</em>).</p>
</div>
</div>
</div>
<div id="estimate_mlr" class="section level2" number="34.2">
<h2><span class="header-section-number">34.2</span> Estimate Multiple Linear Regression Model</h2>
<p>Let’s assume that evidence of <em>criterion-related validity</em> was already found for the three selection tools (<code>SJT</code>, <code>EI</code>, <code>Interview</code>) using correlations; that is, the <em>validity coefficient</em> associated with each selection tool and the criterion (<code>Performance</code>) was statistically significant. If you want, you can run the correlations to verify this – or you can trust me on this. For a review of correlation in this context, check out the <a href="criterionrelatedvalidity.html#criterionrelatedvalidity">chapter on estimating criterion-related validity using a correlation</a>.</p>
<p>Given that evidence of criterion-related validity was already found for the three selection tools, our next step is to include all three selection tools as predictors in a multiple linear regression model. The criterion (<code>Performance</code>) variable will serve as our sole outcome variable in the model. In doing so, we can evaluate which selection tools show evidence of <strong>incremental validity</strong>.</p>
<div id="teststatisticalassumptions_mlr" class="section level3" number="34.2.1">
<h3><span class="header-section-number">34.2.1</span> Test Statistical Assumptions</h3>
<p>To determine whether it’s appropriate to interpret the results of a multiple linear regression model, we need to first test the <a href="incrementalvalidity.html#statisticalassumptions_mlr">statistical assumptions</a>. Fortunately, the <code>Regression</code> function from the <code>lessR</code> package automatically produces common tests of statistical assumptions. So to get started, let’s install and access the <code>lessR</code> package using the <code>install.packages</code> and <code>library</code> functions, respectively. In the chapter supplement, you can learn how to carry how the same tests using the <code>lm</code> function from base R.</p>
<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1056-1"><a href="incrementalvalidity.html#cb1056-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1056-2"><a href="incrementalvalidity.html#cb1056-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;lessR&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1057"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1057-1"><a href="incrementalvalidity.html#cb1057-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1057-2"><a href="incrementalvalidity.html#cb1057-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lessR)</span></code></pre></div>
<p>To use the <code>Regression</code> function from the <code>lessR</code> package, type the name of the <code>Regression</code> function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (<code>Performance</code>) to the left of the <code>~</code> operator and the names of the predictor variables (<code>SJT</code>, <code>EI</code>, <code>Interview</code>) to the right of the <code>~</code> operator; note that we use the <code>+</code> to add additional predictor variables to the model. We are telling the function to “regress <code>Performance</code> on <code>SJT</code>, <code>EI</code>, and <code>Interview</code>.” As the second argument, type <code>data=</code> followed by the name of the data frame object to which the variables in your model belong (<code>df</code>).</p>
<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1058-1"><a href="incrementalvalidity.html#cb1058-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model</span></span>
<span id="cb1058-2"><a href="incrementalvalidity.html#cb1058-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Regression</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> EI <span class="sc">+</span> Interview, <span class="at">data=</span>df)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-625-1.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-625-2.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-625-3.png" width="672" /></p>
<pre><code>## &gt;&gt;&gt; Suggestion
## # Create an R markdown file for interpretative output with  Rmd = &quot;file_name&quot;
## Regression(my_formula=Performance ~ SJT + EI + Interview, data=df, Rmd=&quot;eg&quot;)  
## 
## 
##   BACKGROUND
## 
## Data Frame:  df 
##  
## Response Variable: Performance 
## Predictor Variable 1: SJT 
## Predictor Variable 2: EI 
## Predictor Variable 3: Interview 
##  
## Number of cases (rows) of data:  300 
## Number of cases retained for analysis:  300 
## 
## 
##   BASIC ANALYSIS
## 
##              Estimate    Std Err  t-value  p-value   Lower 95%   Upper 95% 
## (Intercept)     5.602      0.574    9.756    0.000       4.472       6.732 
##         SJT     0.485      0.231    2.099    0.037       0.030       0.939 
##          EI     0.091      0.237    0.385    0.700      -0.376       0.559 
##   Interview     0.378      0.067    5.686    0.000       0.247       0.509 
## 
## 
## Standard deviation of residuals:  2.996 for 296 degrees of freedom 
##  
## R-squared:  0.265    Adjusted R-squared:  0.257    PRESS R-squared:  0.233 
## 
## Null hypothesis that all population slope coefficients are 0:
##   F-statistic: 35.507     df: 3 and 296     p-value:  0.000 
## 
## 
##               df    Sum Sq   Mean Sq   F-value   p-value 
##         SJT    1   629.432   629.432    70.101     0.000 
##          EI    1    36.702    36.702     4.088     0.044 
##   Interview    1   290.301   290.301    32.331     0.000 
##  
## Model          3   956.435   318.812    35.507     0.000 
## Residuals    296  2657.762     8.979 
## Performance  299  3614.197    12.088 
## 
## 
##   K-FOLD CROSS-VALIDATION
## 
##   RELATIONS AMONG THE VARIABLES
## 
##               Performance  SJT   EI Interview 
##   Performance        1.00 0.42 0.43      0.39 
##           SJT        0.42 1.00 0.93      0.24 
##            EI        0.43 0.93 1.00      0.32 
##     Interview        0.39 0.24 0.32      1.00 
## 
## 
##             Tolerance       VIF 
##         SJT     0.127     7.887 
##          EI     0.121     8.278 
##   Interview     0.873     1.146 
## 
## 
##  SJT  EI Interview    R2adj    X&#39;s 
##    1   0         1    0.259      2 
##    1   1         1    0.257      3 
##    0   1         1    0.249      2 
##    1   1         0    0.179      2 
##    0   1         0    0.178      1 
##    1   0         0    0.171      1 
##    0   0         1    0.150      1 
##  
## [based on Thomas Lumley&#39;s leaps function from the leaps package] 
##  
## 
## 
##   RESIDUALS AND INFLUENCE
## 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance 
##    [sorted by Cook&#39;s Distance] 
##    [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] 
## --------------------------------------------------------------------------- 
##          SJT    EI Interview Performance fitted   resid rstdnt dffits cooks 
##   201  9.000 8.000     1.000      24.000 11.074  12.926  4.516  0.765 0.137 
##    70 10.000 8.000    12.000       5.000 15.720 -10.720 -3.728 -0.753 0.136 
##   170 10.000 8.000    12.000       5.000 15.720 -10.720 -3.728 -0.753 0.136 
##   270 10.000 8.000    12.000       5.000 15.720 -10.720 -3.728 -0.753 0.136 
##   233 10.000 9.000     6.000      27.000 13.542  13.458  4.693  0.654 0.100 
##     1  9.000 8.000     2.000      22.000 11.452  10.548  3.634  0.556 0.074 
##   101  9.000 8.000     2.000      22.000 11.452  10.548  3.634  0.556 0.074 
##    69  2.000 1.000     1.000      18.000  7.041  10.959  3.772  0.505 0.061 
##   169  2.000 1.000     1.000      18.000  7.041  10.959  3.772  0.505 0.061 
##    92  8.000 7.000    15.000      10.000 15.794  -5.794 -1.989 -0.437 0.047 
##   192  8.000 7.000    15.000      10.000 15.794  -5.794 -1.989 -0.437 0.047 
##   292  8.000 7.000    15.000      10.000 15.794  -5.794 -1.989 -0.437 0.047 
##    20  2.000 1.000     8.000      18.000  9.689   8.311  2.837  0.422 0.044 
##   120  2.000 1.000     8.000      18.000  9.689   8.311  2.837  0.422 0.044 
##   283  9.000 7.000     5.000      22.000 12.496   9.504  3.249  0.418 0.042 
##   269  2.000 1.000     5.000      18.000  8.554   9.446  3.225  0.385 0.036 
##    97  2.000 1.000     3.000      14.000  7.797   6.203  2.096  0.248 0.015 
##   197  2.000 1.000     3.000      14.000  7.797   6.203  2.096  0.248 0.015 
##   297  2.000 1.000     3.000      14.000  7.797   6.203  2.096  0.248 0.015 
##    82  2.000 1.000     4.000      14.000  8.176   5.824  1.966  0.230 0.013 
## 
## 
##   PREDICTION ERROR 
## 
## Data, Predicted, Standard Error of Forecast, 
## 95% Prediction Intervals 
##    [sorted by lower bound of prediction interval] 
##    [to see all intervals do pred_rows=&quot;all&quot;] 
##  ----------------------------------------------
## 
##         SJT    EI Interview Performance   pred    sf pi.lwr pi.upr  width 
##    69 2.000 1.000     1.000      18.000  7.041 3.023  1.092 12.990 11.898 
##   169 2.000 1.000     1.000      18.000  7.041 3.023  1.092 12.990 11.898 
##    41 1.000 2.000     3.000       8.000  7.404 3.052  1.398 13.410 12.012 
## ... 
##   210 8.000 7.000     2.000      10.000 10.876 3.021  4.930 16.822 11.892 
##     4 6.000 5.000     5.000      11.000 10.859 3.002  4.951 16.766 11.815 
##    47 6.000 5.000     5.000      11.000 10.859 3.002  4.951 16.766 11.815 
## ... 
##   225 7.000 6.000     9.000      16.000 12.948 3.011  7.022 18.874 11.852 
##    28 4.000 7.000    14.000      12.000 13.477 3.149  7.278 19.675 12.396 
##   128 4.000 7.000    14.000      12.000 13.477 3.149  7.278 19.675 12.396 
## 
## 
## ---------------------------------- 
## Plot 1: Distribution of Residuals 
## Plot 2: Residuals vs Fitted Values 
## Plot 3: ScatterPlot Matrix 
## ----------------------------------</code></pre>
<p>By default, the output for the <code>Regression</code> function produces three plots that are useful for assessing statistical assumptions and for interpreting the results, as well as text output.</p>
<p>Let’s begin by reviewing the first two plots depicting the residuals and the section of the text called <em>Residuals and Influence</em>. Let’s take a look at the second plot in your Plot window; you may need to hit the back arrow button to review the three plots.</p>
<p><strong>Fitted Values &amp; Residuals Plot:</strong> The second plot is scatterplot that shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimates – or in other words, how much our predicted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may <em>not</em> be equal for across levels of the predictor variables (violation of the assumption of homoscedasticity), (b) the average residual error value may <em>not</em> be zero for all levels of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to deviate slightly from zero, which may indicate some degree of heteroscedasticity, and one case that is flagged as a potential multivariate outlier (case associated with row number <em>201</em>). This case was flagged based on an outlier/influence statistic called <em>Cook’s distance (D)</em>. So we may have a slight violation of the homscedasticity of residuals assumption.</p>
<p><strong>Residuals &amp; Influence Output:</strong> Moving to the text output section called <em>Residuals and Influence</em>, we see a table with a unique identifiers column (that shows the row number in your data frame object), the observed (actual) predictor and outcome variable values, the fitted (predicted) outcome variable values, the residual (error) between the fitted values and the observed outcome variable values, and the following three outlier/influence statistics: (a) studentized residual (rstdnt), (b) number of standard error units that a fitted value shifts when the flagged case is removed (dffits), and (c) Cook’s distance (cooks). Corroborating what we saw in the plot, the case associated with row number <em>201</em> has the highest Cook’s distance value (.137), followed closely by the cases associated with row numbers <em>70</em>, <em>170</em>, and <em>270</em>. There are several different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply <em>1</em> <span class="citation">(<a href="references.html#ref-bollen1985regression" role="doc-biblioref">Bollen and Jackman 1985</a>)</span>. None of our Cook’s distance values exceed 1, where values in excess of 1 would indicate a problematic case. Regardless, as a sensitivity analysis, we would perhaps want to estimate our model once more after removing the cases associated with row number <em>201</em>, <em>70</em>, <em>170</em>, and <em>270</em> from our data frame. Overall, we may have found what seems to be four potentially influential multivariate outlier cases, and we should be a bit concerned about satisfying the homoscedasticity of residuals, as the average residuals deviated slightly from zero. It’s possible that removing the four aforementioned cases might also help to address the slight violation of the homoscedasticity of residuals assumption. Next, let’s consider the following statistical assumption: Residual errors are normally distributed for each level of the predictor variable.</p>
<p><strong>Distribution of Residuals Plot:</strong> Moving on to the first plot, which displays the distribution of the residuals, we can use the display to determine whether or not we have satisfied the statistical assumption that the residuals are normally distributed. We see a histogram and density distribution of our residuals with the shape of a normal distribution superimposed. As you can see, our residuals show a mostly normal distribution, which is great and in line with the assumption.</p>
<p><strong>Collinearity:</strong> Moving onto the third diagnostic plot containing the scatterplot matrix, let’s see if we might have any <a href="incrementalvalidity.html#statisticalsignficance_mlr">collinearity concerns</a>. Specifically, let’s focus in on the correlations between the predictor variables (i.e., selection tool variables), as these can be indicators of whether collinearity might be of concern in this model. The correlation between <code>SJT</code> and <code>Interview</code> is .24, and the correlation between <code>Interview</code> and <code>EI</code> is .32 – both of which are acceptable from a collinearity perspective. In contrast, the correlation between <code>SJT</code> and <code>EI</code> is <em>HUGE</em> at .93. Generally speaking, a correlation that is .85 or higher can indicate that two variables are practically indistinguishable from one another, and here the correlation between <code>SJT</code> and <code>EI</code> is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error.</p>
<p>To further explore potential collinearity in a different way, let’s check out the table in our text output called <strong>Collinearity</strong>. The corresponding table shows two indices of collinearity: <strong>tolerance</strong> and <strong>valence inflation factor (VIF)</strong>. Because the VIF is just the reciprocal of the tolerance (i.e., 1/tolerance), let’s focus just on the tolerance statistic. The tolerance statistic is computed based on the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - <em>R</em><sup>2</sup>), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. <em>We typically get concerned about collinearity when the tolerance statistics falls below .20, with a tolerance of .00 being the worst and thus the strongest level of collinearity.</em> Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. In the table, we can see that <code>SJT</code> has a tolerance statistic of .127, which is lower than .20 and indicates that <code>SJT</code> overlaps considerably with the other predictor variables (<code>EI</code>, <code>Interview</code>) in the model in terms of shared variance. Similarly, <code>EI</code> has a troublesome tolerance statistic of .121, which suggests a similar problem. The tolerance for <code>Interview</code> is .873, which suggests low levels of collinearity (which is a good thing). Because <code>SJT</code> and <code>EI</code> both have low tolerance statistics, in this situation, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the correlation matrix (see above). As such, in the current model, we definitely have a collinearity problem.</p>
<p><strong>Summary of Statistical Assumptions Tests:</strong> Given the apparent slight violation of the assumption of homoscedasticity, the four potential multivariate outlier cases (i.e., <em>201</em>, <em>70</em>, <em>170</em>, and <em>270</em>, which correspond to <code>EmployeeID</code> values of <em>EE223</em>, <em>EE92</em>, <em>EE292</em>, <em>EE192</em>, respectively), and the very concerning levels of collinearity involving our <code>SJT</code> and <code>EI</code> variables, we should avoid interpreting the model as currently specified. In fact, we should at the very least remove either the <code>SJT</code> or <code>EI</code> predictor variable from the model to address the most concerning statistical assumption violation. So how do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource-intensive tool. In addition, we might consider applicants’ reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, let’s assume that we have a compelling reason for dropping <code>EI</code> and retaining <code>SJT</code>.</p>
<p><strong>Re-Specify Model to Address Collinearity Issue:</strong> Given our decision to drop <code>EI</code> as a selection tool due to a severe violation of the collinearity assumption (see above), let’s re-specify and re-estimate our multiple linear regression model with just <code>SJT</code> and <code>Interview</code> as predictor variables.</p>
<div class="sourceCode" id="cb1060"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1060-1"><a href="incrementalvalidity.html#cb1060-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-estimate multiple linear regression model </span></span>
<span id="cb1060-2"><a href="incrementalvalidity.html#cb1060-2" aria-hidden="true" tabindex="-1"></a><span class="co"># (with EI variable dropped due to collinearity)</span></span>
<span id="cb1060-3"><a href="incrementalvalidity.html#cb1060-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Regression</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> Interview, <span class="at">data=</span>df)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-626-1.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-626-2.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-626-3.png" width="672" /></p>
<pre><code>## &gt;&gt;&gt; Suggestion
## # Create an R markdown file for interpretative output with  Rmd = &quot;file_name&quot;
## Regression(my_formula=Performance ~ SJT + Interview, data=df, Rmd=&quot;eg&quot;)  
## 
## 
##   BACKGROUND
## 
## Data Frame:  df 
##  
## Response Variable: Performance 
## Predictor Variable 1: SJT 
## Predictor Variable 2: Interview 
##  
## Number of cases (rows) of data:  300 
## Number of cases retained for analysis:  300 
## 
## 
##   BASIC ANALYSIS
## 
##              Estimate    Std Err  t-value  p-value   Lower 95%   Upper 95% 
## (Intercept)     5.527      0.539   10.249    0.000       4.465       6.588 
##         SJT     0.567      0.085    6.714    0.000       0.401       0.734 
##   Interview     0.385      0.064    6.031    0.000       0.260       0.511 
## 
## 
## Standard deviation of residuals:  2.992 for 297 degrees of freedom 
##  
## R-squared:  0.264    Adjusted R-squared:  0.259    PRESS R-squared:  0.236 
## 
## Null hypothesis that all population slope coefficients are 0:
##   F-statistic: 53.339     df: 2 and 297     p-value:  0.000 
## 
## 
##               df    Sum Sq   Mean Sq   F-value   p-value 
##         SJT    1   629.432   629.432    70.303     0.000 
##   Interview    1   325.670   325.670    36.375     0.000 
##  
## Model          2   955.102   477.551    53.339     0.000 
## Residuals    297  2659.095     8.953 
## Performance  299  3614.197    12.088 
## 
## 
##   K-FOLD CROSS-VALIDATION
## 
##   RELATIONS AMONG THE VARIABLES
## 
##               Performance  SJT Interview 
##   Performance        1.00 0.42      0.39 
##           SJT        0.42 1.00      0.24 
##     Interview        0.39 0.24      1.00 
## 
## 
##             Tolerance       VIF 
##         SJT     0.944     1.060 
##   Interview     0.944     1.060 
## 
## 
##  SJT Interview    R2adj    X&#39;s 
##    1         1    0.259      2 
##    1         0    0.171      1 
##    0         1    0.150      1 
##  
## [based on Thomas Lumley&#39;s leaps function from the leaps package] 
##  
## 
## 
##   RESIDUALS AND INFLUENCE
## 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance 
##    [sorted by Cook&#39;s Distance] 
##    [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] 
## --------------------------------------------------------------------- 
##          SJT Interview Performance fitted   resid rstdnt dffits cooks 
##   201  9.000     1.000      24.000 11.019  12.981  4.537  0.736 0.169 
##    70 10.000    12.000       5.000 15.825 -10.825 -3.755 -0.670 0.143 
##   170 10.000    12.000       5.000 15.825 -10.825 -3.755 -0.670 0.143 
##   270 10.000    12.000       5.000 15.825 -10.825 -3.755 -0.670 0.143 
##   233 10.000     6.000      27.000 13.513  13.487  4.709  0.645 0.129 
##     1  9.000     2.000      22.000 11.404  10.596  3.653  0.537 0.092 
##   101  9.000     2.000      22.000 11.404  10.596  3.653  0.537 0.092 
##    69  2.000     1.000      18.000  7.047  10.953  3.775  0.505 0.081 
##   169  2.000     1.000      18.000  7.047  10.953  3.775  0.505 0.081 
##    92  8.000    15.000      10.000 15.846  -5.846 -2.008 -0.430 0.061 
##   192  8.000    15.000      10.000 15.846  -5.846 -2.008 -0.430 0.061 
##   292  8.000    15.000      10.000 15.846  -5.846 -2.008 -0.430 0.061 
##    20  2.000     8.000      18.000  9.744   8.256  2.819  0.397 0.051 
##   120  2.000     8.000      18.000  9.744   8.256  2.819  0.397 0.051 
##   283  9.000     5.000      22.000 12.560   9.440  3.226  0.372 0.045 
##   269  2.000     5.000      18.000  8.588   9.412  3.216  0.371 0.045 
##    97  2.000     3.000      14.000  7.817   6.183  2.092  0.245 0.020 
##   197  2.000     3.000      14.000  7.817   6.183  2.092  0.245 0.020 
##   297  2.000     3.000      14.000  7.817   6.183  2.092  0.245 0.020 
##    82  2.000     4.000      14.000  8.203   5.797  1.959  0.224 0.017 
## 
## 
##   PREDICTION ERROR 
## 
## Data, Predicted, Standard Error of Forecast, 
## 95% Prediction Intervals 
##    [sorted by lower bound of prediction interval] 
##    [to see all intervals do pred_rows=&quot;all&quot;] 
##  ----------------------------------------------
## 
##         SJT Interview Performance   pred    sf pi.lwr pi.upr  width 
##    69 2.000     1.000      18.000  7.047 3.018  1.107 12.987 11.880 
##   169 2.000     1.000      18.000  7.047 3.018  1.107 12.987 11.880 
##    41 1.000     3.000       8.000  7.250 3.021  1.305 13.195 11.891 
## ... 
##   210 8.000     2.000      10.000 10.837 3.015  4.903 16.771 11.868 
##     4 6.000     5.000      11.000 10.858 2.998  4.959 16.757 11.798 
##    47 6.000     5.000      11.000 10.858 2.998  4.959 16.757 11.798 
## ... 
##   222 9.000     9.000      11.000 14.102 3.015  8.168 20.035 11.867 
##    92 8.000    15.000      10.000 15.846 3.057  9.829 21.862 12.033 
##   192 8.000    15.000      10.000 15.846 3.057  9.829 21.862 12.033 
## 
## 
## ---------------------------------- 
## Plot 1: Distribution of Residuals 
## Plot 2: Residuals vs Fitted Values 
## Plot 3: ScatterPlot Matrix 
## ----------------------------------</code></pre>
<p>In a real-world situation, we would once again work through the statistical assumption tests that we did above; however, for sake of brevity, we will assume that the statistical assumptions have been reasonably satisfied in this re-specified model in which only the <code>SJT</code> and <code>Interview</code> variables are included as predictors. Thus, we will feel confident that we can interpret our statistical tests, confidence intervals, and prediction intervals in a meaningful way, beginning with the <em>Background</em> section of the output.</p>
</div>
<div id="interpret_mlr" class="section level3" number="34.2.2">
<h3><span class="header-section-number">34.2.2</span> Interpret Multiple Linear Regression Model Results</h3>
<p><strong>Background:</strong> The <em>Background</em> section of the text output section shows which data frame object was used to estimate the model, the name of the response (outcome, criterion) variable, and the name of the predictor variable. In addition, it shows the number of cases in the data frame as well as how many were used in the estimation of the model; by default, the <code>Regression</code> function uses listwise deletion when one or more of the variables in the model has a missing value, which means that a case with any missing value on one of the focal variables is removed as part of the analysis. Here we can see that all 300 cases in the data frame were retained for the analysis, which means that none of the variables in the model had any missing values.</p>
<p><strong>Basic Analysis:</strong> The <em>Basic Analysis</em> section of the output first displays a table containing the estimated regression model (<strong>Estimated Model for [INSERT OUTCOME VARIABLE NAME]</strong>), including the regression coefficients (slopes, weights) and their standard errors, <em>t</em>-values, <em>p</em>-values, and lower and upper limits of their 95% confidence intervals. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (more on that later). The regression coefficients associated with the predictor variables (<code>SJT</code> and <code>Interview</code>) in relation to the outcome variable (<code>Performance</code>) are, however, of substantive interest. Here, we see that the <em>unstandardized</em> regression coefficient for <code>SJT</code> is .567, and its associated <em>p</em>-value is less than .001 (<em>b</em> = .567, <em>p</em> &lt; .001). [<em>NOTE: Because the regression coefficient is unstandardized, its practical significance cannot be directly interpreted, and it is not a standardized effect size like a correlation coefficient.</em>] Given that the <em>p</em>-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero <em>when statistically controlling for the effects of <code>Interview</code></em>. Further, the 95% confidence interval ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: <em>For every one point increase in situational judgment test (<code>SJT</code>) scores, job performance (<code>Performance</code>) scores increase by .567 points when controlling for the effect of structured inteview (<code>Interview</code>) scores.</em> Next, the <em>unstandardized</em> regression coefficient for <code>Interview</code> is .385, and its associated <em>p</em>-value is less than .001 (<em>b</em> = .385, <em>p</em> &lt; .001). Given that the <em>p</em>-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero <em>when statistically controlling for the effects of <code>SJT</code></em>. Further, the 95% confidence interval ranges from .260 to .511 (i.e., 95% CI[.260, .511]), which indicates that the true population parameter for the association likely falls somewhere between those two values. We can interpret the significant regression coefficient as follows: <em>For every one point increase in structured interview (<code>Interview</code>) scores, job performance (<code>Performance</code>) scores increase by .385 points when controlling for the effect of situational judgment test (<code>SJT</code>) scores.</em></p>
<p>Using the intercept and predictor variable coefficient estimates from our multiple linear regression model, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\)</span></p>
<p>Let’s assume, for example, that a future applicant scores 5 points on the <code>SJT</code> and 7 points on the <code>Interview</code>. If we plug those values into our equation, we get a predictor criterion score (i.e., <code>Performance</code> score) of 11.057:</p>
<p><span class="math inline">\(11.057 = 5.527 + (.567 * 5) + (.385 * 7)\)</span></p>
<p>Thus, based on our estimate model (i.e., equation), we are able to predict scores on the criterion variable <code>Performance</code> – something we’ll cover in greater depth in the next chapter.</p>
<p>The <strong>Model Fit</strong> section of the output appears below the table containing the regression coefficient estimates. In this section, you will find the (unadjusted) <em>R</em>-squared (<em>R</em><sup>2</sup>) estimate, which is an indicator of the model’s fit to the data as well as the extent to which the predictor variable explains variance (i.e., variability) in the outcome variable. The <em>R</em>-squared (<em>R</em><sup>2</sup>) value of .264 indicates the extent to which the predictor variables collectively explain variance in the outcome variable in this sample – or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in <code>Performance</code> is explained by <code>SJT</code> and <code>Interview</code> collectively. You can also think of the <em>R</em><sup>2</sup> values as <strong>effect sizes</strong> (i.e., indicators of <em>practical</em> significance) at the model level. Here are some rules of thumb for qualitatively interpreting the magnitude of the <em>R</em><sup>2</sup> effect size, which is another way of saying “determining the level of practical significance”:</p>
<table>
<thead>
<tr class="header">
<th><em>R</em><sup>2</sup></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.01</td>
<td>Small</td>
</tr>
<tr class="even">
<td>.09</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>.25</td>
<td>Large</td>
</tr>
</tbody>
</table>
<p>The raw, unadjusted <em>R</em>-squared (<em>R</em><sup>2</sup>) value, however, is sensitive to the sample size and the number of predictor variables in the model. The <em>adjusted</em> <em>R</em><sup>2</sup> value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The <em>adjusted</em> <em>R</em><sup>2</sup> is a better indicator of the magnitude of the association in the underlying <em>population</em> and thus tends to be more accurate. Here we see that the adjusted <em>R</em><sup>2</sup> value is slightly smaller at .259 (or 25.9%). If space permits, it’s a good idea to report both values, but given how close the unadjusted and adjusted <em>R</em><sup>2</sup> estimates tend to be, reporting and interpreting just the unadjusted <em>R</em><sup>2</sup> is usually fine – and is typically customary.</p>
<p>The <em>Model Fit</em> section also contains the sum of squares, <em>F</em>-value, and <em>p</em>-value includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS).. In this table, we are mostly interested in the <em>F</em>-value and its associated <em>p</em>-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the <em>null model</em> does not include any predictor variables. In other words, the <em>F</em>-value and <em>p</em>-value are associated with the <em>R</em><sup>2</sup> value and whether the unadjusted and adjusted <em>R</em><sup>2</sup> values are significantly different from zero. In this example, we see that the <em>F</em>-value is 45.317 and that its associated <em>p</em>-value is less than .001 – the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the <em>R</em><sup>2</sup> values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. Again, you can think of the <em>R</em><sup>2</sup> value as an indicator of <strong>effect size</strong> at the model level, and in the table above, you will find the conventional thresholds for qualitatively describing a small, medium, or large <em>R</em><sup>2</sup> value.</p>
<p><strong>Relations Among the Variables:</strong> The section called <em>Relations Among the Variables</em> displays the zero-order (Pearson product-moment) correlation between the predictor variable and outcome variable. This correlation can be used to gauge the <strong>effect size</strong> of a predictor variable in relation to the outcome variable, as a correlation coefficient is a standardized metric that can be compared across samples - unlike an unstandardized regression coefficient; however, it’s important to note that such a correlation represents a bivariate (i.e., two-variable) association – and thus doesn’t involve statistical control like our multiple linear regression model.</p>
<p><strong>Collinearity:</strong> The <em>Colinearity</em> section displays the tolerance statistics. As you can see, both tolerance statistics are .944 and thus close to 1.00, which indicates very low levels of collinearity. We would be concerned if a tolerance statistic fell below .20, which is not the case for this model when applied to this sample of employees.</p>
<p><strong>Prediction Error:</strong> In the output section called <em>Prediction Error</em>, information about the forecasting error and prediction intervals. This section moves us toward what would be considered true <em>predictive analytics</em> and <em>machine learning</em>; however, because we only have a single dataset to train our model and test it, we’re not performing true predictive analytics. As such, we won’t pay much attention to interpreting this section of the output in this tutorial. With that said, if you’re curious, feel free to read on. When performing true predictive analytics, we typically divide our data into at least two datasets. Often, we have at least one <em>training</em> dataset that we use to “train” or estimate a given model; often, we have more than one training dataset, though. After training the model on one or more training datasets, we then evaluate the model on a <em>test</em> dataset that should contain data from an entirely different set of cases than the training dataset(s). As a more rigorous approach, we can instead use a <em>validation</em> dataset to evaluate the training dataset(s), and after we’ve picked the model that performs best on the validation set, we then pass the model along to the test dataset to see if we can confirm the results. What <code>lessR</code> is doing in the <em>Prediction Error</em> section is taking the model you estimated using the focal dataset, which we could call our training dataset, and then it takes the values for our predictor and outcome variables from our sample and plugs them into the model and accounts for forecasting error for each set of values. Specifically, the standard error of forecast (sf) for each set of values is base on a combination of the standard deviation of the residuals for the entire model (modeling error) and the sampling error for the value on the regression line. Consequently, each set of values is assigned a lower and upper bound of a prediction interval for the outcome variable. The width of the prediction interval is specific to the values used to test the model, so the widths vary across the values. In fact, the further one gets from the mean of the outcome variable (in either direction), the wider the prediction intervals become. The 95% prediction intervals, along with the 95% confidence intervals and regression line of best fit, are plotted on the third and final plot of the function output. As you can, see the prediction intervals are the outermost lines as they include both sampling error and the modeling error, whereas the confidence intervals are the inner lines, as they reflect just the sampling error.</p>
<p><strong>Sample Technical Write-Up of Results:</strong> A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (<code>SJT</code>), an emotional intelligence assessment (<code>EI</code>), and a structured interview (<code>Interview</code>) in relation to job performance (<code>Performance</code>). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (<em>b</em> = .567, <em>p</em> &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (<em>b</em> = .385, <em>p</em> &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (<em>R</em><sup>2</sup> = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion.</p>
</div>
<div id="standardized_mlr" class="section level3" number="34.2.3">
<h3><span class="header-section-number">34.2.3</span> Optional: Obtaining Standardized Coefficients</h3>
<p>As an optional detour, if you would like to estimate the same simple linear regression model but view the <em>standardized</em> regression coefficients, simply add the argument <code>new_scale="z"</code> to your previous <code>Regression</code> function; that argument rescales your outcome and predictor variables to <em>z</em>-scores prior to estimating the model, which in effect produces standardized coefficients.</p>
<div class="sourceCode" id="cb1062"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1062-1"><a href="incrementalvalidity.html#cb1062-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model with standardized coefficients</span></span>
<span id="cb1062-2"><a href="incrementalvalidity.html#cb1062-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Regression</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> EI, <span class="at">data=</span>df, </span>
<span id="cb1062-3"><a href="incrementalvalidity.html#cb1062-3" aria-hidden="true" tabindex="-1"></a>           <span class="at">new_scale=</span><span class="st">&quot;z&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Rescaled Data, First Six Rows
##   Performance    SJT    EI
## 1       3.240  1.632 1.609
## 2       0.076  1.158 0.657
## 3      -1.650  0.683 0.657
## 4       0.076  0.209 0.181
## 5       0.363  0.209 0.181
## 6       0.363 -0.266 0.181</code></pre>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-627-1.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-627-2.png" width="672" /><img src="R-for-HR_files/figure-html/unnamed-chunk-627-3.png" width="672" /></p>
<pre><code>## &gt;&gt;&gt; Suggestion
## # Create an R markdown file for interpretative output with  Rmd = &quot;file_name&quot;
## Regression(my_formula=Performance ~ SJT + EI, data=df, new_scale=&quot;z&quot;, Rmd=&quot;eg&quot;)  
## 
## 
##   BACKGROUND
## 
## Data Frame:  df 
##  
## Response Variable: Performance 
## Predictor Variable 1: SJT 
## Predictor Variable 2: EI 
##  
## Number of cases (rows) of data:  300 
## Number of cases retained for analysis:  300 
##  
## Data are Standardized 
## 
## 
##   BASIC ANALYSIS
## 
##              Estimate    Std Err  t-value  p-value   Lower 95%   Upper 95% 
## (Intercept)    -0.000      0.052   -0.000    1.000      -0.103       0.103 
##         SJT     0.158      0.145    1.089    0.277      -0.127       0.443 
##          EI     0.278      0.145    1.919    0.056      -0.007       0.564 
## 
## 
## Standard deviation of residuals:  0.906 for 297 degrees of freedom 
##  
## R-squared:  0.184    Adjusted R-squared:  0.179    PRESS R-squared:  0.165 
## 
## Null hypothesis that all population slope coefficients are 0:
##   F-statistic: 33.551     df: 2 and 297     p-value:  0.000 
## 
## 
##               df    Sum Sq   Mean Sq   F-value   p-value 
##         SJT    1    52.079    52.079    63.418     0.000 
##          EI    1     3.024     3.024     3.683     0.056 
##  
## Model          2    55.104    27.552    33.551     0.000 
## Residuals    297   243.898     0.821 
## Performance  299   299.002     1.000 
## 
## 
##   K-FOLD CROSS-VALIDATION
## 
##   RELATIONS AMONG THE VARIABLES
## 
##               Performance  SJT   EI 
##   Performance        1.00 0.42 0.43 
##           SJT        0.42 1.00 0.93 
##            EI        0.43 0.93 1.00 
## 
## 
##       Tolerance       VIF 
##   SJT     0.131     7.655 
##    EI     0.131     7.655 
## 
## 
##  SJT  EI    R2adj    X&#39;s 
##    1   1    0.179      2 
##    0   1    0.178      1 
##    1   0    0.171      1 
##  
## [based on Thomas Lumley&#39;s leaps function from the leaps package] 
##  
## 
## 
##   RESIDUALS AND INFLUENCE
## 
## Data, Fitted, Residual, Studentized Residual, Dffits, Cook&#39;s Distance 
##    [sorted by Cook&#39;s Distance] 
##    [res_rows = 20, out of 300 rows of data, or do res_rows=&quot;all&quot;] 
## ----------------------------------------------------------------- 
##          SJT     EI Performance fitted  resid rstdnt dffits cooks 
##   233  2.107  2.086       4.678  0.913  3.765  4.316  0.594 0.111 
##    70  2.107  1.609      -1.650  0.780 -2.430 -2.741 -0.405 0.054 
##   170  2.107  1.609      -1.650  0.780 -2.430 -2.741 -0.405 0.054 
##   270  2.107  1.609      -1.650  0.780 -2.430 -2.741 -0.405 0.054 
##   201  1.632  1.609       3.815  0.705  3.110  3.519  0.395 0.050 
##   283  1.632  1.133       3.240  0.573  2.667  3.007  0.385 0.048 
##    20 -1.689 -1.724       2.089 -0.746  2.835  3.199  0.373 0.045 
##    69 -1.689 -1.724       2.089 -0.746  2.835  3.199  0.373 0.045 
##   120 -1.689 -1.724       2.089 -0.746  2.835  3.199  0.373 0.045 
##   169 -1.689 -1.724       2.089 -0.746  2.835  3.199  0.373 0.045 
##   269 -1.689 -1.724       2.089 -0.746  2.835  3.199  0.373 0.045 
##     1  1.632  1.609       3.240  0.705  2.535  2.848  0.320 0.033 
##   101  1.632  1.609       3.240  0.705  2.535  2.848  0.320 0.033 
##    31 -2.164 -0.771      -1.362 -0.556 -0.806 -0.916 -0.229 0.017 
##   131 -2.164 -0.771      -1.362 -0.556 -0.806 -0.916 -0.229 0.017 
##   231 -2.164 -0.771      -1.362 -0.556 -0.806 -0.916 -0.229 0.017 
##    82 -1.689 -1.724       0.939 -0.746  1.685  1.881  0.220 0.016 
##    97 -1.689 -1.724       0.939 -0.746  1.685  1.881  0.220 0.016 
##   182 -1.689 -1.724       0.939 -0.746  1.685  1.881  0.220 0.016 
##   197 -1.689 -1.724       0.939 -0.746  1.685  1.881  0.220 0.016 
## 
## 
##   PREDICTION ERROR 
## 
## Data, Predicted, Standard Error of Forecast, 
## 95% Prediction Intervals 
##    [sorted by lower bound of prediction interval] 
##    [to see all intervals do pred_rows=&quot;all&quot;] 
##  ----------------------------------------------
## 
##          SJT     EI Performance   pred    sf pi.lwr pi.upr width 
##    20 -1.689 -1.724       2.089 -0.746 0.912 -2.542  1.049 3.591 
##    40 -1.689 -1.724      -1.650 -0.746 0.912 -2.542  1.049 3.591 
##    55 -1.689 -1.724       0.076 -0.746 0.912 -2.542  1.049 3.591 
## ... 
##   211  0.683 -0.295      -0.212  0.026 0.918 -1.782  1.833 3.615 
##     4  0.209  0.181       0.076  0.083 0.908 -1.703  1.870 3.573 
##     5  0.209  0.181       0.363  0.083 0.908 -1.703  1.870 3.573 
## ... 
##   294  0.209  0.181      -0.212  0.083 0.908 -1.703  1.870 3.573 
##    28 -0.740  1.133       0.363  0.198 0.946 -1.664  2.061 3.724 
##   128 -0.740  1.133       0.363  0.198 0.946 -1.664  2.061 3.724 
## 
## 
## ---------------------------------- 
## Plot 1: Distribution of Residuals 
## Plot 2: Residuals vs Fitted Values 
## Plot 3: ScatterPlot Matrix 
## ----------------------------------</code></pre>
<p>In the <em>Estimated Model</em> section of the output, note that the intercept value is zeroed out due to the standardization, and the regression coefficients associated with <code>SJT</code> and <code>Interview</code> are now in standardized units (<span class="math inline">\(\beta\)</span> = .344, <em>p</em> &lt; .001 and <span class="math inline">\(\beta\)</span> = .309, <em>p</em> &lt; .001, respectively). Note that the <em>p</em>-values are the same as the unstandardized regression coefficients we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When statistically controlling for the effect of <code>Interview</code>, for every one standardized unit increase in <code>SJT</code>, <code>Performance</code> increases by .344 standardized units, and when statistically controlling for the effect of <code>SJT</code>, for every one standardized unit increase in <code>Interview</code>, <code>Performance</code> increases by .309 standardized units.</p>
</div>
</div>
<div id="summary_incrementalvalidity" class="section level2" number="34.3">
<h2><span class="header-section-number">34.3</span> Summary</h2>
<p>In this chapter, we learned how to estimate a multiple linear regression model using the <code>Regression</code> function from the <code>lessR</code> package in order to estimate whether evidence of incremental validity exists.</p>
</div>
<div id="incrementalvalidity_supplement" class="section level2" number="34.4">
<h2><span class="header-section-number">34.4</span> Chapter Supplement</h2>
<p>In addition to the <code>Regression</code> function from the <code>lessR</code> package covered <a href="incrementalvalidity.html#estimate_mlr">above</a>, we can use the <code>lm</code> function from base R to estimate a multiple linear regression model. Because this function comes from base R, we do not need to install and access an additional package. In this supplement, you will also have an opportunity to learn how to make an APA (American Psychological Association) style table of regression results.</p>
<div id="incrementalvalidity_supplement_functions" class="section level4" number="34.4.0.1">
<h4><span class="header-section-number">34.4.0.1</span> Functions &amp; Packages Introduced</h4>
<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>lm</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="even">
<td><code>print</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="odd">
<td><code>vif</code></td>
<td><code>car</code></td>
</tr>
<tr class="even">
<td><code>plot</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="odd">
<td><code>cooks.distance</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="even">
<td><code>sort</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="odd">
<td><code>head</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="even">
<td><code>summary</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="odd">
<td><code>confint</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="even">
<td><code>cor</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="odd">
<td><code>scale</code></td>
<td>base <code>R</code></td>
</tr>
<tr class="even">
<td><code>apa.reg.table</code></td>
<td><code>apaTables</code></td>
</tr>
</tbody>
</table>
</div>
<div id="incrementalvalidity_initsteps_supplement" class="section level4" number="34.4.0.2">
<h4><span class="header-section-number">34.4.0.2</span> Initial Steps</h4>
<p>If required, please refer to the <a href="incrementalvalidity.html#initsteps_mlr">Initial Steps</a> section from this chapter for more information on these initial steps.</p>
<div class="sourceCode" id="cb1065"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1065-1"><a href="incrementalvalidity.html#cb1065-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set your working directory</span></span>
<span id="cb1065-2"><a href="incrementalvalidity.html#cb1065-2" aria-hidden="true" tabindex="-1"></a><span class="fu">setwd</span>(<span class="st">&quot;H:/RWorkshop&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1066"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1066-1"><a href="incrementalvalidity.html#cb1066-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install readr package if you haven&#39;t already</span></span>
<span id="cb1066-2"><a href="incrementalvalidity.html#cb1066-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [Note: You don&#39;t need to install a package every </span></span>
<span id="cb1066-3"><a href="incrementalvalidity.html#cb1066-3" aria-hidden="true" tabindex="-1"></a><span class="co"># time you wish to access it]</span></span>
<span id="cb1066-4"><a href="incrementalvalidity.html#cb1066-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;readr&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1067"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1067-1"><a href="incrementalvalidity.html#cb1067-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access readr package</span></span>
<span id="cb1067-2"><a href="incrementalvalidity.html#cb1067-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb1067-3"><a href="incrementalvalidity.html#cb1067-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1067-4"><a href="incrementalvalidity.html#cb1067-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data and name data frame (tibble) object</span></span>
<span id="cb1067-5"><a href="incrementalvalidity.html#cb1067-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;ConcurrentValidation.csv&quot;</span>)</span></code></pre></div>
<pre><code>## 
## -- Column specification -----------------------------------------------------------------------------------------------------------
## cols(
##   EmployeeID = col_character(),
##   SJT = col_double(),
##   EI = col_double(),
##   Interview = col_double(),
##   Performance = col_double()
## )</code></pre>
<div class="sourceCode" id="cb1069"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1069-1"><a href="incrementalvalidity.html#cb1069-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the names of the variables in the data frame (tibble) objects</span></span>
<span id="cb1069-2"><a href="incrementalvalidity.html#cb1069-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df)</span></code></pre></div>
<pre><code>## [1] &quot;EmployeeID&quot;  &quot;SJT&quot;         &quot;EI&quot;          &quot;Interview&quot;   &quot;Performance&quot;</code></pre>
<div class="sourceCode" id="cb1071"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1071-1"><a href="incrementalvalidity.html#cb1071-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View variable type for each variable in data frame</span></span>
<span id="cb1071-2"><a href="incrementalvalidity.html#cb1071-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## spec_tbl_df[,5] [300 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
##  $ EmployeeID : chr [1:300] &quot;EE23&quot; &quot;EE24&quot; &quot;EE25&quot; &quot;EE26&quot; ...
##  $ SJT        : num [1:300] 9 8 7 6 6 5 5 4 3 8 ...
##  $ EI         : num [1:300] 8 6 6 5 5 5 4 2 2 7 ...
##  $ Interview  : num [1:300] 2 3 4 5 6 7 7 8 9 2 ...
##  $ Performance: num [1:300] 22 11 5 11 12 12 12 12 12 10 ...
##  - attr(*, &quot;spec&quot;)=
##   .. cols(
##   ..   EmployeeID = col_character(),
##   ..   SJT = col_double(),
##   ..   EI = col_double(),
##   ..   Interview = col_double(),
##   ..   Performance = col_double()
##   .. )</code></pre>
<div class="sourceCode" id="cb1073"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1073-1"><a href="incrementalvalidity.html#cb1073-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View first 6 rows of data frame</span></span>
<span id="cb1073-2"><a href="incrementalvalidity.html#cb1073-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 5
##   EmployeeID   SJT    EI Interview Performance
##   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 EE23           9     8         2          22
## 2 EE24           8     6         3          11
## 3 EE25           7     6         4           5
## 4 EE26           6     5         5          11
## 5 EE27           6     5         6          12
## 6 EE28           5     5         7          12</code></pre>
</div>
<div id="lm_function_mlr" class="section level3" number="34.4.1">
<h3><span class="header-section-number">34.4.1</span> <code>lm</code> Function from Base R</h3>
<p>In the following section, we will learn how to apply the <code>lm</code> function from base R to estimate a multiple linear regression model. Please note that we are doing some of the operations in a different order than what we did with the <code>Regression</code> function from <code>lessR</code>. Why? Well, the <code>Regression</code> function from <code>lessR</code> generates a number of diagnostics automatically (by default), and thus we took advantage of that in the previous section. With the <code>lm</code> function from base R, we have to piece together the diagnostics the old-fashioned way, which also happens to mean that we have more control over the order in which we do things.</p>
<p>As a critical first step, we must specify the regression model using the <code>lm</code> function. To use the <code>lm</code> (linear model) function, create a name for your regression model (<code>reg.mod1</code>) using the <code>&lt;-</code> symbol. Next, type the name of the <code>lm</code> function. As the first argument, specify the regression model you wish to estimate. Specifically, type the name of the outcome variable (<code>Performance</code>) to the left of the <code>~</code> operator and the name of the predictor variables (<code>SJT</code>, <code>EI</code>, <code>Interview</code>) to the right of the <code>~</code> operator. We are telling the function to “regress <code>Performance</code> on <code>SJT</code>, <code>EI</code>, and <code>Interview</code>.” As the second argument, type <code>data=</code> followed by the name of the data frame object to which the variables in your model belong (<code>df</code>). Now we are ready to determine whether we have satisfied key statistical assumptions and, if so, review the summary of our model estimation results.</p>
<div class="sourceCode" id="cb1075"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1075-1"><a href="incrementalvalidity.html#cb1075-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify multiple linear regression model</span></span>
<span id="cb1075-2"><a href="incrementalvalidity.html#cb1075-2" aria-hidden="true" tabindex="-1"></a>reg.mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> EI <span class="sc">+</span> Interview, <span class="at">data=</span>df)</span></code></pre></div>
<p><em>Note:</em> You won’t see any output in your console by specifying the regression model above. If you print the model (<code>reg.mod1</code>) to your console using the <code>print</code> function from base R, you only get the regression coefficients (but no statistical tests or model fit information). Later on, we’ll apply a different function to obtain the full model results.</p>
<div class="sourceCode" id="cb1076"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1076-1"><a href="incrementalvalidity.html#cb1076-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print very brief model information (NOT NECESSARY)</span></span>
<span id="cb1076-2"><a href="incrementalvalidity.html#cb1076-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(reg.mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ SJT + EI + Interview, data = df)
## 
## Coefficients:
## (Intercept)          SJT           EI    Interview  
##     5.60170      0.48470      0.09147      0.37827</code></pre>
<p>Now that we have specified our model, let’s see if we might have any worrisome collinearity among our predictors, which if you recall, a key statistical assumption is that the model is free of collinearity. Identifying problematic collinearity typically means we need to re-specify our model by perhaps dropping a predictor variable or two. Thus, let’s start with testing the assumption related to collinearity. Creating a correlation matrix is one great way to identify possible collinearity issues, so let’s start with a correlation matrix of the variables in our data frame. Let’s use the <code>cor</code> function from base R. Before doing, so let’s create a temporary data frame object (<code>temp</code>) in which we drop the <code>EmployeeID</code> variable from the original <code>df</code> data frame object; we need to do this because the <code>cor</code> function only accepts numeric variables, and the <code>EmployeeID</code> variable is non-numeric.
, and enter the name of our data frame as the sole argument. To learn more about how to remove variables, feel free to check out the <a href="filter.html#filter">chapter on filtering</a></p>
<div class="sourceCode" id="cb1078"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1078-1"><a href="incrementalvalidity.html#cb1078-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create temporary data frame object</span></span>
<span id="cb1078-2"><a href="incrementalvalidity.html#cb1078-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with EmployeeID variable removed</span></span>
<span id="cb1078-3"><a href="incrementalvalidity.html#cb1078-3" aria-hidden="true" tabindex="-1"></a>temp <span class="ot">&lt;-</span> <span class="fu">subset</span>(df, <span class="at">select=</span><span class="sc">-</span>EmployeeID)</span>
<span id="cb1078-4"><a href="incrementalvalidity.html#cb1078-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1078-5"><a href="incrementalvalidity.html#cb1078-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create basic correlation matrix</span></span>
<span id="cb1078-6"><a href="incrementalvalidity.html#cb1078-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(temp)</span></code></pre></div>
<pre><code>##                   SJT        EI Interview Performance
## SJT         1.0000000 0.9324020 0.2373394   0.4173192
## EI          0.9324020 1.0000000 0.3175592   0.4255306
## Interview   0.2373394 0.3175592 1.0000000   0.3906502
## Performance 0.4173192 0.4255306 0.3906502   1.0000000</code></pre>
<p>High correlations between pairs of predictor variables are indicative of high collinearity. The correlation between <code>SJT</code> and <code>Interview</code> is approximately .24, and the correlation between <code>EI</code> and <code>Interview</code> is .32, both of which are in the acceptable range. The correlation between <code>SJT</code> and <code>EI</code> is <em>HUGE</em> at .93. Generally speaking, a correlation that is .85 or higher can indicate that two predictors are practically distinguishable from one another, and here the correlation between <code>SJT</code> and <code>EmotionalIntelligence</code> is a whopping .93! These two selection tools seem to be measuring the same thing and are mostly redundant aside from some likely measurement error.</p>
<p>To further explore this collinearity issue, let’s estimate two indices of collinearity: tolerance and valence inflation factor (VIF). If you haven’t already, install the <code>car</code> package which contains the <code>vif</code> function we will use.</p>
<div class="sourceCode" id="cb1080"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1080-1"><a href="incrementalvalidity.html#cb1080-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install car package if you haven&#39;t already</span></span>
<span id="cb1080-2"><a href="incrementalvalidity.html#cb1080-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;car&quot;</span>)</span></code></pre></div>
<p>Now, access the <code>car</code> package using the <code>library</code> function.</p>
<div class="sourceCode" id="cb1081"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1081-1"><a href="incrementalvalidity.html#cb1081-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access car package</span></span>
<span id="cb1081-2"><a href="incrementalvalidity.html#cb1081-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code></pre></div>
<div class="sourceCode" id="cb1082"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1082-1"><a href="incrementalvalidity.html#cb1082-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute VIF statistic</span></span>
<span id="cb1082-2"><a href="incrementalvalidity.html#cb1082-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(reg.mod1)</span></code></pre></div>
<pre><code>##       SJT        EI Interview 
##  7.887216  8.277683  1.145830</code></pre>
<p>Next, the tolerance statistic is just the reciprocal of the VIF (1/VIF), and generally, I find it to be easier to interpret because the tolerance statistic represents the shared variance (<em>R</em><sup>2</sup>) of just the predictor variables in a single model (excluding the outcome variable) and subtracting that value from 1 (1 - <em>R</em><sup>2</sup>), where a focal predictor variable serves as the outcome and the other(s) (collectively) explain variance in that predictor variable. We typically get concerned when the tolerance statistics approaches .20, as the closer it gets to .00, the higher the collinearity. Ideally, we want the tolerance statistic to approach 1.00, as this indicates that there are lower levels of collinearity. To compute the tolerance statistic, we just divide <em>1</em> by the VIF.</p>
<div class="sourceCode" id="cb1084"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1084-1"><a href="incrementalvalidity.html#cb1084-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute tolerance statistic</span></span>
<span id="cb1084-2"><a href="incrementalvalidity.html#cb1084-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> <span class="fu">vif</span>(reg.mod1)</span></code></pre></div>
<pre><code>##       SJT        EI Interview 
## 0.1267875 0.1208068 0.8727299</code></pre>
<p>In the table, we can see that <code>SJT</code> has a tolerance statistic of .127, which is lower than .20 and indicates that <code>SJT</code> overlaps considerable with the other predictor variables (<code>EI</code>, <code>Interview</code>) in terms of shared variance. Similarly, the <code>EI</code> has a tolerance statistic of .121, which suggests a similar problem. Because <code>SJT</code> and <code>EI</code> both have very low tolerance statistics that fall below .20, we can deduce that they must be the collinearity culprits. And this corroborates what we saw in the scatterplot matrix plot. As such, in the current model, we definitely have a collinearity problem involving <code>SJT</code> and <code>EI</code>, which means interpreting the rest of the output would not be appropriate.</p>
<p>When we face severe collinearity between scores on two selection tools, as we do in this scenario, we need to make a thoughtful decision. Given that the correlation between <code>SJT</code> and <code>EI</code> is extremely high at .93, these variables are from a statistical perspective essentially redundant. How do we determine which selection tool to retain? Well, that depends on a number of practical factors. For example, we would likely be concerned with the amount of resources (e.g., time, money) it requires to administer each of these selection tools, and given that they are apparently redundant, we might go with the least resource intensive tool. In addition, we might consider applicants’ reactions to each of the tools. If applicants tend to dislike one of the tools or perceive it as unfair, then that one might be a candidate for removal. As another consideration, we might review whether one of these tools results in adverse (disparate) impact, but given there high correlation, if one results in adverse impact, then the other most certainly will too. For the sake of this tutorial, let’s assume that we have a compelling reason for dropping <code>EI</code> and retaining <code>SJT</code>.</p>
<p>Given our decision to drop <code>EI</code> as a selection tool due to a violation of the collinearity assumption, let’s re-specify and re-estimate our multiple linear regression model with just <code>SJT</code> and <code>Interview</code> as predictor variables. This time, let’s name our model object <code>reg.mod2</code>.</p>
<div class="sourceCode" id="cb1086"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1086-1"><a href="incrementalvalidity.html#cb1086-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-specify multiple linear regression model </span></span>
<span id="cb1086-2"><a href="incrementalvalidity.html#cb1086-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with just SJT &amp; Interview as predictor variables</span></span>
<span id="cb1086-3"><a href="incrementalvalidity.html#cb1086-3" aria-hidden="true" tabindex="-1"></a>reg.mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> Interview, <span class="at">data=</span>df)</span></code></pre></div>
<p>Let’s quickly run the VIF and tolerance statistics to see if collinearity is an issue with the two predictors in this new model.</p>
<div class="sourceCode" id="cb1087"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1087-1"><a href="incrementalvalidity.html#cb1087-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute VIF statistic</span></span>
<span id="cb1087-2"><a href="incrementalvalidity.html#cb1087-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(reg.mod2)</span></code></pre></div>
<pre><code>##       SJT Interview 
##  1.059692  1.059692</code></pre>
<div class="sourceCode" id="cb1089"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1089-1"><a href="incrementalvalidity.html#cb1089-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute tolerance statistic</span></span>
<span id="cb1089-2"><a href="incrementalvalidity.html#cb1089-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> <span class="fu">vif</span>(reg.mod2)</span></code></pre></div>
<pre><code>##       SJT Interview 
##   0.94367   0.94367</code></pre>
<p>Both tolerance statistics are identical (because we only have two predictors in the model), and both values are .943, which is nearly 1.00. Accordingly, we don’t have any concerns with collinearity in this new model.</p>
<p>We’re ready to move on to evaluating other important statistical assumptions.</p>
<p><strong>Statistical Assumptions:</strong> Let’s look at some diagnostics to determine whether we have reason to believe we have met the other statistical assumptions described at the beginning of this tutorial. We will generate plots and other output to inform our conclusions whether we have satisfied certain statistical assumptions.</p>
<p>We will begin by generating a scatterplot displaying the association between the <em>fitted (predicted) values and residuals</em>. To do so, we will use the <code>plot</code> function from base <code>R</code>. As the first argument, enter the name of the regression model object you created above (<code>reg.mod2</code>). As the second argument, type the numeral <em>1</em>, which will request the first of four possible diagnostic plots, of which we will review three.</p>
<div class="sourceCode" id="cb1091"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1091-1"><a href="incrementalvalidity.html#cb1091-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics plot: fitted values &amp; residuals</span></span>
<span id="cb1091-2"><a href="incrementalvalidity.html#cb1091-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(reg.mod2, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-643-1.png" width="672" /></p>
<p>The resulting plot shows the association between the fitted values (x-axis) and the residuals (y-axis). If you recall, the residuals are the error in our estimations - or in other words, how much our fitted values for the outcome variable deviate from the observed (actual) values for the outcome variable. The term “fitted values” is another way of saying predicted values for the outcome variable, but the language “fitted” is more precise here. The horizontal dotted line is drawn at the residual value of zero; remember, our goal is to minimize the size of residuals (i.e., error). The solid line shows the deviations from zero of the residuals for each fitted value, and the greater the solid line deviates from the dotted line, the more likely that (a) variances of the residuals may <em>not</em> be equal for each level of the predictor variable (violation of the assumption of homoscedasticity), (b) the average residual error value may <em>not</em> be zero for each level of the predictor variable, and (c) there may be potential multivariate outliers influencing the fitted (predicted) values. In this plot, we can see that the variances of the residuals seem to be about equal across observations (evidence of homoscedasticity), the average residual error value appears to be about zero (which is good), and there appears to be three cases that are flagged as a potential multivariate outlier (i.e., row numbers <em>69</em>, <em>201</em>, and <em>233</em>).</p>
<p>As an additional diagnostic tool, we can plot a Q-Q plot, which provides an indication as to whether the residuals are normality distributed (one of our statistical assumptions). Simply adapt the <code>plot</code> script from above, but this time, enter the numeral <em>2</em> (instead of <em>1</em>) to request the second diagnostic plot.</p>
<div class="sourceCode" id="cb1092"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1092-1"><a href="incrementalvalidity.html#cb1092-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics plot: normal Q-Q plot</span></span>
<span id="cb1092-2"><a href="incrementalvalidity.html#cb1092-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(reg.mod2, <span class="dv">2</span>)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-644-1.png" width="672" /></p>
<p>Normally distributed residuals will fall along the dotted diagonal line. As you can see, many of the residuals fall on or near the line with the exception of those three potential outlier cases that we identified in the previous plot: row numbers <em>69</em>, <em>201</em>, and <em>233</em>. We also see some deviations from the dotted line at the low and high ends of the theoretical quantiles, which shows some departure from normality in the residuals.</p>
<p>As the last diagnostic plot, let’s look at Cook’s distance (D) across cases. Once again, adapt the <code>plot</code> script from above, but this time, enter the numeral <em>4</em> to request the fourth diagnostic plot. We’re skipping the third plot.</p>
<div class="sourceCode" id="cb1093"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1093-1"><a href="incrementalvalidity.html#cb1093-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Diagnostics plot: Cook&#39;s Distance plot</span></span>
<span id="cb1093-2"><a href="incrementalvalidity.html#cb1093-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(reg.mod2, <span class="dv">4</span>)</span></code></pre></div>
<p><img src="R-for-HR_files/figure-html/unnamed-chunk-645-1.png" width="672" /></p>
<p>Here we see case associated with row number <em>201</em> again, but we also see cases associate with row numbers <em>70</em>, <em>170</em>, and <em>201</em> based on Cook’s distance.</p>
<p>We can also grab the cases with the highest Cook’s distances. Let’s create an object called <code>cooksD</code> that we will assign a vector of Cook’s distance values to using the <code>cooks.distance</code> function from base <code>R</code>. Just enter the name of the regression model object (<code>reg.mod1</code>) as the sole parenthetical argument. Next, update the object we called <code>cooksD</code> by applying the <code>sort</code> function from base <code>R</code> and entering the <code>cooksD</code> object as the first object and <code>decreasing=TRUE</code> as the second argument; this will sort the Cook’s distance values in descending order. Finally, apply the <code>head</code> function from base <code>R</code>, and as the first argument enter the name of the <code>cooksD</code> object that we just sorted; as the second argument, type <code>n=20</code> to show the top 20 rows, as opposed to the default top 6 rows.</p>
<div class="sourceCode" id="cb1094"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1094-1"><a href="incrementalvalidity.html#cb1094-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate Cook&#39;s distance values</span></span>
<span id="cb1094-2"><a href="incrementalvalidity.html#cb1094-2" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">cooks.distance</span>(reg.mod2)</span>
<span id="cb1094-3"><a href="incrementalvalidity.html#cb1094-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1094-4"><a href="incrementalvalidity.html#cb1094-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort Cook&#39;s distance values in descending order               </span></span>
<span id="cb1094-5"><a href="incrementalvalidity.html#cb1094-5" aria-hidden="true" tabindex="-1"></a>cooksD <span class="ot">&lt;-</span> <span class="fu">sort</span>(cooksD, <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb1094-6"><a href="incrementalvalidity.html#cb1094-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1094-7"><a href="incrementalvalidity.html#cb1094-7" aria-hidden="true" tabindex="-1"></a><span class="co"># View top-20 Cook&#39;s distance values</span></span>
<span id="cb1094-8"><a href="incrementalvalidity.html#cb1094-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(cooksD, <span class="at">n=</span><span class="dv">20</span>)</span></code></pre></div>
<pre><code>##        201         70        170        270        233        101          1         69        169         92        192 
## 0.16917874 0.14329599 0.14329599 0.14329599 0.12946844 0.09214623 0.09214623 0.08140835 0.08140835 0.06114519 0.06114519 
##        292         20        120        283        269         97        197        297         82 
## 0.06114519 0.05121764 0.05121764 0.04481510 0.04456115 0.01981777 0.01981777 0.01981777 0.01656404</code></pre>
<p>Here we get the numeric values associated with Cook’s distances, which corroborates what we saw in the plot of Cook’s distances above. Again, the case associated with row <em>201</em> is by far the largest, and the next largest values are clustered closer together, which include the cases associated with row numbers <em>70</em>, <em>170</em>, and <em>270</em>. There are many different rules of thumbs for what constitutes an extreme Cook’s distance value. One common approach is to determine the threshold by dividing 4 by the sample size. There is, however, an even more liberal cutoff, which is simply <em>1</em> <span class="citation">(<a href="references.html#ref-bollen1985regression" role="doc-biblioref">Bollen and Jackman 1985</a>)</span>.</p>
<p>Across the three diagnostic plots and the vector of Cook’s distances, <em>201</em> appears to be consistently the most concerning case, and cases associated with row numbers <em>69</em>, <em>70</em>, <em>170</em>, <em>270</em>, and <em>233</em> may also be problematic. For the most part, the distribution of the residuals look mostly normal. As such, as a sensitivity analysis and for the sake of demonstration, we will estimate our model once more after removing the cases associated with row numbers <em>201</em>, <em>69</em>, <em>70</em>, <em>170</em>, <em>270</em>, and <em>233</em> from our data frame. If you’ve completed the entire tutorial thus far, you may have noted that some of the plots we are using with the <code>lm</code> function are different than those that come with the <code>Regression</code> function from <code>lessR</code> (see above), and further, you may have noticed that we have flagged more potential problematic cases in using these plots than the ones for the <code>Regression</code> function. This goes to show, again, how thoughtful we must be with the tools that we use, and again, I recommend erring on the side of caution when it comes to removing cases for subsequent analyses. And, remember, no model will ever be perfect. For now, let’s go ahead and interpret the results of the multiple linear regression model.</p>
<p><strong>Obtaining the Model Results:</strong> Type the name of the <code>summary</code> function from base <code>R</code> and include whatever you named your regression model (<code>reg.mod1</code>) as the sole parenthetical argument; we specified the regression model object called <code>reg.mod2</code>) earlier in the tutorial. The <code>summary</code> function simply returns a summary of your estimated regression model results.</p>
<div class="sourceCode" id="cb1096"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1096-1"><a href="incrementalvalidity.html#cb1096-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get summary of multiple linear regression model results</span></span>
<span id="cb1096-2"><a href="incrementalvalidity.html#cb1096-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg.mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ SJT + Interview, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.8249  -1.5198  -0.0401   1.0396  13.4868 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  5.52654    0.53925  10.249 &lt; 0.0000000000000002 ***
## SJT          0.56748    0.08453   6.714      0.0000000000964 ***
## Interview    0.38530    0.06388   6.031      0.0000000048298 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.992 on 297 degrees of freedom
## Multiple R-squared:  0.2643, Adjusted R-squared:  0.2593 
## F-statistic: 53.34 on 2 and 297 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>The output first displays the model you specified, followed by descriptive statistics about the residuals (i.e., estimation errors). The table called <em>Coefficients</em> contains the estimated regression model, including the regression coefficients (slopes, weights) and their standard errors, <em>t</em>-values, and <em>p</em>-values. Typically, the intercept value and its significance test are not of interest, unless we wish to use the value to specify the regression model equation (which we do later in this tutorial). The estimate of the regression coefficients for the predictor variables in relation to the outcome variable are often of substantive interest. Here, we see that the unstandardized regression coefficient for <code>SJT</code> is .567, and its associated <em>p</em>-value is less than .001 (<em>b</em> = .567, <em>p</em> &lt; .001). Given that the <em>p</em>-value is less than our conventional two-tailed alpha level of .05, we reject the null hypothesis that the regression coefficient is equal to zero, which means that we conclude that the regression coefficient is statistically significantly different from zero. We can interpret the significant regression coefficient as follows: <em>Controlling for the effects of <code>Interview</code>, for every one point increase in <code>SJT</code>, <code>Performance</code> increases by .567 points</em> Let’s move on to the regression coefficient for <code>Interview</code>, which is statistically significant and positive too (<em>b</em> = .385, <em>p</em> &lt; .001). Because these two regression coefficients are unstandardized, it would not be appropriate to compare their magnitudes. We can interpret the significant regression coefficient as follows: <em>Controlling for the effects of <code>SJT</code>, for every one point increase in <code>Interview</code>, <code>Performance</code> increases by .385 points.</em></p>
<p>Using the intercept and predictor variable coefficient estimates, we can write out the equation for the regression model as follows:</p>
<p><span class="math inline">\(Performance_{predicted} = 5.527 + (.567 * SJT_{observed}) + (.385 * Interview_{observed})\)</span></p>
<p>If we plug in, for example, the value 6 as an observed value of <code>SJT</code> and the value 7 as an observed value of <code>Interview</code>, then we get 11.624, as shown below:</p>
<p><span class="math inline">\(11.624 = 5.527 + (.567 * 6) + (.385 * 7)\)</span></p>
<p>Thus, we are able to predict future values of <code>Performance</code> based on our estimated regression model.</p>
<p>Below the table containing the regression coefficient estimates, the (unadjusted multiple) <em>R</em>-squared (<em>R</em><sup>2</sup>) and <em>adjusted</em> <em>R</em><sup>2</sup> values appear, which are indicators of the model’s fit to the data as well as the extent to which the predictor variables collectively explain variability in the outcome variable. First, the (multiple) <em>R</em>-squared (<em>R</em><sup>2</sup>) value of .264 indicates the extent to which the predictor variable explains variance in the outcome variable in this sample - or in other words, how much errors are minimized in the sample given the specified model; if you multiply the value by 100, you get a percentage. In this case, we find that 26.4% of the variance in <code>Performance</code> is explained by <code>SJT</code>. This raw, unadjusted <em>R</em>-squared (<em>R</em><sup>2</sup>) value, however, is sensitive to the sample size and the number of predictor variables in the model. The <em>adjusted</em> <em>R</em><sup>2</sup> value corrects for the sample size relative to the number of predictor variables in the model, which results in a lower estimate than its unadjusted counterpart. The <em>adjusted</em> <em>R</em><sup>2</sup> is a better indicator of the magnitude of the association in the underlying <em>population</em> and thus tends to be more accurate. Here we see that the adjusted <em>R</em><sup>2</sup> value is slightly smaller at .259 (or 25.9%). Typically, it’s a good idea to report both values.</p>
<p>The table containing the sum of squares, <em>F</em>-values, and <em>p</em>-values includes detailed information about overall model fit, which was estimated using ordinary least squares (OLS), as described earlier in this tutorial. In this table, we are mostly interested in the <em>F</em>-value associated with the overall model and its associated <em>p</em>-value, as it indicates whether the estimated model fits the data significantly better than the null model, where the <em>null model</em> does not include any predictor variables. In other words, the <em>F</em>-value and <em>p</em>-value are associated with the <em>R</em><sup>2</sup> value and whether the unadjusted and adjusted <em>R</em><sup>2</sup> values are significantly different from zero. In this example, we see that the <em>F</em>-value is 53.733 and its associated <em>p</em>-value is less than .001, the latter of which is less than our conventional alpha level of .05. Thus, we reject the null hypothesis that the <em>R</em><sup>2</sup> values are equal to zero, which leads us to conclude that the estimated model outperforms a model with no predictor variables. You can think of the <em>R</em><sup>2</sup> values as indicators of <strong>effect size</strong> at the model level. I provide some rules of thumb for qualitatively interpreting the magnitude of the effect size, which is another way of saying “determining the level of practical significance” (see table below). As you can see, our statistically significant unadjusted and adjusted <em>R</em><sup>2</sup> value can both be described as large by conventional standards. Thus, we seem to have a good model here.</p>
<table>
<thead>
<tr class="header">
<th><em>R</em><sup>2</sup></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.01</td>
<td>Small</td>
</tr>
<tr class="even">
<td>.09</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>.25</td>
<td>Large</td>
</tr>
</tbody>
</table>
<p>To estimate the 95% confidence intervals, we can apply the <code>confint</code> function from base <code>R</code>, and enter the name of the regression model (<code>reg.mod1</code>) as the sole argument.</p>
<div class="sourceCode" id="cb1098"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1098-1"><a href="incrementalvalidity.html#cb1098-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate 95% confidence intervals</span></span>
<span id="cb1098-2"><a href="incrementalvalidity.html#cb1098-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(reg.mod2)</span></code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 4.4652963 6.5877787
## SJT         0.4011348 0.7338281
## Interview   0.2595753 0.5110242</code></pre>
<p>The 95% confidence interval for the <code>SJT</code> regression coefficient ranges from .401 to .734 (i.e., 95% CI[.401, .734]), which indicates that the true population parameter for association likely falls somewhere between those two values. The 95% confidence interval for the <code>Interview</code> regression coefficient ranges from .260 to .511. (i.e., 95% CI[.260, .511.]).</p>
<p>As a direct indicator of the effect size (practical significance), we can calculate the zero-order (Pearson product-moment) correlations between <code>SJT</code> and <code>Performance</code> and between <code>Interview</code> and <code>Performance</code>. Keep the <code>method="pearson"</code> as is to request a Pearson product-moment correlation, and be sure to include the name of the data frame object (<code>df</code>) followed by the <code>$</code> operator in front of each variable.</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1100-1"><a href="incrementalvalidity.html#cb1100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate zero-order correlations (effect sizes)</span></span>
<span id="cb1100-2"><a href="incrementalvalidity.html#cb1100-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>SJT, df<span class="sc">$</span>Performance, <span class="at">method=</span><span class="st">&quot;pearson&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.4173192</code></pre>
<div class="sourceCode" id="cb1102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1102-1"><a href="incrementalvalidity.html#cb1102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>Interview, df<span class="sc">$</span>Performance, <span class="at">method=</span><span class="st">&quot;pearson&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.3906502</code></pre>
<p>The correlation coefficients between <code>SJT</code> and <code>Performance</code> and between <code>Interview</code> and <code>Performance</code> are medium-large in magnitude (<em>r</em> = .42 and <em>r</em> = .39, respectively), which indicates a moderate-strong, positive, effects for each of these selection tools with respect to <code>Performance</code>.</p>
<table>
<thead>
<tr class="header">
<th><em>r</em></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>.10</td>
<td>Small</td>
</tr>
<tr class="even">
<td>.30</td>
<td>Medium</td>
</tr>
<tr class="odd">
<td>.50</td>
<td>Large</td>
</tr>
</tbody>
</table>
<p><strong>Sample Technical Write-Up of Results:</strong> A concurrent validation study was conducted to evaluate whether evidence of incremental validities existed for a situational judgment test (<code>SJT</code>), an emotional intelligence assessment (<code>EI</code>), and a structured interview (<code>Interview</code>) in relation to job performance (<code>Performance</code>). An initial multiple linear regression model was estimated with all three selection tools included as predictor variables and job performance specified as the criterion (i.e., outcome) variable. Due to very high collinearity (i.e., a very high correlation) associated with the situational judgment test and the emotional intelligence assessment, we made the decision to drop the emotional intelligence assessment, as it was more expensive to administer and overlapped with the situational judgment test in terms of assessed content. In a follow-up multiple linear regression model, only the situational judgment test and structured interview variables were included as predictor variables. In this model, we found that each of the these selection tools showed evidence of incremental validity in relation to the criterion of job performance. Specifically, when statistically controlling for the effect of structure interview scores, model results indicated that scores on the situational judgment test were positively associated with job performance scores and to a statistically significant extent (<em>b</em> = .567, <em>p</em> &lt; .001). In other words, controlling for the effect of the structured interview, for every one point increase in situational judgment test scores, job performance increased by .567 points. Further, when statistically controlling for the effect of situational judgment test scores, model results indicated that scores on the structured interview were positively associated with job performance scores and to a statistically significant extent (<em>b</em> = .385, <em>p</em> &lt; .001). In other words, controlling for the effect of the situational judgment test, for every one point increase in structured interview scores, job performance increased by .385 points. Collectively, scores on the situational judgment test and the structured interview explained 26.4% of the variance in the criterion of job performance (<em>R</em><sup>2</sup> = .264), which can be described as a large amount of variance explained by the model. In sum, of the three selection tools, the situational judgment test and the structured interview showed the most promise, as they showed acceptable levels of collinearity and evidence of incremental validity with respect to the criterion.</p>
<p><strong>Obtaining Standardized Coefficients:</strong> If you would like to estimate the same multiple linear regression model but view the <em>standardized</em> regression coefficients, just do some small tweaks to the <code>lm</code> code/script that we specified above. First, let’s name the model something different given that it will include standardized coefficients; here, I decided to name the model <code>st_reg.mod2</code>. Next, within the <code>lm</code> function, apply the <code>scale</code> function from base R to the predictor and outcome variables as shown; doing so will standardize our variables and center them around zero.</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1104-1"><a href="incrementalvalidity.html#cb1104-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model </span></span>
<span id="cb1104-2"><a href="incrementalvalidity.html#cb1104-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with standardized coefficient estimates</span></span>
<span id="cb1104-3"><a href="incrementalvalidity.html#cb1104-3" aria-hidden="true" tabindex="-1"></a>st_reg.mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">scale</span>(Performance) <span class="sc">~</span> <span class="fu">scale</span>(SJT) <span class="sc">+</span> <span class="fu">scale</span>(Interview), <span class="at">data=</span>df)</span>
<span id="cb1104-4"><a href="incrementalvalidity.html#cb1104-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(st_reg.mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = scale(Performance) ~ scale(SJT) + scale(Interview), 
##     data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1135 -0.4371 -0.0115  0.2990  3.8792 
## 
## Coefficients:
##                                  Estimate               Std. Error t value        Pr(&gt;|t|)    
## (Intercept)      -0.000000000000000002772  0.049688712929418260567   0.000               1    
## scale(SJT)        0.343978856791380960267  0.051235702957663491197   6.714 0.0000000000964 ***
## scale(Interview)  0.309010486383703486535  0.051235702957663539769   6.031 0.0000000048298 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8606 on 297 degrees of freedom
## Multiple R-squared:  0.2643, Adjusted R-squared:  0.2593 
## F-statistic: 53.34 on 2 and 297 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>In the <em>Coefficients</em> portion of the output, note that the intercept value is virtually zeroed out due to the standardization, and the regression coefficients associated with <code>SJT</code> and <code>Interview</code> are now in standardized units (<span class="math inline">\(\beta\)</span> = .340, <em>p</em> &lt; .001 and <span class="math inline">\(\beta\)</span> = .309, <em>p</em> &lt; .001, respectively). The reason that the intercept value is not exactly zero (in scientific notation) is rounding error. Note that the <em>p</em>-value is the same as the unstandardized regression coefficient we saw above. The interpretation of this statistically significant standardized regression coefficient is as follows: When controlling for <code>Interview</code>, for every one standardized unit increase in <code>SJT</code>, <code>Performance</code> increases by .344 standardized units, and when controlling for <code>SJT</code>, for every one standardized unit increase in <code>Interview</code>, <code>Performance</code> increases by .309 standardized units.</p>
<p><strong>Dealing with Multivariate Outliers:</strong> If you recall above, we found that the case associated with row numbers <em>201</em>, <em>70</em>, <em>270</em>, and <em>170</em> in this sample may be candidates for removal. I tend to be quite wary of eliminating cases that are members of the population of interest and who seem to have plausible data (i.e., cleaned). As such, I am typically reluctant to jettison cases, unless the cases appear to have a dramatic influence on the estimated regression line. Some might argue that we should retain cases with row numbers <em>201</em>, <em>70</em>, <em>270</em>, and <em>170</em>, and others might argue that we should remove them; really, it comes down to your own logic, rationale, and justification, and I recommend pulling in other information you might have about these cases (even beyond data contained in this dataset) to inform your decision. If you were to decide to remove these cases, here’s what you would do. First, look at the data frame (using the <code>View</code> function) and determine which cases row number <em>201</em>, <em>70</em>, <em>270</em>, and <em>170</em> are associated with; because we have a unique identifier variable (<code>ID</code>) in our data frame, we can see that row numbers <em>201</em>, <em>70</em>, <em>270</em>, and <em>170</em> in our data frame are associated with <code>EmployeeID</code>s <em>EE223</em>, <em>EE92</em>, <em>EE292</em>, and <em>EE192</em>, respectively. Next, with respect to estimating the regression model, I suggest naming the unstandardized regression model something different, and here I name it <code>reg.mod3</code>. The model should be specified just as it was earlier in the tutorial, but now let’s add an additional argument: <code>subset=(!EmployeeID %in% c("EE223", "EE92", "EE292", "EE192"))</code>; the <code>subset</code> argument subsets the data frame within the <code>lm</code> function by whatever logical/conditional statement you provide. In this instance, we indicate that we want to retain every case in which <code>EmployeeID</code> is <em>not equal to</em> <em>EE223</em>, <em>EE92</em>, <em>EE292</em>, and <em>EE192</em>. Remember, the logical operator <code>!</code> means “not” and the <code>%in%</code> operator means “within”. Revisit the <a href="filter.html#filter">chapter on filtering data</a> if you want to see the full list of logical operators and more information on removing multiple cases.</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1106-1"><a href="incrementalvalidity.html#cb1106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model </span></span>
<span id="cb1106-2"><a href="incrementalvalidity.html#cb1106-2" aria-hidden="true" tabindex="-1"></a><span class="co"># but remove row numbers 201, 70, 270, and 170</span></span>
<span id="cb1106-3"><a href="incrementalvalidity.html#cb1106-3" aria-hidden="true" tabindex="-1"></a><span class="co"># which correspond to EmployeeID numbers of</span></span>
<span id="cb1106-4"><a href="incrementalvalidity.html#cb1106-4" aria-hidden="true" tabindex="-1"></a><span class="co"># EE223, EE92, EE292, and EE192</span></span>
<span id="cb1106-5"><a href="incrementalvalidity.html#cb1106-5" aria-hidden="true" tabindex="-1"></a>reg.mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> Interview, <span class="at">data=</span>df, </span>
<span id="cb1106-6"><a href="incrementalvalidity.html#cb1106-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">subset=</span>(<span class="sc">!</span>EmployeeID <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;EE223&quot;</span>, <span class="st">&quot;EE92&quot;</span>, <span class="st">&quot;EE292&quot;</span>, <span class="st">&quot;EE192&quot;</span>)))</span>
<span id="cb1106-7"><a href="incrementalvalidity.html#cb1106-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg.mod3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ SJT + Interview, data = df, subset = (!EmployeeID %in% 
##     c(&quot;EE223&quot;, &quot;EE92&quot;, &quot;EE292&quot;, &quot;EE192&quot;)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1832 -1.2913 -0.0509  0.8278 13.1206 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  4.70328    0.49639   9.475 &lt; 0.0000000000000002 ***
## SJT          0.61531    0.07723   7.968 0.000000000000036184 ***
## Interview    0.50383    0.05873   8.579 0.000000000000000567 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.677 on 293 degrees of freedom
## Multiple R-squared:  0.371,  Adjusted R-squared:  0.3667 
## F-statistic: 86.41 on 2 and 293 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>The pattern of results remains mostly the same; however, the model fit (as evidenced <em>R</em><sup>2</sup>) improved after removing these outliers. Even with an improved model fit, We should remain hesitant to use the model based on the reduced sample (without these multivariate outliers), as this could potentially hurt our ability to generalize our findings to future samples of applicants. Then again, if the outlier cases really to do seem to be atypical for the population of interest, then you might make a case for removing them. Basically, it’s up to you to justify your decision and document it.</p>
<p>If your data frame does not have a unique identifier variable, you could remove the outlier by referencing the outlier by row number. Instead of using the <code>subset=</code> argument, you would follow up your <code>data=df</code> argument with <code>[-c(201, 70, 270, 170),]</code> to indicate that you wish to remove row numbers <em>201</em>, <em>70</em>, <em>270</em>, and <em>170</em>. The minus sign (<code>-</code>) signals “not.”</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1108-1"><a href="incrementalvalidity.html#cb1108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model </span></span>
<span id="cb1108-2"><a href="incrementalvalidity.html#cb1108-2" aria-hidden="true" tabindex="-1"></a><span class="co"># but remove row numbers 201, 70, 270, and 170</span></span>
<span id="cb1108-3"><a href="incrementalvalidity.html#cb1108-3" aria-hidden="true" tabindex="-1"></a>reg.mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> Interview, <span class="at">data=</span>df[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">201</span>, <span class="dv">70</span>, <span class="dv">270</span>, <span class="dv">170</span>),])</span>
<span id="cb1108-4"><a href="incrementalvalidity.html#cb1108-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(reg.mod3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Performance ~ SJT + Interview, data = df[-c(201, 
##     70, 270, 170), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1832 -1.2913 -0.0509  0.8278 13.1206 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  4.70328    0.49639   9.475 &lt; 0.0000000000000002 ***
## SJT          0.61531    0.07723   7.968 0.000000000000036184 ***
## Interview    0.50383    0.05873   8.579 0.000000000000000567 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.677 on 293 degrees of freedom
## Multiple R-squared:  0.371,  Adjusted R-squared:  0.3667 
## F-statistic: 86.41 on 2 and 293 DF,  p-value: &lt; 0.00000000000000022</code></pre>
</div>
<div id="apatable_mlr" class="section level3" number="34.4.2">
<h3><span class="header-section-number">34.4.2</span> APA-Style Results Table</h3>
<p>If you want to present the results of your multiple linear regression to a more statistically inclined audience, particularly an audience that prefers American Psychological Association (APA) style, consider using functions from the <code>apaTables</code> package.</p>
<p>Using the <code>lm</code> function from base R, as we did above, let’s begin by estimating a multiple linear regression model and naming the model object (<code>reg.mod1</code>).</p>
<div class="sourceCode" id="cb1110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1110-1"><a href="incrementalvalidity.html#cb1110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate multiple linear regression model</span></span>
<span id="cb1110-2"><a href="incrementalvalidity.html#cb1110-2" aria-hidden="true" tabindex="-1"></a>reg.mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Performance <span class="sc">~</span> SJT <span class="sc">+</span> Interview, <span class="at">data=</span>df)</span></code></pre></div>
<p>If you haven’t already, install and access the <code>apaTables</code> package using the <code>install.packages</code> and <code>library</code> functions, respectively.</p>
<div class="sourceCode" id="cb1111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1111-1"><a href="incrementalvalidity.html#cb1111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install package</span></span>
<span id="cb1111-2"><a href="incrementalvalidity.html#cb1111-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;apaTables&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb1112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1112-1"><a href="incrementalvalidity.html#cb1112-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Access package</span></span>
<span id="cb1112-2"><a href="incrementalvalidity.html#cb1112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(apaTables)</span></code></pre></div>
<p>The <code>apa.reg.table</code> function from <code>apaTables</code> is pretty straightforward. Simply enter your regression model object (<code>reg.mod1</code>) as the sole parenthetical argument. This will generate a table as output in your Console.</p>
<div class="sourceCode" id="cb1113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1113-1"><a href="incrementalvalidity.html#cb1113-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create APA-style regression table</span></span>
<span id="cb1113-2"><a href="incrementalvalidity.html#cb1113-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apa.reg.table</span>(reg.mod1)</span></code></pre></div>
<pre><code>## 
## 
## Regression results using Performance as the criterion
##  
## 
##    Predictor      b     b_95%_CI beta  beta_95%_CI sr2 sr2_95%_CI     r             Fit
##  (Intercept) 5.53** [4.47, 6.59]                                                       
##          SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42**                
##    Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39**                
##                                                                             R2 = .264**
##                                                                         95% CI[.18,.34]
##                                                                                        
## 
## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant.
## b represents unstandardized regression weights. beta indicates the standardized regression weights. 
## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation.
## Square brackets are used to enclose the lower and upper limits of a confidence interval.
## * indicates p &lt; .05. ** indicates p &lt; .01.
## </code></pre>
<p>If we add a <code>filename=</code> as a second argument, we can specify the name of a .doc file that we can write to our working directory. Here, I name the file “APA Multiple Linear Regression Table.doc”. The .doc file that appears in your working directory will be nicely formatted and include critical regression model results in an organized manner.</p>
<div class="sourceCode" id="cb1115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1115-1"><a href="incrementalvalidity.html#cb1115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create APA-style regression table and write to working directory</span></span>
<span id="cb1115-2"><a href="incrementalvalidity.html#cb1115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apa.reg.table</span>(reg.mod1, <span class="at">filename=</span><span class="st">&quot;APA Multiple Linear Regression Table.doc&quot;</span>)</span></code></pre></div>
<pre><code>## 
## 
## Regression results using Performance as the criterion
##  
## 
##    Predictor      b     b_95%_CI beta  beta_95%_CI sr2 sr2_95%_CI     r             Fit
##  (Intercept) 5.53** [4.47, 6.59]                                                       
##          SJT 0.57** [0.40, 0.73] 0.34 [0.24, 0.44] .11 [.05, .17] .42**                
##    Interview 0.39** [0.26, 0.51] 0.31 [0.21, 0.41] .09 [.03, .15] .39**                
##                                                                             R2 = .264**
##                                                                         95% CI[.18,.34]
##                                                                                        
## 
## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant.
## b represents unstandardized regression weights. beta indicates the standardized regression weights. 
## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation.
## Square brackets are used to enclose the lower and upper limits of a confidence interval.
## * indicates p &lt; .05. ** indicates p &lt; .01.
## </code></pre>
<div class="figure">
<img src="apa_mlr_table.PNG" alt="" />
<p class="caption">The <code>apa.reg.table</code> function from the <code>apaTables</code> package can table multiple linear regression model results in a manner that is consistent with the American Psychological Association (APA) style guide. APA-style tables are useful when presenting to academic audiences or audiences with high levels of technical/statistical expertise.</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="predictingcriterionscores.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="compensatory.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
